INFO - Log file for this run: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_10_epoch80_20221108-070318/MobileNetv2_imagenet_a8w8_10_epoch80_20221108-070318.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_10_epoch80_20221108-070318/tb_runs
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/LSQ/quan/quantizer/lsq.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.005
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Validation [   20/  391]   Loss 27.494899   Top1 0.000000   Top5 0.117188   BatchTime 0.847856
INFO - Validation [   40/  391]   Loss 28.453143   Top1 0.000000   Top5 0.058594   BatchTime 0.500127
INFO - Validation [   60/  391]   Loss 27.431588   Top1 0.000000   Top5 0.338542   BatchTime 0.381702
INFO - Validation [   80/  391]   Loss 24.830360   Top1 0.000000   Top5 0.302734   BatchTime 0.324170
INFO - Validation [  100/  391]   Loss 23.232639   Top1 0.000000   Top5 0.242188   BatchTime 0.289863
INFO - Validation [  120/  391]   Loss 23.381565   Top1 0.305990   Top5 0.559896   BatchTime 0.267684
INFO - Validation [  140/  391]   Loss 24.094356   Top1 0.262277   Top5 0.479911   BatchTime 0.252198
INFO - Validation [  160/  391]   Loss 24.122167   Top1 0.229492   Top5 0.419922   BatchTime 0.239078
INFO - Validation [  180/  391]   Loss 23.957889   Top1 0.203993   Top5 0.373264   BatchTime 0.230385
INFO - Validation [  200/  391]   Loss 24.037360   Top1 0.183594   Top5 0.335938   BatchTime 0.232230
INFO - Validation [  220/  391]   Loss 24.155474   Top1 0.170455   Top5 0.482955   BatchTime 0.227363
INFO - Validation [  240/  391]   Loss 24.176308   Top1 0.156250   Top5 0.445964   BatchTime 0.221076
INFO - Validation [  260/  391]   Loss 24.185363   Top1 0.144231   Top5 0.411659   BatchTime 0.215349
INFO - Validation [  280/  391]   Loss 24.248383   Top1 0.133929   Top5 0.465960   BatchTime 0.211016
INFO - Validation [  300/  391]   Loss 24.188895   Top1 0.125000   Top5 0.606771   BatchTime 0.207910
INFO - Validation [  320/  391]   Loss 24.236692   Top1 0.117188   Top5 0.568848   BatchTime 0.210146
INFO - Validation [  340/  391]   Loss 24.114940   Top1 0.110294   Top5 0.569853   BatchTime 0.208062
INFO - Validation [  360/  391]   Loss 24.022330   Top1 0.104167   Top5 0.538194   BatchTime 0.203383
INFO - Validation [  380/  391]   Loss 24.205605   Top1 0.098684   Top5 0.509868   BatchTime 0.199348
INFO - ==> Top1: 0.096    Top5: 0.496    Loss: 24.301
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.096   Top5: 0.496] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.0418, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0624, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0610, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0296, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0329, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0609, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0207, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0220, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0871, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0175, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0351, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0728, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0206, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0135, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0847, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0161, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0112, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0934, device='cuda:0', requires_grad=True)
Parameter containing:
Parameter containing:='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0374, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0619, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0165, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0094, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0723, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0106, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0088, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0764, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0083, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0094, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0794, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0148, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0243, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0478, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0082, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0093, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0565, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0065, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0102, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0647, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0065, device='cuda:0', requires_grad=True)
Parameter containing:='cuda:0', requires_grad=True)
tensor(0.0276, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0359, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0108, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0098, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0455, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0084, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0111, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0580, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0036, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0219, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0308, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 5.455694   Top1 8.554688   Top5 21.328125   BatchTime 0.970856   LR 0.005000
INFO - Training [0][   40/10010]   Loss 5.180968   Top1 10.761719   Top5 25.332031   BatchTime 0.669414   LR 0.005000
INFO - Training [0][   60/10010]   Loss 4.903129   Top1 13.606771   Top5 29.570312   BatchTime 0.569421   LR 0.005000
INFO - Training [0][   80/10010]   Loss 4.646111   Top1 15.869141   Top5 33.994141   BatchTime 0.512078   LR 0.005000
INFO - Training [0][  100/10010]   Loss 4.439148   Top1 18.062500   Top5 37.421875   BatchTime 0.476035   LR 0.005000
INFO - Training [0][  120/10010]   Loss 4.261992   Top1 19.973958   Top5 40.371094   BatchTime 0.459626   LR 0.005000
INFO - Training [0][  140/10010]   Loss 4.114757   Top1 21.601562   Top5 42.840402   BatchTime 0.441163   LR 0.005000
INFO - Training [0][  160/10010]   Loss 3.994084   Top1 23.276367   Top5 44.902344   BatchTime 0.427739   LR 0.005000
INFO - Training [0][  180/10010]   Loss 3.897450   Top1 24.496528   Top5 46.536458   BatchTime 0.417667   LR 0.005000
INFO - Training [0][  200/10010]   Loss 3.812351   Top1 25.730469   Top5 48.007812   BatchTime 0.409827   LR 0.005000
INFO - Training [0][  220/10010]   Loss 3.720718   Top1 26.985085   Top5 49.549006   BatchTime 0.402943   LR 0.005000
INFO - Training [0][  240/10010]   Loss 3.643291   Top1 28.089193   Top5 50.836589   BatchTime 0.397016   LR 0.005000
INFO - Training [0][  260/10010]   Loss 3.573655   Top1 29.125601   Top5 51.992188   BatchTime 0.392066   LR 0.005000
INFO - Training [0][  280/10010]   Loss 3.518265   Top1 30.002790   Top5 52.885045   BatchTime 0.387702   LR 0.005000
INFO - Training [0][  300/10010]   Loss 3.450275   Top1 31.010417   Top5 53.963542   BatchTime 0.384084   LR 0.005000
INFO - Training [0][  320/10010]   Loss 3.384529   Top1 31.979980   Top5 55.043945   BatchTime 0.380701   LR 0.005000
INFO - Training [0][  340/10010]   Loss 3.330606   Top1 32.752757   Top5 55.999540   BatchTime 0.377670   LR 0.005000
INFO - Training [0][  360/10010]   Loss 3.281339   Top1 33.437500   Top5 56.788194   BatchTime 0.375162   LR 0.005000
INFO - Training [0][  380/10010]   Loss 3.240485   Top1 34.056332   Top5 57.495888   BatchTime 0.373440   LR 0.005000
INFO - Training [0][  400/10010]   Loss 3.199232   Top1 34.671875   Top5 58.152344   BatchTime 0.371422   LR 0.005000
INFO - Training [0][  420/10010]   Loss 3.155590   Top1 35.370164   Top5 58.893229   BatchTime 0.369595   LR 0.005000
INFO - Training [0][  440/10010]   Loss 3.119563   Top1 35.916193   Top5 59.477983   BatchTime 0.368216   LR 0.005000
INFO - Training [0][  460/10010]   Loss 3.082398   Top1 36.480978   Top5 60.081522   BatchTime 0.366685   LR 0.005000
INFO - Training [0][  480/10010]   Loss 3.047262   Top1 37.060547   Top5 60.668945   BatchTime 0.365191   LR 0.005000
INFO - Training [0][  500/10010]   Loss 3.012812   Top1 37.609375   Top5 61.234375   BatchTime 0.363980   LR 0.005000
INFO - Training [0][  520/10010]   Loss 2.983978   Top1 38.061899   Top5 61.661659   BatchTime 0.362973   LR 0.005000
INFO - Training [0][  540/10010]   Loss 2.958175   Top1 38.479456   Top5 62.096354   BatchTime 0.361803   LR 0.005000
INFO - Training [0][  560/10010]   Loss 2.931246   Top1 38.911830   Top5 62.543248   BatchTime 0.360759   LR 0.005000
INFO - Training [0][  580/10010]   Loss 2.904010   Top1 39.356142   Top5 63.006466   BatchTime 0.359820   LR 0.005000
INFO - Training [0][  600/10010]   Loss 2.878568   Top1 39.766927   Top5 63.411458   BatchTime 0.359046   LR 0.005000
INFO - Training [0][  620/10010]   Loss 2.853093   Top1 40.173891   Top5 63.825605   BatchTime 0.358310   LR 0.005000
INFO - Training [0][  640/10010]   Loss 2.829314   Top1 40.568848   Top5 64.223633   BatchTime 0.357507   LR 0.005000
INFO - Training [0][  660/10010]   Loss 2.808217   Top1 40.903172   Top5 64.564394   BatchTime 0.356796   LR 0.005000
INFO - Training [0][  680/10010]   Loss 2.786658   Top1 41.309743   Top5 64.905790   BatchTime 0.356017   LR 0.005000
INFO - Training [0][  700/10010]   Loss 2.765432   Top1 41.648438   Top5 65.271205   BatchTime 0.355412   LR 0.005000
INFO - Training [0][  720/10010]   Loss 2.743269   Top1 41.987847   Top5 65.628255   BatchTime 0.354705   LR 0.005000
INFO - Training [0][  740/10010]   Loss 2.720918   Top1 42.374367   Top5 65.993454   BatchTime 0.354073   LR 0.005000
INFO - Training [0][  760/10010]   Loss 2.703562   Top1 42.630551   Top5 66.276727   BatchTime 0.353539   LR 0.005000
INFO - Training [0][  780/10010]   Loss 2.685171   Top1 42.967748   Top5 66.569511   BatchTime 0.352967   LR 0.005000
INFO - Training [0][  800/10010]   Loss 2.670631   Top1 43.238281   Top5 66.818359   BatchTime 0.352506   LR 0.005000
INFO - Training [0][  820/10010]   Loss 2.655679   Top1 43.482279   Top5 67.061738   BatchTime 0.351887   LR 0.005000
INFO - Training [0][  840/10010]   Loss 2.639739   Top1 43.777902   Top5 67.314918   BatchTime 0.351202   LR 0.005000
INFO - Training [0][  860/10010]   Loss 2.623165   Top1 44.046148   Top5 67.589935   BatchTime 0.350717   LR 0.005000
INFO - Training [0][  880/10010]   Loss 2.610801   Top1 44.248047   Top5 67.793857   BatchTime 0.350166   LR 0.005000
INFO - Training [0][  900/10010]   Loss 2.598320   Top1 44.427951   Top5 68.010417   BatchTime 0.349623   LR 0.005000
INFO - Training [0][  920/10010]   Loss 2.585590   Top1 44.643342   Top5 68.220958   BatchTime 0.349094   LR 0.005000
INFO - Training [0][  940/10010]   Loss 2.570935   Top1 44.883644   Top5 68.453291   BatchTime 0.348665   LR 0.005000
INFO - Training [0][  960/10010]   Loss 2.559792   Top1 45.047201   Top5 68.651530   BatchTime 0.348290   LR 0.005000
INFO - Training [0][  980/10010]   Loss 2.545935   Top1 45.293367   Top5 68.866390   BatchTime 0.347911   LR 0.005000
INFO - Training [0][ 1000/10010]   Loss 2.532302   Top1 45.517187   Top5 69.078906   BatchTime 0.347643   LR 0.005000
INFO - Training [0][ 1020/10010]   Loss 2.520021   Top1 45.723805   Top5 69.285386   BatchTime 0.347396   LR 0.005000
INFO - Training [0][ 1040/10010]   Loss 2.508020   Top1 45.917969   Top5 69.477163   BatchTime 0.347256   LR 0.005000
INFO - Training [0][ 1060/10010]   Loss 2.496420   Top1 46.127653   Top5 69.666863   BatchTime 0.346970   LR 0.005000
INFO - Training [0][ 1080/10010]   Loss 2.486247   Top1 46.301360   Top5 69.827836   BatchTime 0.346697   LR 0.005000
INFO - Training [0][ 1100/10010]   Loss 2.476215   Top1 46.472301   Top5 69.980824   BatchTime 0.346268   LR 0.005000
INFO - Training [0][ 1120/10010]   Loss 2.466648   Top1 46.652483   Top5 70.122070   BatchTime 0.345882   LR 0.005000
INFO - Training [0][ 1140/10010]   Loss 2.454481   Top1 46.855811   Top5 70.324836   BatchTime 0.345661   LR 0.005000
INFO - Training [0][ 1160/10010]   Loss 2.443326   Top1 47.025862   Top5 70.514547   BatchTime 0.345382   LR 0.005000
INFO - Training [0][ 1180/10010]   Loss 2.432991   Top1 47.212659   Top5 70.675318   BatchTime 0.345163   LR 0.005000
INFO - Training [0][ 1200/10010]   Loss 2.423796   Top1 47.394531   Top5 70.828125   BatchTime 0.344918   LR 0.005000
INFO - Training [0][ 1220/10010]   Loss 2.414355   Top1 47.564037   Top5 70.988089   BatchTime 0.344555   LR 0.005000
INFO - Training [0][ 1240/10010]   Loss 2.405676   Top1 47.722404   Top5 71.123362   BatchTime 0.344261   LR 0.005000
INFO - Training [0][ 1260/10010]   Loss 2.396661   Top1 47.883185   Top5 71.274802   BatchTime 0.344085   LR 0.005000
INFO - Training [0][ 1280/10010]   Loss 2.388024   Top1 48.042603   Top5 71.406860   BatchTime 0.344796   LR 0.005000
INFO - Training [0][ 1300/10010]   Loss 2.380890   Top1 48.175481   Top5 71.512620   BatchTime 0.344853   LR 0.005000
INFO - Training [0][ 1320/10010]   Loss 2.374189   Top1 48.296638   Top5 71.618134   BatchTime 0.344808   LR 0.005000
INFO - Training [0][ 1340/10010]   Loss 2.365948   Top1 48.429921   Top5 71.739156   BatchTime 0.344883   LR 0.005000
INFO - Training [0][ 1360/10010]   Loss 2.358959   Top1 48.550092   Top5 71.843405   BatchTime 0.345014   LR 0.005000
INFO - Training [0][ 1380/10010]   Loss 2.350991   Top1 48.674139   Top5 71.971241   BatchTime 0.345109   LR 0.005000
INFO - Training [0][ 1400/10010]   Loss 2.342953   Top1 48.803013   Top5 72.099330   BatchTime 0.344973   LR 0.005000
INFO - Training [0][ 1420/10010]   Loss 2.335555   Top1 48.952465   Top5 72.212808   BatchTime 0.344806   LR 0.005000
INFO - Training [0][ 1440/10010]   Loss 2.328515   Top1 49.080404   Top5 72.315538   BatchTime 0.345850   LR 0.005000
INFO - Training [0][ 1460/10010]   Loss 2.320645   Top1 49.229452   Top5 72.431507   BatchTime 0.345659   LR 0.005000
INFO - Training [0][ 1480/10010]   Loss 2.314935   Top1 49.329603   Top5 72.522698   BatchTime 0.345499   LR 0.005000
INFO - Training [0][ 1500/10010]   Loss 2.308078   Top1 49.441667   Top5 72.619271   BatchTime 0.345317   LR 0.005000
INFO - Training [0][ 1520/10010]   Loss 2.302406   Top1 49.546155   Top5 72.715872   BatchTime 0.345120   LR 0.005000
INFO - Training [0][ 1540/10010]   Loss 2.296487   Top1 49.641335   Top5 72.813007   BatchTime 0.344989   LR 0.005000
INFO - Training [0][ 1560/10010]   Loss 2.289715   Top1 49.762119   Top5 72.919671   BatchTime 0.344831   LR 0.005000
INFO - Training [0][ 1580/10010]   Loss 2.283918   Top1 49.883307   Top5 73.004351   BatchTime 0.345480   LR 0.005000
INFO - Training [0][ 1600/10010]   Loss 2.278242   Top1 49.965820   Top5 73.095215   BatchTime 0.345256   LR 0.005000
INFO - Training [0][ 1620/10010]   Loss 2.271333   Top1 50.082465   Top5 73.214217   BatchTime 0.345213   LR 0.005000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f29ca317d30>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 59, in train
    inputs = inputs.to(args.device.type)
KeyboardInterrupt