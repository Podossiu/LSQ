INFO - Log file for this run: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_20_epoch80_20221108-065105/MobileNetv2_imagenet_a8w8_20_epoch80_20221108-065105.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_20_epoch80_20221108-065105/tb_runs
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/LSQ/quan/quantizer/lsq.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.005
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Validation [   20/  391]   Loss 12.927166   Top1 0.000000   Top5 0.000000   BatchTime 0.763334
INFO - Validation [   40/  391]   Loss 13.225460   Top1 0.000000   Top5 0.000000   BatchTime 0.449665
INFO - Validation [   60/  391]   Loss 12.783325   Top1 0.000000   Top5 0.000000   BatchTime 0.346214
INFO - Validation [   80/  391]   Loss 11.983866   Top1 0.000000   Top5 0.488281   BatchTime 0.294143
INFO - Validation [  100/  391]   Loss 11.471573   Top1 0.000000   Top5 0.781250   BatchTime 0.262149
INFO - Validation [  120/  391]   Loss 11.341034   Top1 0.000000   Top5 0.976562   BatchTime 0.240353
INFO - Validation [  140/  391]   Loss 11.538778   Top1 0.000000   Top5 0.837054   BatchTime 0.225358
INFO - Validation [  160/  391]   Loss 11.389091   Top1 0.000000   Top5 0.732422   BatchTime 0.213539
INFO - Validation [  180/  391]   Loss 11.399139   Top1 0.000000   Top5 0.651042   BatchTime 0.204659
INFO - Validation [  200/  391]   Loss 11.465668   Top1 0.000000   Top5 0.585938   BatchTime 0.197430
INFO - Validation [  220/  391]   Loss 11.544815   Top1 0.000000   Top5 0.532670   BatchTime 0.191562
INFO - Validation [  240/  391]   Loss 11.596521   Top1 0.000000   Top5 0.488281   BatchTime 0.187022
INFO - Validation [  260/  391]   Loss 11.627833   Top1 0.000000   Top5 0.450721   BatchTime 0.183470
INFO - Validation [  280/  391]   Loss 11.677776   Top1 0.000000   Top5 0.418527   BatchTime 0.180481
INFO - Validation [  300/  391]   Loss 11.678604   Top1 0.130208   Top5 0.520833   BatchTime 0.177862
INFO - Validation [  320/  391]   Loss 11.729943   Top1 0.122070   Top5 0.488281   BatchTime 0.175510
INFO - Validation [  340/  391]   Loss 11.704121   Top1 0.114890   Top5 0.574449   BatchTime 0.172849
INFO - Validation [  360/  391]   Loss 11.660252   Top1 0.108507   Top5 0.542535   BatchTime 0.169588
INFO - Validation [  380/  391]   Loss 11.746508   Top1 0.102796   Top5 0.513980   BatchTime 0.166693
INFO - ==> Top1: 0.100    Top5: 0.500    Loss: 11.797
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.100   Top5: 0.500] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
0.010000000000000002
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.0592, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0884, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0872, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0421, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0468, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0863, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0293, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0313, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1239, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0248, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0497, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1032, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0292, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0191, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1201, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0159, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1325, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0232, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0530, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0878, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1026, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0150, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0124, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1082, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1126, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0210, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0344, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0678, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0116, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0132, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0802, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0093, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0144, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0917, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0092, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0392, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0512, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0154, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0139, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0651, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0119, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0158, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0835, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0055, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0318, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0435, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 5.499844   Top1 7.773438   Top5 20.976562   BatchTime 0.651447   LR 0.005000
INFO - Training [0][   40/10010]   Loss 5.206928   Top1 9.765625   Top5 25.019531   BatchTime 0.486420   LR 0.005000
INFO - Training [0][   60/10010]   Loss 4.956801   Top1 11.914062   Top5 28.476562   BatchTime 0.433271   LR 0.005000
INFO - Training [0][   80/10010]   Loss 4.708599   Top1 14.482422   Top5 32.519531   BatchTime 0.407245   LR 0.005000
INFO - Training [0][  100/10010]   Loss 4.494437   Top1 16.828125   Top5 36.031250   BatchTime 0.390815   LR 0.005000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f83b35b5d30>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 83, in train
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt