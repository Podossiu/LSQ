INFO - Log file for this run: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_15_epoch80_20221108-065540/MobileNetv2_imagenet_a8w8_15_epoch80_20221108-065540.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_15_epoch80_20221108-065540/tb_runs
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/LSQ/quan/quantizer/lsq.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.005
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Validation [   20/  391]   Loss 12.927401   Top1 0.000000   Top5 0.000000   BatchTime 0.839128
INFO - Validation [   40/  391]   Loss 13.225690   Top1 0.000000   Top5 0.000000   BatchTime 0.491679
INFO - Validation [   60/  391]   Loss 12.783485   Top1 0.000000   Top5 0.000000   BatchTime 0.375168
INFO - Validation [   80/  391]   Loss 11.984020   Top1 0.000000   Top5 0.488281   BatchTime 0.320491
INFO - Validation [  100/  391]   Loss 11.471709   Top1 0.000000   Top5 0.781250   BatchTime 0.285876
INFO - Validation [  120/  391]   Loss 11.341164   Top1 0.000000   Top5 0.976562   BatchTime 0.262527
INFO - Validation [  140/  391]   Loss 11.538913   Top1 0.000000   Top5 0.837054   BatchTime 0.246077
INFO - Validation [  160/  391]   Loss 11.389241   Top1 0.000000   Top5 0.732422   BatchTime 0.234851
INFO - Validation [  180/  391]   Loss 11.399283   Top1 0.000000   Top5 0.651042   BatchTime 0.226136
INFO - Validation [  200/  391]   Loss 11.465818   Top1 0.000000   Top5 0.585938   BatchTime 0.217550
INFO - Validation [  220/  391]   Loss 11.544956   Top1 0.000000   Top5 0.532670   BatchTime 0.213675
INFO - Validation [  240/  391]   Loss 11.596657   Top1 0.000000   Top5 0.488281   BatchTime 0.210120
INFO - Validation [  260/  391]   Loss 11.627968   Top1 0.000000   Top5 0.450721   BatchTime 0.204914
INFO - Validation [  280/  391]   Loss 11.677904   Top1 0.000000   Top5 0.418527   BatchTime 0.204560
INFO - Validation [  300/  391]   Loss 11.678717   Top1 0.130208   Top5 0.520833   BatchTime 0.200274
INFO - Validation [  320/  391]   Loss 11.730051   Top1 0.122070   Top5 0.488281   BatchTime 0.197226
INFO - Validation [  340/  391]   Loss 11.704230   Top1 0.114890   Top5 0.574449   BatchTime 0.193552
INFO - Validation [  360/  391]   Loss 11.660352   Top1 0.108507   Top5 0.542535   BatchTime 0.189447
INFO - Validation [  380/  391]   Loss 11.746606   Top1 0.102796   Top5 0.513980   BatchTime 0.185786
INFO - ==> Top1: 0.100    Top5: 0.500    Loss: 11.797
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.100   Top5: 0.500] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.0592, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0884, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0873, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0421, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0468, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0864, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0293, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0313, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0247, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0497, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1031, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0292, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0191, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1200, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0159, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1322, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0231, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0530, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0877, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1025, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0150, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0124, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1082, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1123, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0210, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0345, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0678, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0132, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0801, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0093, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0144, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0913, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0092, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0392, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0510, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0153, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0139, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0646, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0119, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0158, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0826, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0053, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0314, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0436, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 5.777149   Top1 5.820312   Top5 16.093750   BatchTime 0.803045   LR 0.005000
INFO - Training [0][   40/10010]   Loss 5.441485   Top1 8.242188   Top5 20.996094   BatchTime 0.577981   LR 0.005000
INFO - Training [0][   60/10010]   Loss 5.154740   Top1 10.520833   Top5 25.598958   BatchTime 0.504935   LR 0.005000
INFO - Training [0][   80/10010]   Loss 4.871889   Top1 13.251953   Top5 30.009766   BatchTime 0.468303   LR 0.005000
INFO - Training [0][  100/10010]   Loss 4.625737   Top1 15.859375   Top5 34.046875   BatchTime 0.444801   LR 0.005000
INFO - Training [0][  120/10010]   Loss 4.431103   Top1 18.001302   Top5 37.259115   BatchTime 0.440043   LR 0.005000
INFO - Training [0][  140/10010]   Loss 4.271179   Top1 19.882812   Top5 39.977679   BatchTime 0.427764   LR 0.005000
INFO - Training [0][  160/10010]   Loss 4.144159   Top1 21.538086   Top5 42.055664   BatchTime 0.417678   LR 0.005000
INFO - Training [0][  180/10010]   Loss 4.036102   Top1 22.799479   Top5 43.797743   BatchTime 0.409776   LR 0.005000
INFO - Training [0][  200/10010]   Loss 3.940478   Top1 24.015625   Top5 45.437500   BatchTime 0.403045   LR 0.005000
INFO - Training [0][  220/10010]   Loss 3.844890   Top1 25.351562   Top5 47.123580   BatchTime 0.401836   LR 0.005000
INFO - Training [0][  240/10010]   Loss 3.761584   Top1 26.627604   Top5 48.505859   BatchTime 0.396490   LR 0.005000
INFO - Training [0][  260/10010]   Loss 3.688265   Top1 27.713341   Top5 49.738582   BatchTime 0.392525   LR 0.005000
INFO - Training [0][  280/10010]   Loss 3.627393   Top1 28.618862   Top5 50.789621   BatchTime 0.389017   LR 0.005000
INFO - Training [0][  300/10010]   Loss 3.556738   Top1 29.533854   Top5 52.010417   BatchTime 0.385448   LR 0.005000
INFO - Training [0][  320/10010]   Loss 3.488765   Top1 30.502930   Top5 53.120117   BatchTime 0.382500   LR 0.005000
INFO - Training [0][  340/10010]   Loss 3.437109   Top1 31.236213   Top5 54.002757   BatchTime 0.379738   LR 0.005000
INFO - Training [0][  360/10010]   Loss 3.384046   Top1 32.005208   Top5 54.880642   BatchTime 0.377465   LR 0.005000
INFO - Training [0][  380/10010]   Loss 3.340825   Top1 32.615132   Top5 55.622944   BatchTime 0.376159   LR 0.005000
INFO - Training [0][  400/10010]   Loss 3.296166   Top1 33.312500   Top5 56.353516   BatchTime 0.374262   LR 0.005000
INFO - Training [0][  420/10010]   Loss 3.251323   Top1 34.010417   Top5 57.124256   BatchTime 0.372664   LR 0.005000
INFO - Training [0][  440/10010]   Loss 3.211339   Top1 34.625355   Top5 57.810724   BatchTime 0.371343   LR 0.005000
INFO - Training [0][  460/10010]   Loss 3.170747   Top1 35.251359   Top5 58.496943   BatchTime 0.369827   LR 0.005000
INFO - Training [0][  480/10010]   Loss 3.134766   Top1 35.833333   Top5 59.103190   BatchTime 0.368387   LR 0.005000
INFO - Training [0][  500/10010]   Loss 3.098167   Top1 36.401562   Top5 59.714062   BatchTime 0.367131   LR 0.005000
INFO - Training [0][  520/10010]   Loss 3.067088   Top1 36.867488   Top5 60.201322   BatchTime 0.366172   LR 0.005000
INFO - Training [0][  540/10010]   Loss 3.039367   Top1 37.322049   Top5 60.656829   BatchTime 0.365227   LR 0.005000
INFO - Training [0][  560/10010]   Loss 3.011031   Top1 37.760882   Top5 61.139788   BatchTime 0.364268   LR 0.005000
INFO - Training [0][  580/10010]   Loss 2.983787   Top1 38.196390   Top5 61.606950   BatchTime 0.363467   LR 0.005000
INFO - Training [0][  600/10010]   Loss 2.958201   Top1 38.569010   Top5 62.036458   BatchTime 0.362762   LR 0.005000
INFO - Training [0][  620/10010]   Loss 2.931999   Top1 38.954133   Top5 62.476058   BatchTime 0.362085   LR 0.005000
INFO - Training [0][  640/10010]   Loss 2.906609   Top1 39.346924   Top5 62.906494   BatchTime 0.361452   LR 0.005000
INFO - Training [0][  660/10010]   Loss 2.883992   Top1 39.720644   Top5 63.242188   BatchTime 0.360993   LR 0.005000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7efc78aa9d30>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 84, in train
    optimizer.step()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/sgd.py", line 146, in step
    sgd(params_with_grad,
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/sgd.py", line 197, in sgd
    func(params,
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/optim/sgd.py", line 224, in _single_tensor_sgd
    d_p = d_p.add(param, alpha=weight_decay)
KeyboardInterrupt