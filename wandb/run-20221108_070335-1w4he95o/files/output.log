INFO - Log file for this run: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_15_epoch80_20221108-070336/MobileNetv2_imagenet_a8w8_15_epoch80_20221108-070336.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_15_epoch80_20221108-070336/tb_runs
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/LSQ/quan/quantizer/lsq.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.005
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
INFO - Validation [   20/  391]   Loss 12.927401   Top1 0.000000   Top5 0.000000   BatchTime 1.064757
INFO - Validation [   40/  391]   Loss 13.225690   Top1 0.000000   Top5 0.000000   BatchTime 0.634607
INFO - Validation [   60/  391]   Loss 12.783485   Top1 0.000000   Top5 0.000000   BatchTime 0.500541
INFO - Validation [   80/  391]   Loss 11.984020   Top1 0.000000   Top5 0.488281   BatchTime 0.413040
INFO - Validation [  100/  391]   Loss 11.471709   Top1 0.000000   Top5 0.781250   BatchTime 0.358188
INFO - Validation [  120/  391]   Loss 11.341164   Top1 0.000000   Top5 0.976562   BatchTime 0.323805
INFO - Validation [  140/  391]   Loss 11.538913   Top1 0.000000   Top5 0.837054   BatchTime 0.300463
INFO - Validation [  160/  391]   Loss 11.389241   Top1 0.000000   Top5 0.732422   BatchTime 0.302811
INFO - Validation [  180/  391]   Loss 11.399283   Top1 0.000000   Top5 0.651042   BatchTime 0.293115
INFO - Validation [  200/  391]   Loss 11.465818   Top1 0.000000   Top5 0.585938   BatchTime 0.281307
INFO - Validation [  220/  391]   Loss 11.544956   Top1 0.000000   Top5 0.532670   BatchTime 0.271471
INFO - Validation [  240/  391]   Loss 11.596657   Top1 0.000000   Top5 0.488281   BatchTime 0.263513
INFO - Validation [  260/  391]   Loss 11.627968   Top1 0.000000   Top5 0.450721   BatchTime 0.256242
INFO - Validation [  280/  391]   Loss 11.677904   Top1 0.000000   Top5 0.418527   BatchTime 0.250176
INFO - Validation [  300/  391]   Loss 11.678717   Top1 0.130208   Top5 0.520833   BatchTime 0.244962
INFO - Validation [  320/  391]   Loss 11.730051   Top1 0.122070   Top5 0.488281   BatchTime 0.238943
INFO - Validation [  340/  391]   Loss 11.704230   Top1 0.114890   Top5 0.574449   BatchTime 0.233012
INFO - Validation [  360/  391]   Loss 11.660352   Top1 0.108507   Top5 0.542535   BatchTime 0.226664
INFO - Validation [  380/  391]   Loss 11.746606   Top1 0.102796   Top5 0.513980   BatchTime 0.221068
INFO - ==> Top1: 0.100    Top5: 0.500    Loss: 11.797
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.100   Top5: 0.500] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.0592, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0884, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0873, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0421, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0468, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0864, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0293, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0313, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0247, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0497, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1031, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0292, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0191, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1200, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0159, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1322, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0231, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0530, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0877, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1025, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0150, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0124, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1082, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1123, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0210, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0345, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0678, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0132, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0801, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0093, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0144, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0913, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0092, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0392, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0510, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0153, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0139, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0646, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0119, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0158, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0826, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0053, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0314, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0436, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 5.770683   Top1 6.054688   Top5 16.054688   BatchTime 0.655932   LR 0.005000
INFO - Training [0][   40/10010]   Loss 5.337116   Top1 9.394531   Top5 22.792969   BatchTime 0.492281   LR 0.005000
INFO - Training [0][   60/10010]   Loss 5.046666   Top1 11.940104   Top5 27.018229   BatchTime 0.440046   LR 0.005000
INFO - Training [0][   80/10010]   Loss 4.747122   Top1 14.755859   Top5 31.708984   BatchTime 0.413398   LR 0.005000
INFO - Training [0][  100/10010]   Loss 4.515442   Top1 17.460938   Top5 35.562500   BatchTime 0.398608   LR 0.005000
INFO - Training [0][  120/10010]   Loss 4.335036   Top1 19.427083   Top5 38.580729   BatchTime 0.388919   LR 0.005000
INFO - Training [0][  140/10010]   Loss 4.190393   Top1 20.909598   Top5 41.026786   BatchTime 0.381070   LR 0.005000
INFO - Training [0][  160/10010]   Loss 4.068400   Top1 22.441406   Top5 43.110352   BatchTime 0.375691   LR 0.005000
INFO - Training [0][  180/10010]   Loss 3.967588   Top1 23.680556   Top5 44.947917   BatchTime 0.371178   LR 0.005000
INFO - Training [0][  200/10010]   Loss 3.879076   Top1 24.902344   Top5 46.539062   BatchTime 0.367467   LR 0.005000
INFO - Training [0][  220/10010]   Loss 3.788757   Top1 26.083097   Top5 48.196023   BatchTime 0.364503   LR 0.005000
INFO - Training [0][  240/10010]   Loss 3.707006   Top1 27.298177   Top5 49.472656   BatchTime 0.362086   LR 0.005000
INFO - Training [0][  260/10010]   Loss 3.636166   Top1 28.368389   Top5 50.679087   BatchTime 0.360199   LR 0.005000
INFO - Training [0][  280/10010]   Loss 3.580290   Top1 29.199219   Top5 51.604353   BatchTime 0.358182   LR 0.005000
INFO - Training [0][  300/10010]   Loss 3.511478   Top1 30.138021   Top5 52.742188   BatchTime 0.356849   LR 0.005000
INFO - Training [0][  320/10010]   Loss 3.448432   Top1 31.018066   Top5 53.845215   BatchTime 0.355617   LR 0.005000
INFO - Training [0][  340/10010]   Loss 3.396004   Top1 31.801471   Top5 54.767923   BatchTime 0.354382   LR 0.005000
INFO - Training [0][  360/10010]   Loss 3.346800   Top1 32.521701   Top5 55.601128   BatchTime 0.353215   LR 0.005000
INFO - Training [0][  380/10010]   Loss 3.304722   Top1 33.122944   Top5 56.334293   BatchTime 0.353063   LR 0.005000
INFO - Training [0][  400/10010]   Loss 3.261627   Top1 33.802734   Top5 57.054688   BatchTime 0.352119   LR 0.005000
INFO - Training [0][  420/10010]   Loss 3.216899   Top1 34.512649   Top5 57.823661   BatchTime 0.350919   LR 0.005000
INFO - Training [0][  440/10010]   Loss 3.176255   Top1 35.110085   Top5 58.529830   BatchTime 0.350248   LR 0.005000
INFO - Training [0][  460/10010]   Loss 3.139922   Top1 35.591033   Top5 59.104959   BatchTime 0.349888   LR 0.005000
INFO - Training [0][  480/10010]   Loss 3.103539   Top1 36.181641   Top5 59.711914   BatchTime 0.349170   LR 0.005000
INFO - Training [0][  500/10010]   Loss 3.068701   Top1 36.712500   Top5 60.303125   BatchTime 0.348574   LR 0.005000
INFO - Training [0][  520/10010]   Loss 3.039192   Top1 37.205529   Top5 60.746695   BatchTime 0.348067   LR 0.005000
INFO - Training [0][  540/10010]   Loss 3.012207   Top1 37.676505   Top5 61.177662   BatchTime 0.347390   LR 0.005000
INFO - Training [0][  560/10010]   Loss 2.985516   Top1 38.087333   Top5 61.633650   BatchTime 0.346988   LR 0.005000
INFO - Training [0][  580/10010]   Loss 2.959176   Top1 38.506196   Top5 62.071659   BatchTime 0.346509   LR 0.005000
INFO - Training [0][  600/10010]   Loss 2.934628   Top1 38.915365   Top5 62.475260   BatchTime 0.346011   LR 0.005000
INFO - Training [0][  620/10010]   Loss 2.910213   Top1 39.275454   Top5 62.871724   BatchTime 0.345685   LR 0.005000
INFO - Training [0][  640/10010]   Loss 2.885748   Top1 39.654541   Top5 63.305664   BatchTime 0.345444   LR 0.005000
INFO - Training [0][  660/10010]   Loss 2.863802   Top1 40.028409   Top5 63.642282   BatchTime 0.345110   LR 0.005000
INFO - Training [0][  680/10010]   Loss 2.842347   Top1 40.386029   Top5 63.993566   BatchTime 0.344737   LR 0.005000
INFO - Training [0][  700/10010]   Loss 2.821446   Top1 40.687500   Top5 64.343750   BatchTime 0.344370   LR 0.005000
INFO - Training [0][  720/10010]   Loss 2.799275   Top1 41.046007   Top5 64.680990   BatchTime 0.344031   LR 0.005000
INFO - Training [0][  740/10010]   Loss 2.777700   Top1 41.377745   Top5 65.041174   BatchTime 0.343829   LR 0.005000
INFO - Training [0][  760/10010]   Loss 2.760294   Top1 41.627262   Top5 65.312500   BatchTime 0.343536   LR 0.005000
INFO - Training [0][  780/10010]   Loss 2.742701   Top1 41.955128   Top5 65.600962   BatchTime 0.343112   LR 0.005000
INFO - Training [0][  800/10010]   Loss 2.728929   Top1 42.213867   Top5 65.815430   BatchTime 0.343001   LR 0.005000
INFO - Training [0][  820/10010]   Loss 2.714455   Top1 42.457127   Top5 66.048018   BatchTime 0.342736   LR 0.005000
INFO - Training [0][  840/10010]   Loss 2.698640   Top1 42.748326   Top5 66.309524   BatchTime 0.342676   LR 0.005000
INFO - Training [0][  860/10010]   Loss 2.682072   Top1 43.024164   Top5 66.579760   BatchTime 0.342672   LR 0.005000
INFO - Training [0][  880/10010]   Loss 2.668836   Top1 43.252841   Top5 66.787997   BatchTime 0.342456   LR 0.005000
INFO - Training [0][  900/10010]   Loss 2.656607   Top1 43.447049   Top5 66.986979   BatchTime 0.342328   LR 0.005000
INFO - Training [0][  920/10010]   Loss 2.643982   Top1 43.638757   Top5 67.192595   BatchTime 0.342216   LR 0.005000
INFO - Training [0][  940/10010]   Loss 2.629258   Top1 43.887965   Top5 67.430186   BatchTime 0.342026   LR 0.005000
INFO - Training [0][  960/10010]   Loss 2.618332   Top1 44.053548   Top5 67.605794   BatchTime 0.341956   LR 0.005000
INFO - Training [0][  980/10010]   Loss 2.605611   Top1 44.272162   Top5 67.807717   BatchTime 0.341692   LR 0.005000
INFO - Training [0][ 1000/10010]   Loss 2.591943   Top1 44.485937   Top5 68.032031   BatchTime 0.341595   LR 0.005000
INFO - Training [0][ 1020/10010]   Loss 2.579845   Top1 44.669118   Top5 68.235294   BatchTime 0.341568   LR 0.005000
INFO - Training [0][ 1040/10010]   Loss 2.568723   Top1 44.828726   Top5 68.418720   BatchTime 0.341599   LR 0.005000
INFO - Training [0][ 1060/10010]   Loss 2.557515   Top1 45.039800   Top5 68.612913   BatchTime 0.341497   LR 0.005000
INFO - Training [0][ 1080/10010]   Loss 2.547263   Top1 45.210503   Top5 68.765914   BatchTime 0.341398   LR 0.005000
INFO - Training [0][ 1100/10010]   Loss 2.537394   Top1 45.376420   Top5 68.928977   BatchTime 0.341281   LR 0.005000
INFO - Training [0][ 1120/10010]   Loss 2.528112   Top1 45.539900   Top5 69.073661   BatchTime 0.341154   LR 0.005000
INFO - Training [0][ 1140/10010]   Loss 2.516388   Top1 45.729852   Top5 69.277001   BatchTime 0.341071   LR 0.005000
INFO - Training [0][ 1160/10010]   Loss 2.505033   Top1 45.929418   Top5 69.460533   BatchTime 0.342042   LR 0.005000
INFO - Training [0][ 1180/10010]   Loss 2.494588   Top1 46.102357   Top5 69.619968   BatchTime 0.341910   LR 0.005000
INFO - Training [0][ 1200/10010]   Loss 2.485794   Top1 46.264974   Top5 69.778646   BatchTime 0.342122   LR 0.005000
INFO - Training [0][ 1220/10010]   Loss 2.476622   Top1 46.418417   Top5 69.935323   BatchTime 0.342289   LR 0.005000
INFO - Training [0][ 1240/10010]   Loss 2.468551   Top1 46.570060   Top5 70.071195   BatchTime 0.342481   LR 0.005000
INFO - Training [0][ 1260/10010]   Loss 2.459847   Top1 46.720610   Top5 70.202133   BatchTime 0.342810   LR 0.005000
INFO - Training [0][ 1280/10010]   Loss 2.451241   Top1 46.861572   Top5 70.340576   BatchTime 0.342738   LR 0.005000
INFO - Training [0][ 1300/10010]   Loss 2.444306   Top1 47.000000   Top5 70.438702   BatchTime 0.342654   LR 0.005000
INFO - Training [0][ 1320/10010]   Loss 2.437915   Top1 47.108191   Top5 70.541548   BatchTime 0.342578   LR 0.005000
INFO - Training [0][ 1340/10010]   Loss 2.429832   Top1 47.261544   Top5 70.661147   BatchTime 0.342469   LR 0.005000
INFO - Training [0][ 1360/10010]   Loss 2.422503   Top1 47.377642   Top5 70.776654   BatchTime 0.342371   LR 0.005000
INFO - Training [0][ 1380/10010]   Loss 2.414533   Top1 47.526042   Top5 70.909760   BatchTime 0.342283   LR 0.005000
INFO - Training [0][ 1400/10010]   Loss 2.406562   Top1 47.649554   Top5 71.034598   BatchTime 0.342200   LR 0.005000
INFO - Training [0][ 1420/10010]   Loss 2.399563   Top1 47.768486   Top5 71.150968   BatchTime 0.342043   LR 0.005000
INFO - Training [0][ 1440/10010]   Loss 2.392942   Top1 47.887370   Top5 71.250000   BatchTime 0.341875   LR 0.005000
INFO - Training [0][ 1460/10010]   Loss 2.385102   Top1 48.029217   Top5 71.377890   BatchTime 0.342412   LR 0.005000
INFO - Training [0][ 1480/10010]   Loss 2.379300   Top1 48.133974   Top5 71.474345   BatchTime 0.342351   LR 0.005000
INFO - Training [0][ 1500/10010]   Loss 2.372108   Top1 48.265104   Top5 71.576042   BatchTime 0.342465   LR 0.005000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fbe05371d30>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 83, in train
    loss.backward()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt