INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106.log
2022-11-03 22:41:06.671812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-03 22:41:06.795507: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-03 22:41:07.180206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-03 22:41:07.180259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-03 22:41:07.180265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/tb_runs
Files already downloaded and verified
Files already downloaded and verified
hello
********************pre-trained*****************
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
INFO - Created `MobileNetv2` model for `cifar10` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): QuanConv2d(
          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): IdentityQuan()
          (quan_a_fn): IdentityQuan()
        )
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv): Sequential(
      (0): QuanConv2d(
        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
        (quan_w_fn): SLsqQuan()
        (quan_a_fn): LsqQuan()
      )
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): QuanLinear(
      in_features=1280, out_features=10, bias=True
      (quan_w_fn): IdentityQuan()
      (quan_a_fn): IdentityQuan()
    )
  )
)
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 2.545371   Top1 10.429688   Top5 49.101562   BatchTime 0.183324
INFO - Validation [   40/   79]   Loss 2.549466   Top1 10.175781   Top5 49.941406   BatchTime 0.113624
INFO - Validation [   60/   79]   Loss 2.541519   Top1 10.117188   Top5 50.377604   BatchTime 0.090532
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.546
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - >>>>>>>> Epoch   0
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [0][   20/  391]   Loss 1.547804   Top1 69.296875   Top5 96.757812   BatchTime 0.205804   LR 0.010000
INFO - Training [0][   40/  391]   Loss 1.298149   Top1 70.527344   Top5 97.246094   BatchTime 0.153136   LR 0.010000
INFO - Training [0][   60/  391]   Loss 1.118628   Top1 71.940104   Top5 97.565104   BatchTime 0.135877   LR 0.010000
INFO - Training [0][   80/  391]   Loss 1.001836   Top1 73.535156   Top5 97.949219   BatchTime 0.126868   LR 0.010000
INFO - Training [0][  100/  391]   Loss 0.911430   Top1 74.882812   Top5 98.210938   BatchTime 0.121497   LR 0.010000
INFO - Training [0][  120/  391]   Loss 0.842658   Top1 76.178385   Top5 98.398438   BatchTime 0.118047   LR 0.010000
INFO - Training [0][  140/  391]   Loss 0.793718   Top1 77.170759   Top5 98.504464   BatchTime 0.115378   LR 0.010000
INFO - Training [0][  160/  391]   Loss 0.753082   Top1 77.958984   Top5 98.627930   BatchTime 0.113351   LR 0.010000
INFO - Training [0][  180/  391]   Loss 0.717213   Top1 78.750000   Top5 98.710938   BatchTime 0.111767   LR 0.010000
INFO - Training [0][  200/  391]   Loss 0.692259   Top1 79.214844   Top5 98.761719   BatchTime 0.110634   LR 0.010000
INFO - Training [0][  220/  391]   Loss 0.669018   Top1 79.715909   Top5 98.824574   BatchTime 0.109587   LR 0.010000
INFO - Training [0][  240/  391]   Loss 0.648714   Top1 80.169271   Top5 98.886719   BatchTime 0.108864   LR 0.010000
INFO - Training [0][  260/  391]   Loss 0.628460   Top1 80.646034   Top5 98.948317   BatchTime 0.108047   LR 0.010000
INFO - Training [0][  280/  391]   Loss 0.612703   Top1 81.015625   Top5 98.989955   BatchTime 0.107364   LR 0.010000
INFO - Training [0][  300/  391]   Loss 0.599784   Top1 81.286458   Top5 99.005208   BatchTime 0.106745   LR 0.010000
INFO - Training [0][  320/  391]   Loss 0.585583   Top1 81.652832   Top5 99.047852   BatchTime 0.106081   LR 0.010000
INFO - Training [0][  340/  391]   Loss 0.573082   Top1 81.953125   Top5 99.076287   BatchTime 0.104599   LR 0.010000
INFO - Training [0][  360/  391]   Loss 0.560171   Top1 82.280816   Top5 99.116753   BatchTime 0.103344   LR 0.010000
INFO - Training [0][  380/  391]   Loss 0.550116   Top1 82.571957   Top5 99.140625   BatchTime 0.102447   LR 0.010000
INFO - ==> Top1: 82.714    Top5: 99.150    Loss: 0.545
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [0][   20/   79]   Loss 0.456158   Top1 85.039062   Top5 99.414062   BatchTime 0.140300
INFO - Validation [0][   40/   79]   Loss 0.458075   Top1 85.039062   Top5 99.257812   BatchTime 0.092449
INFO - Validation [0][   60/   79]   Loss 0.461006   Top1 84.882812   Top5 99.296875   BatchTime 0.076460
INFO - ==> Top1: 84.570    Top5: 99.370    Loss: 0.460
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 84.570   Top5: 99.370] Sparsity : 0.593
INFO - Scoreboard best 2 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   1
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [1][   20/  391]   Loss 0.334199   Top1 88.359375   Top5 99.726562   BatchTime 0.192167   LR 0.010000
INFO - Training [1][   40/  391]   Loss 0.319475   Top1 89.062500   Top5 99.648438   BatchTime 0.146415   LR 0.010000
INFO - Training [1][   60/  391]   Loss 0.318866   Top1 89.088542   Top5 99.700521   BatchTime 0.131014   LR 0.010000
INFO - Training [1][   80/  391]   Loss 0.318700   Top1 88.955078   Top5 99.707031   BatchTime 0.123208   LR 0.010000
INFO - Training [1][  100/  391]   Loss 0.312545   Top1 89.164062   Top5 99.718750   BatchTime 0.118735   LR 0.010000
INFO - Training [1][  120/  391]   Loss 0.309983   Top1 89.290365   Top5 99.713542   BatchTime 0.115654   LR 0.010000
INFO - Training [1][  140/  391]   Loss 0.304069   Top1 89.531250   Top5 99.743304   BatchTime 0.113423   LR 0.010000
INFO - Training [1][  160/  391]   Loss 0.302743   Top1 89.521484   Top5 99.750977   BatchTime 0.111826   LR 0.010000
INFO - Training [1][  180/  391]   Loss 0.300818   Top1 89.513889   Top5 99.748264   BatchTime 0.110567   LR 0.010000
INFO - Training [1][  200/  391]   Loss 0.298429   Top1 89.582031   Top5 99.738281   BatchTime 0.109656   LR 0.010000
INFO - Training [1][  220/  391]   Loss 0.297907   Top1 89.651989   Top5 99.730114   BatchTime 0.108835   LR 0.010000
INFO - Training [1][  240/  391]   Loss 0.295798   Top1 89.720052   Top5 99.742839   BatchTime 0.108130   LR 0.010000
INFO - Training [1][  260/  391]   Loss 0.293711   Top1 89.786659   Top5 99.759615   BatchTime 0.107524   LR 0.010000
INFO - Training [1][  280/  391]   Loss 0.291411   Top1 89.852121   Top5 99.762835   BatchTime 0.106962   LR 0.010000
INFO - Training [1][  300/  391]   Loss 0.289720   Top1 89.903646   Top5 99.765625   BatchTime 0.106432   LR 0.010000
INFO - Training [1][  320/  391]   Loss 0.288012   Top1 89.914551   Top5 99.772949   BatchTime 0.105950   LR 0.010000
INFO - Training [1][  340/  391]   Loss 0.287279   Top1 89.949449   Top5 99.772518   BatchTime 0.104432   LR 0.010000
INFO - Training [1][  360/  391]   Loss 0.285411   Top1 89.982639   Top5 99.780816   BatchTime 0.103282   LR 0.010000
INFO - Training [1][  380/  391]   Loss 0.284007   Top1 90.010280   Top5 99.786184   BatchTime 0.102032   LR 0.010000
INFO - ==> Top1: 90.036    Top5: 99.792    Loss: 0.283
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [1][   20/   79]   Loss 0.449490   Top1 85.625000   Top5 99.453125   BatchTime 0.131483
INFO - Validation [1][   40/   79]   Loss 0.449532   Top1 85.800781   Top5 99.375000   BatchTime 0.088487
INFO - Validation [1][   60/   79]   Loss 0.439498   Top1 86.145833   Top5 99.440104   BatchTime 0.073776
INFO - ==> Top1: 86.370    Top5: 99.460    Loss: 0.435
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 86.370   Top5: 99.460] Sparsity : 0.630
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 84.570   Top5: 99.370] Sparsity : 0.593
INFO - Scoreboard best 3 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   2
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [2][   20/  391]   Loss 0.221092   Top1 93.046875   Top5 99.765625   BatchTime 0.190740   LR 0.010000
INFO - Training [2][   40/  391]   Loss 0.219095   Top1 92.656250   Top5 99.863281   BatchTime 0.145481   LR 0.010000
INFO - Training [2][   60/  391]   Loss 0.220840   Top1 92.486979   Top5 99.856771   BatchTime 0.130721   LR 0.010000
INFO - Training [2][   80/  391]   Loss 0.219047   Top1 92.402344   Top5 99.824219   BatchTime 0.123138   LR 0.010000
INFO - Training [2][  100/  391]   Loss 0.221442   Top1 92.273438   Top5 99.828125   BatchTime 0.118474   LR 0.010000
INFO - Training [2][  120/  391]   Loss 0.221635   Top1 92.246094   Top5 99.843750   BatchTime 0.115463   LR 0.010000
INFO - Training [2][  140/  391]   Loss 0.221780   Top1 92.293527   Top5 99.860491   BatchTime 0.113294   LR 0.010000
INFO - Training [2][  160/  391]   Loss 0.224275   Top1 92.182617   Top5 99.853516   BatchTime 0.111600   LR 0.010000
INFO - Training [2][  180/  391]   Loss 0.224761   Top1 92.157118   Top5 99.869792   BatchTime 0.110362   LR 0.010000
INFO - Training [2][  200/  391]   Loss 0.223797   Top1 92.218750   Top5 99.875000   BatchTime 0.109447   LR 0.010000
INFO - Training [2][  220/  391]   Loss 0.226984   Top1 92.095170   Top5 99.872159   BatchTime 0.108620   LR 0.010000
INFO - Training [2][  240/  391]   Loss 0.227760   Top1 92.067057   Top5 99.873047   BatchTime 0.107993   LR 0.010000
INFO - Training [2][  260/  391]   Loss 0.228599   Top1 92.043269   Top5 99.867788   BatchTime 0.107423   LR 0.010000
INFO - Training [2][  280/  391]   Loss 0.229196   Top1 92.045201   Top5 99.871652   BatchTime 0.106847   LR 0.010000
INFO - Training [2][  300/  391]   Loss 0.230066   Top1 92.041667   Top5 99.864583   BatchTime 0.106322   LR 0.010000
INFO - Training [2][  320/  391]   Loss 0.230063   Top1 92.019043   Top5 99.858398   BatchTime 0.105893   LR 0.010000
INFO - Training [2][  340/  391]   Loss 0.230986   Top1 91.985294   Top5 99.852941   BatchTime 0.104924   LR 0.010000
INFO - Training [2][  360/  391]   Loss 0.230435   Top1 91.996528   Top5 99.856771   BatchTime 0.103599   LR 0.010000
INFO - Training [2][  380/  391]   Loss 0.229963   Top1 92.010691   Top5 99.849918   BatchTime 0.102272   LR 0.010000
INFO - ==> Top1: 92.008    Top5: 99.854    Loss: 0.230
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [2][   20/   79]   Loss 0.401712   Top1 87.421875   Top5 99.296875   BatchTime 0.131151
INFO - Validation [2][   40/   79]   Loss 0.409924   Top1 87.519531   Top5 99.199219   BatchTime 0.087395
INFO - Validation [2][   60/   79]   Loss 0.405759   Top1 87.565104   Top5 99.348958   BatchTime 0.073870
INFO - ==> Top1: 87.500    Top5: 99.430    Loss: 0.408
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 87.500   Top5: 99.430] Sparsity : 0.715
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 86.370   Top5: 99.460] Sparsity : 0.630
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 84.570   Top5: 99.370] Sparsity : 0.593
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   3
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [3][   20/  391]   Loss 0.207268   Top1 92.851562   Top5 99.921875   BatchTime 0.189560   LR 0.010000
INFO - Training [3][   40/  391]   Loss 0.199856   Top1 93.242188   Top5 99.902344   BatchTime 0.144951   LR 0.010000
INFO - Training [3][   60/  391]   Loss 0.202058   Top1 93.177083   Top5 99.882812   BatchTime 0.130145   LR 0.010000
INFO - Training [3][   80/  391]   Loss 0.202730   Top1 93.115234   Top5 99.902344   BatchTime 0.122594   LR 0.010000
INFO - Training [3][  100/  391]   Loss 0.208077   Top1 92.843750   Top5 99.914062   BatchTime 0.118259   LR 0.010000
INFO - Training [3][  120/  391]   Loss 0.212017   Top1 92.721354   Top5 99.908854   BatchTime 0.115256   LR 0.010000
INFO - Training [3][  140/  391]   Loss 0.214771   Top1 92.639509   Top5 99.905134   BatchTime 0.113042   LR 0.010000
INFO - Training [3][  160/  391]   Loss 0.216506   Top1 92.631836   Top5 99.892578   BatchTime 0.111373   LR 0.010000
INFO - Training [3][  180/  391]   Loss 0.221621   Top1 92.460938   Top5 99.887153   BatchTime 0.110063   LR 0.010000
INFO - Training [3][  200/  391]   Loss 0.222040   Top1 92.433594   Top5 99.871094   BatchTime 0.109080   LR 0.010000
INFO - Training [3][  220/  391]   Loss 0.223677   Top1 92.382812   Top5 99.868608   BatchTime 0.108313   LR 0.010000
INFO - Training [3][  240/  391]   Loss 0.224812   Top1 92.343750   Top5 99.876302   BatchTime 0.107565   LR 0.010000
INFO - Training [3][  260/  391]   Loss 0.226039   Top1 92.262620   Top5 99.867788   BatchTime 0.107020   LR 0.010000
INFO - Training [3][  280/  391]   Loss 0.227059   Top1 92.207031   Top5 99.871652   BatchTime 0.106430   LR 0.010000
INFO - Training [3][  300/  391]   Loss 0.225985   Top1 92.216146   Top5 99.880208   BatchTime 0.105906   LR 0.010000
INFO - Training [3][  320/  391]   Loss 0.224551   Top1 92.272949   Top5 99.880371   BatchTime 0.105462   LR 0.010000
INFO - Training [3][  340/  391]   Loss 0.224366   Top1 92.267923   Top5 99.885110   BatchTime 0.104566   LR 0.010000
INFO - Training [3][  360/  391]   Loss 0.224463   Top1 92.250434   Top5 99.891493   BatchTime 0.102988   LR 0.010000
INFO - Training [3][  380/  391]   Loss 0.224017   Top1 92.257401   Top5 99.886924   BatchTime 0.101819   LR 0.010000
INFO - ==> Top1: 92.258    Top5: 99.884    Loss: 0.224
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [3][   20/   79]   Loss 0.423584   Top1 87.265625   Top5 99.296875   BatchTime 0.123572
INFO - Validation [3][   40/   79]   Loss 0.414104   Top1 87.363281   Top5 99.179688   BatchTime 0.084993
INFO - Validation [3][   60/   79]   Loss 0.412683   Top1 87.382812   Top5 99.257812   BatchTime 0.071764
INFO - ==> Top1: 87.590    Top5: 99.360    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 87.590   Top5: 99.360] Sparsity : 0.740
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 87.500   Top5: 99.430] Sparsity : 0.715
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 86.370   Top5: 99.460] Sparsity : 0.630
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   4
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [4][   20/  391]   Loss 0.201467   Top1 92.656250   Top5 99.882812   BatchTime 0.190439   LR 0.010000
INFO - Training [4][   40/  391]   Loss 0.203158   Top1 92.871094   Top5 99.902344   BatchTime 0.145486   LR 0.010000
INFO - Training [4][   60/  391]   Loss 0.199934   Top1 93.072917   Top5 99.921875   BatchTime 0.130555   LR 0.010000
INFO - Training [4][   80/  391]   Loss 0.196938   Top1 93.085938   Top5 99.931641   BatchTime 0.122839   LR 0.010000
INFO - Training [4][  100/  391]   Loss 0.197364   Top1 93.156250   Top5 99.945312   BatchTime 0.118288   LR 0.010000
INFO - Training [4][  120/  391]   Loss 0.199027   Top1 93.072917   Top5 99.941406   BatchTime 0.115214   LR 0.010000
INFO - Training [4][  140/  391]   Loss 0.201855   Top1 93.030134   Top5 99.944196   BatchTime 0.113041   LR 0.010000
INFO - Training [4][  160/  391]   Loss 0.199964   Top1 93.125000   Top5 99.926758   BatchTime 0.111537   LR 0.010000
INFO - Training [4][  180/  391]   Loss 0.198613   Top1 93.107639   Top5 99.921875   BatchTime 0.110359   LR 0.010000
INFO - Training [4][  200/  391]   Loss 0.196892   Top1 93.136719   Top5 99.925781   BatchTime 0.109321   LR 0.010000
INFO - Training [4][  220/  391]   Loss 0.195335   Top1 93.220881   Top5 99.928977   BatchTime 0.108445   LR 0.010000
INFO - Training [4][  240/  391]   Loss 0.192756   Top1 93.297526   Top5 99.931641   BatchTime 0.107799   LR 0.010000
INFO - Training [4][  260/  391]   Loss 0.192335   Top1 93.350361   Top5 99.930889   BatchTime 0.107200   LR 0.010000
INFO - Training [4][  280/  391]   Loss 0.191881   Top1 93.351004   Top5 99.927455   BatchTime 0.106617   LR 0.010000
INFO - Training [4][  300/  391]   Loss 0.190972   Top1 93.401042   Top5 99.924479   BatchTime 0.106131   LR 0.010000
INFO - Training [4][  320/  391]   Loss 0.189618   Top1 93.432617   Top5 99.929199   BatchTime 0.105678   LR 0.010000
INFO - Training [4][  340/  391]   Loss 0.188536   Top1 93.478860   Top5 99.933364   BatchTime 0.105116   LR 0.010000
INFO - Training [4][  360/  391]   Loss 0.188205   Top1 93.478733   Top5 99.934896   BatchTime 0.103635   LR 0.010000
INFO - Training [4][  380/  391]   Loss 0.187744   Top1 93.495066   Top5 99.936266   BatchTime 0.102375   LR 0.010000
INFO - ==> Top1: 93.500    Top5: 99.936    Loss: 0.188
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [4][   20/   79]   Loss 0.401370   Top1 88.164062   Top5 99.453125   BatchTime 0.121062
INFO - Validation [4][   40/   79]   Loss 0.392270   Top1 88.457031   Top5 99.414062   BatchTime 0.079939
INFO - Validation [4][   60/   79]   Loss 0.387324   Top1 88.346354   Top5 99.492188   BatchTime 0.066697
INFO - ==> Top1: 88.410    Top5: 99.540    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.410   Top5: 99.540] Sparsity : 0.745
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 87.590   Top5: 99.360] Sparsity : 0.740
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 87.500   Top5: 99.430] Sparsity : 0.715
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   5
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [5][   20/  391]   Loss 0.174046   Top1 94.335938   Top5 99.921875   BatchTime 0.194593   LR 0.010000
INFO - Training [5][   40/  391]   Loss 0.152183   Top1 95.078125   Top5 99.941406   BatchTime 0.147610   LR 0.010000
INFO - Training [5][   60/  391]   Loss 0.158089   Top1 94.713542   Top5 99.934896   BatchTime 0.131954   LR 0.010000
INFO - Training [5][   80/  391]   Loss 0.155257   Top1 94.687500   Top5 99.941406   BatchTime 0.123834   LR 0.010000
INFO - Training [5][  100/  391]   Loss 0.153881   Top1 94.742188   Top5 99.929688   BatchTime 0.119174   LR 0.010000
INFO - Training [5][  120/  391]   Loss 0.153272   Top1 94.720052   Top5 99.934896   BatchTime 0.116068   LR 0.010000
INFO - Training [5][  140/  391]   Loss 0.152384   Top1 94.743304   Top5 99.938616   BatchTime 0.113906   LR 0.010000
INFO - Training [5][  160/  391]   Loss 0.154932   Top1 94.687500   Top5 99.936523   BatchTime 0.112156   LR 0.010000
INFO - Training [5][  180/  391]   Loss 0.154236   Top1 94.717882   Top5 99.943576   BatchTime 0.110833   LR 0.010000
INFO - Training [5][  200/  391]   Loss 0.154830   Top1 94.625000   Top5 99.949219   BatchTime 0.109788   LR 0.010000
INFO - Training [5][  220/  391]   Loss 0.158061   Top1 94.502841   Top5 99.943182   BatchTime 0.108931   LR 0.010000
INFO - Training [5][  240/  391]   Loss 0.156991   Top1 94.557292   Top5 99.941406   BatchTime 0.108267   LR 0.010000
INFO - Training [5][  260/  391]   Loss 0.156754   Top1 94.561298   Top5 99.945913   BatchTime 0.107631   LR 0.010000
INFO - Training [5][  280/  391]   Loss 0.157078   Top1 94.578683   Top5 99.946987   BatchTime 0.107111   LR 0.010000
INFO - Training [5][  300/  391]   Loss 0.156813   Top1 94.557292   Top5 99.950521   BatchTime 0.106603   LR 0.010000
INFO - Training [5][  320/  391]   Loss 0.156095   Top1 94.577637   Top5 99.943848   BatchTime 0.106135   LR 0.010000
INFO - Training [5][  340/  391]   Loss 0.157354   Top1 94.545037   Top5 99.928768   BatchTime 0.106021   LR 0.010000
INFO - Training [5][  360/  391]   Loss 0.156787   Top1 94.552951   Top5 99.928385   BatchTime 0.104167   LR 0.010000
INFO - Training [5][  380/  391]   Loss 0.157156   Top1 94.520970   Top5 99.928043   BatchTime 0.103011   LR 0.010000
INFO - ==> Top1: 94.498    Top5: 99.930    Loss: 0.158
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [5][   20/   79]   Loss 0.402134   Top1 88.164062   Top5 99.414062   BatchTime 0.121901
INFO - Validation [5][   40/   79]   Loss 0.399562   Top1 88.398438   Top5 99.355469   BatchTime 0.075068
INFO - Validation [5][   60/   79]   Loss 0.392018   Top1 88.632812   Top5 99.453125   BatchTime 0.065631
INFO - ==> Top1: 88.470    Top5: 99.490    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.470   Top5: 99.490] Sparsity : 0.751
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.410   Top5: 99.540] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 87.590   Top5: 99.360] Sparsity : 0.740
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   6
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [6][   20/  391]   Loss 0.155695   Top1 94.101562   Top5 99.921875   BatchTime 0.190843   LR 0.010000
INFO - Training [6][   40/  391]   Loss 0.148453   Top1 94.804688   Top5 99.921875   BatchTime 0.145153   LR 0.010000
INFO - Training [6][   60/  391]   Loss 0.145516   Top1 94.921875   Top5 99.947917   BatchTime 0.130000   LR 0.010000
INFO - Training [6][   80/  391]   Loss 0.143396   Top1 95.009766   Top5 99.941406   BatchTime 0.122549   LR 0.010000
INFO - Training [6][  100/  391]   Loss 0.146371   Top1 94.929688   Top5 99.937500   BatchTime 0.117904   LR 0.010000
INFO - Training [6][  120/  391]   Loss 0.144588   Top1 95.019531   Top5 99.941406   BatchTime 0.114918   LR 0.010000
INFO - Training [6][  140/  391]   Loss 0.145236   Top1 95.011161   Top5 99.949777   BatchTime 0.112898   LR 0.010000
INFO - Training [6][  160/  391]   Loss 0.142803   Top1 95.087891   Top5 99.956055   BatchTime 0.111374   LR 0.010000
INFO - Training [6][  180/  391]   Loss 0.142723   Top1 95.082465   Top5 99.956597   BatchTime 0.110168   LR 0.010000
INFO - Training [6][  200/  391]   Loss 0.143053   Top1 95.105469   Top5 99.953125   BatchTime 0.109176   LR 0.010000
INFO - Training [6][  220/  391]   Loss 0.142508   Top1 95.074574   Top5 99.957386   BatchTime 0.108372   LR 0.010000
INFO - Training [6][  240/  391]   Loss 0.143604   Top1 95.042318   Top5 99.954427   BatchTime 0.107756   LR 0.010000
INFO - Training [6][  260/  391]   Loss 0.143453   Top1 95.036058   Top5 99.951923   BatchTime 0.107181   LR 0.010000
INFO - Training [6][  280/  391]   Loss 0.143090   Top1 95.053013   Top5 99.952567   BatchTime 0.106657   LR 0.010000
INFO - Training [6][  300/  391]   Loss 0.143508   Top1 95.039062   Top5 99.950521   BatchTime 0.106166   LR 0.010000
INFO - Training [6][  320/  391]   Loss 0.143342   Top1 95.065918   Top5 99.953613   BatchTime 0.105731   LR 0.010000
INFO - Training [6][  340/  391]   Loss 0.144477   Top1 95.002298   Top5 99.956342   BatchTime 0.105287   LR 0.010000
INFO - Training [6][  360/  391]   Loss 0.145078   Top1 94.956597   Top5 99.958767   BatchTime 0.104067   LR 0.010000
INFO - Training [6][  380/  391]   Loss 0.146432   Top1 94.893092   Top5 99.960938   BatchTime 0.102998   LR 0.010000
INFO - ==> Top1: 94.876    Top5: 99.958    Loss: 0.147
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [6][   20/   79]   Loss 0.392419   Top1 88.046875   Top5 99.414062   BatchTime 0.120614
INFO - Validation [6][   40/   79]   Loss 0.394724   Top1 88.339844   Top5 99.394531   BatchTime 0.073447
INFO - Validation [6][   60/   79]   Loss 0.388793   Top1 88.541667   Top5 99.479167   BatchTime 0.062240
INFO - ==> Top1: 88.610    Top5: 99.530    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 88.610   Top5: 99.530] Sparsity : 0.764
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.470   Top5: 99.490] Sparsity : 0.751
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.410   Top5: 99.540] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   7
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [7][   20/  391]   Loss 0.150120   Top1 94.257812   Top5 100.000000   BatchTime 0.188484   LR 0.010000
INFO - Training [7][   40/  391]   Loss 0.158724   Top1 94.394531   Top5 99.941406   BatchTime 0.143746   LR 0.010000
INFO - Training [7][   60/  391]   Loss 0.158347   Top1 94.492188   Top5 99.921875   BatchTime 0.129733   LR 0.010000
INFO - Training [7][   80/  391]   Loss 0.162223   Top1 94.433594   Top5 99.931641   BatchTime 0.124156   LR 0.010000
INFO - Training [7][  100/  391]   Loss 0.160614   Top1 94.429688   Top5 99.929688   BatchTime 0.119458   LR 0.010000
INFO - Training [7][  120/  391]   Loss 0.161658   Top1 94.407552   Top5 99.928385   BatchTime 0.116307   LR 0.010000
INFO - Training [7][  140/  391]   Loss 0.164116   Top1 94.246652   Top5 99.927455   BatchTime 0.114080   LR 0.010000
INFO - Training [7][  160/  391]   Loss 0.164787   Top1 94.272461   Top5 99.907227   BatchTime 0.112424   LR 0.010000
INFO - Training [7][  180/  391]   Loss 0.166794   Top1 94.210069   Top5 99.917535   BatchTime 0.111095   LR 0.010000
INFO - Training [7][  200/  391]   Loss 0.169172   Top1 94.097656   Top5 99.925781   BatchTime 0.109989   LR 0.010000
INFO - Training [7][  220/  391]   Loss 0.170068   Top1 94.080256   Top5 99.921875   BatchTime 0.109124   LR 0.010000
INFO - Training [7][  240/  391]   Loss 0.173522   Top1 93.964844   Top5 99.915365   BatchTime 0.108389   LR 0.010000
INFO - Training [7][  260/  391]   Loss 0.176313   Top1 93.873197   Top5 99.912861   BatchTime 0.107734   LR 0.010000
INFO - Training [7][  280/  391]   Loss 0.178243   Top1 93.822545   Top5 99.910714   BatchTime 0.107204   LR 0.010000
INFO - Training [7][  300/  391]   Loss 0.179369   Top1 93.781250   Top5 99.903646   BatchTime 0.106696   LR 0.010000
INFO - Training [7][  320/  391]   Loss 0.180956   Top1 93.718262   Top5 99.899902   BatchTime 0.106178   LR 0.010000
INFO - Training [7][  340/  391]   Loss 0.181866   Top1 93.690257   Top5 99.887408   BatchTime 0.105683   LR 0.010000
INFO - Training [7][  360/  391]   Loss 0.180946   Top1 93.730469   Top5 99.893663   BatchTime 0.104657   LR 0.010000
INFO - Training [7][  380/  391]   Loss 0.181266   Top1 93.721217   Top5 99.899260   BatchTime 0.103366   LR 0.010000
INFO - ==> Top1: 93.720    Top5: 99.900    Loss: 0.181
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [7][   20/   79]   Loss 0.400650   Top1 87.812500   Top5 99.492188   BatchTime 0.125196
INFO - Validation [7][   40/   79]   Loss 0.406497   Top1 87.929688   Top5 99.355469   BatchTime 0.075761
INFO - Validation [7][   60/   79]   Loss 0.401811   Top1 88.098958   Top5 99.361979   BatchTime 0.063807
INFO - ==> Top1: 88.060    Top5: 99.390    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 88.610   Top5: 99.530] Sparsity : 0.764
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.470   Top5: 99.490] Sparsity : 0.751
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.410   Top5: 99.540] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   8
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [8][   20/  391]   Loss 0.164266   Top1 94.609375   Top5 99.882812   BatchTime 0.190508   LR 0.010000
INFO - Training [8][   40/  391]   Loss 0.175012   Top1 94.199219   Top5 99.902344   BatchTime 0.145778   LR 0.010000
INFO - Training [8][   60/  391]   Loss 0.170517   Top1 94.192708   Top5 99.895833   BatchTime 0.130265   LR 0.010000
INFO - Training [8][   80/  391]   Loss 0.173031   Top1 94.111328   Top5 99.902344   BatchTime 0.122732   LR 0.010000
INFO - Training [8][  100/  391]   Loss 0.172088   Top1 94.140625   Top5 99.890625   BatchTime 0.118277   LR 0.010000
INFO - Training [8][  120/  391]   Loss 0.173764   Top1 93.958333   Top5 99.902344   BatchTime 0.115362   LR 0.010000
INFO - Training [8][  140/  391]   Loss 0.178361   Top1 93.833705   Top5 99.899554   BatchTime 0.113345   LR 0.010000
INFO - Training [8][  160/  391]   Loss 0.178122   Top1 93.813477   Top5 99.902344   BatchTime 0.111682   LR 0.010000
INFO - Training [8][  180/  391]   Loss 0.175373   Top1 93.914931   Top5 99.904514   BatchTime 0.110398   LR 0.010000
INFO - Training [8][  200/  391]   Loss 0.177153   Top1 93.843750   Top5 99.906250   BatchTime 0.109310   LR 0.010000
INFO - Training [8][  220/  391]   Loss 0.177188   Top1 93.874290   Top5 99.904119   BatchTime 0.108363   LR 0.010000
INFO - Training [8][  240/  391]   Loss 0.175946   Top1 93.902995   Top5 99.905599   BatchTime 0.107745   LR 0.010000
INFO - Training [8][  260/  391]   Loss 0.175347   Top1 93.957332   Top5 99.903846   BatchTime 0.107191   LR 0.010000
INFO - Training [8][  280/  391]   Loss 0.176032   Top1 93.931362   Top5 99.902344   BatchTime 0.106649   LR 0.010000
INFO - Training [8][  300/  391]   Loss 0.175701   Top1 93.937500   Top5 99.901042   BatchTime 0.106216   LR 0.010000
INFO - Training [8][  320/  391]   Loss 0.176685   Top1 93.879395   Top5 99.897461   BatchTime 0.105746   LR 0.010000
INFO - Training [8][  340/  391]   Loss 0.175220   Top1 93.926930   Top5 99.901195   BatchTime 0.105299   LR 0.010000
INFO - Training [8][  360/  391]   Loss 0.176107   Top1 93.895399   Top5 99.898003   BatchTime 0.104650   LR 0.010000
INFO - Training [8][  380/  391]   Loss 0.176482   Top1 93.881579   Top5 99.901316   BatchTime 0.103192   LR 0.010000
INFO - ==> Top1: 93.894    Top5: 99.902    Loss: 0.176
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [8][   20/   79]   Loss 0.347790   Top1 89.101562   Top5 99.804688   BatchTime 0.122563
INFO - Validation [8][   40/   79]   Loss 0.362174   Top1 89.199219   Top5 99.609375   BatchTime 0.074394
INFO - Validation [8][   60/   79]   Loss 0.363975   Top1 89.062500   Top5 99.583333   BatchTime 0.059849
INFO - ==> Top1: 88.690    Top5: 99.580    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 88.690   Top5: 99.580] Sparsity : 0.807
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 88.610   Top5: 99.530] Sparsity : 0.764
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 88.470   Top5: 99.490] Sparsity : 0.751
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   9
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [9][   20/  391]   Loss 0.159877   Top1 94.375000   Top5 99.960938   BatchTime 0.191464   LR 0.010000
INFO - Training [9][   40/  391]   Loss 0.160960   Top1 94.355469   Top5 99.941406   BatchTime 0.145783   LR 0.010000
INFO - Training [9][   60/  391]   Loss 0.153648   Top1 94.505208   Top5 99.947917   BatchTime 0.130850   LR 0.010000
INFO - Training [9][   80/  391]   Loss 0.159659   Top1 94.296875   Top5 99.941406   BatchTime 0.122942   LR 0.010000
INFO - Training [9][  100/  391]   Loss 0.161253   Top1 94.265625   Top5 99.945312   BatchTime 0.118631   LR 0.010000
INFO - Training [9][  120/  391]   Loss 0.163851   Top1 94.173177   Top5 99.928385   BatchTime 0.116427   LR 0.010000
INFO - Training [9][  140/  391]   Loss 0.161717   Top1 94.296875   Top5 99.933036   BatchTime 0.114445   LR 0.010000
INFO - Training [9][  160/  391]   Loss 0.160550   Top1 94.311523   Top5 99.926758   BatchTime 0.112790   LR 0.010000
INFO - Training [9][  180/  391]   Loss 0.160039   Top1 94.357639   Top5 99.930556   BatchTime 0.111481   LR 0.010000
INFO - Training [9][  200/  391]   Loss 0.159358   Top1 94.382812   Top5 99.937500   BatchTime 0.110372   LR 0.010000
INFO - Training [9][  220/  391]   Loss 0.158221   Top1 94.438920   Top5 99.943182   BatchTime 0.109489   LR 0.010000
INFO - Training [9][  240/  391]   Loss 0.157274   Top1 94.492188   Top5 99.944661   BatchTime 0.108803   LR 0.010000
INFO - Training [9][  260/  391]   Loss 0.157456   Top1 94.477163   Top5 99.945913   BatchTime 0.108196   LR 0.010000
INFO - Training [9][  280/  391]   Loss 0.157161   Top1 94.475446   Top5 99.941406   BatchTime 0.107659   LR 0.010000
INFO - Training [9][  300/  391]   Loss 0.157430   Top1 94.471354   Top5 99.940104   BatchTime 0.107169   LR 0.010000
INFO - Training [9][  320/  391]   Loss 0.158928   Top1 94.389648   Top5 99.931641   BatchTime 0.106714   LR 0.010000
INFO - Training [9][  340/  391]   Loss 0.159564   Top1 94.375000   Top5 99.933364   BatchTime 0.106217   LR 0.010000
INFO - Training [9][  360/  391]   Loss 0.159373   Top1 94.414062   Top5 99.928385   BatchTime 0.105563   LR 0.010000
INFO - Training [9][  380/  391]   Loss 0.160033   Top1 94.379112   Top5 99.932155   BatchTime 0.103932   LR 0.010000
INFO - ==> Top1: 94.384    Top5: 99.930    Loss: 0.161
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [9][   20/   79]   Loss 0.384872   Top1 88.710938   Top5 99.492188   BatchTime 0.122738
INFO - Validation [9][   40/   79]   Loss 0.386882   Top1 88.613281   Top5 99.550781   BatchTime 0.074603
INFO - Validation [9][   60/   79]   Loss 0.386903   Top1 88.632812   Top5 99.583333   BatchTime 0.059466
INFO - ==> Top1: 88.600    Top5: 99.580    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 88.690   Top5: 99.580] Sparsity : 0.807
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 88.610   Top5: 99.530] Sparsity : 0.764
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 88.600   Top5: 99.580] Sparsity : 0.809
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  10
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [10][   20/  391]   Loss 0.149043   Top1 94.960938   Top5 99.921875   BatchTime 0.190376   LR 0.010000
INFO - Training [10][   40/  391]   Loss 0.144221   Top1 95.097656   Top5 99.941406   BatchTime 0.145581   LR 0.010000
INFO - Training [10][   60/  391]   Loss 0.143945   Top1 94.947917   Top5 99.960938   BatchTime 0.130565   LR 0.010000
INFO - Training [10][   80/  391]   Loss 0.141526   Top1 95.078125   Top5 99.960938   BatchTime 0.122951   LR 0.010000
INFO - Training [10][  100/  391]   Loss 0.142572   Top1 95.015625   Top5 99.968750   BatchTime 0.118410   LR 0.010000
INFO - Training [10][  120/  391]   Loss 0.142282   Top1 95.032552   Top5 99.967448   BatchTime 0.115308   LR 0.010000
INFO - Training [10][  140/  391]   Loss 0.143130   Top1 94.994420   Top5 99.972098   BatchTime 0.113160   LR 0.010000
INFO - Training [10][  160/  391]   Loss 0.144384   Top1 94.936523   Top5 99.970703   BatchTime 0.111504   LR 0.010000
INFO - Training [10][  180/  391]   Loss 0.142526   Top1 95.043403   Top5 99.965278   BatchTime 0.110449   LR 0.010000
INFO - Training [10][  200/  391]   Loss 0.142580   Top1 95.093750   Top5 99.957031   BatchTime 0.109399   LR 0.010000
INFO - Training [10][  220/  391]   Loss 0.143624   Top1 95.056818   Top5 99.957386   BatchTime 0.108569   LR 0.010000
INFO - Training [10][  240/  391]   Loss 0.144428   Top1 94.980469   Top5 99.960938   BatchTime 0.107899   LR 0.010000
INFO - Training [10][  260/  391]   Loss 0.145555   Top1 94.915865   Top5 99.951923   BatchTime 0.107350   LR 0.010000
INFO - Training [10][  280/  391]   Loss 0.146421   Top1 94.891183   Top5 99.952567   BatchTime 0.106830   LR 0.010000
INFO - Training [10][  300/  391]   Loss 0.146933   Top1 94.877604   Top5 99.947917   BatchTime 0.106385   LR 0.010000
INFO - Training [10][  320/  391]   Loss 0.146929   Top1 94.877930   Top5 99.948730   BatchTime 0.105957   LR 0.010000
INFO - Training [10][  340/  391]   Loss 0.147027   Top1 94.866728   Top5 99.951746   BatchTime 0.105467   LR 0.010000
INFO - Training [10][  360/  391]   Loss 0.147836   Top1 94.843750   Top5 99.952257   BatchTime 0.104946   LR 0.010000
INFO - Training [10][  380/  391]   Loss 0.147912   Top1 94.821135   Top5 99.954770   BatchTime 0.103225   LR 0.010000
INFO - ==> Top1: 94.830    Top5: 99.956    Loss: 0.147
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [10][   20/   79]   Loss 0.358742   Top1 89.453125   Top5 99.609375   BatchTime 0.123993
INFO - Validation [10][   40/   79]   Loss 0.366556   Top1 89.394531   Top5 99.511719   BatchTime 0.075163
INFO - Validation [10][   60/   79]   Loss 0.369487   Top1 89.427083   Top5 99.531250   BatchTime 0.058923
INFO - ==> Top1: 89.240    Top5: 99.580    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 88.690   Top5: 99.580] Sparsity : 0.807
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 88.610   Top5: 99.530] Sparsity : 0.764
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  11
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [11][   20/  391]   Loss 0.126214   Top1 95.429688   Top5 99.960938   BatchTime 0.193608   LR 0.010000
INFO - Training [11][   40/  391]   Loss 0.128262   Top1 95.371094   Top5 99.960938   BatchTime 0.148068   LR 0.010000
INFO - Training [11][   60/  391]   Loss 0.128845   Top1 95.377604   Top5 99.973958   BatchTime 0.132242   LR 0.010000
INFO - Training [11][   80/  391]   Loss 0.129715   Top1 95.390625   Top5 99.960938   BatchTime 0.124168   LR 0.010000
INFO - Training [11][  100/  391]   Loss 0.129314   Top1 95.312500   Top5 99.968750   BatchTime 0.119399   LR 0.010000
INFO - Training [11][  120/  391]   Loss 0.130038   Top1 95.234375   Top5 99.967448   BatchTime 0.116243   LR 0.010000
INFO - Training [11][  140/  391]   Loss 0.131579   Top1 95.212054   Top5 99.972098   BatchTime 0.113914   LR 0.010000
INFO - Training [11][  160/  391]   Loss 0.133932   Top1 95.161133   Top5 99.970703   BatchTime 0.112979   LR 0.010000
INFO - Training [11][  180/  391]   Loss 0.133707   Top1 95.217014   Top5 99.969618   BatchTime 0.111562   LR 0.010000
INFO - Training [11][  200/  391]   Loss 0.135373   Top1 95.167969   Top5 99.968750   BatchTime 0.110537   LR 0.010000
INFO - Training [11][  220/  391]   Loss 0.134360   Top1 95.205966   Top5 99.968040   BatchTime 0.109596   LR 0.010000
INFO - Training [11][  240/  391]   Loss 0.134056   Top1 95.250651   Top5 99.964193   BatchTime 0.108920   LR 0.010000
INFO - Training [11][  260/  391]   Loss 0.133688   Top1 95.288462   Top5 99.963942   BatchTime 0.108188   LR 0.010000
INFO - Training [11][  280/  391]   Loss 0.133861   Top1 95.256696   Top5 99.963728   BatchTime 0.107678   LR 0.010000
INFO - Training [11][  300/  391]   Loss 0.134562   Top1 95.218750   Top5 99.966146   BatchTime 0.107170   LR 0.010000
INFO - Training [11][  320/  391]   Loss 0.134118   Top1 95.227051   Top5 99.963379   BatchTime 0.106634   LR 0.010000
INFO - Training [11][  340/  391]   Loss 0.134955   Top1 95.218290   Top5 99.965533   BatchTime 0.106100   LR 0.010000
INFO - Training [11][  360/  391]   Loss 0.135771   Top1 95.203993   Top5 99.967448   BatchTime 0.105658   LR 0.010000
INFO - Training [11][  380/  391]   Loss 0.137038   Top1 95.152138   Top5 99.965049   BatchTime 0.103827   LR 0.010000
INFO - ==> Top1: 95.138    Top5: 99.964    Loss: 0.137
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [11][   20/   79]   Loss 0.380352   Top1 89.687500   Top5 99.492188   BatchTime 0.119738
INFO - Validation [11][   40/   79]   Loss 0.369744   Top1 89.648438   Top5 99.570312   BatchTime 0.073012
INFO - Validation [11][   60/   79]   Loss 0.369597   Top1 89.674479   Top5 99.518229   BatchTime 0.057429
INFO - ==> Top1: 89.360    Top5: 99.540    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 88.690   Top5: 99.580] Sparsity : 0.807
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  12
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [12][   20/  391]   Loss 0.131530   Top1 95.234375   Top5 99.960938   BatchTime 0.192799   LR 0.010000
INFO - Training [12][   40/  391]   Loss 0.126109   Top1 95.605469   Top5 99.980469   BatchTime 0.146499   LR 0.010000
INFO - Training [12][   60/  391]   Loss 0.122221   Top1 95.729167   Top5 99.960938   BatchTime 0.131126   LR 0.010000
INFO - Training [12][   80/  391]   Loss 0.121228   Top1 95.742188   Top5 99.960938   BatchTime 0.123522   LR 0.010000
INFO - Training [12][  100/  391]   Loss 0.123760   Top1 95.601562   Top5 99.960938   BatchTime 0.118994   LR 0.010000
INFO - Training [12][  120/  391]   Loss 0.124721   Top1 95.644531   Top5 99.954427   BatchTime 0.115961   LR 0.010000
INFO - Training [12][  140/  391]   Loss 0.124804   Top1 95.664062   Top5 99.960938   BatchTime 0.113799   LR 0.010000
INFO - Training [12][  160/  391]   Loss 0.125499   Top1 95.634766   Top5 99.965820   BatchTime 0.112132   LR 0.010000
INFO - Training [12][  180/  391]   Loss 0.125740   Top1 95.638021   Top5 99.960938   BatchTime 0.110799   LR 0.010000
INFO - Training [12][  200/  391]   Loss 0.125381   Top1 95.601562   Top5 99.960938   BatchTime 0.109810   LR 0.010000
INFO - Training [12][  220/  391]   Loss 0.125193   Top1 95.607244   Top5 99.964489   BatchTime 0.108921   LR 0.010000
INFO - Training [12][  240/  391]   Loss 0.124713   Top1 95.572917   Top5 99.967448   BatchTime 0.108193   LR 0.010000
INFO - Training [12][  260/  391]   Loss 0.124347   Top1 95.600962   Top5 99.969952   BatchTime 0.107538   LR 0.010000
INFO - Training [12][  280/  391]   Loss 0.125340   Top1 95.580357   Top5 99.963728   BatchTime 0.107033   LR 0.010000
INFO - Training [12][  300/  391]   Loss 0.125749   Top1 95.546875   Top5 99.963542   BatchTime 0.106513   LR 0.010000
INFO - Training [12][  320/  391]   Loss 0.125543   Top1 95.573730   Top5 99.965820   BatchTime 0.106024   LR 0.010000
INFO - Training [12][  340/  391]   Loss 0.125432   Top1 95.562960   Top5 99.965533   BatchTime 0.105535   LR 0.010000
INFO - Training [12][  360/  391]   Loss 0.124432   Top1 95.598958   Top5 99.967448   BatchTime 0.105220   LR 0.010000
INFO - Training [12][  380/  391]   Loss 0.124678   Top1 95.577714   Top5 99.967105   BatchTime 0.103420   LR 0.010000
INFO - ==> Top1: 95.572    Top5: 99.966    Loss: 0.125
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [12][   20/   79]   Loss 0.367166   Top1 89.960938   Top5 99.687500   BatchTime 0.124980
INFO - Validation [12][   40/   79]   Loss 0.378632   Top1 89.648438   Top5 99.648438   BatchTime 0.075900
INFO - Validation [12][   60/   79]   Loss 0.382682   Top1 89.375000   Top5 99.622396   BatchTime 0.059268
INFO - ==> Top1: 89.240    Top5: 99.630    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  13
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [13][   20/  391]   Loss 0.111434   Top1 96.406250   Top5 99.960938   BatchTime 0.191547   LR 0.010000
INFO - Training [13][   40/  391]   Loss 0.108539   Top1 96.445312   Top5 99.960938   BatchTime 0.146199   LR 0.010000
INFO - Training [13][   60/  391]   Loss 0.108472   Top1 96.419271   Top5 99.973958   BatchTime 0.131561   LR 0.010000
INFO - Training [13][   80/  391]   Loss 0.117167   Top1 95.986328   Top5 99.960938   BatchTime 0.123747   LR 0.010000
INFO - Training [13][  100/  391]   Loss 0.118547   Top1 95.843750   Top5 99.960938   BatchTime 0.118848   LR 0.010000
INFO - Training [13][  120/  391]   Loss 0.119482   Top1 95.807292   Top5 99.954427   BatchTime 0.115768   LR 0.010000
INFO - Training [13][  140/  391]   Loss 0.118260   Top1 95.820312   Top5 99.960938   BatchTime 0.113448   LR 0.010000
INFO - Training [13][  160/  391]   Loss 0.119238   Top1 95.859375   Top5 99.960938   BatchTime 0.111689   LR 0.010000
INFO - Training [13][  180/  391]   Loss 0.119018   Top1 95.820312   Top5 99.960938   BatchTime 0.110462   LR 0.010000
INFO - Training [13][  200/  391]   Loss 0.118788   Top1 95.835938   Top5 99.960938   BatchTime 0.109461   LR 0.010000
INFO - Training [13][  220/  391]   Loss 0.119125   Top1 95.820312   Top5 99.960938   BatchTime 0.108534   LR 0.010000
INFO - Training [13][  240/  391]   Loss 0.120200   Top1 95.774740   Top5 99.960938   BatchTime 0.108351   LR 0.010000
INFO - Training [13][  260/  391]   Loss 0.120372   Top1 95.763221   Top5 99.963942   BatchTime 0.107769   LR 0.010000
INFO - Training [13][  280/  391]   Loss 0.120216   Top1 95.770089   Top5 99.966518   BatchTime 0.107244   LR 0.010000
INFO - Training [13][  300/  391]   Loss 0.120801   Top1 95.736979   Top5 99.968750   BatchTime 0.106730   LR 0.010000
INFO - Training [13][  320/  391]   Loss 0.121480   Top1 95.700684   Top5 99.970703   BatchTime 0.106287   LR 0.010000
INFO - Training [13][  340/  391]   Loss 0.121764   Top1 95.712316   Top5 99.970129   BatchTime 0.105782   LR 0.010000
INFO - Training [13][  360/  391]   Loss 0.122691   Top1 95.690104   Top5 99.971788   BatchTime 0.105299   LR 0.010000
INFO - Training [13][  380/  391]   Loss 0.123702   Top1 95.647615   Top5 99.971217   BatchTime 0.104037   LR 0.010000
INFO - ==> Top1: 95.624    Top5: 99.970    Loss: 0.124
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [13][   20/   79]   Loss 0.396842   Top1 88.710938   Top5 99.570312   BatchTime 0.123500
INFO - Validation [13][   40/   79]   Loss 0.396989   Top1 88.789062   Top5 99.433594   BatchTime 0.074819
INFO - Validation [13][   60/   79]   Loss 0.390936   Top1 89.179688   Top5 99.492188   BatchTime 0.058582
INFO - ==> Top1: 89.110    Top5: 99.550    Loss: 0.392
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  14
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [14][   20/  391]   Loss 0.110964   Top1 96.289062   Top5 100.000000   BatchTime 0.191116   LR 0.010000
INFO - Training [14][   40/  391]   Loss 0.111046   Top1 96.230469   Top5 99.960938   BatchTime 0.145887   LR 0.010000
INFO - Training [14][   60/  391]   Loss 0.116583   Top1 95.937500   Top5 99.947917   BatchTime 0.130427   LR 0.010000
INFO - Training [14][   80/  391]   Loss 0.121593   Top1 95.800781   Top5 99.941406   BatchTime 0.123257   LR 0.010000
INFO - Training [14][  100/  391]   Loss 0.125558   Top1 95.695312   Top5 99.953125   BatchTime 0.118535   LR 0.010000
INFO - Training [14][  120/  391]   Loss 0.128198   Top1 95.592448   Top5 99.934896   BatchTime 0.115365   LR 0.010000
INFO - Training [14][  140/  391]   Loss 0.130434   Top1 95.479911   Top5 99.938616   BatchTime 0.113383   LR 0.010000
INFO - Training [14][  160/  391]   Loss 0.131257   Top1 95.444336   Top5 99.946289   BatchTime 0.111709   LR 0.010000
INFO - Training [14][  180/  391]   Loss 0.133565   Top1 95.342882   Top5 99.952257   BatchTime 0.110415   LR 0.010000
INFO - Training [14][  200/  391]   Loss 0.134550   Top1 95.289062   Top5 99.957031   BatchTime 0.109454   LR 0.010000
INFO - Training [14][  220/  391]   Loss 0.136597   Top1 95.262784   Top5 99.953835   BatchTime 0.108648   LR 0.010000
INFO - Training [14][  240/  391]   Loss 0.137860   Top1 95.221354   Top5 99.947917   BatchTime 0.107945   LR 0.010000
INFO - Training [14][  260/  391]   Loss 0.139033   Top1 95.186298   Top5 99.942909   BatchTime 0.107299   LR 0.010000
INFO - Training [14][  280/  391]   Loss 0.139762   Top1 95.150670   Top5 99.935826   BatchTime 0.106828   LR 0.010000
INFO - Training [14][  300/  391]   Loss 0.140016   Top1 95.135417   Top5 99.940104   BatchTime 0.106408   LR 0.010000
INFO - Training [14][  320/  391]   Loss 0.141824   Top1 95.085449   Top5 99.943848   BatchTime 0.105897   LR 0.010000
INFO - Training [14][  340/  391]   Loss 0.144714   Top1 94.993107   Top5 99.937960   BatchTime 0.105433   LR 0.010000
INFO - Training [14][  360/  391]   Loss 0.147018   Top1 94.900174   Top5 99.937066   BatchTime 0.105003   LR 0.010000
INFO - Training [14][  380/  391]   Loss 0.147823   Top1 94.849918   Top5 99.936266   BatchTime 0.103539   LR 0.010000
INFO - ==> Top1: 94.810    Top5: 99.938    Loss: 0.149
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [14][   20/   79]   Loss 0.387841   Top1 88.476562   Top5 99.492188   BatchTime 0.124705
INFO - Validation [14][   40/   79]   Loss 0.402598   Top1 88.164062   Top5 99.375000   BatchTime 0.075390
INFO - Validation [14][   60/   79]   Loss 0.394640   Top1 88.606771   Top5 99.453125   BatchTime 0.058945
INFO - ==> Top1: 88.330    Top5: 99.500    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  15
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [15][   20/  391]   Loss 0.158062   Top1 93.984375   Top5 99.921875   BatchTime 0.192487   LR 0.010000
INFO - Training [15][   40/  391]   Loss 0.165079   Top1 93.554688   Top5 99.960938   BatchTime 0.146532   LR 0.010000
INFO - Training [15][   60/  391]   Loss 0.161124   Top1 93.932292   Top5 99.960938   BatchTime 0.130837   LR 0.010000
INFO - Training [15][   80/  391]   Loss 0.163861   Top1 93.896484   Top5 99.951172   BatchTime 0.123043   LR 0.010000
INFO - Training [15][  100/  391]   Loss 0.165373   Top1 93.953125   Top5 99.953125   BatchTime 0.118682   LR 0.010000
INFO - Training [15][  120/  391]   Loss 0.163065   Top1 94.082031   Top5 99.947917   BatchTime 0.115628   LR 0.010000
INFO - Training [15][  140/  391]   Loss 0.166616   Top1 93.973214   Top5 99.955357   BatchTime 0.113464   LR 0.010000
INFO - Training [15][  160/  391]   Loss 0.167294   Top1 93.969727   Top5 99.951172   BatchTime 0.111913   LR 0.010000
INFO - Training [15][  180/  391]   Loss 0.169315   Top1 93.849826   Top5 99.943576   BatchTime 0.110630   LR 0.010000
INFO - Training [15][  200/  391]   Loss 0.168980   Top1 93.906250   Top5 99.937500   BatchTime 0.109592   LR 0.010000
INFO - Training [15][  220/  391]   Loss 0.171475   Top1 93.913352   Top5 99.939631   BatchTime 0.108759   LR 0.010000
INFO - Training [15][  240/  391]   Loss 0.171799   Top1 93.916016   Top5 99.931641   BatchTime 0.108045   LR 0.010000
INFO - Training [15][  260/  391]   Loss 0.170260   Top1 93.981370   Top5 99.933894   BatchTime 0.107401   LR 0.010000
INFO - Training [15][  280/  391]   Loss 0.171074   Top1 93.936942   Top5 99.933036   BatchTime 0.106892   LR 0.010000
INFO - Training [15][  300/  391]   Loss 0.170339   Top1 93.979167   Top5 99.932292   BatchTime 0.106273   LR 0.010000
INFO - Training [15][  320/  391]   Loss 0.170640   Top1 93.986816   Top5 99.931641   BatchTime 0.106220   LR 0.010000
INFO - Training [15][  340/  391]   Loss 0.170731   Top1 93.988971   Top5 99.935662   BatchTime 0.105756   LR 0.010000
INFO - Training [15][  360/  391]   Loss 0.171383   Top1 93.958333   Top5 99.937066   BatchTime 0.105324   LR 0.010000
INFO - Training [15][  380/  391]   Loss 0.171725   Top1 93.972039   Top5 99.938322   BatchTime 0.104116   LR 0.010000
INFO - ==> Top1: 93.966    Top5: 99.936    Loss: 0.172
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [15][   20/   79]   Loss 0.394852   Top1 88.750000   Top5 99.414062   BatchTime 0.125334
INFO - Validation [15][   40/   79]   Loss 0.396972   Top1 88.691406   Top5 99.414062   BatchTime 0.076124
INFO - Validation [15][   60/   79]   Loss 0.388625   Top1 88.645833   Top5 99.479167   BatchTime 0.059569
INFO - ==> Top1: 88.550    Top5: 99.510    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  16
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [16][   20/  391]   Loss 0.151543   Top1 94.570312   Top5 100.000000   BatchTime 0.193197   LR 0.010000
INFO - Training [16][   40/  391]   Loss 0.146911   Top1 94.707031   Top5 99.960938   BatchTime 0.147129   LR 0.010000
INFO - Training [16][   60/  391]   Loss 0.153606   Top1 94.518229   Top5 99.921875   BatchTime 0.131334   LR 0.010000
INFO - Training [16][   80/  391]   Loss 0.157253   Top1 94.570312   Top5 99.921875   BatchTime 0.123313   LR 0.010000
INFO - Training [16][  100/  391]   Loss 0.158942   Top1 94.445312   Top5 99.898438   BatchTime 0.118608   LR 0.010000
INFO - Training [16][  120/  391]   Loss 0.158902   Top1 94.472656   Top5 99.889323   BatchTime 0.115595   LR 0.010000
INFO - Training [16][  140/  391]   Loss 0.160114   Top1 94.414062   Top5 99.899554   BatchTime 0.113611   LR 0.010000
INFO - Training [16][  160/  391]   Loss 0.158931   Top1 94.423828   Top5 99.912109   BatchTime 0.111917   LR 0.010000
INFO - Training [16][  180/  391]   Loss 0.160355   Top1 94.292535   Top5 99.917535   BatchTime 0.110674   LR 0.010000
INFO - Training [16][  200/  391]   Loss 0.159814   Top1 94.304688   Top5 99.921875   BatchTime 0.109684   LR 0.010000
INFO - Training [16][  220/  391]   Loss 0.159933   Top1 94.303977   Top5 99.921875   BatchTime 0.108829   LR 0.010000
INFO - Training [16][  240/  391]   Loss 0.159797   Top1 94.335938   Top5 99.921875   BatchTime 0.108093   LR 0.010000
INFO - Training [16][  260/  391]   Loss 0.159822   Top1 94.378005   Top5 99.927885   BatchTime 0.107448   LR 0.010000
INFO - Training [16][  280/  391]   Loss 0.161561   Top1 94.313616   Top5 99.924665   BatchTime 0.106951   LR 0.010000
INFO - Training [16][  300/  391]   Loss 0.161951   Top1 94.283854   Top5 99.929688   BatchTime 0.106474   LR 0.010000
INFO - Training [16][  320/  391]   Loss 0.161760   Top1 94.299316   Top5 99.924316   BatchTime 0.106034   LR 0.010000
INFO - Training [16][  340/  391]   Loss 0.162285   Top1 94.287684   Top5 99.926471   BatchTime 0.105549   LR 0.010000
INFO - Training [16][  360/  391]   Loss 0.161805   Top1 94.320747   Top5 99.928385   BatchTime 0.105138   LR 0.010000
INFO - Training [16][  380/  391]   Loss 0.160709   Top1 94.368832   Top5 99.932155   BatchTime 0.104392   LR 0.010000
INFO - ==> Top1: 94.332    Top5: 99.930    Loss: 0.161
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [16][   20/   79]   Loss 0.399983   Top1 88.671875   Top5 99.335938   BatchTime 0.126252
INFO - Validation [16][   40/   79]   Loss 0.410728   Top1 88.164062   Top5 99.375000   BatchTime 0.080147
INFO - Validation [16][   60/   79]   Loss 0.403071   Top1 88.242188   Top5 99.466146   BatchTime 0.062193
INFO - ==> Top1: 88.470    Top5: 99.550    Loss: 0.394
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  17
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [17][   20/  391]   Loss 0.122526   Top1 95.312500   Top5 100.000000   BatchTime 0.194919   LR 0.010000
INFO - Training [17][   40/  391]   Loss 0.141464   Top1 94.921875   Top5 99.960938   BatchTime 0.147454   LR 0.010000
INFO - Training [17][   60/  391]   Loss 0.142265   Top1 94.882812   Top5 99.947917   BatchTime 0.131861   LR 0.010000
INFO - Training [17][   80/  391]   Loss 0.146302   Top1 94.697266   Top5 99.960938   BatchTime 0.123884   LR 0.010000
INFO - Training [17][  100/  391]   Loss 0.145281   Top1 94.757812   Top5 99.968750   BatchTime 0.119072   LR 0.010000
INFO - Training [17][  120/  391]   Loss 0.150894   Top1 94.459635   Top5 99.960938   BatchTime 0.115945   LR 0.010000
INFO - Training [17][  140/  391]   Loss 0.151171   Top1 94.464286   Top5 99.949777   BatchTime 0.113758   LR 0.010000
INFO - Training [17][  160/  391]   Loss 0.151781   Top1 94.511719   Top5 99.956055   BatchTime 0.112141   LR 0.010000
INFO - Training [17][  180/  391]   Loss 0.151077   Top1 94.505208   Top5 99.952257   BatchTime 0.110864   LR 0.010000
INFO - Training [17][  200/  391]   Loss 0.150478   Top1 94.570312   Top5 99.953125   BatchTime 0.109913   LR 0.010000
INFO - Training [17][  220/  391]   Loss 0.149913   Top1 94.602273   Top5 99.953835   BatchTime 0.109081   LR 0.010000
INFO - Training [17][  240/  391]   Loss 0.151056   Top1 94.547526   Top5 99.954427   BatchTime 0.108386   LR 0.010000
INFO - Training [17][  260/  391]   Loss 0.151697   Top1 94.549279   Top5 99.954928   BatchTime 0.107788   LR 0.010000
INFO - Training [17][  280/  391]   Loss 0.152130   Top1 94.547991   Top5 99.944196   BatchTime 0.107312   LR 0.010000
INFO - Training [17][  300/  391]   Loss 0.152796   Top1 94.515625   Top5 99.942708   BatchTime 0.106846   LR 0.010000
INFO - Training [17][  320/  391]   Loss 0.153498   Top1 94.494629   Top5 99.941406   BatchTime 0.106446   LR 0.010000
INFO - Training [17][  340/  391]   Loss 0.153934   Top1 94.473805   Top5 99.940257   BatchTime 0.106007   LR 0.010000
INFO - Training [17][  360/  391]   Loss 0.153989   Top1 94.474826   Top5 99.937066   BatchTime 0.105721   LR 0.010000
INFO - Training [17][  380/  391]   Loss 0.154430   Top1 94.467516   Top5 99.938322   BatchTime 0.105346   LR 0.010000
INFO - ==> Top1: 94.474    Top5: 99.940    Loss: 0.155
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [17][   20/   79]   Loss 0.396419   Top1 88.437500   Top5 99.453125   BatchTime 0.126010
INFO - Validation [17][   40/   79]   Loss 0.403003   Top1 88.105469   Top5 99.414062   BatchTime 0.080360
INFO - Validation [17][   60/   79]   Loss 0.391342   Top1 88.463542   Top5 99.479167   BatchTime 0.064297
INFO - ==> Top1: 88.590    Top5: 99.490    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  18
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [18][   20/  391]   Loss 0.133129   Top1 95.390625   Top5 100.000000   BatchTime 0.195073   LR 0.010000
INFO - Training [18][   40/  391]   Loss 0.143657   Top1 94.941406   Top5 100.000000   BatchTime 0.148485   LR 0.010000
INFO - Training [18][   60/  391]   Loss 0.137836   Top1 95.234375   Top5 100.000000   BatchTime 0.133140   LR 0.010000
INFO - Training [18][   80/  391]   Loss 0.138870   Top1 95.107422   Top5 99.990234   BatchTime 0.125103   LR 0.010000
INFO - Training [18][  100/  391]   Loss 0.137056   Top1 95.093750   Top5 99.976562   BatchTime 0.120198   LR 0.010000
INFO - Training [18][  120/  391]   Loss 0.138419   Top1 95.091146   Top5 99.980469   BatchTime 0.116881   LR 0.010000
INFO - Training [18][  140/  391]   Loss 0.137717   Top1 95.178571   Top5 99.983259   BatchTime 0.114549   LR 0.010000
INFO - Training [18][  160/  391]   Loss 0.136830   Top1 95.229492   Top5 99.975586   BatchTime 0.112807   LR 0.010000
INFO - Training [18][  180/  391]   Loss 0.138495   Top1 95.182292   Top5 99.973958   BatchTime 0.111419   LR 0.010000
INFO - Training [18][  200/  391]   Loss 0.139146   Top1 95.171875   Top5 99.972656   BatchTime 0.110333   LR 0.010000
INFO - Training [18][  220/  391]   Loss 0.140437   Top1 95.120739   Top5 99.975142   BatchTime 0.109450   LR 0.010000
INFO - Training [18][  240/  391]   Loss 0.142539   Top1 95.052083   Top5 99.970703   BatchTime 0.108721   LR 0.010000
INFO - Training [18][  260/  391]   Loss 0.143519   Top1 95.033053   Top5 99.972957   BatchTime 0.108014   LR 0.010000
INFO - Training [18][  280/  391]   Loss 0.143406   Top1 95.044643   Top5 99.974888   BatchTime 0.107435   LR 0.010000
INFO - Training [18][  300/  391]   Loss 0.142605   Top1 95.072917   Top5 99.976562   BatchTime 0.106954   LR 0.010000
INFO - Training [18][  320/  391]   Loss 0.142525   Top1 95.056152   Top5 99.973145   BatchTime 0.106504   LR 0.010000
INFO - Training [18][  340/  391]   Loss 0.143848   Top1 95.000000   Top5 99.972426   BatchTime 0.105997   LR 0.010000
INFO - Training [18][  360/  391]   Loss 0.144656   Top1 94.939236   Top5 99.969618   BatchTime 0.105523   LR 0.010000
INFO - Training [18][  380/  391]   Loss 0.144733   Top1 94.913651   Top5 99.969161   BatchTime 0.105096   LR 0.010000
INFO - ==> Top1: 94.886    Top5: 99.968    Loss: 0.145
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [18][   20/   79]   Loss 0.395291   Top1 88.867188   Top5 99.375000   BatchTime 0.128259
INFO - Validation [18][   40/   79]   Loss 0.393361   Top1 88.828125   Top5 99.453125   BatchTime 0.081568
INFO - Validation [18][   60/   79]   Loss 0.378223   Top1 89.179688   Top5 99.518229   BatchTime 0.066047
INFO - ==> Top1: 88.980    Top5: 99.560    Loss: 0.378
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  19
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [19][   20/  391]   Loss 0.150730   Top1 94.687500   Top5 99.921875   BatchTime 0.194216   LR 0.010000
INFO - Training [19][   40/  391]   Loss 0.134561   Top1 95.390625   Top5 99.921875   BatchTime 0.147573   LR 0.010000
INFO - Training [19][   60/  391]   Loss 0.136962   Top1 95.325521   Top5 99.947917   BatchTime 0.132433   LR 0.010000
INFO - Training [19][   80/  391]   Loss 0.134956   Top1 95.292969   Top5 99.960938   BatchTime 0.124589   LR 0.010000
INFO - Training [19][  100/  391]   Loss 0.136143   Top1 95.195312   Top5 99.968750   BatchTime 0.119725   LR 0.010000
INFO - Training [19][  120/  391]   Loss 0.135903   Top1 95.253906   Top5 99.967448   BatchTime 0.116592   LR 0.010000
INFO - Training [19][  140/  391]   Loss 0.137451   Top1 95.167411   Top5 99.966518   BatchTime 0.114422   LR 0.010000
INFO - Training [19][  160/  391]   Loss 0.138965   Top1 95.102539   Top5 99.965820   BatchTime 0.112736   LR 0.010000
INFO - Training [19][  180/  391]   Loss 0.138713   Top1 95.091146   Top5 99.960938   BatchTime 0.111396   LR 0.010000
INFO - Training [19][  200/  391]   Loss 0.140261   Top1 95.062500   Top5 99.960938   BatchTime 0.110375   LR 0.010000
INFO - Training [19][  220/  391]   Loss 0.141753   Top1 95.035511   Top5 99.960938   BatchTime 0.109411   LR 0.010000
INFO - Training [19][  240/  391]   Loss 0.144577   Top1 94.934896   Top5 99.954427   BatchTime 0.108701   LR 0.010000
INFO - Training [19][  260/  391]   Loss 0.146485   Top1 94.858774   Top5 99.954928   BatchTime 0.108021   LR 0.010000
INFO - Training [19][  280/  391]   Loss 0.146357   Top1 94.829799   Top5 99.958147   BatchTime 0.107495   LR 0.010000
INFO - Training [19][  300/  391]   Loss 0.147533   Top1 94.783854   Top5 99.958333   BatchTime 0.107058   LR 0.010000
INFO - Training [19][  320/  391]   Loss 0.148379   Top1 94.738770   Top5 99.958496   BatchTime 0.106694   LR 0.010000
INFO - Training [19][  340/  391]   Loss 0.150443   Top1 94.687500   Top5 99.954044   BatchTime 0.106279   LR 0.010000
INFO - Training [19][  360/  391]   Loss 0.151239   Top1 94.633247   Top5 99.947917   BatchTime 0.105808   LR 0.010000
INFO - Training [19][  380/  391]   Loss 0.151640   Top1 94.619655   Top5 99.948602   BatchTime 0.105387   LR 0.010000
INFO - ==> Top1: 94.580    Top5: 99.944    Loss: 0.153
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [19][   20/   79]   Loss 0.417603   Top1 87.851562   Top5 99.375000   BatchTime 0.129529
INFO - Validation [19][   40/   79]   Loss 0.416720   Top1 87.949219   Top5 99.355469   BatchTime 0.081857
INFO - Validation [19][   60/   79]   Loss 0.407086   Top1 88.385417   Top5 99.375000   BatchTime 0.066112
INFO - ==> Top1: 88.370    Top5: 99.490    Loss: 0.400
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.240   Top5: 99.580] Sparsity : 0.811
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  20
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [20][   20/  391]   Loss 0.164867   Top1 94.335938   Top5 99.921875   BatchTime 0.202920   LR 0.001000
INFO - Training [20][   40/  391]   Loss 0.153200   Top1 94.824219   Top5 99.921875   BatchTime 0.151468   LR 0.001000
INFO - Training [20][   60/  391]   Loss 0.145090   Top1 95.039062   Top5 99.921875   BatchTime 0.134510   LR 0.001000
INFO - Training [20][   80/  391]   Loss 0.147949   Top1 94.892578   Top5 99.912109   BatchTime 0.125885   LR 0.001000
INFO - Training [20][  100/  391]   Loss 0.150567   Top1 94.773438   Top5 99.914062   BatchTime 0.120880   LR 0.001000
INFO - Training [20][  120/  391]   Loss 0.147205   Top1 94.908854   Top5 99.908854   BatchTime 0.117454   LR 0.001000
INFO - Training [20][  140/  391]   Loss 0.145368   Top1 95.000000   Top5 99.921875   BatchTime 0.115181   LR 0.001000
INFO - Training [20][  160/  391]   Loss 0.145871   Top1 94.956055   Top5 99.921875   BatchTime 0.113385   LR 0.001000
INFO - Training [20][  180/  391]   Loss 0.144642   Top1 95.013021   Top5 99.926215   BatchTime 0.111974   LR 0.001000
INFO - Training [20][  200/  391]   Loss 0.144122   Top1 95.019531   Top5 99.929688   BatchTime 0.110817   LR 0.001000
INFO - Training [20][  220/  391]   Loss 0.144901   Top1 95.000000   Top5 99.932528   BatchTime 0.109832   LR 0.001000
INFO - Training [20][  240/  391]   Loss 0.143798   Top1 95.091146   Top5 99.934896   BatchTime 0.109129   LR 0.001000
INFO - Training [20][  260/  391]   Loss 0.143510   Top1 95.090144   Top5 99.933894   BatchTime 0.108506   LR 0.001000
INFO - Training [20][  280/  391]   Loss 0.142749   Top1 95.094866   Top5 99.938616   BatchTime 0.107939   LR 0.001000
INFO - Training [20][  300/  391]   Loss 0.142313   Top1 95.083333   Top5 99.942708   BatchTime 0.107469   LR 0.001000
INFO - Training [20][  320/  391]   Loss 0.142513   Top1 95.087891   Top5 99.941406   BatchTime 0.107010   LR 0.001000
INFO - Training [20][  340/  391]   Loss 0.142448   Top1 95.080423   Top5 99.944853   BatchTime 0.106489   LR 0.001000
INFO - Training [20][  360/  391]   Loss 0.141829   Top1 95.101997   Top5 99.943576   BatchTime 0.106002   LR 0.001000
INFO - Training [20][  380/  391]   Loss 0.140537   Top1 95.143914   Top5 99.946546   BatchTime 0.105540   LR 0.001000
INFO - ==> Top1: 95.138    Top5: 99.948    Loss: 0.141
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [20][   20/   79]   Loss 0.377790   Top1 88.554688   Top5 99.531250   BatchTime 0.129391
INFO - Validation [20][   40/   79]   Loss 0.374131   Top1 88.984375   Top5 99.531250   BatchTime 0.082028
INFO - Validation [20][   60/   79]   Loss 0.366042   Top1 89.361979   Top5 99.557292   BatchTime 0.066212
INFO - ==> Top1: 89.410    Top5: 99.600    Loss: 0.358
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.410   Top5: 99.600] Sparsity : 0.848
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.240   Top5: 99.630] Sparsity : 0.815
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  21
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [21][   20/  391]   Loss 0.124501   Top1 95.585938   Top5 99.960938   BatchTime 0.201143   LR 0.001000
INFO - Training [21][   40/  391]   Loss 0.125631   Top1 95.527344   Top5 99.960938   BatchTime 0.150619   LR 0.001000
INFO - Training [21][   60/  391]   Loss 0.128587   Top1 95.611979   Top5 99.960938   BatchTime 0.134161   LR 0.001000
INFO - Training [21][   80/  391]   Loss 0.131776   Top1 95.458984   Top5 99.960938   BatchTime 0.125846   LR 0.001000
INFO - Training [21][  100/  391]   Loss 0.128382   Top1 95.593750   Top5 99.953125   BatchTime 0.120810   LR 0.001000
INFO - Training [21][  120/  391]   Loss 0.130153   Top1 95.520833   Top5 99.947917   BatchTime 0.117506   LR 0.001000
INFO - Training [21][  140/  391]   Loss 0.127709   Top1 95.608259   Top5 99.949777   BatchTime 0.115046   LR 0.001000
INFO - Training [21][  160/  391]   Loss 0.128018   Top1 95.585938   Top5 99.956055   BatchTime 0.113186   LR 0.001000
INFO - Training [21][  180/  391]   Loss 0.126448   Top1 95.611979   Top5 99.960938   BatchTime 0.111742   LR 0.001000
INFO - Training [21][  200/  391]   Loss 0.125425   Top1 95.625000   Top5 99.960938   BatchTime 0.110592   LR 0.001000
INFO - Training [21][  220/  391]   Loss 0.125372   Top1 95.621449   Top5 99.960938   BatchTime 0.109612   LR 0.001000
INFO - Training [21][  240/  391]   Loss 0.124104   Top1 95.686849   Top5 99.964193   BatchTime 0.108808   LR 0.001000
INFO - Training [21][  260/  391]   Loss 0.124392   Top1 95.667067   Top5 99.966947   BatchTime 0.108174   LR 0.001000
INFO - Training [21][  280/  391]   Loss 0.125695   Top1 95.625000   Top5 99.960938   BatchTime 0.107555   LR 0.001000
INFO - Training [21][  300/  391]   Loss 0.125651   Top1 95.596354   Top5 99.955729   BatchTime 0.107130   LR 0.001000
INFO - Training [21][  320/  391]   Loss 0.126149   Top1 95.588379   Top5 99.956055   BatchTime 0.106766   LR 0.001000
INFO - Training [21][  340/  391]   Loss 0.126463   Top1 95.579044   Top5 99.956342   BatchTime 0.106325   LR 0.001000
INFO - Training [21][  360/  391]   Loss 0.126890   Top1 95.536024   Top5 99.956597   BatchTime 0.105858   LR 0.001000
INFO - Training [21][  380/  391]   Loss 0.127933   Top1 95.493421   Top5 99.952714   BatchTime 0.105433   LR 0.001000
INFO - ==> Top1: 95.494    Top5: 99.950    Loss: 0.128
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [21][   20/   79]   Loss 0.372772   Top1 89.140625   Top5 99.453125   BatchTime 0.132005
INFO - Validation [21][   40/   79]   Loss 0.376170   Top1 89.140625   Top5 99.492188   BatchTime 0.083381
INFO - Validation [21][   60/   79]   Loss 0.365878   Top1 89.609375   Top5 99.544271   BatchTime 0.067260
INFO - ==> Top1: 89.540    Top5: 99.590    Loss: 0.359
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 89.540   Top5: 99.590] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 89.410   Top5: 99.600] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 89.360   Top5: 99.540] Sparsity : 0.813
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  22
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [22][   20/  391]   Loss 0.113050   Top1 96.132812   Top5 100.000000   BatchTime 0.201122   LR 0.001000
INFO - Training [22][   40/  391]   Loss 0.118515   Top1 95.917969   Top5 100.000000   BatchTime 0.150558   LR 0.001000
INFO - Training [22][   60/  391]   Loss 0.121543   Top1 95.768229   Top5 99.973958   BatchTime 0.133623   LR 0.001000
INFO - Training [22][   80/  391]   Loss 0.125838   Top1 95.527344   Top5 99.960938   BatchTime 0.124979   LR 0.001000
INFO - Training [22][  100/  391]   Loss 0.125639   Top1 95.585938   Top5 99.945312   BatchTime 0.121164   LR 0.001000
INFO - Training [22][  120/  391]   Loss 0.125736   Top1 95.579427   Top5 99.954427   BatchTime 0.117825   LR 0.001000
INFO - Training [22][  140/  391]   Loss 0.126744   Top1 95.563616   Top5 99.955357   BatchTime 0.115447   LR 0.001000
INFO - Training [22][  160/  391]   Loss 0.126488   Top1 95.639648   Top5 99.951172   BatchTime 0.113623   LR 0.001000
INFO - Training [22][  180/  391]   Loss 0.126982   Top1 95.638021   Top5 99.956597   BatchTime 0.112213   LR 0.001000
INFO - Training [22][  200/  391]   Loss 0.127504   Top1 95.609375   Top5 99.953125   BatchTime 0.111112   LR 0.001000
INFO - Training [22][  220/  391]   Loss 0.126736   Top1 95.607244   Top5 99.957386   BatchTime 0.110135   LR 0.001000
INFO - Training [22][  240/  391]   Loss 0.125573   Top1 95.651042   Top5 99.960938   BatchTime 0.109344   LR 0.001000
INFO - Training [22][  260/  391]   Loss 0.125517   Top1 95.661058   Top5 99.963942   BatchTime 0.108673   LR 0.001000
INFO - Training [22][  280/  391]   Loss 0.125355   Top1 95.672433   Top5 99.960938   BatchTime 0.108094   LR 0.001000
INFO - Training [22][  300/  391]   Loss 0.125859   Top1 95.630208   Top5 99.960938   BatchTime 0.107594   LR 0.001000
INFO - Training [22][  320/  391]   Loss 0.124832   Top1 95.668945   Top5 99.960938   BatchTime 0.107087   LR 0.001000
INFO - Training [22][  340/  391]   Loss 0.125655   Top1 95.641085   Top5 99.960938   BatchTime 0.106627   LR 0.001000
INFO - Training [22][  360/  391]   Loss 0.125131   Top1 95.648872   Top5 99.960938   BatchTime 0.106163   LR 0.001000
INFO - Training [22][  380/  391]   Loss 0.126406   Top1 95.594161   Top5 99.962993   BatchTime 0.105718   LR 0.001000
INFO - ==> Top1: 95.594    Top5: 99.964    Loss: 0.126
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [22][   20/   79]   Loss 0.375069   Top1 89.492188   Top5 99.531250   BatchTime 0.132486
INFO - Validation [22][   40/   79]   Loss 0.373156   Top1 89.550781   Top5 99.531250   BatchTime 0.083845
INFO - Validation [22][   60/   79]   Loss 0.366059   Top1 89.817708   Top5 99.583333   BatchTime 0.067530
INFO - ==> Top1: 89.820    Top5: 99.610    Loss: 0.360
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 89.540   Top5: 99.590] Sparsity : 0.849
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 89.410   Top5: 99.600] Sparsity : 0.848
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  23
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [23][   20/  391]   Loss 0.127223   Top1 95.195312   Top5 100.000000   BatchTime 0.200478   LR 0.001000
INFO - Training [23][   40/  391]   Loss 0.123186   Top1 95.429688   Top5 99.960938   BatchTime 0.151051   LR 0.001000
INFO - Training [23][   60/  391]   Loss 0.122560   Top1 95.572917   Top5 99.973958   BatchTime 0.134582   LR 0.001000
INFO - Training [23][   80/  391]   Loss 0.124500   Top1 95.527344   Top5 99.970703   BatchTime 0.126290   LR 0.001000
INFO - Training [23][  100/  391]   Loss 0.121954   Top1 95.632812   Top5 99.976562   BatchTime 0.121269   LR 0.001000
INFO - Training [23][  120/  391]   Loss 0.120052   Top1 95.709635   Top5 99.973958   BatchTime 0.117845   LR 0.001000
INFO - Training [23][  140/  391]   Loss 0.120762   Top1 95.725446   Top5 99.955357   BatchTime 0.115417   LR 0.001000
INFO - Training [23][  160/  391]   Loss 0.123507   Top1 95.595703   Top5 99.951172   BatchTime 0.113514   LR 0.001000
INFO - Training [23][  180/  391]   Loss 0.121466   Top1 95.694444   Top5 99.956597   BatchTime 0.112024   LR 0.001000
INFO - Training [23][  200/  391]   Loss 0.121998   Top1 95.695312   Top5 99.957031   BatchTime 0.110849   LR 0.001000
INFO - Training [23][  220/  391]   Loss 0.122621   Top1 95.692472   Top5 99.960938   BatchTime 0.109870   LR 0.001000
INFO - Training [23][  240/  391]   Loss 0.123822   Top1 95.667318   Top5 99.960938   BatchTime 0.109080   LR 0.001000
INFO - Training [23][  260/  391]   Loss 0.124174   Top1 95.673077   Top5 99.963942   BatchTime 0.108433   LR 0.001000
INFO - Training [23][  280/  391]   Loss 0.124138   Top1 95.661272   Top5 99.966518   BatchTime 0.107900   LR 0.001000
INFO - Training [23][  300/  391]   Loss 0.124569   Top1 95.658854   Top5 99.968750   BatchTime 0.107363   LR 0.001000
INFO - Training [23][  320/  391]   Loss 0.123878   Top1 95.700684   Top5 99.970703   BatchTime 0.106975   LR 0.001000
INFO - Training [23][  340/  391]   Loss 0.122856   Top1 95.755974   Top5 99.972426   BatchTime 0.106507   LR 0.001000
INFO - Training [23][  360/  391]   Loss 0.123098   Top1 95.755208   Top5 99.973958   BatchTime 0.106016   LR 0.001000
INFO - Training [23][  380/  391]   Loss 0.123245   Top1 95.721628   Top5 99.975329   BatchTime 0.105573   LR 0.001000
INFO - ==> Top1: 95.732    Top5: 99.974    Loss: 0.123
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [23][   20/   79]   Loss 0.374280   Top1 89.296875   Top5 99.492188   BatchTime 0.131650
INFO - Validation [23][   40/   79]   Loss 0.375572   Top1 89.218750   Top5 99.433594   BatchTime 0.083283
INFO - Validation [23][   60/   79]   Loss 0.364615   Top1 89.661458   Top5 99.518229   BatchTime 0.066990
INFO - ==> Top1: 89.610    Top5: 99.570    Loss: 0.362
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 89.610   Top5: 99.570] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 89.540   Top5: 99.590] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  24
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [24][   20/  391]   Loss 0.132640   Top1 95.195312   Top5 100.000000   BatchTime 0.198092   LR 0.001000
INFO - Training [24][   40/  391]   Loss 0.129796   Top1 95.371094   Top5 99.960938   BatchTime 0.149332   LR 0.001000
INFO - Training [24][   60/  391]   Loss 0.127272   Top1 95.546875   Top5 99.960938   BatchTime 0.133116   LR 0.001000
INFO - Training [24][   80/  391]   Loss 0.122545   Top1 95.683594   Top5 99.960938   BatchTime 0.124845   LR 0.001000
INFO - Training [24][  100/  391]   Loss 0.121621   Top1 95.796875   Top5 99.968750   BatchTime 0.119941   LR 0.001000
INFO - Training [24][  120/  391]   Loss 0.121689   Top1 95.761719   Top5 99.973958   BatchTime 0.116648   LR 0.001000
INFO - Training [24][  140/  391]   Loss 0.120845   Top1 95.770089   Top5 99.977679   BatchTime 0.114915   LR 0.001000
INFO - Training [24][  160/  391]   Loss 0.118614   Top1 95.883789   Top5 99.975586   BatchTime 0.113044   LR 0.001000
INFO - Training [24][  180/  391]   Loss 0.119005   Top1 95.828993   Top5 99.973958   BatchTime 0.111613   LR 0.001000
INFO - Training [24][  200/  391]   Loss 0.121188   Top1 95.738281   Top5 99.976562   BatchTime 0.110562   LR 0.001000
INFO - Training [24][  220/  391]   Loss 0.119803   Top1 95.791903   Top5 99.978693   BatchTime 0.109668   LR 0.001000
INFO - Training [24][  240/  391]   Loss 0.119817   Top1 95.804036   Top5 99.977214   BatchTime 0.108958   LR 0.001000
INFO - Training [24][  260/  391]   Loss 0.118413   Top1 95.829327   Top5 99.978966   BatchTime 0.108266   LR 0.001000
INFO - Training [24][  280/  391]   Loss 0.117836   Top1 95.837054   Top5 99.980469   BatchTime 0.107699   LR 0.001000
INFO - Training [24][  300/  391]   Loss 0.118841   Top1 95.812500   Top5 99.979167   BatchTime 0.107219   LR 0.001000
INFO - Training [24][  320/  391]   Loss 0.118653   Top1 95.817871   Top5 99.980469   BatchTime 0.106755   LR 0.001000
INFO - Training [24][  340/  391]   Loss 0.118855   Top1 95.785846   Top5 99.979320   BatchTime 0.106294   LR 0.001000
INFO - Training [24][  360/  391]   Loss 0.119664   Top1 95.770399   Top5 99.971788   BatchTime 0.105824   LR 0.001000
INFO - Training [24][  380/  391]   Loss 0.119945   Top1 95.748355   Top5 99.973273   BatchTime 0.105406   LR 0.001000
INFO - ==> Top1: 95.766    Top5: 99.974    Loss: 0.120
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [24][   20/   79]   Loss 0.366483   Top1 89.648438   Top5 99.492188   BatchTime 0.131420
INFO - Validation [24][   40/   79]   Loss 0.373403   Top1 89.433594   Top5 99.433594   BatchTime 0.083368
INFO - Validation [24][   60/   79]   Loss 0.362761   Top1 89.778646   Top5 99.531250   BatchTime 0.067323
INFO - ==> Top1: 89.770    Top5: 99.590    Loss: 0.361
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 89.610   Top5: 99.570] Sparsity : 0.850
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  25
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [25][   20/  391]   Loss 0.130360   Top1 95.507812   Top5 99.960938   BatchTime 0.197987   LR 0.001000
INFO - Training [25][   40/  391]   Loss 0.125402   Top1 95.664062   Top5 99.980469   BatchTime 0.149684   LR 0.001000
INFO - Training [25][   60/  391]   Loss 0.119094   Top1 95.833333   Top5 99.986979   BatchTime 0.132798   LR 0.001000
INFO - Training [25][   80/  391]   Loss 0.121064   Top1 95.751953   Top5 99.990234   BatchTime 0.124787   LR 0.001000
INFO - Training [25][  100/  391]   Loss 0.122043   Top1 95.710938   Top5 99.992188   BatchTime 0.119845   LR 0.001000
INFO - Training [25][  120/  391]   Loss 0.119485   Top1 95.761719   Top5 99.993490   BatchTime 0.116556   LR 0.001000
INFO - Training [25][  140/  391]   Loss 0.118444   Top1 95.825893   Top5 99.988839   BatchTime 0.114282   LR 0.001000
INFO - Training [25][  160/  391]   Loss 0.119764   Top1 95.810547   Top5 99.985352   BatchTime 0.112736   LR 0.001000
INFO - Training [25][  180/  391]   Loss 0.119031   Top1 95.820312   Top5 99.982639   BatchTime 0.111353   LR 0.001000
INFO - Training [25][  200/  391]   Loss 0.119187   Top1 95.832031   Top5 99.976562   BatchTime 0.110372   LR 0.001000
INFO - Training [25][  220/  391]   Loss 0.120524   Top1 95.727983   Top5 99.971591   BatchTime 0.109435   LR 0.001000
INFO - Training [25][  240/  391]   Loss 0.119621   Top1 95.761719   Top5 99.970703   BatchTime 0.108662   LR 0.001000
INFO - Training [25][  260/  391]   Loss 0.118884   Top1 95.787260   Top5 99.969952   BatchTime 0.108035   LR 0.001000
INFO - Training [25][  280/  391]   Loss 0.117684   Top1 95.851004   Top5 99.969308   BatchTime 0.107502   LR 0.001000
INFO - Training [25][  300/  391]   Loss 0.117537   Top1 95.856771   Top5 99.971354   BatchTime 0.107026   LR 0.001000
INFO - Training [25][  320/  391]   Loss 0.117823   Top1 95.859375   Top5 99.970703   BatchTime 0.106595   LR 0.001000
INFO - Training [25][  340/  391]   Loss 0.117028   Top1 95.912224   Top5 99.970129   BatchTime 0.106165   LR 0.001000
INFO - Training [25][  360/  391]   Loss 0.117973   Top1 95.889757   Top5 99.971788   BatchTime 0.105701   LR 0.001000
INFO - Training [25][  380/  391]   Loss 0.117743   Top1 95.892270   Top5 99.971217   BatchTime 0.105279   LR 0.001000
INFO - ==> Top1: 95.906    Top5: 99.972    Loss: 0.118
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [25][   20/   79]   Loss 0.354272   Top1 89.453125   Top5 99.492188   BatchTime 0.131233
INFO - Validation [25][   40/   79]   Loss 0.360681   Top1 89.335938   Top5 99.531250   BatchTime 0.083025
INFO - Validation [25][   60/   79]   Loss 0.355903   Top1 89.726562   Top5 99.544271   BatchTime 0.066963
INFO - ==> Top1: 89.750    Top5: 99.590    Loss: 0.354
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  26
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [26][   20/  391]   Loss 0.122979   Top1 95.507812   Top5 99.960938   BatchTime 0.199708   LR 0.001000
INFO - Training [26][   40/  391]   Loss 0.117689   Top1 95.507812   Top5 99.980469   BatchTime 0.150247   LR 0.001000
INFO - Training [26][   60/  391]   Loss 0.114964   Top1 95.677083   Top5 99.960938   BatchTime 0.134036   LR 0.001000
INFO - Training [26][   80/  391]   Loss 0.114657   Top1 95.703125   Top5 99.970703   BatchTime 0.125590   LR 0.001000
INFO - Training [26][  100/  391]   Loss 0.113897   Top1 95.726562   Top5 99.968750   BatchTime 0.120633   LR 0.001000
INFO - Training [26][  120/  391]   Loss 0.111770   Top1 95.872396   Top5 99.967448   BatchTime 0.117269   LR 0.001000
INFO - Training [26][  140/  391]   Loss 0.113837   Top1 95.837054   Top5 99.972098   BatchTime 0.114769   LR 0.001000
INFO - Training [26][  160/  391]   Loss 0.115340   Top1 95.839844   Top5 99.960938   BatchTime 0.112934   LR 0.001000
INFO - Training [26][  180/  391]   Loss 0.115886   Top1 95.885417   Top5 99.956597   BatchTime 0.112327   LR 0.001000
INFO - Training [26][  200/  391]   Loss 0.114958   Top1 95.914062   Top5 99.957031   BatchTime 0.111204   LR 0.001000
INFO - Training [26][  220/  391]   Loss 0.114647   Top1 95.916193   Top5 99.960938   BatchTime 0.110133   LR 0.001000
INFO - Training [26][  240/  391]   Loss 0.114030   Top1 95.927734   Top5 99.957682   BatchTime 0.109251   LR 0.001000
INFO - Training [26][  260/  391]   Loss 0.113548   Top1 95.949519   Top5 99.951923   BatchTime 0.108608   LR 0.001000
INFO - Training [26][  280/  391]   Loss 0.113694   Top1 95.915179   Top5 99.955357   BatchTime 0.107976   LR 0.001000
INFO - Training [26][  300/  391]   Loss 0.113206   Top1 95.929688   Top5 99.958333   BatchTime 0.107449   LR 0.001000
INFO - Training [26][  320/  391]   Loss 0.114607   Top1 95.881348   Top5 99.958496   BatchTime 0.107024   LR 0.001000
INFO - Training [26][  340/  391]   Loss 0.114431   Top1 95.889246   Top5 99.960938   BatchTime 0.106528   LR 0.001000
INFO - Training [26][  360/  391]   Loss 0.115085   Top1 95.863715   Top5 99.963108   BatchTime 0.106042   LR 0.001000
INFO - Training [26][  380/  391]   Loss 0.115527   Top1 95.867599   Top5 99.962993   BatchTime 0.105579   LR 0.001000
INFO - ==> Top1: 95.858    Top5: 99.964    Loss: 0.116
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [26][   20/   79]   Loss 0.375242   Top1 89.335938   Top5 99.531250   BatchTime 0.128129
INFO - Validation [26][   40/   79]   Loss 0.374064   Top1 89.277344   Top5 99.511719   BatchTime 0.081363
INFO - Validation [26][   60/   79]   Loss 0.363261   Top1 89.687500   Top5 99.557292   BatchTime 0.065745
INFO - ==> Top1: 89.670    Top5: 99.600    Loss: 0.361
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  27
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [27][   20/  391]   Loss 0.126437   Top1 95.468750   Top5 99.960938   BatchTime 0.199037   LR 0.001000
INFO - Training [27][   40/  391]   Loss 0.122099   Top1 95.449219   Top5 99.980469   BatchTime 0.150257   LR 0.001000
INFO - Training [27][   60/  391]   Loss 0.124019   Top1 95.416667   Top5 99.973958   BatchTime 0.133845   LR 0.001000
INFO - Training [27][   80/  391]   Loss 0.119354   Top1 95.693359   Top5 99.980469   BatchTime 0.125607   LR 0.001000
INFO - Training [27][  100/  391]   Loss 0.118107   Top1 95.750000   Top5 99.976562   BatchTime 0.120569   LR 0.001000
INFO - Training [27][  120/  391]   Loss 0.117700   Top1 95.768229   Top5 99.973958   BatchTime 0.117165   LR 0.001000
INFO - Training [27][  140/  391]   Loss 0.116853   Top1 95.731027   Top5 99.977679   BatchTime 0.114958   LR 0.001000
INFO - Training [27][  160/  391]   Loss 0.117822   Top1 95.732422   Top5 99.975586   BatchTime 0.113138   LR 0.001000
INFO - Training [27][  180/  391]   Loss 0.116337   Top1 95.746528   Top5 99.978299   BatchTime 0.111724   LR 0.001000
INFO - Training [27][  200/  391]   Loss 0.118637   Top1 95.664062   Top5 99.972656   BatchTime 0.110626   LR 0.001000
INFO - Training [27][  220/  391]   Loss 0.117935   Top1 95.735085   Top5 99.971591   BatchTime 0.109665   LR 0.001000
INFO - Training [27][  240/  391]   Loss 0.118057   Top1 95.761719   Top5 99.973958   BatchTime 0.108803   LR 0.001000
INFO - Training [27][  260/  391]   Loss 0.116242   Top1 95.820312   Top5 99.975962   BatchTime 0.108166   LR 0.001000
INFO - Training [27][  280/  391]   Loss 0.116661   Top1 95.823103   Top5 99.974888   BatchTime 0.107666   LR 0.001000
INFO - Training [27][  300/  391]   Loss 0.117344   Top1 95.804688   Top5 99.973958   BatchTime 0.107209   LR 0.001000
INFO - Training [27][  320/  391]   Loss 0.117478   Top1 95.812988   Top5 99.973145   BatchTime 0.106887   LR 0.001000
INFO - Training [27][  340/  391]   Loss 0.117256   Top1 95.808824   Top5 99.974724   BatchTime 0.106351   LR 0.001000
INFO - Training [27][  360/  391]   Loss 0.117577   Top1 95.785590   Top5 99.976128   BatchTime 0.105860   LR 0.001000
INFO - Training [27][  380/  391]   Loss 0.117731   Top1 95.760691   Top5 99.977385   BatchTime 0.105416   LR 0.001000
INFO - ==> Top1: 95.768    Top5: 99.974    Loss: 0.118
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [27][   20/   79]   Loss 0.371816   Top1 89.531250   Top5 99.531250   BatchTime 0.130820
INFO - Validation [27][   40/   79]   Loss 0.375045   Top1 89.394531   Top5 99.433594   BatchTime 0.082853
INFO - Validation [27][   60/   79]   Loss 0.367475   Top1 89.739583   Top5 99.492188   BatchTime 0.066811
INFO - ==> Top1: 89.660    Top5: 99.570    Loss: 0.364
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  28
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [28][   20/  391]   Loss 0.114163   Top1 95.781250   Top5 99.921875   BatchTime 0.197567   LR 0.001000
INFO - Training [28][   40/  391]   Loss 0.117383   Top1 95.937500   Top5 99.960938   BatchTime 0.148928   LR 0.001000
INFO - Training [28][   60/  391]   Loss 0.123737   Top1 95.690104   Top5 99.973958   BatchTime 0.132444   LR 0.001000
INFO - Training [28][   80/  391]   Loss 0.122039   Top1 95.800781   Top5 99.980469   BatchTime 0.124622   LR 0.001000
INFO - Training [28][  100/  391]   Loss 0.120941   Top1 95.742188   Top5 99.984375   BatchTime 0.119774   LR 0.001000
INFO - Training [28][  120/  391]   Loss 0.122691   Top1 95.709635   Top5 99.980469   BatchTime 0.116552   LR 0.001000
INFO - Training [28][  140/  391]   Loss 0.121005   Top1 95.775670   Top5 99.977679   BatchTime 0.114303   LR 0.001000
INFO - Training [28][  160/  391]   Loss 0.119272   Top1 95.820312   Top5 99.980469   BatchTime 0.112510   LR 0.001000
INFO - Training [28][  180/  391]   Loss 0.118909   Top1 95.807292   Top5 99.978299   BatchTime 0.111128   LR 0.001000
INFO - Training [28][  200/  391]   Loss 0.118064   Top1 95.847656   Top5 99.972656   BatchTime 0.110096   LR 0.001000
INFO - Training [28][  220/  391]   Loss 0.116791   Top1 95.901989   Top5 99.968040   BatchTime 0.109146   LR 0.001000
INFO - Training [28][  240/  391]   Loss 0.116259   Top1 95.927734   Top5 99.967448   BatchTime 0.108988   LR 0.001000
INFO - Training [28][  260/  391]   Loss 0.116572   Top1 95.901442   Top5 99.969952   BatchTime 0.108259   LR 0.001000
INFO - Training [28][  280/  391]   Loss 0.117443   Top1 95.859375   Top5 99.969308   BatchTime 0.107673   LR 0.001000
INFO - Training [28][  300/  391]   Loss 0.117508   Top1 95.880208   Top5 99.968750   BatchTime 0.107130   LR 0.001000
INFO - Training [28][  320/  391]   Loss 0.118023   Top1 95.876465   Top5 99.970703   BatchTime 0.106693   LR 0.001000
INFO - Training [28][  340/  391]   Loss 0.119069   Top1 95.834099   Top5 99.967831   BatchTime 0.106197   LR 0.001000
INFO - Training [28][  360/  391]   Loss 0.118958   Top1 95.828993   Top5 99.967448   BatchTime 0.105747   LR 0.001000
INFO - Training [28][  380/  391]   Loss 0.120213   Top1 95.795641   Top5 99.969161   BatchTime 0.105362   LR 0.001000
INFO - ==> Top1: 95.782    Top5: 99.968    Loss: 0.120
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [28][   20/   79]   Loss 0.362071   Top1 89.140625   Top5 99.492188   BatchTime 0.131418
INFO - Validation [28][   40/   79]   Loss 0.368215   Top1 89.101562   Top5 99.394531   BatchTime 0.083321
INFO - Validation [28][   60/   79]   Loss 0.364431   Top1 89.466146   Top5 99.453125   BatchTime 0.067051
INFO - ==> Top1: 89.520    Top5: 99.520    Loss: 0.362
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  29
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [29][   20/  391]   Loss 0.117814   Top1 95.742188   Top5 100.000000   BatchTime 0.199300   LR 0.001000
INFO - Training [29][   40/  391]   Loss 0.123308   Top1 95.507812   Top5 99.960938   BatchTime 0.150462   LR 0.001000
INFO - Training [29][   60/  391]   Loss 0.115920   Top1 95.833333   Top5 99.973958   BatchTime 0.133807   LR 0.001000
INFO - Training [29][   80/  391]   Loss 0.116954   Top1 95.839844   Top5 99.980469   BatchTime 0.125799   LR 0.001000
INFO - Training [29][  100/  391]   Loss 0.117552   Top1 95.859375   Top5 99.968750   BatchTime 0.120978   LR 0.001000
INFO - Training [29][  120/  391]   Loss 0.116606   Top1 95.865885   Top5 99.973958   BatchTime 0.117686   LR 0.001000
INFO - Training [29][  140/  391]   Loss 0.115664   Top1 95.909598   Top5 99.972098   BatchTime 0.115245   LR 0.001000
INFO - Training [29][  160/  391]   Loss 0.117645   Top1 95.820312   Top5 99.965820   BatchTime 0.113505   LR 0.001000
INFO - Training [29][  180/  391]   Loss 0.117641   Top1 95.842014   Top5 99.960938   BatchTime 0.112137   LR 0.001000
INFO - Training [29][  200/  391]   Loss 0.119848   Top1 95.824219   Top5 99.953125   BatchTime 0.110958   LR 0.001000
INFO - Training [29][  220/  391]   Loss 0.120452   Top1 95.795455   Top5 99.953835   BatchTime 0.110004   LR 0.001000
INFO - Training [29][  240/  391]   Loss 0.119977   Top1 95.826823   Top5 99.954427   BatchTime 0.109243   LR 0.001000
INFO - Training [29][  260/  391]   Loss 0.119828   Top1 95.820312   Top5 99.957933   BatchTime 0.108603   LR 0.001000
INFO - Training [29][  280/  391]   Loss 0.120029   Top1 95.789621   Top5 99.960938   BatchTime 0.108040   LR 0.001000
INFO - Training [29][  300/  391]   Loss 0.118933   Top1 95.825521   Top5 99.963542   BatchTime 0.107537   LR 0.001000
INFO - Training [29][  320/  391]   Loss 0.119473   Top1 95.795898   Top5 99.965820   BatchTime 0.107128   LR 0.001000
INFO - Training [29][  340/  391]   Loss 0.119376   Top1 95.788143   Top5 99.967831   BatchTime 0.106633   LR 0.001000
INFO - Training [29][  360/  391]   Loss 0.119550   Top1 95.779080   Top5 99.969618   BatchTime 0.106129   LR 0.001000
INFO - Training [29][  380/  391]   Loss 0.119750   Top1 95.756579   Top5 99.969161   BatchTime 0.105700   LR 0.001000
INFO - ==> Top1: 95.746    Top5: 99.970    Loss: 0.120
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [29][   20/   79]   Loss 0.377933   Top1 88.906250   Top5 99.570312   BatchTime 0.134045
INFO - Validation [29][   40/   79]   Loss 0.379870   Top1 89.179688   Top5 99.492188   BatchTime 0.084827
INFO - Validation [29][   60/   79]   Loss 0.373334   Top1 89.270833   Top5 99.518229   BatchTime 0.068258
INFO - ==> Top1: 89.260    Top5: 99.570    Loss: 0.374
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  30
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [30][   20/  391]   Loss 0.119518   Top1 95.507812   Top5 100.000000   BatchTime 0.199884   LR 0.001000
INFO - Training [30][   40/  391]   Loss 0.119987   Top1 95.644531   Top5 100.000000   BatchTime 0.150432   LR 0.001000
INFO - Training [30][   60/  391]   Loss 0.116333   Top1 95.885417   Top5 99.973958   BatchTime 0.133682   LR 0.001000
INFO - Training [30][   80/  391]   Loss 0.116713   Top1 95.869141   Top5 99.980469   BatchTime 0.125631   LR 0.001000
INFO - Training [30][  100/  391]   Loss 0.117875   Top1 95.750000   Top5 99.984375   BatchTime 0.120835   LR 0.001000
INFO - Training [30][  120/  391]   Loss 0.117924   Top1 95.755208   Top5 99.980469   BatchTime 0.117554   LR 0.001000
INFO - Training [30][  140/  391]   Loss 0.119124   Top1 95.742188   Top5 99.983259   BatchTime 0.115216   LR 0.001000
INFO - Training [30][  160/  391]   Loss 0.119942   Top1 95.751953   Top5 99.985352   BatchTime 0.113406   LR 0.001000
INFO - Training [30][  180/  391]   Loss 0.119711   Top1 95.772569   Top5 99.982639   BatchTime 0.111909   LR 0.001000
INFO - Training [30][  200/  391]   Loss 0.120166   Top1 95.757812   Top5 99.984375   BatchTime 0.110725   LR 0.001000
INFO - Training [30][  220/  391]   Loss 0.120379   Top1 95.738636   Top5 99.982244   BatchTime 0.109804   LR 0.001000
INFO - Training [30][  240/  391]   Loss 0.120530   Top1 95.712891   Top5 99.980469   BatchTime 0.109017   LR 0.001000
INFO - Training [30][  260/  391]   Loss 0.121649   Top1 95.679087   Top5 99.981971   BatchTime 0.108349   LR 0.001000
INFO - Training [30][  280/  391]   Loss 0.121896   Top1 95.680804   Top5 99.980469   BatchTime 0.107569   LR 0.001000
INFO - Training [30][  300/  391]   Loss 0.121508   Top1 95.718750   Top5 99.981771   BatchTime 0.107141   LR 0.001000
INFO - Training [30][  320/  391]   Loss 0.121543   Top1 95.703125   Top5 99.980469   BatchTime 0.107007   LR 0.001000
INFO - Training [30][  340/  391]   Loss 0.121868   Top1 95.714614   Top5 99.979320   BatchTime 0.106568   LR 0.001000
INFO - Training [30][  360/  391]   Loss 0.122496   Top1 95.653212   Top5 99.978299   BatchTime 0.106094   LR 0.001000
INFO - Training [30][  380/  391]   Loss 0.122435   Top1 95.647615   Top5 99.975329   BatchTime 0.105659   LR 0.001000
INFO - ==> Top1: 95.630    Top5: 99.974    Loss: 0.123
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [30][   20/   79]   Loss 0.361802   Top1 89.453125   Top5 99.492188   BatchTime 0.133914
INFO - Validation [30][   40/   79]   Loss 0.372811   Top1 89.355469   Top5 99.375000   BatchTime 0.084436
INFO - Validation [30][   60/   79]   Loss 0.370299   Top1 89.674479   Top5 99.453125   BatchTime 0.067845
INFO - ==> Top1: 89.500    Top5: 99.530    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  31
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [31][   20/  391]   Loss 0.128980   Top1 95.312500   Top5 100.000000   BatchTime 0.197633   LR 0.001000
INFO - Training [31][   40/  391]   Loss 0.131123   Top1 95.312500   Top5 100.000000   BatchTime 0.149034   LR 0.001000
INFO - Training [31][   60/  391]   Loss 0.132538   Top1 95.195312   Top5 99.973958   BatchTime 0.133462   LR 0.001000
INFO - Training [31][   80/  391]   Loss 0.134074   Top1 95.185547   Top5 99.970703   BatchTime 0.125077   LR 0.001000
INFO - Training [31][  100/  391]   Loss 0.133224   Top1 95.304688   Top5 99.976562   BatchTime 0.120142   LR 0.001000
INFO - Training [31][  120/  391]   Loss 0.133663   Top1 95.345052   Top5 99.967448   BatchTime 0.116784   LR 0.001000
INFO - Training [31][  140/  391]   Loss 0.132967   Top1 95.306920   Top5 99.972098   BatchTime 0.114403   LR 0.001000
INFO - Training [31][  160/  391]   Loss 0.132068   Top1 95.395508   Top5 99.965820   BatchTime 0.112746   LR 0.001000
INFO - Training [31][  180/  391]   Loss 0.130560   Top1 95.464410   Top5 99.969618   BatchTime 0.111431   LR 0.001000
INFO - Training [31][  200/  391]   Loss 0.128882   Top1 95.531250   Top5 99.972656   BatchTime 0.110449   LR 0.001000
INFO - Training [31][  220/  391]   Loss 0.129048   Top1 95.536222   Top5 99.975142   BatchTime 0.109698   LR 0.001000
INFO - Training [31][  240/  391]   Loss 0.128940   Top1 95.504557   Top5 99.970703   BatchTime 0.108984   LR 0.001000
INFO - Training [31][  260/  391]   Loss 0.128327   Top1 95.552885   Top5 99.972957   BatchTime 0.108368   LR 0.001000
INFO - Training [31][  280/  391]   Loss 0.127179   Top1 95.588728   Top5 99.974888   BatchTime 0.107829   LR 0.001000
INFO - Training [31][  300/  391]   Loss 0.126938   Top1 95.575521   Top5 99.976562   BatchTime 0.107326   LR 0.001000
INFO - Training [31][  320/  391]   Loss 0.126040   Top1 95.622559   Top5 99.970703   BatchTime 0.106894   LR 0.001000
INFO - Training [31][  340/  391]   Loss 0.126827   Top1 95.595129   Top5 99.972426   BatchTime 0.106424   LR 0.001000
INFO - Training [31][  360/  391]   Loss 0.126795   Top1 95.609809   Top5 99.973958   BatchTime 0.105937   LR 0.001000
INFO - Training [31][  380/  391]   Loss 0.127458   Top1 95.577714   Top5 99.973273   BatchTime 0.105510   LR 0.001000
INFO - ==> Top1: 95.584    Top5: 99.972    Loss: 0.127
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [31][   20/   79]   Loss 0.361376   Top1 89.257812   Top5 99.531250   BatchTime 0.132296
INFO - Validation [31][   40/   79]   Loss 0.374426   Top1 89.414062   Top5 99.394531   BatchTime 0.083745
INFO - Validation [31][   60/   79]   Loss 0.371510   Top1 89.674479   Top5 99.453125   BatchTime 0.067464
INFO - ==> Top1: 89.480    Top5: 99.520    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  32
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [32][   20/  391]   Loss 0.123272   Top1 96.054688   Top5 99.960938   BatchTime 0.202675   LR 0.001000
INFO - Training [32][   40/  391]   Loss 0.125510   Top1 95.644531   Top5 99.941406   BatchTime 0.152170   LR 0.001000
INFO - Training [32][   60/  391]   Loss 0.126552   Top1 95.625000   Top5 99.947917   BatchTime 0.135058   LR 0.001000
INFO - Training [32][   80/  391]   Loss 0.127396   Top1 95.527344   Top5 99.960938   BatchTime 0.126535   LR 0.001000
INFO - Training [32][  100/  391]   Loss 0.133281   Top1 95.265625   Top5 99.960938   BatchTime 0.121441   LR 0.001000
INFO - Training [32][  120/  391]   Loss 0.131943   Top1 95.325521   Top5 99.954427   BatchTime 0.117937   LR 0.001000
INFO - Training [32][  140/  391]   Loss 0.130780   Top1 95.385045   Top5 99.955357   BatchTime 0.115377   LR 0.001000
INFO - Training [32][  160/  391]   Loss 0.131038   Top1 95.366211   Top5 99.956055   BatchTime 0.113492   LR 0.001000
INFO - Training [32][  180/  391]   Loss 0.130930   Top1 95.364583   Top5 99.956597   BatchTime 0.112023   LR 0.001000
INFO - Training [32][  200/  391]   Loss 0.132996   Top1 95.261719   Top5 99.957031   BatchTime 0.110851   LR 0.001000
INFO - Training [32][  220/  391]   Loss 0.133594   Top1 95.248580   Top5 99.953835   BatchTime 0.109958   LR 0.001000
INFO - Training [32][  240/  391]   Loss 0.133266   Top1 95.253906   Top5 99.957682   BatchTime 0.109295   LR 0.001000
INFO - Training [32][  260/  391]   Loss 0.133172   Top1 95.234375   Top5 99.951923   BatchTime 0.108681   LR 0.001000
INFO - Training [32][  280/  391]   Loss 0.132777   Top1 95.251116   Top5 99.955357   BatchTime 0.108178   LR 0.001000
INFO - Training [32][  300/  391]   Loss 0.133269   Top1 95.244792   Top5 99.955729   BatchTime 0.107679   LR 0.001000
INFO - Training [32][  320/  391]   Loss 0.133814   Top1 95.229492   Top5 99.958496   BatchTime 0.107293   LR 0.001000
INFO - Training [32][  340/  391]   Loss 0.135482   Top1 95.199908   Top5 99.956342   BatchTime 0.106836   LR 0.001000
INFO - Training [32][  360/  391]   Loss 0.135742   Top1 95.203993   Top5 99.954427   BatchTime 0.106329   LR 0.001000
INFO - Training [32][  380/  391]   Loss 0.136914   Top1 95.160362   Top5 99.952714   BatchTime 0.105869   LR 0.001000
INFO - ==> Top1: 95.144    Top5: 99.954    Loss: 0.137
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [32][   20/   79]   Loss 0.393123   Top1 89.062500   Top5 99.570312   BatchTime 0.130827
INFO - Validation [32][   40/   79]   Loss 0.396243   Top1 89.062500   Top5 99.472656   BatchTime 0.083240
INFO - Validation [32][   60/   79]   Loss 0.387301   Top1 89.322917   Top5 99.518229   BatchTime 0.067093
INFO - ==> Top1: 89.200    Top5: 99.590    Loss: 0.385
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  33
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [33][   20/  391]   Loss 0.143930   Top1 95.000000   Top5 100.000000   BatchTime 0.197702   LR 0.001000
INFO - Training [33][   40/  391]   Loss 0.144330   Top1 94.902344   Top5 99.960938   BatchTime 0.149292   LR 0.001000
INFO - Training [33][   60/  391]   Loss 0.137443   Top1 95.117188   Top5 99.973958   BatchTime 0.133088   LR 0.001000
INFO - Training [33][   80/  391]   Loss 0.137838   Top1 95.146484   Top5 99.970703   BatchTime 0.124908   LR 0.001000
INFO - Training [33][  100/  391]   Loss 0.137698   Top1 95.148438   Top5 99.960938   BatchTime 0.120350   LR 0.001000
INFO - Training [33][  120/  391]   Loss 0.136813   Top1 95.156250   Top5 99.967448   BatchTime 0.117083   LR 0.001000
INFO - Training [33][  140/  391]   Loss 0.136978   Top1 95.145089   Top5 99.972098   BatchTime 0.114874   LR 0.001000
INFO - Training [33][  160/  391]   Loss 0.138949   Top1 95.053711   Top5 99.975586   BatchTime 0.113044   LR 0.001000
INFO - Training [33][  180/  391]   Loss 0.140004   Top1 95.004340   Top5 99.969618   BatchTime 0.111600   LR 0.001000
INFO - Training [33][  200/  391]   Loss 0.141539   Top1 94.929688   Top5 99.964844   BatchTime 0.110496   LR 0.001000
INFO - Training [33][  220/  391]   Loss 0.142583   Top1 94.865057   Top5 99.957386   BatchTime 0.109528   LR 0.001000
INFO - Training [33][  240/  391]   Loss 0.140177   Top1 94.957682   Top5 99.954427   BatchTime 0.108749   LR 0.001000
INFO - Training [33][  260/  391]   Loss 0.140056   Top1 94.960938   Top5 99.957933   BatchTime 0.108115   LR 0.001000
INFO - Training [33][  280/  391]   Loss 0.141463   Top1 94.930246   Top5 99.958147   BatchTime 0.107565   LR 0.001000
INFO - Training [33][  300/  391]   Loss 0.142255   Top1 94.932292   Top5 99.947917   BatchTime 0.107072   LR 0.001000
INFO - Training [33][  320/  391]   Loss 0.142078   Top1 94.941406   Top5 99.951172   BatchTime 0.106656   LR 0.001000
INFO - Training [33][  340/  391]   Loss 0.143114   Top1 94.905790   Top5 99.951746   BatchTime 0.106235   LR 0.001000
INFO - Training [33][  360/  391]   Loss 0.144303   Top1 94.865451   Top5 99.947917   BatchTime 0.105759   LR 0.001000
INFO - Training [33][  380/  391]   Loss 0.145005   Top1 94.837582   Top5 99.950658   BatchTime 0.105361   LR 0.001000
INFO - ==> Top1: 94.848    Top5: 99.950    Loss: 0.145
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [33][   20/   79]   Loss 0.397115   Top1 88.476562   Top5 99.570312   BatchTime 0.130444
INFO - Validation [33][   40/   79]   Loss 0.394939   Top1 88.945312   Top5 99.492188   BatchTime 0.082814
INFO - Validation [33][   60/   79]   Loss 0.382242   Top1 89.153646   Top5 99.544271   BatchTime 0.066882
INFO - ==> Top1: 89.200    Top5: 99.580    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  34
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [34][   20/  391]   Loss 0.144907   Top1 95.039062   Top5 99.921875   BatchTime 0.197870   LR 0.001000
INFO - Training [34][   40/  391]   Loss 0.149888   Top1 94.707031   Top5 99.941406   BatchTime 0.149515   LR 0.001000
INFO - Training [34][   60/  391]   Loss 0.143178   Top1 94.986979   Top5 99.947917   BatchTime 0.133303   LR 0.001000
INFO - Training [34][   80/  391]   Loss 0.141282   Top1 94.990234   Top5 99.951172   BatchTime 0.125422   LR 0.001000
INFO - Training [34][  100/  391]   Loss 0.138768   Top1 95.093750   Top5 99.937500   BatchTime 0.120570   LR 0.001000
INFO - Training [34][  120/  391]   Loss 0.142868   Top1 94.947917   Top5 99.941406   BatchTime 0.117283   LR 0.001000
INFO - Training [34][  140/  391]   Loss 0.143064   Top1 94.966518   Top5 99.938616   BatchTime 0.115075   LR 0.001000
INFO - Training [34][  160/  391]   Loss 0.143375   Top1 95.000000   Top5 99.941406   BatchTime 0.113322   LR 0.001000
INFO - Training [34][  180/  391]   Loss 0.145777   Top1 94.809028   Top5 99.943576   BatchTime 0.111887   LR 0.001000
INFO - Training [34][  200/  391]   Loss 0.147587   Top1 94.781250   Top5 99.949219   BatchTime 0.110778   LR 0.001000
INFO - Training [34][  220/  391]   Loss 0.148593   Top1 94.765625   Top5 99.950284   BatchTime 0.109816   LR 0.001000
INFO - Training [34][  240/  391]   Loss 0.148388   Top1 94.762370   Top5 99.951172   BatchTime 0.109053   LR 0.001000
INFO - Training [34][  260/  391]   Loss 0.149935   Top1 94.735577   Top5 99.945913   BatchTime 0.108364   LR 0.001000
INFO - Training [34][  280/  391]   Loss 0.149975   Top1 94.701451   Top5 99.946987   BatchTime 0.107764   LR 0.001000
INFO - Training [34][  300/  391]   Loss 0.150889   Top1 94.682292   Top5 99.947917   BatchTime 0.107242   LR 0.001000
INFO - Training [34][  320/  391]   Loss 0.151353   Top1 94.665527   Top5 99.948730   BatchTime 0.106845   LR 0.001000
INFO - Training [34][  340/  391]   Loss 0.152665   Top1 94.581801   Top5 99.949449   BatchTime 0.106388   LR 0.001000
INFO - Training [34][  360/  391]   Loss 0.154392   Top1 94.518229   Top5 99.947917   BatchTime 0.105890   LR 0.001000
INFO - Training [34][  380/  391]   Loss 0.154770   Top1 94.488076   Top5 99.948602   BatchTime 0.105270   LR 0.001000
INFO - ==> Top1: 94.474    Top5: 99.948    Loss: 0.155
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [34][   20/   79]   Loss 0.392424   Top1 88.320312   Top5 99.609375   BatchTime 0.122672
INFO - Validation [34][   40/   79]   Loss 0.395064   Top1 88.457031   Top5 99.511719   BatchTime 0.080447
INFO - Validation [34][   60/   79]   Loss 0.394468   Top1 88.593750   Top5 99.518229   BatchTime 0.065156
INFO - ==> Top1: 88.540    Top5: 99.540    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  35
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [35][   20/  391]   Loss 0.153596   Top1 94.804688   Top5 99.921875   BatchTime 0.202675   LR 0.001000
INFO - Training [35][   40/  391]   Loss 0.154134   Top1 94.570312   Top5 99.960938   BatchTime 0.151993   LR 0.001000
INFO - Training [35][   60/  391]   Loss 0.157207   Top1 94.492188   Top5 99.934896   BatchTime 0.134778   LR 0.001000
INFO - Training [35][   80/  391]   Loss 0.156642   Top1 94.433594   Top5 99.951172   BatchTime 0.126210   LR 0.001000
INFO - Training [35][  100/  391]   Loss 0.156353   Top1 94.453125   Top5 99.945312   BatchTime 0.121252   LR 0.001000
INFO - Training [35][  120/  391]   Loss 0.158320   Top1 94.355469   Top5 99.941406   BatchTime 0.117910   LR 0.001000
INFO - Training [35][  140/  391]   Loss 0.156143   Top1 94.408482   Top5 99.949777   BatchTime 0.115580   LR 0.001000
INFO - Training [35][  160/  391]   Loss 0.156185   Top1 94.414062   Top5 99.956055   BatchTime 0.113727   LR 0.001000
INFO - Training [35][  180/  391]   Loss 0.158177   Top1 94.335938   Top5 99.952257   BatchTime 0.112258   LR 0.001000
INFO - Training [35][  200/  391]   Loss 0.158561   Top1 94.328125   Top5 99.945312   BatchTime 0.111174   LR 0.001000
INFO - Training [35][  220/  391]   Loss 0.157501   Top1 94.396307   Top5 99.943182   BatchTime 0.110153   LR 0.001000
INFO - Training [35][  240/  391]   Loss 0.158226   Top1 94.381510   Top5 99.944661   BatchTime 0.109392   LR 0.001000
INFO - Training [35][  260/  391]   Loss 0.158202   Top1 94.371995   Top5 99.939904   BatchTime 0.108729   LR 0.001000
INFO - Training [35][  280/  391]   Loss 0.159342   Top1 94.358259   Top5 99.941406   BatchTime 0.108162   LR 0.001000
INFO - Training [35][  300/  391]   Loss 0.160100   Top1 94.320312   Top5 99.934896   BatchTime 0.107668   LR 0.001000
INFO - Training [35][  320/  391]   Loss 0.162910   Top1 94.228516   Top5 99.931641   BatchTime 0.107222   LR 0.001000
INFO - Training [35][  340/  391]   Loss 0.162370   Top1 94.276195   Top5 99.928768   BatchTime 0.106745   LR 0.001000
INFO - Training [35][  360/  391]   Loss 0.162801   Top1 94.275174   Top5 99.930556   BatchTime 0.106244   LR 0.001000
INFO - Training [35][  380/  391]   Loss 0.162901   Top1 94.266036   Top5 99.928043   BatchTime 0.105787   LR 0.001000
INFO - ==> Top1: 94.256    Top5: 99.928    Loss: 0.163
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [35][   20/   79]   Loss 0.407888   Top1 87.734375   Top5 99.453125   BatchTime 0.125485
INFO - Validation [35][   40/   79]   Loss 0.399000   Top1 88.164062   Top5 99.511719   BatchTime 0.080541
INFO - Validation [35][   60/   79]   Loss 0.394034   Top1 88.294271   Top5 99.492188   BatchTime 0.065496
INFO - ==> Top1: 88.230    Top5: 99.520    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  36
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [36][   20/  391]   Loss 0.177044   Top1 93.437500   Top5 100.000000   BatchTime 0.193081   LR 0.001000
INFO - Training [36][   40/  391]   Loss 0.172284   Top1 93.652344   Top5 99.980469   BatchTime 0.147027   LR 0.001000
INFO - Training [36][   60/  391]   Loss 0.165905   Top1 93.841146   Top5 99.986979   BatchTime 0.131713   LR 0.001000
INFO - Training [36][   80/  391]   Loss 0.163566   Top1 94.013672   Top5 99.980469   BatchTime 0.124092   LR 0.001000
INFO - Training [36][  100/  391]   Loss 0.163185   Top1 94.031250   Top5 99.960938   BatchTime 0.119750   LR 0.001000
INFO - Training [36][  120/  391]   Loss 0.162440   Top1 94.042969   Top5 99.960938   BatchTime 0.116610   LR 0.001000
INFO - Training [36][  140/  391]   Loss 0.165389   Top1 93.922991   Top5 99.949777   BatchTime 0.114448   LR 0.001000
INFO - Training [36][  160/  391]   Loss 0.167952   Top1 93.872070   Top5 99.946289   BatchTime 0.112821   LR 0.001000
INFO - Training [36][  180/  391]   Loss 0.169545   Top1 93.849826   Top5 99.943576   BatchTime 0.111500   LR 0.001000
INFO - Training [36][  200/  391]   Loss 0.169875   Top1 93.824219   Top5 99.949219   BatchTime 0.110471   LR 0.001000
INFO - Training [36][  220/  391]   Loss 0.169490   Top1 93.821023   Top5 99.950284   BatchTime 0.109630   LR 0.001000
INFO - Training [36][  240/  391]   Loss 0.168773   Top1 93.821615   Top5 99.951172   BatchTime 0.108909   LR 0.001000
INFO - Training [36][  260/  391]   Loss 0.169743   Top1 93.795072   Top5 99.942909   BatchTime 0.108191   LR 0.001000
INFO - Training [36][  280/  391]   Loss 0.169833   Top1 93.803013   Top5 99.946987   BatchTime 0.107614   LR 0.001000
INFO - Training [36][  300/  391]   Loss 0.170047   Top1 93.776042   Top5 99.945312   BatchTime 0.107190   LR 0.001000
INFO - Training [36][  320/  391]   Loss 0.170277   Top1 93.793945   Top5 99.943848   BatchTime 0.106750   LR 0.001000
INFO - Training [36][  340/  391]   Loss 0.170247   Top1 93.821232   Top5 99.942555   BatchTime 0.106338   LR 0.001000
INFO - Training [36][  360/  391]   Loss 0.170766   Top1 93.802083   Top5 99.943576   BatchTime 0.105867   LR 0.001000
INFO - Training [36][  380/  391]   Loss 0.170719   Top1 93.830181   Top5 99.946546   BatchTime 0.105426   LR 0.001000
INFO - ==> Top1: 93.818    Top5: 99.946    Loss: 0.171
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [36][   20/   79]   Loss 0.401592   Top1 88.046875   Top5 99.414062   BatchTime 0.125727
INFO - Validation [36][   40/   79]   Loss 0.399804   Top1 88.398438   Top5 99.433594   BatchTime 0.081644
INFO - Validation [36][   60/   79]   Loss 0.391107   Top1 88.606771   Top5 99.531250   BatchTime 0.066155
INFO - ==> Top1: 88.530    Top5: 99.590    Loss: 0.392
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  37
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [37][   20/  391]   Loss 0.169771   Top1 94.062500   Top5 99.960938   BatchTime 0.196718   LR 0.001000
INFO - Training [37][   40/  391]   Loss 0.163282   Top1 94.121094   Top5 99.960938   BatchTime 0.147549   LR 0.001000
INFO - Training [37][   60/  391]   Loss 0.163925   Top1 94.218750   Top5 99.973958   BatchTime 0.132200   LR 0.001000
INFO - Training [37][   80/  391]   Loss 0.163572   Top1 94.238281   Top5 99.960938   BatchTime 0.124493   LR 0.001000
INFO - Training [37][  100/  391]   Loss 0.164554   Top1 94.187500   Top5 99.968750   BatchTime 0.120605   LR 0.001000
INFO - Training [37][  120/  391]   Loss 0.164224   Top1 94.205729   Top5 99.967448   BatchTime 0.117270   LR 0.001000
INFO - Training [37][  140/  391]   Loss 0.164138   Top1 94.213170   Top5 99.960938   BatchTime 0.114922   LR 0.001000
INFO - Training [37][  160/  391]   Loss 0.166217   Top1 94.169922   Top5 99.960938   BatchTime 0.113258   LR 0.001000
INFO - Training [37][  180/  391]   Loss 0.165673   Top1 94.171007   Top5 99.965278   BatchTime 0.112000   LR 0.001000
INFO - Training [37][  200/  391]   Loss 0.166150   Top1 94.144531   Top5 99.964844   BatchTime 0.110962   LR 0.001000
INFO - Training [37][  220/  391]   Loss 0.166694   Top1 94.119318   Top5 99.960938   BatchTime 0.110060   LR 0.001000
INFO - Training [37][  240/  391]   Loss 0.168878   Top1 94.042969   Top5 99.960938   BatchTime 0.109323   LR 0.001000
INFO - Training [37][  260/  391]   Loss 0.169976   Top1 93.993389   Top5 99.963942   BatchTime 0.108651   LR 0.001000
INFO - Training [37][  280/  391]   Loss 0.171073   Top1 93.964844   Top5 99.960938   BatchTime 0.108088   LR 0.001000
INFO - Training [37][  300/  391]   Loss 0.172447   Top1 93.921875   Top5 99.958333   BatchTime 0.107621   LR 0.001000
INFO - Training [37][  320/  391]   Loss 0.172787   Top1 93.894043   Top5 99.956055   BatchTime 0.107203   LR 0.001000
INFO - Training [37][  340/  391]   Loss 0.173527   Top1 93.848805   Top5 99.956342   BatchTime 0.106792   LR 0.001000
INFO - Training [37][  360/  391]   Loss 0.173683   Top1 93.841146   Top5 99.950087   BatchTime 0.106282   LR 0.001000
INFO - Training [37][  380/  391]   Loss 0.173711   Top1 93.830181   Top5 99.948602   BatchTime 0.105865   LR 0.001000
INFO - ==> Top1: 93.792    Top5: 99.946    Loss: 0.175
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [37][   20/   79]   Loss 0.404111   Top1 88.242188   Top5 99.453125   BatchTime 0.123858
INFO - Validation [37][   40/   79]   Loss 0.396450   Top1 88.378906   Top5 99.335938   BatchTime 0.080042
INFO - Validation [37][   60/   79]   Loss 0.388677   Top1 88.489583   Top5 99.401042   BatchTime 0.064980
INFO - ==> Top1: 88.470    Top5: 99.510    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  38
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [38][   20/  391]   Loss 0.165445   Top1 94.062500   Top5 99.921875   BatchTime 0.191644   LR 0.001000
INFO - Training [38][   40/  391]   Loss 0.162381   Top1 94.160156   Top5 99.941406   BatchTime 0.146296   LR 0.001000
INFO - Training [38][   60/  391]   Loss 0.162401   Top1 94.231771   Top5 99.947917   BatchTime 0.131298   LR 0.001000
INFO - Training [38][   80/  391]   Loss 0.165830   Top1 94.169922   Top5 99.941406   BatchTime 0.123959   LR 0.001000
INFO - Training [38][  100/  391]   Loss 0.168366   Top1 94.039062   Top5 99.953125   BatchTime 0.119499   LR 0.001000
INFO - Training [38][  120/  391]   Loss 0.170806   Top1 93.919271   Top5 99.954427   BatchTime 0.116373   LR 0.001000
INFO - Training [38][  140/  391]   Loss 0.173637   Top1 93.772321   Top5 99.949777   BatchTime 0.114129   LR 0.001000
INFO - Training [38][  160/  391]   Loss 0.172368   Top1 93.867188   Top5 99.951172   BatchTime 0.112401   LR 0.001000
INFO - Training [38][  180/  391]   Loss 0.173081   Top1 93.854167   Top5 99.952257   BatchTime 0.111112   LR 0.001000
INFO - Training [38][  200/  391]   Loss 0.173180   Top1 93.808594   Top5 99.957031   BatchTime 0.110006   LR 0.001000
INFO - Training [38][  220/  391]   Loss 0.173880   Top1 93.799716   Top5 99.953835   BatchTime 0.109264   LR 0.001000
INFO - Training [38][  240/  391]   Loss 0.173555   Top1 93.818359   Top5 99.951172   BatchTime 0.108500   LR 0.001000
INFO - Training [38][  260/  391]   Loss 0.174255   Top1 93.840144   Top5 99.939904   BatchTime 0.107927   LR 0.001000
INFO - Training [38][  280/  391]   Loss 0.174001   Top1 93.833705   Top5 99.935826   BatchTime 0.107437   LR 0.001000
INFO - Training [38][  300/  391]   Loss 0.174312   Top1 93.820312   Top5 99.934896   BatchTime 0.106929   LR 0.001000
INFO - Training [38][  320/  391]   Loss 0.173205   Top1 93.869629   Top5 99.929199   BatchTime 0.106525   LR 0.001000
INFO - Training [38][  340/  391]   Loss 0.172923   Top1 93.883272   Top5 99.933364   BatchTime 0.106147   LR 0.001000
INFO - Training [38][  360/  391]   Loss 0.171993   Top1 93.914931   Top5 99.937066   BatchTime 0.105695   LR 0.001000
INFO - Training [38][  380/  391]   Loss 0.172242   Top1 93.916530   Top5 99.938322   BatchTime 0.105276   LR 0.001000
INFO - ==> Top1: 93.918    Top5: 99.938    Loss: 0.172
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [38][   20/   79]   Loss 0.396134   Top1 88.320312   Top5 99.335938   BatchTime 0.122246
INFO - Validation [38][   40/   79]   Loss 0.385579   Top1 88.417969   Top5 99.355469   BatchTime 0.078851
INFO - Validation [38][   60/   79]   Loss 0.377126   Top1 88.710938   Top5 99.466146   BatchTime 0.064328
INFO - ==> Top1: 88.820    Top5: 99.560    Loss: 0.374
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  39
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [39][   20/  391]   Loss 0.179136   Top1 93.554688   Top5 99.804688   BatchTime 0.190334   LR 0.001000
INFO - Training [39][   40/  391]   Loss 0.172288   Top1 93.789062   Top5 99.902344   BatchTime 0.145956   LR 0.001000
INFO - Training [39][   60/  391]   Loss 0.173886   Top1 93.763021   Top5 99.908854   BatchTime 0.129873   LR 0.001000
INFO - Training [39][   80/  391]   Loss 0.178101   Top1 93.671875   Top5 99.902344   BatchTime 0.122612   LR 0.001000
INFO - Training [39][  100/  391]   Loss 0.178504   Top1 93.703125   Top5 99.914062   BatchTime 0.118232   LR 0.001000
INFO - Training [39][  120/  391]   Loss 0.176949   Top1 93.847656   Top5 99.921875   BatchTime 0.115276   LR 0.001000
INFO - Training [39][  140/  391]   Loss 0.177494   Top1 93.906250   Top5 99.916295   BatchTime 0.113193   LR 0.001000
INFO - Training [39][  160/  391]   Loss 0.175313   Top1 93.901367   Top5 99.916992   BatchTime 0.112260   LR 0.001000
INFO - Training [39][  180/  391]   Loss 0.173203   Top1 93.936632   Top5 99.926215   BatchTime 0.111080   LR 0.001000
INFO - Training [39][  200/  391]   Loss 0.171099   Top1 93.988281   Top5 99.925781   BatchTime 0.110066   LR 0.001000
INFO - Training [39][  220/  391]   Loss 0.171265   Top1 94.005682   Top5 99.932528   BatchTime 0.109190   LR 0.001000
INFO - Training [39][  240/  391]   Loss 0.171005   Top1 93.981120   Top5 99.931641   BatchTime 0.108520   LR 0.001000
INFO - Training [39][  260/  391]   Loss 0.173338   Top1 93.888221   Top5 99.930889   BatchTime 0.107886   LR 0.001000
INFO - Training [39][  280/  391]   Loss 0.173253   Top1 93.883929   Top5 99.930246   BatchTime 0.107355   LR 0.001000
INFO - Training [39][  300/  391]   Loss 0.173792   Top1 93.877604   Top5 99.932292   BatchTime 0.106888   LR 0.001000
INFO - Training [39][  320/  391]   Loss 0.173374   Top1 93.886719   Top5 99.934082   BatchTime 0.106510   LR 0.001000
INFO - Training [39][  340/  391]   Loss 0.173074   Top1 93.883272   Top5 99.935662   BatchTime 0.106111   LR 0.001000
INFO - Training [39][  360/  391]   Loss 0.173835   Top1 93.838976   Top5 99.937066   BatchTime 0.105670   LR 0.001000
INFO - Training [39][  380/  391]   Loss 0.174443   Top1 93.817845   Top5 99.934211   BatchTime 0.105268   LR 0.001000
INFO - ==> Top1: 93.798    Top5: 99.930    Loss: 0.175
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [39][   20/   79]   Loss 0.393221   Top1 88.437500   Top5 99.335938   BatchTime 0.122464
INFO - Validation [39][   40/   79]   Loss 0.383545   Top1 88.808594   Top5 99.335938   BatchTime 0.078174
INFO - Validation [39][   60/   79]   Loss 0.373439   Top1 88.932292   Top5 99.440104   BatchTime 0.063781
INFO - ==> Top1: 88.990    Top5: 99.530    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  40
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [40][   20/  391]   Loss 0.171101   Top1 93.789062   Top5 99.960938   BatchTime 0.191868   LR 0.000100
INFO - Training [40][   40/  391]   Loss 0.174701   Top1 93.750000   Top5 99.941406   BatchTime 0.146325   LR 0.000100
INFO - Training [40][   60/  391]   Loss 0.169737   Top1 94.114583   Top5 99.947917   BatchTime 0.131225   LR 0.000100
INFO - Training [40][   80/  391]   Loss 0.171764   Top1 94.042969   Top5 99.921875   BatchTime 0.123385   LR 0.000100
INFO - Training [40][  100/  391]   Loss 0.172498   Top1 93.992188   Top5 99.890625   BatchTime 0.118576   LR 0.000100
INFO - Training [40][  120/  391]   Loss 0.172716   Top1 94.036458   Top5 99.889323   BatchTime 0.115682   LR 0.000100
INFO - Training [40][  140/  391]   Loss 0.169161   Top1 94.090402   Top5 99.905134   BatchTime 0.113616   LR 0.000100
INFO - Training [40][  160/  391]   Loss 0.170438   Top1 94.013672   Top5 99.916992   BatchTime 0.111928   LR 0.000100
INFO - Training [40][  180/  391]   Loss 0.170304   Top1 94.032118   Top5 99.917535   BatchTime 0.110708   LR 0.000100
INFO - Training [40][  200/  391]   Loss 0.172149   Top1 93.957031   Top5 99.910156   BatchTime 0.109666   LR 0.000100
INFO - Training [40][  220/  391]   Loss 0.171872   Top1 93.980824   Top5 99.907670   BatchTime 0.108893   LR 0.000100
INFO - Training [40][  240/  391]   Loss 0.172998   Top1 93.922526   Top5 99.908854   BatchTime 0.108229   LR 0.000100
INFO - Training [40][  260/  391]   Loss 0.173511   Top1 93.912260   Top5 99.906851   BatchTime 0.107589   LR 0.000100
INFO - Training [40][  280/  391]   Loss 0.174111   Top1 93.867188   Top5 99.913504   BatchTime 0.107054   LR 0.000100
INFO - Training [40][  300/  391]   Loss 0.173791   Top1 93.877604   Top5 99.914062   BatchTime 0.106639   LR 0.000100
INFO - Training [40][  320/  391]   Loss 0.173030   Top1 93.916016   Top5 99.916992   BatchTime 0.106225   LR 0.000100
INFO - Training [40][  340/  391]   Loss 0.172931   Top1 93.892463   Top5 99.917279   BatchTime 0.105811   LR 0.000100
INFO - Training [40][  360/  391]   Loss 0.172917   Top1 93.880208   Top5 99.921875   BatchTime 0.105334   LR 0.000100
INFO - Training [40][  380/  391]   Loss 0.173292   Top1 93.856908   Top5 99.921875   BatchTime 0.104971   LR 0.000100
INFO - ==> Top1: 93.864    Top5: 99.922    Loss: 0.173
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [40][   20/   79]   Loss 0.396303   Top1 88.125000   Top5 99.453125   BatchTime 0.123406
INFO - Validation [40][   40/   79]   Loss 0.384465   Top1 88.437500   Top5 99.414062   BatchTime 0.077650
INFO - Validation [40][   60/   79]   Loss 0.375121   Top1 88.632812   Top5 99.505208   BatchTime 0.063371
INFO - ==> Top1: 88.650    Top5: 99.600    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  41
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [41][   20/  391]   Loss 0.176803   Top1 93.750000   Top5 100.000000   BatchTime 0.187697   LR 0.000100
INFO - Training [41][   40/  391]   Loss 0.177479   Top1 93.808594   Top5 100.000000   BatchTime 0.144660   LR 0.000100
INFO - Training [41][   60/  391]   Loss 0.179662   Top1 93.854167   Top5 99.934896   BatchTime 0.130256   LR 0.000100
INFO - Training [41][   80/  391]   Loss 0.173668   Top1 93.925781   Top5 99.921875   BatchTime 0.122741   LR 0.000100
INFO - Training [41][  100/  391]   Loss 0.175283   Top1 93.867188   Top5 99.921875   BatchTime 0.118403   LR 0.000100
INFO - Training [41][  120/  391]   Loss 0.173410   Top1 93.938802   Top5 99.928385   BatchTime 0.115567   LR 0.000100
INFO - Training [41][  140/  391]   Loss 0.172589   Top1 93.917411   Top5 99.933036   BatchTime 0.113409   LR 0.000100
INFO - Training [41][  160/  391]   Loss 0.170590   Top1 93.974609   Top5 99.936523   BatchTime 0.111802   LR 0.000100
INFO - Training [41][  180/  391]   Loss 0.171340   Top1 93.949653   Top5 99.939236   BatchTime 0.110523   LR 0.000100
INFO - Training [41][  200/  391]   Loss 0.171697   Top1 93.964844   Top5 99.929688   BatchTime 0.109628   LR 0.000100
INFO - Training [41][  220/  391]   Loss 0.171613   Top1 93.952415   Top5 99.914773   BatchTime 0.109176   LR 0.000100
INFO - Training [41][  240/  391]   Loss 0.170559   Top1 93.987630   Top5 99.918620   BatchTime 0.108451   LR 0.000100
INFO - Training [41][  260/  391]   Loss 0.170518   Top1 93.960337   Top5 99.915865   BatchTime 0.107822   LR 0.000100
INFO - Training [41][  280/  391]   Loss 0.170033   Top1 94.009487   Top5 99.916295   BatchTime 0.107326   LR 0.000100
INFO - Training [41][  300/  391]   Loss 0.169655   Top1 94.007812   Top5 99.919271   BatchTime 0.106905   LR 0.000100
INFO - Training [41][  320/  391]   Loss 0.169286   Top1 94.040527   Top5 99.919434   BatchTime 0.106550   LR 0.000100
INFO - Training [41][  340/  391]   Loss 0.168732   Top1 94.064798   Top5 99.921875   BatchTime 0.106118   LR 0.000100
INFO - Training [41][  360/  391]   Loss 0.169474   Top1 94.051649   Top5 99.924045   BatchTime 0.105642   LR 0.000100
INFO - Training [41][  380/  391]   Loss 0.170192   Top1 94.021382   Top5 99.925987   BatchTime 0.105238   LR 0.000100
INFO - ==> Top1: 93.994    Top5: 99.926    Loss: 0.170
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [41][   20/   79]   Loss 0.399550   Top1 88.398438   Top5 99.492188   BatchTime 0.122918
INFO - Validation [41][   40/   79]   Loss 0.389511   Top1 88.691406   Top5 99.433594   BatchTime 0.077553
INFO - Validation [41][   60/   79]   Loss 0.377265   Top1 88.997396   Top5 99.518229   BatchTime 0.063406
INFO - ==> Top1: 89.060    Top5: 99.620    Loss: 0.373
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  42
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [42][   20/  391]   Loss 0.180374   Top1 93.710938   Top5 99.960938   BatchTime 0.183399   LR 0.000100
INFO - Training [42][   40/  391]   Loss 0.167474   Top1 94.238281   Top5 99.941406   BatchTime 0.142105   LR 0.000100
INFO - Training [42][   60/  391]   Loss 0.166358   Top1 94.348958   Top5 99.921875   BatchTime 0.128594   LR 0.000100
INFO - Training [42][   80/  391]   Loss 0.168696   Top1 94.355469   Top5 99.892578   BatchTime 0.121536   LR 0.000100
INFO - Training [42][  100/  391]   Loss 0.169072   Top1 94.250000   Top5 99.906250   BatchTime 0.117457   LR 0.000100
INFO - Training [42][  120/  391]   Loss 0.168512   Top1 94.199219   Top5 99.921875   BatchTime 0.114638   LR 0.000100
INFO - Training [42][  140/  391]   Loss 0.166033   Top1 94.274554   Top5 99.927455   BatchTime 0.112780   LR 0.000100
INFO - Training [42][  160/  391]   Loss 0.163929   Top1 94.360352   Top5 99.931641   BatchTime 0.111228   LR 0.000100
INFO - Training [42][  180/  391]   Loss 0.169677   Top1 94.175347   Top5 99.926215   BatchTime 0.110078   LR 0.000100
INFO - Training [42][  200/  391]   Loss 0.167853   Top1 94.257812   Top5 99.929688   BatchTime 0.109254   LR 0.000100
INFO - Training [42][  220/  391]   Loss 0.167915   Top1 94.254261   Top5 99.936080   BatchTime 0.108451   LR 0.000100
INFO - Training [42][  240/  391]   Loss 0.166856   Top1 94.254557   Top5 99.931641   BatchTime 0.107749   LR 0.000100
INFO - Training [42][  260/  391]   Loss 0.165419   Top1 94.311899   Top5 99.936899   BatchTime 0.107198   LR 0.000100
INFO - Training [42][  280/  391]   Loss 0.165513   Top1 94.316406   Top5 99.938616   BatchTime 0.106745   LR 0.000100
INFO - Training [42][  300/  391]   Loss 0.167258   Top1 94.234375   Top5 99.940104   BatchTime 0.106337   LR 0.000100
INFO - Training [42][  320/  391]   Loss 0.167016   Top1 94.233398   Top5 99.934082   BatchTime 0.105963   LR 0.000100
INFO - Training [42][  340/  391]   Loss 0.166320   Top1 94.246324   Top5 99.935662   BatchTime 0.105595   LR 0.000100
INFO - Training [42][  360/  391]   Loss 0.167014   Top1 94.242622   Top5 99.934896   BatchTime 0.105219   LR 0.000100
INFO - Training [42][  380/  391]   Loss 0.166954   Top1 94.212582   Top5 99.934211   BatchTime 0.104828   LR 0.000100
INFO - ==> Top1: 94.226    Top5: 99.934    Loss: 0.167
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [42][   20/   79]   Loss 0.399893   Top1 88.554688   Top5 99.492188   BatchTime 0.124232
INFO - Validation [42][   40/   79]   Loss 0.384755   Top1 88.750000   Top5 99.414062   BatchTime 0.077842
INFO - Validation [42][   60/   79]   Loss 0.372306   Top1 88.997396   Top5 99.505208   BatchTime 0.063757
INFO - ==> Top1: 89.020    Top5: 99.590    Loss: 0.370
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  43
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [43][   20/  391]   Loss 0.167647   Top1 93.671875   Top5 99.960938   BatchTime 0.186635   LR 0.000100
INFO - Training [43][   40/  391]   Loss 0.166383   Top1 93.886719   Top5 99.941406   BatchTime 0.144011   LR 0.000100
INFO - Training [43][   60/  391]   Loss 0.165813   Top1 93.854167   Top5 99.947917   BatchTime 0.129379   LR 0.000100
INFO - Training [43][   80/  391]   Loss 0.165393   Top1 93.984375   Top5 99.921875   BatchTime 0.122098   LR 0.000100
INFO - Training [43][  100/  391]   Loss 0.161805   Top1 94.187500   Top5 99.921875   BatchTime 0.117821   LR 0.000100
INFO - Training [43][  120/  391]   Loss 0.161078   Top1 94.244792   Top5 99.934896   BatchTime 0.114897   LR 0.000100
INFO - Training [43][  140/  391]   Loss 0.161328   Top1 94.252232   Top5 99.938616   BatchTime 0.112972   LR 0.000100
INFO - Training [43][  160/  391]   Loss 0.161808   Top1 94.218750   Top5 99.926758   BatchTime 0.111404   LR 0.000100
INFO - Training [43][  180/  391]   Loss 0.160229   Top1 94.283854   Top5 99.934896   BatchTime 0.110189   LR 0.000100
INFO - Training [43][  200/  391]   Loss 0.161423   Top1 94.261719   Top5 99.933594   BatchTime 0.109162   LR 0.000100
INFO - Training [43][  220/  391]   Loss 0.161663   Top1 94.186790   Top5 99.936080   BatchTime 0.108302   LR 0.000100
INFO - Training [43][  240/  391]   Loss 0.162085   Top1 94.166667   Top5 99.941406   BatchTime 0.107734   LR 0.000100
INFO - Training [43][  260/  391]   Loss 0.161987   Top1 94.182692   Top5 99.936899   BatchTime 0.107318   LR 0.000100
INFO - Training [43][  280/  391]   Loss 0.162936   Top1 94.118304   Top5 99.938616   BatchTime 0.107250   LR 0.000100
INFO - Training [43][  300/  391]   Loss 0.162704   Top1 94.130208   Top5 99.937500   BatchTime 0.106797   LR 0.000100
INFO - Training [43][  320/  391]   Loss 0.163163   Top1 94.125977   Top5 99.938965   BatchTime 0.106409   LR 0.000100
INFO - Training [43][  340/  391]   Loss 0.163330   Top1 94.094669   Top5 99.942555   BatchTime 0.106053   LR 0.000100
INFO - Training [43][  360/  391]   Loss 0.163509   Top1 94.077691   Top5 99.941406   BatchTime 0.105583   LR 0.000100
INFO - Training [43][  380/  391]   Loss 0.163952   Top1 94.099507   Top5 99.940378   BatchTime 0.105186   LR 0.000100
INFO - ==> Top1: 94.086    Top5: 99.942    Loss: 0.164
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [43][   20/   79]   Loss 0.388609   Top1 88.437500   Top5 99.414062   BatchTime 0.125707
INFO - Validation [43][   40/   79]   Loss 0.383854   Top1 88.730469   Top5 99.414062   BatchTime 0.077534
INFO - Validation [43][   60/   79]   Loss 0.375060   Top1 88.932292   Top5 99.505208   BatchTime 0.063567
INFO - ==> Top1: 88.990    Top5: 99.590    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  44
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [44][   20/  391]   Loss 0.187350   Top1 93.945312   Top5 99.921875   BatchTime 0.190748   LR 0.000100
INFO - Training [44][   40/  391]   Loss 0.181325   Top1 93.906250   Top5 99.921875   BatchTime 0.146213   LR 0.000100
INFO - Training [44][   60/  391]   Loss 0.179262   Top1 93.750000   Top5 99.934896   BatchTime 0.131246   LR 0.000100
INFO - Training [44][   80/  391]   Loss 0.178887   Top1 93.750000   Top5 99.941406   BatchTime 0.123413   LR 0.000100
INFO - Training [44][  100/  391]   Loss 0.175796   Top1 93.890625   Top5 99.953125   BatchTime 0.118983   LR 0.000100
INFO - Training [44][  120/  391]   Loss 0.175367   Top1 93.815104   Top5 99.941406   BatchTime 0.116007   LR 0.000100
INFO - Training [44][  140/  391]   Loss 0.171299   Top1 93.928571   Top5 99.949777   BatchTime 0.113829   LR 0.000100
INFO - Training [44][  160/  391]   Loss 0.171058   Top1 93.940430   Top5 99.951172   BatchTime 0.112109   LR 0.000100
INFO - Training [44][  180/  391]   Loss 0.172092   Top1 93.875868   Top5 99.934896   BatchTime 0.110907   LR 0.000100
INFO - Training [44][  200/  391]   Loss 0.172451   Top1 93.835938   Top5 99.937500   BatchTime 0.109917   LR 0.000100
INFO - Training [44][  220/  391]   Loss 0.170873   Top1 93.920455   Top5 99.936080   BatchTime 0.109078   LR 0.000100
INFO - Training [44][  240/  391]   Loss 0.169990   Top1 93.922526   Top5 99.941406   BatchTime 0.108419   LR 0.000100
INFO - Training [44][  260/  391]   Loss 0.169013   Top1 93.978365   Top5 99.942909   BatchTime 0.107798   LR 0.000100
INFO - Training [44][  280/  391]   Loss 0.169332   Top1 93.948103   Top5 99.946987   BatchTime 0.107330   LR 0.000100
INFO - Training [44][  300/  391]   Loss 0.168538   Top1 93.986979   Top5 99.947917   BatchTime 0.106856   LR 0.000100
INFO - Training [44][  320/  391]   Loss 0.168769   Top1 93.977051   Top5 99.948730   BatchTime 0.106436   LR 0.000100
INFO - Training [44][  340/  391]   Loss 0.168934   Top1 93.991268   Top5 99.951746   BatchTime 0.106070   LR 0.000100
INFO - Training [44][  360/  391]   Loss 0.168467   Top1 93.980035   Top5 99.952257   BatchTime 0.105596   LR 0.000100
INFO - Training [44][  380/  391]   Loss 0.168828   Top1 93.969984   Top5 99.952714   BatchTime 0.105182   LR 0.000100
INFO - ==> Top1: 93.980    Top5: 99.952    Loss: 0.169
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [44][   20/   79]   Loss 0.392934   Top1 88.515625   Top5 99.609375   BatchTime 0.121967
INFO - Validation [44][   40/   79]   Loss 0.383372   Top1 88.750000   Top5 99.472656   BatchTime 0.076857
INFO - Validation [44][   60/   79]   Loss 0.372676   Top1 88.997396   Top5 99.531250   BatchTime 0.062772
INFO - ==> Top1: 89.040    Top5: 99.610    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  45
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [45][   20/  391]   Loss 0.147460   Top1 94.687500   Top5 99.960938   BatchTime 0.183009   LR 0.000100
INFO - Training [45][   40/  391]   Loss 0.165233   Top1 94.003906   Top5 99.921875   BatchTime 0.141478   LR 0.000100
INFO - Training [45][   60/  391]   Loss 0.166663   Top1 93.880208   Top5 99.921875   BatchTime 0.127708   LR 0.000100
INFO - Training [45][   80/  391]   Loss 0.163741   Top1 93.974609   Top5 99.912109   BatchTime 0.120877   LR 0.000100
INFO - Training [45][  100/  391]   Loss 0.171503   Top1 93.726562   Top5 99.914062   BatchTime 0.116886   LR 0.000100
INFO - Training [45][  120/  391]   Loss 0.168155   Top1 93.867188   Top5 99.928385   BatchTime 0.114046   LR 0.000100
INFO - Training [45][  140/  391]   Loss 0.168551   Top1 93.822545   Top5 99.938616   BatchTime 0.112110   LR 0.000100
INFO - Training [45][  160/  391]   Loss 0.167864   Top1 93.925781   Top5 99.941406   BatchTime 0.110741   LR 0.000100
INFO - Training [45][  180/  391]   Loss 0.169356   Top1 93.910590   Top5 99.930556   BatchTime 0.109591   LR 0.000100
INFO - Training [45][  200/  391]   Loss 0.170277   Top1 93.890625   Top5 99.929688   BatchTime 0.108669   LR 0.000100
INFO - Training [45][  220/  391]   Loss 0.169989   Top1 93.916903   Top5 99.936080   BatchTime 0.107884   LR 0.000100
INFO - Training [45][  240/  391]   Loss 0.168875   Top1 93.990885   Top5 99.938151   BatchTime 0.107296   LR 0.000100
INFO - Training [45][  260/  391]   Loss 0.167621   Top1 94.047476   Top5 99.933894   BatchTime 0.106750   LR 0.000100
INFO - Training [45][  280/  391]   Loss 0.168498   Top1 94.012277   Top5 99.933036   BatchTime 0.106268   LR 0.000100
INFO - Training [45][  300/  391]   Loss 0.169272   Top1 94.007812   Top5 99.932292   BatchTime 0.105839   LR 0.000100
INFO - Training [45][  320/  391]   Loss 0.169231   Top1 94.028320   Top5 99.934082   BatchTime 0.105563   LR 0.000100
INFO - Training [45][  340/  391]   Loss 0.168497   Top1 94.041820   Top5 99.937960   BatchTime 0.105244   LR 0.000100
INFO - Training [45][  360/  391]   Loss 0.168042   Top1 94.055990   Top5 99.939236   BatchTime 0.105219   LR 0.000100
INFO - Training [45][  380/  391]   Loss 0.167997   Top1 94.068668   Top5 99.938322   BatchTime 0.104800   LR 0.000100
INFO - ==> Top1: 94.072    Top5: 99.938    Loss: 0.168
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [45][   20/   79]   Loss 0.391858   Top1 88.437500   Top5 99.492188   BatchTime 0.122881
INFO - Validation [45][   40/   79]   Loss 0.383859   Top1 88.769531   Top5 99.472656   BatchTime 0.076593
INFO - Validation [45][   60/   79]   Loss 0.377521   Top1 88.763021   Top5 99.531250   BatchTime 0.062860
INFO - ==> Top1: 88.830    Top5: 99.620    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  46
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [46][   20/  391]   Loss 0.151304   Top1 94.531250   Top5 99.960938   BatchTime 0.189148   LR 0.000100
INFO - Training [46][   40/  391]   Loss 0.163753   Top1 93.828125   Top5 99.941406   BatchTime 0.144777   LR 0.000100
INFO - Training [46][   60/  391]   Loss 0.161933   Top1 94.114583   Top5 99.921875   BatchTime 0.130234   LR 0.000100
INFO - Training [46][   80/  391]   Loss 0.165153   Top1 93.994141   Top5 99.931641   BatchTime 0.122918   LR 0.000100
INFO - Training [46][  100/  391]   Loss 0.162285   Top1 94.148438   Top5 99.929688   BatchTime 0.118430   LR 0.000100
INFO - Training [46][  120/  391]   Loss 0.160700   Top1 94.140625   Top5 99.934896   BatchTime 0.115398   LR 0.000100
INFO - Training [46][  140/  391]   Loss 0.160042   Top1 94.151786   Top5 99.938616   BatchTime 0.113286   LR 0.000100
INFO - Training [46][  160/  391]   Loss 0.161037   Top1 94.135742   Top5 99.936523   BatchTime 0.111710   LR 0.000100
INFO - Training [46][  180/  391]   Loss 0.164127   Top1 94.019097   Top5 99.939236   BatchTime 0.110464   LR 0.000100
INFO - Training [46][  200/  391]   Loss 0.163033   Top1 94.082031   Top5 99.941406   BatchTime 0.109502   LR 0.000100
INFO - Training [46][  220/  391]   Loss 0.164535   Top1 94.026989   Top5 99.943182   BatchTime 0.108684   LR 0.000100
INFO - Training [46][  240/  391]   Loss 0.163963   Top1 94.055990   Top5 99.944661   BatchTime 0.107922   LR 0.000100
INFO - Training [46][  260/  391]   Loss 0.162999   Top1 94.116587   Top5 99.945913   BatchTime 0.107297   LR 0.000100
INFO - Training [46][  280/  391]   Loss 0.161648   Top1 94.146205   Top5 99.944196   BatchTime 0.106742   LR 0.000100
INFO - Training [46][  300/  391]   Loss 0.162701   Top1 94.106771   Top5 99.940104   BatchTime 0.106376   LR 0.000100
INFO - Training [46][  320/  391]   Loss 0.163692   Top1 94.079590   Top5 99.943848   BatchTime 0.105964   LR 0.000100
INFO - Training [46][  340/  391]   Loss 0.163931   Top1 94.087776   Top5 99.931066   BatchTime 0.105567   LR 0.000100
INFO - Training [46][  360/  391]   Loss 0.163202   Top1 94.114583   Top5 99.934896   BatchTime 0.105148   LR 0.000100
INFO - Training [46][  380/  391]   Loss 0.164371   Top1 94.070724   Top5 99.936266   BatchTime 0.104733   LR 0.000100
INFO - ==> Top1: 94.094    Top5: 99.936    Loss: 0.164
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [46][   20/   79]   Loss 0.392110   Top1 88.203125   Top5 99.492188   BatchTime 0.123102
INFO - Validation [46][   40/   79]   Loss 0.384223   Top1 88.574219   Top5 99.433594   BatchTime 0.077573
INFO - Validation [46][   60/   79]   Loss 0.373490   Top1 88.893229   Top5 99.518229   BatchTime 0.063413
INFO - ==> Top1: 88.950    Top5: 99.610    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  47
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [47][   20/  391]   Loss 0.160167   Top1 94.257812   Top5 99.921875   BatchTime 0.186661   LR 0.000100
INFO - Training [47][   40/  391]   Loss 0.162156   Top1 94.082031   Top5 99.960938   BatchTime 0.143688   LR 0.000100
INFO - Training [47][   60/  391]   Loss 0.164771   Top1 94.153646   Top5 99.947917   BatchTime 0.129037   LR 0.000100
INFO - Training [47][   80/  391]   Loss 0.162754   Top1 94.287109   Top5 99.941406   BatchTime 0.121820   LR 0.000100
INFO - Training [47][  100/  391]   Loss 0.165239   Top1 94.132812   Top5 99.945312   BatchTime 0.117493   LR 0.000100
INFO - Training [47][  120/  391]   Loss 0.169887   Top1 94.016927   Top5 99.941406   BatchTime 0.114673   LR 0.000100
INFO - Training [47][  140/  391]   Loss 0.168204   Top1 94.040179   Top5 99.944196   BatchTime 0.112606   LR 0.000100
INFO - Training [47][  160/  391]   Loss 0.167828   Top1 94.086914   Top5 99.946289   BatchTime 0.111084   LR 0.000100
INFO - Training [47][  180/  391]   Loss 0.166549   Top1 94.131944   Top5 99.947917   BatchTime 0.109832   LR 0.000100
INFO - Training [47][  200/  391]   Loss 0.166838   Top1 94.140625   Top5 99.953125   BatchTime 0.108973   LR 0.000100
INFO - Training [47][  220/  391]   Loss 0.166763   Top1 94.140625   Top5 99.943182   BatchTime 0.108150   LR 0.000100
INFO - Training [47][  240/  391]   Loss 0.166682   Top1 94.108073   Top5 99.947917   BatchTime 0.107462   LR 0.000100
INFO - Training [47][  260/  391]   Loss 0.167452   Top1 94.086538   Top5 99.948918   BatchTime 0.106950   LR 0.000100
INFO - Training [47][  280/  391]   Loss 0.166738   Top1 94.101562   Top5 99.944196   BatchTime 0.106405   LR 0.000100
INFO - Training [47][  300/  391]   Loss 0.166883   Top1 94.085938   Top5 99.942708   BatchTime 0.105984   LR 0.000100
INFO - Training [47][  320/  391]   Loss 0.166798   Top1 94.091797   Top5 99.941406   BatchTime 0.105564   LR 0.000100
INFO - Training [47][  340/  391]   Loss 0.167109   Top1 94.094669   Top5 99.940257   BatchTime 0.105170   LR 0.000100
INFO - Training [47][  360/  391]   Loss 0.167148   Top1 94.092882   Top5 99.937066   BatchTime 0.104789   LR 0.000100
INFO - Training [47][  380/  391]   Loss 0.167877   Top1 94.083059   Top5 99.940378   BatchTime 0.104415   LR 0.000100
INFO - ==> Top1: 94.082    Top5: 99.938    Loss: 0.168
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [47][   20/   79]   Loss 0.385611   Top1 88.710938   Top5 99.414062   BatchTime 0.121821
INFO - Validation [47][   40/   79]   Loss 0.381411   Top1 88.847656   Top5 99.433594   BatchTime 0.075213
INFO - Validation [47][   60/   79]   Loss 0.372459   Top1 88.971354   Top5 99.544271   BatchTime 0.061810
INFO - ==> Top1: 88.990    Top5: 99.630    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  48
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [48][   20/  391]   Loss 0.175134   Top1 93.203125   Top5 99.921875   BatchTime 0.194619   LR 0.000100
INFO - Training [48][   40/  391]   Loss 0.170912   Top1 93.496094   Top5 99.941406   BatchTime 0.147813   LR 0.000100
INFO - Training [48][   60/  391]   Loss 0.166977   Top1 93.736979   Top5 99.947917   BatchTime 0.132172   LR 0.000100
INFO - Training [48][   80/  391]   Loss 0.162201   Top1 93.925781   Top5 99.951172   BatchTime 0.124141   LR 0.000100
INFO - Training [48][  100/  391]   Loss 0.157646   Top1 94.109375   Top5 99.953125   BatchTime 0.119402   LR 0.000100
INFO - Training [48][  120/  391]   Loss 0.159808   Top1 94.134115   Top5 99.954427   BatchTime 0.116231   LR 0.000100
INFO - Training [48][  140/  391]   Loss 0.161582   Top1 94.073661   Top5 99.944196   BatchTime 0.113840   LR 0.000100
INFO - Training [48][  160/  391]   Loss 0.162208   Top1 94.072266   Top5 99.936523   BatchTime 0.112166   LR 0.000100
INFO - Training [48][  180/  391]   Loss 0.161401   Top1 94.166667   Top5 99.939236   BatchTime 0.110893   LR 0.000100
INFO - Training [48][  200/  391]   Loss 0.161982   Top1 94.164062   Top5 99.941406   BatchTime 0.109849   LR 0.000100
INFO - Training [48][  220/  391]   Loss 0.163426   Top1 94.122869   Top5 99.946733   BatchTime 0.108859   LR 0.000100
INFO - Training [48][  240/  391]   Loss 0.162301   Top1 94.156901   Top5 99.944661   BatchTime 0.108116   LR 0.000100
INFO - Training [48][  260/  391]   Loss 0.163043   Top1 94.122596   Top5 99.948918   BatchTime 0.107524   LR 0.000100
INFO - Training [48][  280/  391]   Loss 0.164683   Top1 94.059710   Top5 99.946987   BatchTime 0.106924   LR 0.000100
INFO - Training [48][  300/  391]   Loss 0.164239   Top1 94.088542   Top5 99.945312   BatchTime 0.106500   LR 0.000100
INFO - Training [48][  320/  391]   Loss 0.163230   Top1 94.150391   Top5 99.943848   BatchTime 0.106101   LR 0.000100
INFO - Training [48][  340/  391]   Loss 0.162398   Top1 94.177390   Top5 99.947151   BatchTime 0.105695   LR 0.000100
INFO - Training [48][  360/  391]   Loss 0.163082   Top1 94.151476   Top5 99.945747   BatchTime 0.105236   LR 0.000100
INFO - Training [48][  380/  391]   Loss 0.162164   Top1 94.183799   Top5 99.944490   BatchTime 0.104810   LR 0.000100
INFO - ==> Top1: 94.138    Top5: 99.944    Loss: 0.163
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [48][   20/   79]   Loss 0.382857   Top1 88.750000   Top5 99.492188   BatchTime 0.123715
INFO - Validation [48][   40/   79]   Loss 0.377965   Top1 88.828125   Top5 99.355469   BatchTime 0.078248
INFO - Validation [48][   60/   79]   Loss 0.370854   Top1 89.023438   Top5 99.427083   BatchTime 0.063780
INFO - ==> Top1: 88.950    Top5: 99.530    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  49
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [49][   20/  391]   Loss 0.181889   Top1 93.398438   Top5 99.921875   BatchTime 0.188550   LR 0.000100
INFO - Training [49][   40/  391]   Loss 0.168075   Top1 93.945312   Top5 99.941406   BatchTime 0.145100   LR 0.000100
INFO - Training [49][   60/  391]   Loss 0.167896   Top1 93.997396   Top5 99.947917   BatchTime 0.130310   LR 0.000100
INFO - Training [49][   80/  391]   Loss 0.167214   Top1 94.072266   Top5 99.951172   BatchTime 0.122874   LR 0.000100
INFO - Training [49][  100/  391]   Loss 0.166439   Top1 94.148438   Top5 99.937500   BatchTime 0.118284   LR 0.000100
INFO - Training [49][  120/  391]   Loss 0.168720   Top1 94.055990   Top5 99.934896   BatchTime 0.115422   LR 0.000100
INFO - Training [49][  140/  391]   Loss 0.170245   Top1 93.928571   Top5 99.938616   BatchTime 0.113346   LR 0.000100
INFO - Training [49][  160/  391]   Loss 0.170467   Top1 93.930664   Top5 99.936523   BatchTime 0.111705   LR 0.000100
INFO - Training [49][  180/  391]   Loss 0.170074   Top1 93.919271   Top5 99.943576   BatchTime 0.110450   LR 0.000100
INFO - Training [49][  200/  391]   Loss 0.169452   Top1 93.957031   Top5 99.949219   BatchTime 0.109536   LR 0.000100
INFO - Training [49][  220/  391]   Loss 0.170005   Top1 93.902699   Top5 99.950284   BatchTime 0.108668   LR 0.000100
INFO - Training [49][  240/  391]   Loss 0.168990   Top1 93.912760   Top5 99.951172   BatchTime 0.107898   LR 0.000100
INFO - Training [49][  260/  391]   Loss 0.167680   Top1 93.963341   Top5 99.951923   BatchTime 0.107364   LR 0.000100
INFO - Training [49][  280/  391]   Loss 0.166658   Top1 93.978795   Top5 99.946987   BatchTime 0.106849   LR 0.000100
INFO - Training [49][  300/  391]   Loss 0.166112   Top1 93.989583   Top5 99.945312   BatchTime 0.106197   LR 0.000100
INFO - Training [49][  320/  391]   Loss 0.166225   Top1 93.994141   Top5 99.941406   BatchTime 0.105791   LR 0.000100
INFO - Training [49][  340/  391]   Loss 0.166143   Top1 94.011949   Top5 99.944853   BatchTime 0.105390   LR 0.000100
INFO - Training [49][  360/  391]   Loss 0.167405   Top1 94.010417   Top5 99.945747   BatchTime 0.104962   LR 0.000100
INFO - Training [49][  380/  391]   Loss 0.166977   Top1 94.048109   Top5 99.940378   BatchTime 0.104599   LR 0.000100
INFO - ==> Top1: 94.074    Top5: 99.942    Loss: 0.166
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [49][   20/   79]   Loss 0.393833   Top1 88.164062   Top5 99.531250   BatchTime 0.120934
INFO - Validation [49][   40/   79]   Loss 0.384093   Top1 88.710938   Top5 99.394531   BatchTime 0.075296
INFO - Validation [49][   60/   79]   Loss 0.375232   Top1 88.867188   Top5 99.492188   BatchTime 0.061955
INFO - ==> Top1: 88.920    Top5: 99.590    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  50
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [50][   20/  391]   Loss 0.161565   Top1 94.296875   Top5 100.000000   BatchTime 0.181555   LR 0.000010
INFO - Training [50][   40/  391]   Loss 0.152857   Top1 94.511719   Top5 99.980469   BatchTime 0.140400   LR 0.000010
INFO - Training [50][   60/  391]   Loss 0.159584   Top1 94.088542   Top5 99.986979   BatchTime 0.127599   LR 0.000010
INFO - Training [50][   80/  391]   Loss 0.168308   Top1 93.876953   Top5 99.980469   BatchTime 0.120798   LR 0.000010
INFO - Training [50][  100/  391]   Loss 0.164987   Top1 94.007812   Top5 99.984375   BatchTime 0.117598   LR 0.000010
INFO - Training [50][  120/  391]   Loss 0.166839   Top1 93.938802   Top5 99.973958   BatchTime 0.114821   LR 0.000010
INFO - Training [50][  140/  391]   Loss 0.166711   Top1 93.967634   Top5 99.977679   BatchTime 0.112640   LR 0.000010
INFO - Training [50][  160/  391]   Loss 0.167720   Top1 93.935547   Top5 99.970703   BatchTime 0.111089   LR 0.000010
INFO - Training [50][  180/  391]   Loss 0.169281   Top1 93.923611   Top5 99.960938   BatchTime 0.109943   LR 0.000010
INFO - Training [50][  200/  391]   Loss 0.166823   Top1 94.015625   Top5 99.960938   BatchTime 0.108957   LR 0.000010
INFO - Training [50][  220/  391]   Loss 0.167608   Top1 94.002131   Top5 99.957386   BatchTime 0.108169   LR 0.000010
INFO - Training [50][  240/  391]   Loss 0.167153   Top1 94.023438   Top5 99.957682   BatchTime 0.107501   LR 0.000010
INFO - Training [50][  260/  391]   Loss 0.167437   Top1 93.987380   Top5 99.960938   BatchTime 0.106975   LR 0.000010
INFO - Training [50][  280/  391]   Loss 0.166455   Top1 94.023438   Top5 99.963728   BatchTime 0.106465   LR 0.000010
INFO - Training [50][  300/  391]   Loss 0.166770   Top1 93.981771   Top5 99.963542   BatchTime 0.106090   LR 0.000010
INFO - Training [50][  320/  391]   Loss 0.167727   Top1 93.972168   Top5 99.958496   BatchTime 0.105748   LR 0.000010
INFO - Training [50][  340/  391]   Loss 0.167299   Top1 93.991268   Top5 99.960938   BatchTime 0.105450   LR 0.000010
INFO - Training [50][  360/  391]   Loss 0.167309   Top1 93.982205   Top5 99.960938   BatchTime 0.105044   LR 0.000010
INFO - Training [50][  380/  391]   Loss 0.166073   Top1 94.027549   Top5 99.958882   BatchTime 0.104636   LR 0.000010
INFO - ==> Top1: 94.014    Top5: 99.958    Loss: 0.167
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [50][   20/   79]   Loss 0.387509   Top1 88.593750   Top5 99.453125   BatchTime 0.125887
INFO - Validation [50][   40/   79]   Loss 0.382407   Top1 88.925781   Top5 99.414062   BatchTime 0.078340
INFO - Validation [50][   60/   79]   Loss 0.371773   Top1 88.958333   Top5 99.505208   BatchTime 0.064176
INFO - ==> Top1: 88.950    Top5: 99.600    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  51
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [51][   20/  391]   Loss 0.171988   Top1 93.476562   Top5 100.000000   BatchTime 0.189568   LR 0.000010
INFO - Training [51][   40/  391]   Loss 0.164216   Top1 94.101562   Top5 100.000000   BatchTime 0.145217   LR 0.000010
INFO - Training [51][   60/  391]   Loss 0.164622   Top1 94.153646   Top5 99.960938   BatchTime 0.130428   LR 0.000010
INFO - Training [51][   80/  391]   Loss 0.168219   Top1 94.013672   Top5 99.931641   BatchTime 0.123060   LR 0.000010
INFO - Training [51][  100/  391]   Loss 0.168987   Top1 94.031250   Top5 99.937500   BatchTime 0.118702   LR 0.000010
INFO - Training [51][  120/  391]   Loss 0.169108   Top1 94.036458   Top5 99.947917   BatchTime 0.115708   LR 0.000010
INFO - Training [51][  140/  391]   Loss 0.167544   Top1 94.068080   Top5 99.944196   BatchTime 0.113408   LR 0.000010
INFO - Training [51][  160/  391]   Loss 0.166753   Top1 94.150391   Top5 99.941406   BatchTime 0.111736   LR 0.000010
INFO - Training [51][  180/  391]   Loss 0.165846   Top1 94.214410   Top5 99.934896   BatchTime 0.110433   LR 0.000010
INFO - Training [51][  200/  391]   Loss 0.164453   Top1 94.281250   Top5 99.933594   BatchTime 0.109499   LR 0.000010
INFO - Training [51][  220/  391]   Loss 0.165245   Top1 94.289773   Top5 99.936080   BatchTime 0.108676   LR 0.000010
INFO - Training [51][  240/  391]   Loss 0.164020   Top1 94.322917   Top5 99.925130   BatchTime 0.107982   LR 0.000010
INFO - Training [51][  260/  391]   Loss 0.164310   Top1 94.278846   Top5 99.930889   BatchTime 0.107378   LR 0.000010
INFO - Training [51][  280/  391]   Loss 0.163628   Top1 94.291295   Top5 99.933036   BatchTime 0.106887   LR 0.000010
INFO - Training [51][  300/  391]   Loss 0.163692   Top1 94.242188   Top5 99.934896   BatchTime 0.106491   LR 0.000010
INFO - Training [51][  320/  391]   Loss 0.163117   Top1 94.240723   Top5 99.938965   BatchTime 0.106100   LR 0.000010
INFO - Training [51][  340/  391]   Loss 0.163063   Top1 94.253217   Top5 99.942555   BatchTime 0.105691   LR 0.000010
INFO - Training [51][  360/  391]   Loss 0.162974   Top1 94.238281   Top5 99.943576   BatchTime 0.105227   LR 0.000010
INFO - Training [51][  380/  391]   Loss 0.163339   Top1 94.226974   Top5 99.938322   BatchTime 0.104640   LR 0.000010
INFO - ==> Top1: 94.250    Top5: 99.938    Loss: 0.163
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [51][   20/   79]   Loss 0.392831   Top1 88.359375   Top5 99.375000   BatchTime 0.121716
INFO - Validation [51][   40/   79]   Loss 0.385791   Top1 88.789062   Top5 99.316406   BatchTime 0.074017
INFO - Validation [51][   60/   79]   Loss 0.375755   Top1 89.010417   Top5 99.440104   BatchTime 0.062021
INFO - ==> Top1: 89.050    Top5: 99.520    Loss: 0.374
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  52
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [52][   20/  391]   Loss 0.149709   Top1 94.570312   Top5 99.960938   BatchTime 0.184860   LR 0.000010
INFO - Training [52][   40/  391]   Loss 0.163916   Top1 94.003906   Top5 99.960938   BatchTime 0.142569   LR 0.000010
INFO - Training [52][   60/  391]   Loss 0.169426   Top1 93.971354   Top5 99.934896   BatchTime 0.128658   LR 0.000010
INFO - Training [52][   80/  391]   Loss 0.167188   Top1 93.925781   Top5 99.951172   BatchTime 0.121472   LR 0.000010
INFO - Training [52][  100/  391]   Loss 0.169497   Top1 93.898438   Top5 99.937500   BatchTime 0.117382   LR 0.000010
INFO - Training [52][  120/  391]   Loss 0.172948   Top1 93.756510   Top5 99.934896   BatchTime 0.114847   LR 0.000010
INFO - Training [52][  140/  391]   Loss 0.170569   Top1 93.911830   Top5 99.933036   BatchTime 0.113188   LR 0.000010
INFO - Training [52][  160/  391]   Loss 0.170175   Top1 93.886719   Top5 99.936523   BatchTime 0.111641   LR 0.000010
INFO - Training [52][  180/  391]   Loss 0.167838   Top1 93.953993   Top5 99.939236   BatchTime 0.111110   LR 0.000010
INFO - Training [52][  200/  391]   Loss 0.168528   Top1 93.937500   Top5 99.933594   BatchTime 0.110047   LR 0.000010
INFO - Training [52][  220/  391]   Loss 0.169088   Top1 93.892045   Top5 99.928977   BatchTime 0.109179   LR 0.000010
INFO - Training [52][  240/  391]   Loss 0.168960   Top1 93.909505   Top5 99.928385   BatchTime 0.108488   LR 0.000010
INFO - Training [52][  260/  391]   Loss 0.168905   Top1 93.915264   Top5 99.921875   BatchTime 0.107928   LR 0.000010
INFO - Training [52][  280/  391]   Loss 0.167237   Top1 93.992746   Top5 99.919085   BatchTime 0.107396   LR 0.000010
INFO - Training [52][  300/  391]   Loss 0.166821   Top1 94.013021   Top5 99.921875   BatchTime 0.106902   LR 0.000010
INFO - Training [52][  320/  391]   Loss 0.165920   Top1 94.057617   Top5 99.924316   BatchTime 0.106456   LR 0.000010
INFO - Training [52][  340/  391]   Loss 0.165316   Top1 94.073989   Top5 99.926471   BatchTime 0.106102   LR 0.000010
INFO - Training [52][  360/  391]   Loss 0.163854   Top1 94.108073   Top5 99.928385   BatchTime 0.105648   LR 0.000010
INFO - Training [52][  380/  391]   Loss 0.164263   Top1 94.103618   Top5 99.928043   BatchTime 0.105215   LR 0.000010
INFO - ==> Top1: 94.098    Top5: 99.928    Loss: 0.165
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [52][   20/   79]   Loss 0.391449   Top1 88.359375   Top5 99.453125   BatchTime 0.123898
INFO - Validation [52][   40/   79]   Loss 0.383770   Top1 88.808594   Top5 99.375000   BatchTime 0.077966
INFO - Validation [52][   60/   79]   Loss 0.374624   Top1 89.049479   Top5 99.453125   BatchTime 0.063680
INFO - ==> Top1: 89.150    Top5: 99.560    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  53
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [53][   20/  391]   Loss 0.170190   Top1 93.398438   Top5 99.882812   BatchTime 0.187170   LR 0.000010
INFO - Training [53][   40/  391]   Loss 0.161771   Top1 94.062500   Top5 99.921875   BatchTime 0.143917   LR 0.000010
INFO - Training [53][   60/  391]   Loss 0.158240   Top1 94.440104   Top5 99.934896   BatchTime 0.129467   LR 0.000010
INFO - Training [53][   80/  391]   Loss 0.159513   Top1 94.296875   Top5 99.921875   BatchTime 0.122152   LR 0.000010
INFO - Training [53][  100/  391]   Loss 0.161449   Top1 94.226562   Top5 99.914062   BatchTime 0.117895   LR 0.000010
INFO - Training [53][  120/  391]   Loss 0.160251   Top1 94.199219   Top5 99.921875   BatchTime 0.114903   LR 0.000010
INFO - Training [53][  140/  391]   Loss 0.160642   Top1 94.213170   Top5 99.921875   BatchTime 0.112843   LR 0.000010
INFO - Training [53][  160/  391]   Loss 0.164130   Top1 94.091797   Top5 99.916992   BatchTime 0.111309   LR 0.000010
INFO - Training [53][  180/  391]   Loss 0.166453   Top1 94.032118   Top5 99.921875   BatchTime 0.110168   LR 0.000010
INFO - Training [53][  200/  391]   Loss 0.165692   Top1 94.078125   Top5 99.917969   BatchTime 0.109338   LR 0.000010
INFO - Training [53][  220/  391]   Loss 0.164904   Top1 94.140625   Top5 99.918324   BatchTime 0.108585   LR 0.000010
INFO - Training [53][  240/  391]   Loss 0.163841   Top1 94.179688   Top5 99.915365   BatchTime 0.108042   LR 0.000010
INFO - Training [53][  260/  391]   Loss 0.165605   Top1 94.107572   Top5 99.918870   BatchTime 0.107496   LR 0.000010
INFO - Training [53][  280/  391]   Loss 0.164100   Top1 94.135045   Top5 99.924665   BatchTime 0.106999   LR 0.000010
INFO - Training [53][  300/  391]   Loss 0.164494   Top1 94.156250   Top5 99.916667   BatchTime 0.106589   LR 0.000010
INFO - Training [53][  320/  391]   Loss 0.164207   Top1 94.157715   Top5 99.919434   BatchTime 0.106212   LR 0.000010
INFO - Training [53][  340/  391]   Loss 0.163934   Top1 94.159007   Top5 99.912684   BatchTime 0.105869   LR 0.000010
INFO - Training [53][  360/  391]   Loss 0.164914   Top1 94.114583   Top5 99.911024   BatchTime 0.105435   LR 0.000010
INFO - Training [53][  380/  391]   Loss 0.165099   Top1 94.113898   Top5 99.913651   BatchTime 0.105030   LR 0.000010
INFO - ==> Top1: 94.112    Top5: 99.910    Loss: 0.165
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [53][   20/   79]   Loss 0.389490   Top1 88.593750   Top5 99.414062   BatchTime 0.122841
INFO - Validation [53][   40/   79]   Loss 0.385097   Top1 88.710938   Top5 99.355469   BatchTime 0.075296
INFO - Validation [53][   60/   79]   Loss 0.376734   Top1 88.763021   Top5 99.466146   BatchTime 0.062842
INFO - ==> Top1: 88.810    Top5: 99.580    Loss: 0.374
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  54
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [54][   20/  391]   Loss 0.177012   Top1 93.554688   Top5 99.921875   BatchTime 0.181727   LR 0.000010
INFO - Training [54][   40/  391]   Loss 0.173866   Top1 93.828125   Top5 99.921875   BatchTime 0.141095   LR 0.000010
INFO - Training [54][   60/  391]   Loss 0.168783   Top1 94.036458   Top5 99.934896   BatchTime 0.127663   LR 0.000010
INFO - Training [54][   80/  391]   Loss 0.171429   Top1 93.818359   Top5 99.951172   BatchTime 0.120831   LR 0.000010
INFO - Training [54][  100/  391]   Loss 0.169987   Top1 93.953125   Top5 99.960938   BatchTime 0.116840   LR 0.000010
INFO - Training [54][  120/  391]   Loss 0.169630   Top1 93.984375   Top5 99.967448   BatchTime 0.114179   LR 0.000010
INFO - Training [54][  140/  391]   Loss 0.169670   Top1 94.001116   Top5 99.955357   BatchTime 0.112254   LR 0.000010
INFO - Training [54][  160/  391]   Loss 0.166026   Top1 94.111328   Top5 99.960938   BatchTime 0.110887   LR 0.000010
INFO - Training [54][  180/  391]   Loss 0.166973   Top1 94.075521   Top5 99.960938   BatchTime 0.109727   LR 0.000010
INFO - Training [54][  200/  391]   Loss 0.167683   Top1 94.089844   Top5 99.953125   BatchTime 0.108795   LR 0.000010
INFO - Training [54][  220/  391]   Loss 0.167018   Top1 94.133523   Top5 99.953835   BatchTime 0.108447   LR 0.000010
INFO - Training [54][  240/  391]   Loss 0.166573   Top1 94.169922   Top5 99.957682   BatchTime 0.107775   LR 0.000010
INFO - Training [54][  260/  391]   Loss 0.167531   Top1 94.131611   Top5 99.957933   BatchTime 0.107195   LR 0.000010
INFO - Training [54][  280/  391]   Loss 0.168287   Top1 94.095982   Top5 99.955357   BatchTime 0.106749   LR 0.000010
INFO - Training [54][  300/  391]   Loss 0.166993   Top1 94.127604   Top5 99.953125   BatchTime 0.106351   LR 0.000010
INFO - Training [54][  320/  391]   Loss 0.166813   Top1 94.123535   Top5 99.956055   BatchTime 0.105935   LR 0.000010
INFO - Training [54][  340/  391]   Loss 0.166902   Top1 94.110754   Top5 99.956342   BatchTime 0.105572   LR 0.000010
INFO - Training [54][  360/  391]   Loss 0.165771   Top1 94.127604   Top5 99.958767   BatchTime 0.105147   LR 0.000010
INFO - Training [54][  380/  391]   Loss 0.166564   Top1 94.111842   Top5 99.956826   BatchTime 0.104841   LR 0.000010
INFO - ==> Top1: 94.116    Top5: 99.958    Loss: 0.167
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [54][   20/   79]   Loss 0.391292   Top1 88.398438   Top5 99.531250   BatchTime 0.121150
INFO - Validation [54][   40/   79]   Loss 0.384351   Top1 88.769531   Top5 99.394531   BatchTime 0.073579
INFO - Validation [54][   60/   79]   Loss 0.374172   Top1 88.932292   Top5 99.466146   BatchTime 0.060984
INFO - ==> Top1: 88.960    Top5: 99.570    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  55
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [55][   20/  391]   Loss 0.180679   Top1 93.593750   Top5 99.960938   BatchTime 0.190306   LR 0.000010
INFO - Training [55][   40/  391]   Loss 0.177088   Top1 93.671875   Top5 99.960938   BatchTime 0.145706   LR 0.000010
INFO - Training [55][   60/  391]   Loss 0.169571   Top1 93.880208   Top5 99.960938   BatchTime 0.130855   LR 0.000010
INFO - Training [55][   80/  391]   Loss 0.168524   Top1 93.974609   Top5 99.960938   BatchTime 0.123582   LR 0.000010
INFO - Training [55][  100/  391]   Loss 0.167541   Top1 93.960938   Top5 99.960938   BatchTime 0.119093   LR 0.000010
INFO - Training [55][  120/  391]   Loss 0.168709   Top1 93.945312   Top5 99.947917   BatchTime 0.116159   LR 0.000010
INFO - Training [55][  140/  391]   Loss 0.168714   Top1 93.906250   Top5 99.955357   BatchTime 0.113898   LR 0.000010
INFO - Training [55][  160/  391]   Loss 0.167612   Top1 93.925781   Top5 99.951172   BatchTime 0.112222   LR 0.000010
INFO - Training [55][  180/  391]   Loss 0.165195   Top1 94.053819   Top5 99.952257   BatchTime 0.110787   LR 0.000010
INFO - Training [55][  200/  391]   Loss 0.165288   Top1 94.015625   Top5 99.953125   BatchTime 0.109781   LR 0.000010
INFO - Training [55][  220/  391]   Loss 0.165673   Top1 93.980824   Top5 99.957386   BatchTime 0.108907   LR 0.000010
INFO - Training [55][  240/  391]   Loss 0.166035   Top1 93.958333   Top5 99.951172   BatchTime 0.108093   LR 0.000010
INFO - Training [55][  260/  391]   Loss 0.167316   Top1 93.909255   Top5 99.948918   BatchTime 0.107500   LR 0.000010
INFO - Training [55][  280/  391]   Loss 0.166660   Top1 93.925781   Top5 99.949777   BatchTime 0.107062   LR 0.000010
INFO - Training [55][  300/  391]   Loss 0.166651   Top1 93.924479   Top5 99.953125   BatchTime 0.106577   LR 0.000010
INFO - Training [55][  320/  391]   Loss 0.166695   Top1 93.933105   Top5 99.956055   BatchTime 0.106178   LR 0.000010
INFO - Training [55][  340/  391]   Loss 0.166377   Top1 93.949908   Top5 99.954044   BatchTime 0.105755   LR 0.000010
INFO - Training [55][  360/  391]   Loss 0.167294   Top1 93.914931   Top5 99.956597   BatchTime 0.105321   LR 0.000010
INFO - Training [55][  380/  391]   Loss 0.166763   Top1 93.947368   Top5 99.954770   BatchTime 0.104893   LR 0.000010
INFO - ==> Top1: 93.950    Top5: 99.954    Loss: 0.167
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [55][   20/   79]   Loss 0.388625   Top1 88.632812   Top5 99.492188   BatchTime 0.121316
INFO - Validation [55][   40/   79]   Loss 0.385975   Top1 88.671875   Top5 99.453125   BatchTime 0.075879
INFO - Validation [55][   60/   79]   Loss 0.376872   Top1 88.841146   Top5 99.531250   BatchTime 0.062243
INFO - ==> Top1: 88.910    Top5: 99.630    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  56
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [56][   20/  391]   Loss 0.166262   Top1 94.257812   Top5 99.921875   BatchTime 0.189335   LR 0.000010
INFO - Training [56][   40/  391]   Loss 0.166272   Top1 93.886719   Top5 99.941406   BatchTime 0.144544   LR 0.000010
INFO - Training [56][   60/  391]   Loss 0.160438   Top1 94.088542   Top5 99.934896   BatchTime 0.129741   LR 0.000010
INFO - Training [56][   80/  391]   Loss 0.163377   Top1 94.121094   Top5 99.931641   BatchTime 0.122156   LR 0.000010
INFO - Training [56][  100/  391]   Loss 0.163984   Top1 94.101562   Top5 99.921875   BatchTime 0.117840   LR 0.000010
INFO - Training [56][  120/  391]   Loss 0.161285   Top1 94.173177   Top5 99.928385   BatchTime 0.114801   LR 0.000010
INFO - Training [56][  140/  391]   Loss 0.166685   Top1 94.012277   Top5 99.927455   BatchTime 0.112711   LR 0.000010
INFO - Training [56][  160/  391]   Loss 0.166752   Top1 93.989258   Top5 99.931641   BatchTime 0.111049   LR 0.000010
INFO - Training [56][  180/  391]   Loss 0.166227   Top1 94.006076   Top5 99.921875   BatchTime 0.109830   LR 0.000010
INFO - Training [56][  200/  391]   Loss 0.163541   Top1 94.093750   Top5 99.929688   BatchTime 0.108827   LR 0.000010
INFO - Training [56][  220/  391]   Loss 0.164151   Top1 94.083807   Top5 99.928977   BatchTime 0.108075   LR 0.000010
INFO - Training [56][  240/  391]   Loss 0.164227   Top1 94.062500   Top5 99.931641   BatchTime 0.107454   LR 0.000010
INFO - Training [56][  260/  391]   Loss 0.165752   Top1 94.014423   Top5 99.933894   BatchTime 0.106904   LR 0.000010
INFO - Training [56][  280/  391]   Loss 0.165820   Top1 93.995536   Top5 99.930246   BatchTime 0.106422   LR 0.000010
INFO - Training [56][  300/  391]   Loss 0.165532   Top1 94.002604   Top5 99.929688   BatchTime 0.106436   LR 0.000010
INFO - Training [56][  320/  391]   Loss 0.164823   Top1 94.020996   Top5 99.929199   BatchTime 0.106063   LR 0.000010
INFO - Training [56][  340/  391]   Loss 0.165040   Top1 93.998162   Top5 99.933364   BatchTime 0.105678   LR 0.000010
INFO - Training [56][  360/  391]   Loss 0.165851   Top1 93.964844   Top5 99.932726   BatchTime 0.105189   LR 0.000010
INFO - Training [56][  380/  391]   Loss 0.165160   Top1 93.998766   Top5 99.934211   BatchTime 0.104770   LR 0.000010
INFO - ==> Top1: 93.990    Top5: 99.936    Loss: 0.165
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [56][   20/   79]   Loss 0.395219   Top1 88.554688   Top5 99.453125   BatchTime 0.124977
INFO - Validation [56][   40/   79]   Loss 0.385819   Top1 88.847656   Top5 99.394531   BatchTime 0.078767
INFO - Validation [56][   60/   79]   Loss 0.377631   Top1 88.919271   Top5 99.492188   BatchTime 0.064308
INFO - ==> Top1: 89.020    Top5: 99.590    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  57
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [57][   20/  391]   Loss 0.166778   Top1 93.984375   Top5 99.960938   BatchTime 0.192290   LR 0.000010
INFO - Training [57][   40/  391]   Loss 0.176202   Top1 93.691406   Top5 99.941406   BatchTime 0.145790   LR 0.000010
INFO - Training [57][   60/  391]   Loss 0.166340   Top1 93.971354   Top5 99.960938   BatchTime 0.130471   LR 0.000010
INFO - Training [57][   80/  391]   Loss 0.165863   Top1 93.964844   Top5 99.970703   BatchTime 0.122828   LR 0.000010
INFO - Training [57][  100/  391]   Loss 0.167900   Top1 93.851562   Top5 99.976562   BatchTime 0.118446   LR 0.000010
INFO - Training [57][  120/  391]   Loss 0.169225   Top1 93.932292   Top5 99.973958   BatchTime 0.115477   LR 0.000010
INFO - Training [57][  140/  391]   Loss 0.168678   Top1 93.917411   Top5 99.960938   BatchTime 0.113383   LR 0.000010
INFO - Training [57][  160/  391]   Loss 0.168223   Top1 93.906250   Top5 99.956055   BatchTime 0.111867   LR 0.000010
INFO - Training [57][  180/  391]   Loss 0.168071   Top1 93.906250   Top5 99.956597   BatchTime 0.110849   LR 0.000010
INFO - Training [57][  200/  391]   Loss 0.166369   Top1 93.992188   Top5 99.960938   BatchTime 0.109915   LR 0.000010
INFO - Training [57][  220/  391]   Loss 0.165484   Top1 93.995028   Top5 99.957386   BatchTime 0.109124   LR 0.000010
INFO - Training [57][  240/  391]   Loss 0.167228   Top1 93.938802   Top5 99.957682   BatchTime 0.108445   LR 0.000010
INFO - Training [57][  260/  391]   Loss 0.166039   Top1 93.969351   Top5 99.954928   BatchTime 0.107945   LR 0.000010
INFO - Training [57][  280/  391]   Loss 0.165072   Top1 93.989955   Top5 99.949777   BatchTime 0.107424   LR 0.000010
INFO - Training [57][  300/  391]   Loss 0.164565   Top1 93.989583   Top5 99.947917   BatchTime 0.106978   LR 0.000010
INFO - Training [57][  320/  391]   Loss 0.163692   Top1 94.033203   Top5 99.948730   BatchTime 0.106520   LR 0.000010
INFO - Training [57][  340/  391]   Loss 0.163618   Top1 94.044118   Top5 99.944853   BatchTime 0.106108   LR 0.000010
INFO - Training [57][  360/  391]   Loss 0.164103   Top1 94.003906   Top5 99.945747   BatchTime 0.105628   LR 0.000010
INFO - Training [57][  380/  391]   Loss 0.164615   Top1 93.974095   Top5 99.946546   BatchTime 0.105198   LR 0.000010
INFO - ==> Top1: 93.950    Top5: 99.948    Loss: 0.165
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [57][   20/   79]   Loss 0.394929   Top1 88.203125   Top5 99.570312   BatchTime 0.123746
INFO - Validation [57][   40/   79]   Loss 0.388346   Top1 88.554688   Top5 99.414062   BatchTime 0.077474
INFO - Validation [57][   60/   79]   Loss 0.378294   Top1 88.710938   Top5 99.479167   BatchTime 0.063462
INFO - ==> Top1: 88.890    Top5: 99.580    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  58
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [58][   20/  391]   Loss 0.150499   Top1 94.687500   Top5 99.960938   BatchTime 0.187417   LR 0.000010
INFO - Training [58][   40/  391]   Loss 0.159593   Top1 94.199219   Top5 99.980469   BatchTime 0.143791   LR 0.000010
INFO - Training [58][   60/  391]   Loss 0.158578   Top1 94.218750   Top5 99.986979   BatchTime 0.129498   LR 0.000010
INFO - Training [58][   80/  391]   Loss 0.159301   Top1 94.140625   Top5 99.980469   BatchTime 0.122336   LR 0.000010
INFO - Training [58][  100/  391]   Loss 0.162394   Top1 94.000000   Top5 99.976562   BatchTime 0.117950   LR 0.000010
INFO - Training [58][  120/  391]   Loss 0.164182   Top1 93.971354   Top5 99.980469   BatchTime 0.114483   LR 0.000010
INFO - Training [58][  140/  391]   Loss 0.164632   Top1 94.068080   Top5 99.966518   BatchTime 0.112363   LR 0.000010
INFO - Training [58][  160/  391]   Loss 0.166076   Top1 94.077148   Top5 99.970703   BatchTime 0.110840   LR 0.000010
INFO - Training [58][  180/  391]   Loss 0.164443   Top1 94.144965   Top5 99.965278   BatchTime 0.109734   LR 0.000010
INFO - Training [58][  200/  391]   Loss 0.163924   Top1 94.156250   Top5 99.960938   BatchTime 0.108804   LR 0.000010
INFO - Training [58][  220/  391]   Loss 0.163473   Top1 94.222301   Top5 99.960938   BatchTime 0.108056   LR 0.000010
INFO - Training [58][  240/  391]   Loss 0.163004   Top1 94.254557   Top5 99.951172   BatchTime 0.107416   LR 0.000010
INFO - Training [58][  260/  391]   Loss 0.162831   Top1 94.218750   Top5 99.951923   BatchTime 0.106882   LR 0.000010
INFO - Training [58][  280/  391]   Loss 0.163429   Top1 94.199219   Top5 99.955357   BatchTime 0.106449   LR 0.000010
INFO - Training [58][  300/  391]   Loss 0.164330   Top1 94.130208   Top5 99.958333   BatchTime 0.106073   LR 0.000010
INFO - Training [58][  320/  391]   Loss 0.165300   Top1 94.089355   Top5 99.953613   BatchTime 0.105756   LR 0.000010
INFO - Training [58][  340/  391]   Loss 0.164640   Top1 94.122243   Top5 99.951746   BatchTime 0.105446   LR 0.000010
INFO - Training [58][  360/  391]   Loss 0.164348   Top1 94.134115   Top5 99.947917   BatchTime 0.104984   LR 0.000010
INFO - Training [58][  380/  391]   Loss 0.164757   Top1 94.111842   Top5 99.950658   BatchTime 0.104817   LR 0.000010
INFO - ==> Top1: 94.124    Top5: 99.952    Loss: 0.165
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [58][   20/   79]   Loss 0.390119   Top1 88.203125   Top5 99.453125   BatchTime 0.121766
INFO - Validation [58][   40/   79]   Loss 0.384501   Top1 88.632812   Top5 99.414062   BatchTime 0.076161
INFO - Validation [58][   60/   79]   Loss 0.374871   Top1 88.893229   Top5 99.518229   BatchTime 0.062557
INFO - ==> Top1: 89.050    Top5: 99.590    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  59
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [59][   20/  391]   Loss 0.148805   Top1 94.570312   Top5 99.960938   BatchTime 0.188163   LR 0.000010
INFO - Training [59][   40/  391]   Loss 0.159914   Top1 94.238281   Top5 99.921875   BatchTime 0.144877   LR 0.000010
INFO - Training [59][   60/  391]   Loss 0.161609   Top1 94.257812   Top5 99.947917   BatchTime 0.129922   LR 0.000010
INFO - Training [59][   80/  391]   Loss 0.159465   Top1 94.296875   Top5 99.951172   BatchTime 0.122618   LR 0.000010
INFO - Training [59][  100/  391]   Loss 0.160534   Top1 94.265625   Top5 99.937500   BatchTime 0.118199   LR 0.000010
INFO - Training [59][  120/  391]   Loss 0.160682   Top1 94.270833   Top5 99.934896   BatchTime 0.115241   LR 0.000010
INFO - Training [59][  140/  391]   Loss 0.159886   Top1 94.246652   Top5 99.944196   BatchTime 0.113148   LR 0.000010
INFO - Training [59][  160/  391]   Loss 0.161616   Top1 94.184570   Top5 99.941406   BatchTime 0.111576   LR 0.000010
INFO - Training [59][  180/  391]   Loss 0.163975   Top1 94.162326   Top5 99.934896   BatchTime 0.110502   LR 0.000010
INFO - Training [59][  200/  391]   Loss 0.165519   Top1 94.093750   Top5 99.929688   BatchTime 0.109491   LR 0.000010
INFO - Training [59][  220/  391]   Loss 0.164850   Top1 94.122869   Top5 99.928977   BatchTime 0.108714   LR 0.000010
INFO - Training [59][  240/  391]   Loss 0.164911   Top1 94.163411   Top5 99.931641   BatchTime 0.108056   LR 0.000010
INFO - Training [59][  260/  391]   Loss 0.164776   Top1 94.170673   Top5 99.930889   BatchTime 0.107505   LR 0.000010
INFO - Training [59][  280/  391]   Loss 0.164422   Top1 94.202009   Top5 99.930246   BatchTime 0.107049   LR 0.000010
INFO - Training [59][  300/  391]   Loss 0.163991   Top1 94.234375   Top5 99.927083   BatchTime 0.106693   LR 0.000010
INFO - Training [59][  320/  391]   Loss 0.163827   Top1 94.233398   Top5 99.926758   BatchTime 0.106359   LR 0.000010
INFO - Training [59][  340/  391]   Loss 0.165012   Top1 94.191176   Top5 99.928768   BatchTime 0.105941   LR 0.000010
INFO - Training [59][  360/  391]   Loss 0.165343   Top1 94.179688   Top5 99.928385   BatchTime 0.105483   LR 0.000010
INFO - Training [59][  380/  391]   Loss 0.165710   Top1 94.192023   Top5 99.928043   BatchTime 0.105045   LR 0.000010
INFO - ==> Top1: 94.172    Top5: 99.928    Loss: 0.166
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [59][   20/   79]   Loss 0.388225   Top1 88.125000   Top5 99.570312   BatchTime 0.124461
INFO - Validation [59][   40/   79]   Loss 0.383094   Top1 88.730469   Top5 99.453125   BatchTime 0.078379
INFO - Validation [59][   60/   79]   Loss 0.377255   Top1 88.841146   Top5 99.531250   BatchTime 0.064056
INFO - ==> Top1: 88.930    Top5: 99.620    Loss: 0.374
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 89.820   Top5: 99.610] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.770   Top5: 99.590] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 89.750   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch60_20221103-224106/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 0.388225   Top1 88.125000   Top5 99.570312   BatchTime 0.123245
INFO - Validation [   40/   79]   Loss 0.383094   Top1 88.730469   Top5 99.453125   BatchTime 0.079220
INFO - Validation [   60/   79]   Loss 0.377255   Top1 88.841146   Top5 99.531250   BatchTime 0.067295
INFO - ==> Top1: 88.930    Top5: 99.620    Loss: 0.374
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/pruned_model/MobileNetv2_cifar10_a8w8_20_epoch60_checkpoint.pth.tar
INFO - Program completed successfully ... exiting ...
INFO - If you have any questions or suggestions, please visit: github.com/zhutmost/lsq-net