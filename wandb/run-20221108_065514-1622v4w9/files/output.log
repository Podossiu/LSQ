INFO - Log file for this run: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_20_epoch80_20221108-065519/MobileNetv2_imagenet_a8w8_20_epoch80_20221108-065519.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_20_epoch80_20221108-065519/tb_runs
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/LSQ/quan/quantizer/lsq.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.005
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Validation [   20/  391]   Loss 12.927401   Top1 0.000000   Top5 0.000000   BatchTime 0.717409
INFO - Validation [   40/  391]   Loss 13.225690   Top1 0.000000   Top5 0.000000   BatchTime 0.426554
INFO - Validation [   60/  391]   Loss 12.783485   Top1 0.000000   Top5 0.000000   BatchTime 0.330267
INFO - Validation [   80/  391]   Loss 11.984020   Top1 0.000000   Top5 0.488281   BatchTime 0.296917
INFO - Validation [  100/  391]   Loss 11.471709   Top1 0.000000   Top5 0.781250   BatchTime 0.265959
INFO - Validation [  120/  391]   Loss 11.341164   Top1 0.000000   Top5 0.976562   BatchTime 0.243945
INFO - Validation [  140/  391]   Loss 11.538913   Top1 0.000000   Top5 0.837054   BatchTime 0.228824
INFO - Validation [  160/  391]   Loss 11.389241   Top1 0.000000   Top5 0.732422   BatchTime 0.217403
INFO - Validation [  180/  391]   Loss 11.399283   Top1 0.000000   Top5 0.651042   BatchTime 0.209476
INFO - Validation [  200/  391]   Loss 11.465818   Top1 0.000000   Top5 0.585938   BatchTime 0.203008
INFO - Validation [  220/  391]   Loss 11.544956   Top1 0.000000   Top5 0.532670   BatchTime 0.198070
INFO - Validation [  240/  391]   Loss 11.596657   Top1 0.000000   Top5 0.488281   BatchTime 0.193647
INFO - Validation [  260/  391]   Loss 11.627968   Top1 0.000000   Top5 0.450721   BatchTime 0.189791
INFO - Validation [  280/  391]   Loss 11.677904   Top1 0.000000   Top5 0.418527   BatchTime 0.186832
INFO - Validation [  300/  391]   Loss 11.678717   Top1 0.130208   Top5 0.520833   BatchTime 0.184693
INFO - Validation [  320/  391]   Loss 11.730051   Top1 0.122070   Top5 0.488281   BatchTime 0.182550
INFO - Validation [  340/  391]   Loss 11.704230   Top1 0.114890   Top5 0.574449   BatchTime 0.179979
INFO - Validation [  360/  391]   Loss 11.660352   Top1 0.108507   Top5 0.542535   BatchTime 0.177135
INFO - Validation [  380/  391]   Loss 11.746606   Top1 0.102796   Top5 0.513980   BatchTime 0.177893
INFO - ==> Top1: 0.100    Top5: 0.500    Loss: 11.797
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.100   Top5: 0.500] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.0592, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0884, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0873, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0421, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0468, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0864, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0293, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0313, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0247, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0497, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1031, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0292, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0191, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1200, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0159, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1322, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0231, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0530, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0877, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1025, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0150, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0124, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1082, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1123, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0210, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0345, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0678, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0117, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0132, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0801, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0093, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0144, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0913, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0092, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0392, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0510, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0153, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0139, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0646, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0119, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0158, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0827, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0053, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0314, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0436, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 5.716614   Top1 7.031250   Top5 17.500000   BatchTime 0.699021   LR 0.005000
INFO - Training [0][   40/10010]   Loss 5.431418   Top1 8.828125   Top5 21.035156   BatchTime 0.519101   LR 0.005000
INFO - Training [0][   60/10010]   Loss 5.161431   Top1 10.950521   Top5 25.026042   BatchTime 0.480647   LR 0.005000
INFO - Training [0][   80/10010]   Loss 4.886113   Top1 13.388672   Top5 29.287109   BatchTime 0.459434   LR 0.005000
INFO - Training [0][  100/10010]   Loss 4.662987   Top1 15.632812   Top5 33.000000   BatchTime 0.436869   LR 0.005000
INFO - Training [0][  120/10010]   Loss 4.470284   Top1 17.942708   Top5 36.334635   BatchTime 0.422034   LR 0.005000
INFO - Training [0][  140/10010]   Loss 4.313562   Top1 19.614955   Top5 38.956473   BatchTime 0.411019   LR 0.005000
INFO - Training [0][  160/10010]   Loss 4.185475   Top1 21.225586   Top5 40.986328   BatchTime 0.402613   LR 0.005000
INFO - Training [0][  180/10010]   Loss 4.080880   Top1 22.400174   Top5 42.808160   BatchTime 0.397468   LR 0.005000
INFO - Training [0][  200/10010]   Loss 3.982106   Top1 23.632812   Top5 44.488281   BatchTime 0.396976   LR 0.005000
INFO - Training [0][  220/10010]   Loss 3.884775   Top1 24.946733   Top5 46.186080   BatchTime 0.391604   LR 0.005000
INFO - Training [0][  240/10010]   Loss 3.799363   Top1 26.158854   Top5 47.591146   BatchTime 0.387584   LR 0.005000
INFO - Training [0][  260/10010]   Loss 3.726353   Top1 27.235577   Top5 48.933293   BatchTime 0.383533   LR 0.005000
INFO - Training [0][  280/10010]   Loss 3.669733   Top1 28.024554   Top5 49.896763   BatchTime 0.384312   LR 0.005000
INFO - Training [0][  300/10010]   Loss 3.598469   Top1 29.013021   Top5 51.109375   BatchTime 0.381123   LR 0.005000
INFO - Training [0][  320/10010]   Loss 3.530045   Top1 29.904785   Top5 52.270508   BatchTime 0.378578   LR 0.005000
INFO - Training [0][  340/10010]   Loss 3.476684   Top1 30.698529   Top5 53.212316   BatchTime 0.376067   LR 0.005000
INFO - Training [0][  360/10010]   Loss 3.426948   Top1 31.371528   Top5 54.042969   BatchTime 0.373730   LR 0.005000
INFO - Training [0][  380/10010]   Loss 3.383175   Top1 32.006579   Top5 54.798520   BatchTime 0.371895   LR 0.005000
INFO - Training [0][  400/10010]   Loss 3.339035   Top1 32.705078   Top5 55.554688   BatchTime 0.370368   LR 0.005000
INFO - Training [0][  420/10010]   Loss 3.294660   Top1 33.446801   Top5 56.298363   BatchTime 0.368550   LR 0.005000
INFO - Training [0][  440/10010]   Loss 3.254061   Top1 34.078480   Top5 56.944247   BatchTime 0.367229   LR 0.005000
INFO - Training [0][  460/10010]   Loss 3.214631   Top1 34.636549   Top5 57.620584   BatchTime 0.365929   LR 0.005000
INFO - Training [0][  480/10010]   Loss 3.177699   Top1 35.214844   Top5 58.245443   BatchTime 0.364654   LR 0.005000
INFO - Training [0][  500/10010]   Loss 3.141468   Top1 35.742188   Top5 58.889063   BatchTime 0.363367   LR 0.005000
INFO - Training [0][  520/10010]   Loss 3.110563   Top1 36.192909   Top5 59.408053   BatchTime 0.362207   LR 0.005000
INFO - Training [0][  540/10010]   Loss 3.084196   Top1 36.633391   Top5 59.835069   BatchTime 0.361159   LR 0.005000
INFO - Training [0][  560/10010]   Loss 3.056775   Top1 37.078683   Top5 60.297154   BatchTime 0.360291   LR 0.005000
INFO - Training [0][  580/10010]   Loss 3.029239   Top1 37.500000   Top5 60.767780   BatchTime 0.359339   LR 0.005000
INFO - Training [0][  600/10010]   Loss 3.003088   Top1 37.867188   Top5 61.179688   BatchTime 0.358509   LR 0.005000
INFO - Training [0][  620/10010]   Loss 2.977125   Top1 38.272429   Top5 61.585181   BatchTime 0.357845   LR 0.005000
INFO - Training [0][  640/10010]   Loss 2.952933   Top1 38.637695   Top5 62.023926   BatchTime 0.356940   LR 0.005000
INFO - Training [0][  660/10010]   Loss 2.929923   Top1 39.038826   Top5 62.379261   BatchTime 0.356251   LR 0.005000
INFO - Training [0][  680/10010]   Loss 2.908573   Top1 39.403722   Top5 62.753906   BatchTime 0.355712   LR 0.005000
INFO - Training [0][  700/10010]   Loss 2.886426   Top1 39.725446   Top5 63.135045   BatchTime 0.355063   LR 0.005000
INFO - Training [0][  720/10010]   Loss 2.862871   Top1 40.094401   Top5 63.522135   BatchTime 0.354613   LR 0.005000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff0de01dd30>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 62, in train
    outputs = model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/model/mobilenet.py", line 142, in forward
    x = self.features(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/model/mobilenet.py", line 105, in forward
    return self.conv(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/func.py", line 22, in forward
    quantized_weight = self.quan_w_fn(self.weight)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/quantizer/lsq.py", line 118, in forward
    self.p.data.clamp_(0.,self.c.data)
KeyboardInterrupt