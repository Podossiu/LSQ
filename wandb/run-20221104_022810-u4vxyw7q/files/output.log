INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811.log
2022-11-04 02:28:11.736656: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-04 02:28:11.865406: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-04 02:28:12.268902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-04 02:28:12.269005: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-04 02:28:12.269018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/tb_runs
Files already downloaded and verified
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
Files already downloaded and verified
hello
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `cifar10` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): QuanConv2d(
          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): IdentityQuan()
          (quan_a_fn): IdentityQuan()
        )
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv): Sequential(
      (0): QuanConv2d(
        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
        (quan_w_fn): SLsqQuan()
        (quan_a_fn): LsqQuan()
      )
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): QuanLinear(
      in_features=1280, out_features=10, bias=True
      (quan_w_fn): IdentityQuan()
      (quan_a_fn): IdentityQuan()
    )
  )
)
INFO - Validation [   20/   79]   Loss 2.545371   Top1 10.429688   Top5 49.101562   BatchTime 0.235371
INFO - Validation [   40/   79]   Loss 2.549466   Top1 10.175781   Top5 49.941406   BatchTime 0.149830
INFO - Validation [   60/   79]   Loss 2.541519   Top1 10.117188   Top5 50.377604   BatchTime 0.120921
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.546
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - >>>>>>>> Epoch   0
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [0][   20/  391]   Loss 1.636065   Top1 66.953125   Top5 96.484375   BatchTime 0.235209   LR 0.010000
INFO - Training [0][   40/  391]   Loss 1.340914   Top1 68.593750   Top5 97.070312   BatchTime 0.179419   LR 0.010000
INFO - Training [0][   60/  391]   Loss 1.156253   Top1 70.169271   Top5 97.552083   BatchTime 0.160654   LR 0.010000
INFO - Training [0][   80/  391]   Loss 1.037492   Top1 71.718750   Top5 97.910156   BatchTime 0.151152   LR 0.010000
INFO - Training [0][  100/  391]   Loss 0.945780   Top1 73.328125   Top5 98.109375   BatchTime 0.141273   LR 0.010000
INFO - Training [0][  120/  391]   Loss 0.880523   Top1 74.596354   Top5 98.287760   BatchTime 0.132085   LR 0.010000
INFO - Training [0][  140/  391]   Loss 0.826115   Top1 75.686384   Top5 98.398438   BatchTime 0.126523   LR 0.010000
INFO - Training [0][  160/  391]   Loss 0.783328   Top1 76.508789   Top5 98.544922   BatchTime 0.120151   LR 0.010000
INFO - Training [0][  180/  391]   Loss 0.747293   Top1 77.361111   Top5 98.615451   BatchTime 0.120218   LR 0.010000
INFO - Training [0][  200/  391]   Loss 0.720536   Top1 77.937500   Top5 98.687500   BatchTime 0.120589   LR 0.010000
INFO - Training [0][  220/  391]   Loss 0.697475   Top1 78.458807   Top5 98.735795   BatchTime 0.120827   LR 0.010000
INFO - Training [0][  240/  391]   Loss 0.675820   Top1 79.003906   Top5 98.789062   BatchTime 0.121142   LR 0.010000
INFO - Training [0][  260/  391]   Loss 0.655983   Top1 79.459135   Top5 98.858173   BatchTime 0.121282   LR 0.010000
INFO - Training [0][  280/  391]   Loss 0.640203   Top1 79.838170   Top5 98.883929   BatchTime 0.121502   LR 0.010000
INFO - Training [0][  300/  391]   Loss 0.623955   Top1 80.312500   Top5 98.921875   BatchTime 0.121657   LR 0.010000
INFO - Training [0][  320/  391]   Loss 0.608850   Top1 80.690918   Top5 98.955078   BatchTime 0.121709   LR 0.010000
INFO - Training [0][  340/  391]   Loss 0.595466   Top1 81.027114   Top5 99.000460   BatchTime 0.121770   LR 0.010000
INFO - Training [0][  360/  391]   Loss 0.582218   Top1 81.393229   Top5 99.032118   BatchTime 0.121817   LR 0.010000
INFO - Training [0][  380/  391]   Loss 0.571096   Top1 81.733141   Top5 99.050164   BatchTime 0.120139   LR 0.010000
INFO - ==> Top1: 81.876    Top5: 99.068    Loss: 0.565
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [0][   20/   79]   Loss 0.459984   Top1 84.960938   Top5 99.414062   BatchTime 0.123541
INFO - Validation [0][   40/   79]   Loss 0.459103   Top1 84.921875   Top5 99.335938   BatchTime 0.076784
INFO - Validation [0][   60/   79]   Loss 0.464716   Top1 85.117188   Top5 99.335938   BatchTime 0.073052
INFO - ==> Top1: 85.160    Top5: 99.380    Loss: 0.459
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 85.160   Top5: 99.380] Sparsity : 0.598
INFO - Scoreboard best 2 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   1
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [1][   20/  391]   Loss 0.327466   Top1 88.632812   Top5 99.843750   BatchTime 0.213327   LR 0.010000
INFO - Training [1][   40/  391]   Loss 0.317573   Top1 88.847656   Top5 99.726562   BatchTime 0.168503   LR 0.010000
INFO - Training [1][   60/  391]   Loss 0.315200   Top1 88.958333   Top5 99.739583   BatchTime 0.153873   LR 0.010000
INFO - Training [1][   80/  391]   Loss 0.321524   Top1 88.662109   Top5 99.707031   BatchTime 0.146564   LR 0.010000
INFO - Training [1][  100/  391]   Loss 0.317142   Top1 88.781250   Top5 99.718750   BatchTime 0.142036   LR 0.010000
INFO - Training [1][  120/  391]   Loss 0.316072   Top1 88.847656   Top5 99.713542   BatchTime 0.138939   LR 0.010000
INFO - Training [1][  140/  391]   Loss 0.310927   Top1 89.040179   Top5 99.720982   BatchTime 0.136619   LR 0.010000
INFO - Training [1][  160/  391]   Loss 0.307856   Top1 89.121094   Top5 99.736328   BatchTime 0.132864   LR 0.010000
INFO - Training [1][  180/  391]   Loss 0.306427   Top1 89.262153   Top5 99.752604   BatchTime 0.127371   LR 0.010000
INFO - Training [1][  200/  391]   Loss 0.304115   Top1 89.367188   Top5 99.753906   BatchTime 0.123731   LR 0.010000
INFO - Training [1][  220/  391]   Loss 0.304164   Top1 89.353693   Top5 99.754972   BatchTime 0.119955   LR 0.010000
INFO - Training [1][  240/  391]   Loss 0.301692   Top1 89.430339   Top5 99.765625   BatchTime 0.119594   LR 0.010000
INFO - Training [1][  260/  391]   Loss 0.300760   Top1 89.447115   Top5 99.774639   BatchTime 0.119892   LR 0.010000
INFO - Training [1][  280/  391]   Loss 0.298703   Top1 89.531250   Top5 99.773996   BatchTime 0.120192   LR 0.010000
INFO - Training [1][  300/  391]   Loss 0.296623   Top1 89.578125   Top5 99.778646   BatchTime 0.120364   LR 0.010000
INFO - Training [1][  320/  391]   Loss 0.294910   Top1 89.680176   Top5 99.775391   BatchTime 0.120578   LR 0.010000
INFO - Training [1][  340/  391]   Loss 0.293707   Top1 89.705882   Top5 99.774816   BatchTime 0.120725   LR 0.010000
INFO - Training [1][  360/  391]   Loss 0.292562   Top1 89.746094   Top5 99.785156   BatchTime 0.120887   LR 0.010000
INFO - Training [1][  380/  391]   Loss 0.290699   Top1 89.821135   Top5 99.788240   BatchTime 0.120963   LR 0.010000
INFO - ==> Top1: 89.852    Top5: 99.794    Loss: 0.290
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [1][   20/   79]   Loss 0.435670   Top1 85.468750   Top5 99.335938   BatchTime 0.145764
INFO - Validation [1][   40/   79]   Loss 0.433399   Top1 85.664062   Top5 99.316406   BatchTime 0.086618
INFO - Validation [1][   60/   79]   Loss 0.424679   Top1 85.859375   Top5 99.414062   BatchTime 0.066646
INFO - ==> Top1: 85.960    Top5: 99.450    Loss: 0.422
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 85.960   Top5: 99.450] Sparsity : 0.685
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 85.160   Top5: 99.380] Sparsity : 0.598
INFO - Scoreboard best 3 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   2
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [2][   20/  391]   Loss 0.231824   Top1 92.343750   Top5 99.843750   BatchTime 0.193284   LR 0.010000
INFO - Training [2][   40/  391]   Loss 0.242367   Top1 91.660156   Top5 99.824219   BatchTime 0.157339   LR 0.010000
INFO - Training [2][   60/  391]   Loss 0.244319   Top1 91.692708   Top5 99.843750   BatchTime 0.146199   LR 0.010000
INFO - Training [2][   80/  391]   Loss 0.244743   Top1 91.689453   Top5 99.814453   BatchTime 0.140532   LR 0.010000
INFO - Training [2][  100/  391]   Loss 0.246240   Top1 91.640625   Top5 99.843750   BatchTime 0.137138   LR 0.010000
INFO - Training [2][  120/  391]   Loss 0.249761   Top1 91.419271   Top5 99.830729   BatchTime 0.134845   LR 0.010000
INFO - Training [2][  140/  391]   Loss 0.250919   Top1 91.411830   Top5 99.838170   BatchTime 0.133012   LR 0.010000
INFO - Training [2][  160/  391]   Loss 0.252529   Top1 91.401367   Top5 99.838867   BatchTime 0.131930   LR 0.010000
INFO - Training [2][  180/  391]   Loss 0.253733   Top1 91.319444   Top5 99.852431   BatchTime 0.130934   LR 0.010000
INFO - Training [2][  200/  391]   Loss 0.253147   Top1 91.285156   Top5 99.851562   BatchTime 0.130090   LR 0.010000
INFO - Training [2][  220/  391]   Loss 0.254360   Top1 91.253551   Top5 99.847301   BatchTime 0.128961   LR 0.010000
INFO - Training [2][  240/  391]   Loss 0.254217   Top1 91.227214   Top5 99.853516   BatchTime 0.125135   LR 0.010000
INFO - Training [2][  260/  391]   Loss 0.255448   Top1 91.198918   Top5 99.861779   BatchTime 0.122621   LR 0.010000
INFO - Training [2][  280/  391]   Loss 0.255126   Top1 91.208147   Top5 99.860491   BatchTime 0.119884   LR 0.010000
INFO - Training [2][  300/  391]   Loss 0.255937   Top1 91.197917   Top5 99.854167   BatchTime 0.118934   LR 0.010000
INFO - Training [2][  320/  391]   Loss 0.255587   Top1 91.218262   Top5 99.853516   BatchTime 0.119279   LR 0.010000
INFO - Training [2][  340/  391]   Loss 0.257093   Top1 91.162684   Top5 99.848346   BatchTime 0.119503   LR 0.010000
INFO - Training [2][  360/  391]   Loss 0.258142   Top1 91.126302   Top5 99.841580   BatchTime 0.119657   LR 0.010000
INFO - Training [2][  380/  391]   Loss 0.258442   Top1 91.108141   Top5 99.839638   BatchTime 0.119863   LR 0.010000
INFO - ==> Top1: 91.084    Top5: 99.840    Loss: 0.259
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [2][   20/   79]   Loss 0.437461   Top1 85.664062   Top5 99.414062   BatchTime 0.153197
INFO - Validation [2][   40/   79]   Loss 0.430043   Top1 85.957031   Top5 99.257812   BatchTime 0.109478
INFO - Validation [2][   60/   79]   Loss 0.431204   Top1 86.171875   Top5 99.335938   BatchTime 0.094661
INFO - ==> Top1: 86.340    Top5: 99.350    Loss: 0.427
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 86.340   Top5: 99.350] Sparsity : 0.739
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 85.960   Top5: 99.450] Sparsity : 0.685
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 85.160   Top5: 99.380] Sparsity : 0.598
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   3
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [3][   20/  391]   Loss 0.242012   Top1 91.406250   Top5 99.882812   BatchTime 0.177219   LR 0.010000
INFO - Training [3][   40/  391]   Loss 0.232502   Top1 91.738281   Top5 99.882812   BatchTime 0.133640   LR 0.010000
INFO - Training [3][   60/  391]   Loss 0.238553   Top1 91.588542   Top5 99.817708   BatchTime 0.117515   LR 0.010000
INFO - Training [3][   80/  391]   Loss 0.236423   Top1 91.767578   Top5 99.814453   BatchTime 0.116188   LR 0.010000
INFO - Training [3][  100/  391]   Loss 0.235674   Top1 91.859375   Top5 99.820312   BatchTime 0.117810   LR 0.010000
INFO - Training [3][  120/  391]   Loss 0.235913   Top1 91.888021   Top5 99.824219   BatchTime 0.118725   LR 0.010000
INFO - Training [3][  140/  391]   Loss 0.234276   Top1 91.969866   Top5 99.827009   BatchTime 0.119411   LR 0.010000
INFO - Training [3][  160/  391]   Loss 0.233842   Top1 91.958008   Top5 99.829102   BatchTime 0.119932   LR 0.010000
INFO - Training [3][  180/  391]   Loss 0.234893   Top1 91.901042   Top5 99.826389   BatchTime 0.120409   LR 0.010000
INFO - Training [3][  200/  391]   Loss 0.233302   Top1 91.933594   Top5 99.835938   BatchTime 0.120751   LR 0.010000
INFO - Training [3][  220/  391]   Loss 0.233226   Top1 91.906960   Top5 99.843750   BatchTime 0.120940   LR 0.010000
INFO - Training [3][  240/  391]   Loss 0.233171   Top1 91.875000   Top5 99.847005   BatchTime 0.121090   LR 0.010000
INFO - Training [3][  260/  391]   Loss 0.233665   Top1 91.823918   Top5 99.843750   BatchTime 0.121242   LR 0.010000
INFO - Training [3][  280/  391]   Loss 0.233327   Top1 91.855469   Top5 99.849330   BatchTime 0.119407   LR 0.010000
INFO - Training [3][  300/  391]   Loss 0.231519   Top1 91.921875   Top5 99.848958   BatchTime 0.117291   LR 0.010000
INFO - Training [3][  320/  391]   Loss 0.230230   Top1 91.982422   Top5 99.841309   BatchTime 0.115884   LR 0.010000
INFO - Training [3][  340/  391]   Loss 0.230062   Top1 91.999081   Top5 99.848346   BatchTime 0.113491   LR 0.010000
INFO - Training [3][  360/  391]   Loss 0.229818   Top1 92.009549   Top5 99.854601   BatchTime 0.113998   LR 0.010000
INFO - Training [3][  380/  391]   Loss 0.228658   Top1 92.035362   Top5 99.862253   BatchTime 0.114423   LR 0.010000
INFO - ==> Top1: 92.050    Top5: 99.854    Loss: 0.229
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [3][   20/   79]   Loss 0.412670   Top1 87.304688   Top5 99.492188   BatchTime 0.152598
INFO - Validation [3][   40/   79]   Loss 0.406142   Top1 87.558594   Top5 99.433594   BatchTime 0.108987
INFO - Validation [3][   60/   79]   Loss 0.398711   Top1 87.669271   Top5 99.440104   BatchTime 0.094281
INFO - ==> Top1: 87.710    Top5: 99.500    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 87.710   Top5: 99.500] Sparsity : 0.745
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 86.340   Top5: 99.350] Sparsity : 0.739
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 85.960   Top5: 99.450] Sparsity : 0.685
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   4
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [4][   20/  391]   Loss 0.189433   Top1 92.851562   Top5 99.843750   BatchTime 0.215781   LR 0.010000
INFO - Training [4][   40/  391]   Loss 0.203401   Top1 92.617188   Top5 99.902344   BatchTime 0.169501   LR 0.010000
INFO - Training [4][   60/  391]   Loss 0.194225   Top1 93.020833   Top5 99.921875   BatchTime 0.150495   LR 0.010000
INFO - Training [4][   80/  391]   Loss 0.192426   Top1 93.095703   Top5 99.941406   BatchTime 0.133618   LR 0.010000
INFO - Training [4][  100/  391]   Loss 0.193487   Top1 93.093750   Top5 99.937500   BatchTime 0.125768   LR 0.010000
INFO - Training [4][  120/  391]   Loss 0.193805   Top1 93.177083   Top5 99.941406   BatchTime 0.117888   LR 0.010000
INFO - Training [4][  140/  391]   Loss 0.196045   Top1 93.136161   Top5 99.916295   BatchTime 0.117105   LR 0.010000
INFO - Training [4][  160/  391]   Loss 0.195237   Top1 93.173828   Top5 99.912109   BatchTime 0.117926   LR 0.010000
INFO - Training [4][  180/  391]   Loss 0.195481   Top1 93.194444   Top5 99.913194   BatchTime 0.118619   LR 0.010000
INFO - Training [4][  200/  391]   Loss 0.196477   Top1 93.187500   Top5 99.917969   BatchTime 0.119054   LR 0.010000
INFO - Training [4][  220/  391]   Loss 0.195815   Top1 93.206676   Top5 99.907670   BatchTime 0.119540   LR 0.010000
INFO - Training [4][  240/  391]   Loss 0.195660   Top1 93.206380   Top5 99.905599   BatchTime 0.119892   LR 0.010000
INFO - Training [4][  260/  391]   Loss 0.195374   Top1 93.224159   Top5 99.897837   BatchTime 0.120153   LR 0.010000
INFO - Training [4][  280/  391]   Loss 0.195793   Top1 93.211496   Top5 99.902344   BatchTime 0.120407   LR 0.010000
INFO - Training [4][  300/  391]   Loss 0.194972   Top1 93.229167   Top5 99.906250   BatchTime 0.120560   LR 0.010000
INFO - Training [4][  320/  391]   Loss 0.194111   Top1 93.244629   Top5 99.904785   BatchTime 0.120720   LR 0.010000
INFO - Training [4][  340/  391]   Loss 0.193290   Top1 93.249081   Top5 99.903493   BatchTime 0.119149   LR 0.010000
INFO - Training [4][  360/  391]   Loss 0.192490   Top1 93.266059   Top5 99.908854   BatchTime 0.117225   LR 0.010000
INFO - Training [4][  380/  391]   Loss 0.191918   Top1 93.283306   Top5 99.911595   BatchTime 0.115801   LR 0.010000
INFO - ==> Top1: 93.276    Top5: 99.914    Loss: 0.192
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [4][   20/   79]   Loss 0.398903   Top1 88.007812   Top5 99.609375   BatchTime 0.161632
INFO - Validation [4][   40/   79]   Loss 0.399095   Top1 88.027344   Top5 99.492188   BatchTime 0.113138
INFO - Validation [4][   60/   79]   Loss 0.390995   Top1 88.033854   Top5 99.531250   BatchTime 0.096601
INFO - ==> Top1: 87.940    Top5: 99.580    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 87.940   Top5: 99.580] Sparsity : 0.754
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 87.710   Top5: 99.500] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 86.340   Top5: 99.350] Sparsity : 0.739
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   5
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [5][   20/  391]   Loss 0.174384   Top1 93.632812   Top5 99.843750   BatchTime 0.219488   LR 0.010000
INFO - Training [5][   40/  391]   Loss 0.163958   Top1 94.199219   Top5 99.863281   BatchTime 0.172236   LR 0.010000
INFO - Training [5][   60/  391]   Loss 0.169373   Top1 94.127604   Top5 99.895833   BatchTime 0.156010   LR 0.010000
INFO - Training [5][   80/  391]   Loss 0.163855   Top1 94.267578   Top5 99.921875   BatchTime 0.147795   LR 0.010000
INFO - Training [5][  100/  391]   Loss 0.164168   Top1 94.210938   Top5 99.906250   BatchTime 0.142962   LR 0.010000
INFO - Training [5][  120/  391]   Loss 0.165387   Top1 94.147135   Top5 99.915365   BatchTime 0.137222   LR 0.010000
INFO - Training [5][  140/  391]   Loss 0.165652   Top1 94.174107   Top5 99.905134   BatchTime 0.129724   LR 0.010000
INFO - Training [5][  160/  391]   Loss 0.168528   Top1 94.052734   Top5 99.916992   BatchTime 0.124500   LR 0.010000
INFO - Training [5][  180/  391]   Loss 0.171253   Top1 93.936632   Top5 99.921875   BatchTime 0.119956   LR 0.010000
INFO - Training [5][  200/  391]   Loss 0.175305   Top1 93.800781   Top5 99.906250   BatchTime 0.119937   LR 0.010000
INFO - Training [5][  220/  391]   Loss 0.179399   Top1 93.664773   Top5 99.904119   BatchTime 0.120276   LR 0.010000
INFO - Training [5][  240/  391]   Loss 0.181075   Top1 93.636068   Top5 99.899089   BatchTime 0.120582   LR 0.010000
INFO - Training [5][  260/  391]   Loss 0.183150   Top1 93.566707   Top5 99.897837   BatchTime 0.120793   LR 0.010000
INFO - Training [5][  280/  391]   Loss 0.184367   Top1 93.521205   Top5 99.902344   BatchTime 0.120995   LR 0.010000
INFO - Training [5][  300/  391]   Loss 0.187122   Top1 93.479167   Top5 99.890625   BatchTime 0.121160   LR 0.010000
INFO - Training [5][  320/  391]   Loss 0.188713   Top1 93.447266   Top5 99.885254   BatchTime 0.121315   LR 0.010000
INFO - Training [5][  340/  391]   Loss 0.190792   Top1 93.384651   Top5 99.875919   BatchTime 0.121412   LR 0.010000
INFO - Training [5][  360/  391]   Loss 0.191974   Top1 93.355035   Top5 99.874132   BatchTime 0.121481   LR 0.010000
INFO - Training [5][  380/  391]   Loss 0.193983   Top1 93.312089   Top5 99.870477   BatchTime 0.121504   LR 0.010000
INFO - ==> Top1: 93.264    Top5: 99.870    Loss: 0.195
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [5][   20/   79]   Loss 0.396767   Top1 87.968750   Top5 99.609375   BatchTime 0.137121
INFO - Validation [5][   40/   79]   Loss 0.407273   Top1 87.675781   Top5 99.492188   BatchTime 0.084532
INFO - Validation [5][   60/   79]   Loss 0.397227   Top1 87.864583   Top5 99.531250   BatchTime 0.065146
INFO - ==> Top1: 87.820    Top5: 99.550    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 87.940   Top5: 99.580] Sparsity : 0.754
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 87.820   Top5: 99.550] Sparsity : 0.766
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 87.710   Top5: 99.500] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   6
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [6][   20/  391]   Loss 0.209416   Top1 92.851562   Top5 99.765625   BatchTime 0.217428   LR 0.010000
INFO - Training [6][   40/  391]   Loss 0.199568   Top1 93.261719   Top5 99.804688   BatchTime 0.170396   LR 0.010000
INFO - Training [6][   60/  391]   Loss 0.193358   Top1 93.385417   Top5 99.830729   BatchTime 0.154854   LR 0.010000
INFO - Training [6][   80/  391]   Loss 0.194510   Top1 93.281250   Top5 99.843750   BatchTime 0.147313   LR 0.010000
INFO - Training [6][  100/  391]   Loss 0.198824   Top1 93.195312   Top5 99.851562   BatchTime 0.142590   LR 0.010000
INFO - Training [6][  120/  391]   Loss 0.199088   Top1 93.183594   Top5 99.837240   BatchTime 0.139433   LR 0.010000
INFO - Training [6][  140/  391]   Loss 0.197265   Top1 93.286830   Top5 99.854911   BatchTime 0.137054   LR 0.010000
INFO - Training [6][  160/  391]   Loss 0.195070   Top1 93.369141   Top5 99.873047   BatchTime 0.135274   LR 0.010000
INFO - Training [6][  180/  391]   Loss 0.196704   Top1 93.333333   Top5 99.874132   BatchTime 0.133903   LR 0.010000
INFO - Training [6][  200/  391]   Loss 0.196582   Top1 93.359375   Top5 99.878906   BatchTime 0.129004   LR 0.010000
INFO - Training [6][  220/  391]   Loss 0.196960   Top1 93.259943   Top5 99.889915   BatchTime 0.125654   LR 0.010000
INFO - Training [6][  240/  391]   Loss 0.197451   Top1 93.225911   Top5 99.882812   BatchTime 0.122467   LR 0.010000
INFO - Training [6][  260/  391]   Loss 0.198119   Top1 93.191106   Top5 99.885817   BatchTime 0.120776   LR 0.010000
INFO - Training [6][  280/  391]   Loss 0.197153   Top1 93.236607   Top5 99.891183   BatchTime 0.120998   LR 0.010000
INFO - Training [6][  300/  391]   Loss 0.198317   Top1 93.166667   Top5 99.893229   BatchTime 0.121205   LR 0.010000
INFO - Training [6][  320/  391]   Loss 0.198142   Top1 93.166504   Top5 99.895020   BatchTime 0.121282   LR 0.010000
INFO - Training [6][  340/  391]   Loss 0.198936   Top1 93.136489   Top5 99.892004   BatchTime 0.121398   LR 0.010000
INFO - Training [6][  360/  391]   Loss 0.199426   Top1 93.138021   Top5 99.895833   BatchTime 0.121518   LR 0.010000
INFO - Training [6][  380/  391]   Loss 0.199344   Top1 93.141447   Top5 99.895148   BatchTime 0.121591   LR 0.010000
INFO - ==> Top1: 93.136    Top5: 99.896    Loss: 0.199
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [6][   20/   79]   Loss 0.410637   Top1 87.500000   Top5 99.570312   BatchTime 0.153531
INFO - Validation [6][   40/   79]   Loss 0.414710   Top1 87.519531   Top5 99.511719   BatchTime 0.109329
INFO - Validation [6][   60/   79]   Loss 0.404444   Top1 87.747396   Top5 99.518229   BatchTime 0.093730
INFO - ==> Top1: 87.730    Top5: 99.560    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 87.940   Top5: 99.580] Sparsity : 0.754
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 87.820   Top5: 99.550] Sparsity : 0.766
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 87.730   Top5: 99.560] Sparsity : 0.786
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   7
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [7][   20/  391]   Loss 0.180807   Top1 93.906250   Top5 99.921875   BatchTime 0.187941   LR 0.010000
INFO - Training [7][   40/  391]   Loss 0.183256   Top1 93.710938   Top5 99.941406   BatchTime 0.133070   LR 0.010000
INFO - Training [7][   60/  391]   Loss 0.187378   Top1 93.606771   Top5 99.895833   BatchTime 0.132073   LR 0.010000
INFO - Training [7][   80/  391]   Loss 0.183600   Top1 93.662109   Top5 99.912109   BatchTime 0.131161   LR 0.010000
INFO - Training [7][  100/  391]   Loss 0.182144   Top1 93.648438   Top5 99.914062   BatchTime 0.129826   LR 0.010000
INFO - Training [7][  120/  391]   Loss 0.180696   Top1 93.710938   Top5 99.921875   BatchTime 0.128777   LR 0.010000
INFO - Training [7][  140/  391]   Loss 0.181924   Top1 93.660714   Top5 99.921875   BatchTime 0.128052   LR 0.010000
INFO - Training [7][  160/  391]   Loss 0.178504   Top1 93.857422   Top5 99.902344   BatchTime 0.127635   LR 0.010000
INFO - Training [7][  180/  391]   Loss 0.179076   Top1 93.806424   Top5 99.908854   BatchTime 0.127157   LR 0.010000
INFO - Training [7][  200/  391]   Loss 0.181421   Top1 93.761719   Top5 99.910156   BatchTime 0.126714   LR 0.010000
INFO - Training [7][  220/  391]   Loss 0.180387   Top1 93.806818   Top5 99.914773   BatchTime 0.126363   LR 0.010000
INFO - Training [7][  240/  391]   Loss 0.180910   Top1 93.746745   Top5 99.912109   BatchTime 0.125472   LR 0.010000
INFO - Training [7][  260/  391]   Loss 0.181615   Top1 93.716947   Top5 99.912861   BatchTime 0.122141   LR 0.010000
INFO - Training [7][  280/  391]   Loss 0.182533   Top1 93.710938   Top5 99.913504   BatchTime 0.119710   LR 0.010000
INFO - Training [7][  300/  391]   Loss 0.184269   Top1 93.638021   Top5 99.908854   BatchTime 0.117444   LR 0.010000
INFO - Training [7][  320/  391]   Loss 0.183744   Top1 93.676758   Top5 99.902344   BatchTime 0.116269   LR 0.010000
INFO - Training [7][  340/  391]   Loss 0.184911   Top1 93.630515   Top5 99.898897   BatchTime 0.116694   LR 0.010000
INFO - Training [7][  360/  391]   Loss 0.184296   Top1 93.665365   Top5 99.904514   BatchTime 0.117047   LR 0.010000
INFO - Training [7][  380/  391]   Loss 0.185067   Top1 93.618421   Top5 99.899260   BatchTime 0.117341   LR 0.010000
INFO - ==> Top1: 93.620    Top5: 99.900    Loss: 0.185
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [7][   20/   79]   Loss 0.415204   Top1 87.148438   Top5 99.570312   BatchTime 0.152255
INFO - Validation [7][   40/   79]   Loss 0.407435   Top1 87.636719   Top5 99.531250   BatchTime 0.107630
INFO - Validation [7][   60/   79]   Loss 0.397979   Top1 87.877604   Top5 99.583333   BatchTime 0.093437
INFO - ==> Top1: 87.690    Top5: 99.610    Loss: 0.394
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 87.940   Top5: 99.580] Sparsity : 0.754
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 87.820   Top5: 99.550] Sparsity : 0.766
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 87.730   Top5: 99.560] Sparsity : 0.786
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   8
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [8][   20/  391]   Loss 0.160271   Top1 94.140625   Top5 99.921875   BatchTime 0.211688   LR 0.010000
INFO - Training [8][   40/  391]   Loss 0.164479   Top1 93.906250   Top5 99.902344   BatchTime 0.151395   LR 0.010000
INFO - Training [8][   60/  391]   Loss 0.164313   Top1 94.062500   Top5 99.921875   BatchTime 0.130153   LR 0.010000
INFO - Training [8][   80/  391]   Loss 0.163882   Top1 94.208984   Top5 99.931641   BatchTime 0.121069   LR 0.010000
INFO - Training [8][  100/  391]   Loss 0.162054   Top1 94.257812   Top5 99.929688   BatchTime 0.114706   LR 0.010000
INFO - Training [8][  120/  391]   Loss 0.161297   Top1 94.316406   Top5 99.941406   BatchTime 0.116301   LR 0.010000
INFO - Training [8][  140/  391]   Loss 0.163101   Top1 94.224330   Top5 99.938616   BatchTime 0.117337   LR 0.010000
INFO - Training [8][  160/  391]   Loss 0.163701   Top1 94.248047   Top5 99.941406   BatchTime 0.118090   LR 0.010000
INFO - Training [8][  180/  391]   Loss 0.163049   Top1 94.249132   Top5 99.939236   BatchTime 0.118720   LR 0.010000
INFO - Training [8][  200/  391]   Loss 0.162235   Top1 94.269531   Top5 99.941406   BatchTime 0.119285   LR 0.010000
INFO - Training [8][  220/  391]   Loss 0.162565   Top1 94.279119   Top5 99.939631   BatchTime 0.119645   LR 0.010000
INFO - Training [8][  240/  391]   Loss 0.162755   Top1 94.274089   Top5 99.941406   BatchTime 0.120040   LR 0.010000
INFO - Training [8][  260/  391]   Loss 0.163646   Top1 94.224760   Top5 99.942909   BatchTime 0.120261   LR 0.010000
INFO - Training [8][  280/  391]   Loss 0.165733   Top1 94.174107   Top5 99.933036   BatchTime 0.120454   LR 0.010000
INFO - Training [8][  300/  391]   Loss 0.165837   Top1 94.190104   Top5 99.937500   BatchTime 0.120350   LR 0.010000
INFO - Training [8][  320/  391]   Loss 0.168154   Top1 94.113770   Top5 99.924316   BatchTime 0.117758   LR 0.010000
INFO - Training [8][  340/  391]   Loss 0.169208   Top1 94.083180   Top5 99.921875   BatchTime 0.116059   LR 0.010000
INFO - Training [8][  360/  391]   Loss 0.171995   Top1 94.001736   Top5 99.917535   BatchTime 0.114129   LR 0.010000
INFO - Training [8][  380/  391]   Loss 0.173923   Top1 93.947368   Top5 99.917763   BatchTime 0.113322   LR 0.010000
INFO - ==> Top1: 93.936    Top5: 99.920    Loss: 0.175
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [8][   20/   79]   Loss 0.409811   Top1 87.773438   Top5 99.726562   BatchTime 0.152480
INFO - Validation [8][   40/   79]   Loss 0.403182   Top1 88.105469   Top5 99.472656   BatchTime 0.108751
INFO - Validation [8][   60/   79]   Loss 0.398170   Top1 88.242188   Top5 99.531250   BatchTime 0.093274
INFO - ==> Top1: 88.100    Top5: 99.590    Loss: 0.397
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 88.100   Top5: 99.590] Sparsity : 0.817
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 87.940   Top5: 99.580] Sparsity : 0.754
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 87.820   Top5: 99.550] Sparsity : 0.766
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   9
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [9][   20/  391]   Loss 0.194961   Top1 93.281250   Top5 99.804688   BatchTime 0.213461   LR 0.010000
INFO - Training [9][   40/  391]   Loss 0.191190   Top1 93.320312   Top5 99.824219   BatchTime 0.168359   LR 0.010000
INFO - Training [9][   60/  391]   Loss 0.189384   Top1 93.385417   Top5 99.869792   BatchTime 0.153157   LR 0.010000
INFO - Training [9][   80/  391]   Loss 0.196791   Top1 93.144531   Top5 99.863281   BatchTime 0.145696   LR 0.010000
INFO - Training [9][  100/  391]   Loss 0.196755   Top1 93.226562   Top5 99.843750   BatchTime 0.135810   LR 0.010000
INFO - Training [9][  120/  391]   Loss 0.195292   Top1 93.359375   Top5 99.856771   BatchTime 0.127744   LR 0.010000
INFO - Training [9][  140/  391]   Loss 0.191979   Top1 93.437500   Top5 99.866071   BatchTime 0.123391   LR 0.010000
INFO - Training [9][  160/  391]   Loss 0.189118   Top1 93.540039   Top5 99.873047   BatchTime 0.119273   LR 0.010000
INFO - Training [9][  180/  391]   Loss 0.189805   Top1 93.493924   Top5 99.869792   BatchTime 0.119834   LR 0.010000
INFO - Training [9][  200/  391]   Loss 0.189088   Top1 93.511719   Top5 99.878906   BatchTime 0.120279   LR 0.010000
INFO - Training [9][  220/  391]   Loss 0.187938   Top1 93.575994   Top5 99.879261   BatchTime 0.120556   LR 0.010000
INFO - Training [9][  240/  391]   Loss 0.187976   Top1 93.590495   Top5 99.879557   BatchTime 0.120801   LR 0.010000
INFO - Training [9][  260/  391]   Loss 0.189324   Top1 93.539663   Top5 99.882812   BatchTime 0.121026   LR 0.010000
INFO - Training [9][  280/  391]   Loss 0.189173   Top1 93.568638   Top5 99.885603   BatchTime 0.121207   LR 0.010000
INFO - Training [9][  300/  391]   Loss 0.189500   Top1 93.575521   Top5 99.882812   BatchTime 0.121335   LR 0.010000
INFO - Training [9][  320/  391]   Loss 0.191005   Top1 93.483887   Top5 99.882812   BatchTime 0.121450   LR 0.010000
INFO - Training [9][  340/  391]   Loss 0.192632   Top1 93.416820   Top5 99.887408   BatchTime 0.121481   LR 0.010000
INFO - Training [9][  360/  391]   Loss 0.191757   Top1 93.446181   Top5 99.889323   BatchTime 0.121088   LR 0.010000
INFO - Training [9][  380/  391]   Loss 0.192350   Top1 93.408717   Top5 99.893092   BatchTime 0.118737   LR 0.010000
INFO - ==> Top1: 93.414    Top5: 99.894    Loss: 0.193
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [9][   20/   79]   Loss 0.389694   Top1 87.773438   Top5 99.609375   BatchTime 0.122468
INFO - Validation [9][   40/   79]   Loss 0.383046   Top1 88.164062   Top5 99.628906   BatchTime 0.087967
INFO - Validation [9][   60/   79]   Loss 0.380206   Top1 88.359375   Top5 99.635417   BatchTime 0.080076
INFO - ==> Top1: 88.280    Top5: 99.620    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 88.280   Top5: 99.620] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 88.100   Top5: 99.590] Sparsity : 0.817
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 87.940   Top5: 99.580] Sparsity : 0.754
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  10
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [10][   20/  391]   Loss 0.165376   Top1 94.140625   Top5 99.804688   BatchTime 0.212019   LR 0.010000
INFO - Training [10][   40/  391]   Loss 0.171246   Top1 94.003906   Top5 99.843750   BatchTime 0.168387   LR 0.010000
INFO - Training [10][   60/  391]   Loss 0.167846   Top1 94.088542   Top5 99.856771   BatchTime 0.153467   LR 0.010000
INFO - Training [10][   80/  391]   Loss 0.162991   Top1 94.326172   Top5 99.892578   BatchTime 0.145960   LR 0.010000
INFO - Training [10][  100/  391]   Loss 0.163544   Top1 94.289062   Top5 99.906250   BatchTime 0.141371   LR 0.010000
INFO - Training [10][  120/  391]   Loss 0.163455   Top1 94.251302   Top5 99.915365   BatchTime 0.138285   LR 0.010000
INFO - Training [10][  140/  391]   Loss 0.164241   Top1 94.263393   Top5 99.916295   BatchTime 0.136028   LR 0.010000
INFO - Training [10][  160/  391]   Loss 0.165573   Top1 94.238281   Top5 99.921875   BatchTime 0.130034   LR 0.010000
INFO - Training [10][  180/  391]   Loss 0.162957   Top1 94.361979   Top5 99.921875   BatchTime 0.125550   LR 0.010000
INFO - Training [10][  200/  391]   Loss 0.163317   Top1 94.351562   Top5 99.921875   BatchTime 0.122018   LR 0.010000
INFO - Training [10][  220/  391]   Loss 0.164751   Top1 94.282670   Top5 99.928977   BatchTime 0.119543   LR 0.010000
INFO - Training [10][  240/  391]   Loss 0.166372   Top1 94.228516   Top5 99.925130   BatchTime 0.119863   LR 0.010000
INFO - Training [10][  260/  391]   Loss 0.167934   Top1 94.155649   Top5 99.924880   BatchTime 0.120154   LR 0.010000
INFO - Training [10][  280/  391]   Loss 0.169128   Top1 94.118304   Top5 99.919085   BatchTime 0.120495   LR 0.010000
INFO - Training [10][  300/  391]   Loss 0.170049   Top1 94.093750   Top5 99.921875   BatchTime 0.120717   LR 0.010000
INFO - Training [10][  320/  391]   Loss 0.169794   Top1 94.104004   Top5 99.921875   BatchTime 0.120853   LR 0.010000
INFO - Training [10][  340/  391]   Loss 0.169955   Top1 94.119945   Top5 99.924173   BatchTime 0.121045   LR 0.010000
INFO - Training [10][  360/  391]   Loss 0.170483   Top1 94.092882   Top5 99.926215   BatchTime 0.121116   LR 0.010000
INFO - Training [10][  380/  391]   Loss 0.170707   Top1 94.085115   Top5 99.921875   BatchTime 0.121197   LR 0.010000
INFO - ==> Top1: 94.064    Top5: 99.924    Loss: 0.171
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [10][   20/   79]   Loss 0.378075   Top1 89.257812   Top5 99.492188   BatchTime 0.120952
INFO - Validation [10][   40/   79]   Loss 0.382116   Top1 89.062500   Top5 99.531250   BatchTime 0.073635
INFO - Validation [10][   60/   79]   Loss 0.377783   Top1 89.010417   Top5 99.505208   BatchTime 0.057829
INFO - ==> Top1: 88.890    Top5: 99.530    Loss: 0.377
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 88.280   Top5: 99.620] Sparsity : 0.822
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 88.100   Top5: 99.590] Sparsity : 0.817
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  11
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [11][   20/  391]   Loss 0.141914   Top1 95.039062   Top5 100.000000   BatchTime 0.197925   LR 0.010000
INFO - Training [11][   40/  391]   Loss 0.151090   Top1 94.726562   Top5 99.980469   BatchTime 0.160770   LR 0.010000
INFO - Training [11][   60/  391]   Loss 0.149085   Top1 94.882812   Top5 99.960938   BatchTime 0.148310   LR 0.010000
INFO - Training [11][   80/  391]   Loss 0.150695   Top1 94.843750   Top5 99.951172   BatchTime 0.142174   LR 0.010000
INFO - Training [11][  100/  391]   Loss 0.149765   Top1 94.781250   Top5 99.960938   BatchTime 0.138463   LR 0.010000
INFO - Training [11][  120/  391]   Loss 0.152042   Top1 94.707031   Top5 99.967448   BatchTime 0.135984   LR 0.010000
INFO - Training [11][  140/  391]   Loss 0.152880   Top1 94.670759   Top5 99.955357   BatchTime 0.134329   LR 0.010000
INFO - Training [11][  160/  391]   Loss 0.154536   Top1 94.628906   Top5 99.951172   BatchTime 0.132889   LR 0.010000
INFO - Training [11][  180/  391]   Loss 0.155278   Top1 94.557292   Top5 99.952257   BatchTime 0.132196   LR 0.010000
INFO - Training [11][  200/  391]   Loss 0.156015   Top1 94.558594   Top5 99.953125   BatchTime 0.131075   LR 0.010000
INFO - Training [11][  220/  391]   Loss 0.155743   Top1 94.559659   Top5 99.953835   BatchTime 0.128344   LR 0.010000
INFO - Training [11][  240/  391]   Loss 0.156911   Top1 94.501953   Top5 99.951172   BatchTime 0.124468   LR 0.010000
INFO - Training [11][  260/  391]   Loss 0.157111   Top1 94.453125   Top5 99.951923   BatchTime 0.122185   LR 0.010000
INFO - Training [11][  280/  391]   Loss 0.158408   Top1 94.436384   Top5 99.944196   BatchTime 0.119054   LR 0.010000
INFO - Training [11][  300/  391]   Loss 0.159178   Top1 94.424479   Top5 99.940104   BatchTime 0.118982   LR 0.010000
INFO - Training [11][  320/  391]   Loss 0.158929   Top1 94.411621   Top5 99.941406   BatchTime 0.119232   LR 0.010000
INFO - Training [11][  340/  391]   Loss 0.160666   Top1 94.381893   Top5 99.944853   BatchTime 0.119505   LR 0.010000
INFO - Training [11][  360/  391]   Loss 0.161713   Top1 94.331597   Top5 99.943576   BatchTime 0.119662   LR 0.010000
INFO - Training [11][  380/  391]   Loss 0.163960   Top1 94.268092   Top5 99.938322   BatchTime 0.119808   LR 0.010000
INFO - ==> Top1: 94.250    Top5: 99.934    Loss: 0.164
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [11][   20/   79]   Loss 0.413821   Top1 87.851562   Top5 99.257812   BatchTime 0.151949
INFO - Validation [11][   40/   79]   Loss 0.399597   Top1 88.437500   Top5 99.316406   BatchTime 0.108411
INFO - Validation [11][   60/   79]   Loss 0.396611   Top1 88.346354   Top5 99.388021   BatchTime 0.093987
INFO - ==> Top1: 88.230    Top5: 99.460    Loss: 0.397
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 88.280   Top5: 99.620] Sparsity : 0.822
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 88.230   Top5: 99.460] Sparsity : 0.833
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  12
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [12][   20/  391]   Loss 0.170445   Top1 94.375000   Top5 99.804688   BatchTime 0.180236   LR 0.010000
INFO - Training [12][   40/  391]   Loss 0.165630   Top1 94.531250   Top5 99.863281   BatchTime 0.134248   LR 0.010000
INFO - Training [12][   60/  391]   Loss 0.168203   Top1 94.440104   Top5 99.869792   BatchTime 0.118222   LR 0.010000
INFO - Training [12][   80/  391]   Loss 0.172247   Top1 94.189453   Top5 99.853516   BatchTime 0.117523   LR 0.010000
INFO - Training [12][  100/  391]   Loss 0.176693   Top1 93.968750   Top5 99.867188   BatchTime 0.118682   LR 0.010000
INFO - Training [12][  120/  391]   Loss 0.180657   Top1 93.847656   Top5 99.869792   BatchTime 0.119484   LR 0.010000
INFO - Training [12][  140/  391]   Loss 0.184350   Top1 93.655134   Top5 99.871652   BatchTime 0.120082   LR 0.010000
INFO - Training [12][  160/  391]   Loss 0.185521   Top1 93.564453   Top5 99.887695   BatchTime 0.120478   LR 0.010000
INFO - Training [12][  180/  391]   Loss 0.185211   Top1 93.532986   Top5 99.900174   BatchTime 0.120799   LR 0.010000
INFO - Training [12][  200/  391]   Loss 0.186619   Top1 93.449219   Top5 99.898438   BatchTime 0.121092   LR 0.010000
INFO - Training [12][  220/  391]   Loss 0.189673   Top1 93.373580   Top5 99.893466   BatchTime 0.121177   LR 0.010000
INFO - Training [12][  240/  391]   Loss 0.189707   Top1 93.414714   Top5 99.886068   BatchTime 0.121367   LR 0.010000
INFO - Training [12][  260/  391]   Loss 0.189527   Top1 93.437500   Top5 99.885817   BatchTime 0.121461   LR 0.010000
INFO - Training [12][  280/  391]   Loss 0.190563   Top1 93.401228   Top5 99.880022   BatchTime 0.119200   LR 0.010000
INFO - Training [12][  300/  391]   Loss 0.190690   Top1 93.390625   Top5 99.877604   BatchTime 0.117307   LR 0.010000
INFO - Training [12][  320/  391]   Loss 0.190721   Top1 93.398438   Top5 99.877930   BatchTime 0.115712   LR 0.010000
INFO - Training [12][  340/  391]   Loss 0.190119   Top1 93.416820   Top5 99.875919   BatchTime 0.114162   LR 0.010000
INFO - Training [12][  360/  391]   Loss 0.189152   Top1 93.450521   Top5 99.880642   BatchTime 0.114655   LR 0.010000
INFO - Training [12][  380/  391]   Loss 0.189502   Top1 93.400493   Top5 99.886924   BatchTime 0.115082   LR 0.010000
INFO - ==> Top1: 93.374    Top5: 99.888    Loss: 0.190
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [12][   20/   79]   Loss 0.388096   Top1 87.890625   Top5 99.335938   BatchTime 0.154825
INFO - Validation [12][   40/   79]   Loss 0.392804   Top1 88.027344   Top5 99.414062   BatchTime 0.109769
INFO - Validation [12][   60/   79]   Loss 0.387588   Top1 88.229167   Top5 99.466146   BatchTime 0.094033
INFO - ==> Top1: 88.130    Top5: 99.490    Loss: 0.390
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 88.280   Top5: 99.620] Sparsity : 0.822
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 88.230   Top5: 99.460] Sparsity : 0.833
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  13
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [13][   20/  391]   Loss 0.167980   Top1 94.296875   Top5 99.882812   BatchTime 0.211874   LR 0.010000
INFO - Training [13][   40/  391]   Loss 0.168396   Top1 94.042969   Top5 99.921875   BatchTime 0.167405   LR 0.010000
INFO - Training [13][   60/  391]   Loss 0.165899   Top1 94.231771   Top5 99.947917   BatchTime 0.143616   LR 0.010000
INFO - Training [13][   80/  391]   Loss 0.173759   Top1 93.925781   Top5 99.931641   BatchTime 0.129511   LR 0.010000
INFO - Training [13][  100/  391]   Loss 0.173447   Top1 93.890625   Top5 99.929688   BatchTime 0.122197   LR 0.010000
INFO - Training [13][  120/  391]   Loss 0.172459   Top1 93.945312   Top5 99.934896   BatchTime 0.114622   LR 0.010000
INFO - Training [13][  140/  391]   Loss 0.171559   Top1 94.012277   Top5 99.927455   BatchTime 0.117079   LR 0.010000
INFO - Training [13][  160/  391]   Loss 0.173120   Top1 93.989258   Top5 99.936523   BatchTime 0.117994   LR 0.010000
INFO - Training [13][  180/  391]   Loss 0.173686   Top1 93.967014   Top5 99.934896   BatchTime 0.118572   LR 0.010000
INFO - Training [13][  200/  391]   Loss 0.173644   Top1 93.968750   Top5 99.933594   BatchTime 0.119094   LR 0.010000
INFO - Training [13][  220/  391]   Loss 0.174145   Top1 93.966619   Top5 99.936080   BatchTime 0.119547   LR 0.010000
INFO - Training [13][  240/  391]   Loss 0.175130   Top1 93.935547   Top5 99.931641   BatchTime 0.119901   LR 0.010000
INFO - Training [13][  260/  391]   Loss 0.176085   Top1 93.852163   Top5 99.924880   BatchTime 0.120481   LR 0.010000
INFO - Training [13][  280/  391]   Loss 0.175097   Top1 93.892299   Top5 99.919085   BatchTime 0.120652   LR 0.010000
INFO - Training [13][  300/  391]   Loss 0.175532   Top1 93.841146   Top5 99.921875   BatchTime 0.120816   LR 0.010000
INFO - Training [13][  320/  391]   Loss 0.176729   Top1 93.813477   Top5 99.921875   BatchTime 0.120760   LR 0.010000
INFO - Training [13][  340/  391]   Loss 0.177504   Top1 93.789062   Top5 99.914982   BatchTime 0.118328   LR 0.010000
INFO - Training [13][  360/  391]   Loss 0.178126   Top1 93.756510   Top5 99.913194   BatchTime 0.116687   LR 0.010000
INFO - Training [13][  380/  391]   Loss 0.178528   Top1 93.760280   Top5 99.915707   BatchTime 0.114876   LR 0.010000
INFO - ==> Top1: 93.728    Top5: 99.914    Loss: 0.179
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [13][   20/   79]   Loss 0.403026   Top1 88.281250   Top5 99.335938   BatchTime 0.151978
INFO - Validation [13][   40/   79]   Loss 0.401844   Top1 88.457031   Top5 99.375000   BatchTime 0.107151
INFO - Validation [13][   60/   79]   Loss 0.392098   Top1 88.554688   Top5 99.414062   BatchTime 0.092407
INFO - ==> Top1: 88.510    Top5: 99.470    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 88.510   Top5: 99.470] Sparsity : 0.837
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 88.280   Top5: 99.620] Sparsity : 0.822
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  14
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [14][   20/  391]   Loss 0.149856   Top1 94.804688   Top5 99.960938   BatchTime 0.212485   LR 0.010000
INFO - Training [14][   40/  391]   Loss 0.146092   Top1 94.863281   Top5 99.941406   BatchTime 0.168112   LR 0.010000
INFO - Training [14][   60/  391]   Loss 0.153912   Top1 94.557292   Top5 99.921875   BatchTime 0.152995   LR 0.010000
INFO - Training [14][   80/  391]   Loss 0.152566   Top1 94.736328   Top5 99.912109   BatchTime 0.145474   LR 0.010000
INFO - Training [14][  100/  391]   Loss 0.154811   Top1 94.632812   Top5 99.921875   BatchTime 0.141044   LR 0.010000
INFO - Training [14][  120/  391]   Loss 0.155385   Top1 94.648438   Top5 99.915365   BatchTime 0.132390   LR 0.010000
INFO - Training [14][  140/  391]   Loss 0.155538   Top1 94.642857   Top5 99.927455   BatchTime 0.126286   LR 0.010000
INFO - Training [14][  160/  391]   Loss 0.156884   Top1 94.570312   Top5 99.936523   BatchTime 0.122150   LR 0.010000
INFO - Training [14][  180/  391]   Loss 0.156499   Top1 94.539931   Top5 99.939236   BatchTime 0.117058   LR 0.010000
INFO - Training [14][  200/  391]   Loss 0.156753   Top1 94.476562   Top5 99.937500   BatchTime 0.118622   LR 0.010000
INFO - Training [14][  220/  391]   Loss 0.157911   Top1 94.399858   Top5 99.928977   BatchTime 0.119072   LR 0.010000
INFO - Training [14][  240/  391]   Loss 0.158585   Top1 94.410807   Top5 99.931641   BatchTime 0.119515   LR 0.010000
INFO - Training [14][  260/  391]   Loss 0.160202   Top1 94.335938   Top5 99.930889   BatchTime 0.119886   LR 0.010000
INFO - Training [14][  280/  391]   Loss 0.160646   Top1 94.344308   Top5 99.924665   BatchTime 0.120151   LR 0.010000
INFO - Training [14][  300/  391]   Loss 0.162331   Top1 94.289062   Top5 99.924479   BatchTime 0.120339   LR 0.010000
INFO - Training [14][  320/  391]   Loss 0.162865   Top1 94.265137   Top5 99.921875   BatchTime 0.120570   LR 0.010000
INFO - Training [14][  340/  391]   Loss 0.164325   Top1 94.234835   Top5 99.919577   BatchTime 0.120695   LR 0.010000
INFO - Training [14][  360/  391]   Loss 0.164813   Top1 94.220920   Top5 99.921875   BatchTime 0.120813   LR 0.010000
INFO - Training [14][  380/  391]   Loss 0.164958   Top1 94.231086   Top5 99.923931   BatchTime 0.120697   LR 0.010000
INFO - ==> Top1: 94.206    Top5: 99.924    Loss: 0.166
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [14][   20/   79]   Loss 0.378494   Top1 88.828125   Top5 99.335938   BatchTime 0.132486
INFO - Validation [14][   40/   79]   Loss 0.377423   Top1 88.769531   Top5 99.296875   BatchTime 0.079681
INFO - Validation [14][   60/   79]   Loss 0.374469   Top1 88.763021   Top5 99.414062   BatchTime 0.061935
INFO - ==> Top1: 88.650    Top5: 99.480    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 88.650   Top5: 99.480] Sparsity : 0.839
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 88.510   Top5: 99.470] Sparsity : 0.837
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  15
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [15][   20/  391]   Loss 0.146961   Top1 94.453125   Top5 99.960938   BatchTime 0.223217   LR 0.010000
INFO - Training [15][   40/  391]   Loss 0.147783   Top1 94.296875   Top5 99.980469   BatchTime 0.173334   LR 0.010000
INFO - Training [15][   60/  391]   Loss 0.144320   Top1 94.583333   Top5 99.960938   BatchTime 0.156836   LR 0.010000
INFO - Training [15][   80/  391]   Loss 0.151294   Top1 94.365234   Top5 99.951172   BatchTime 0.148504   LR 0.010000
INFO - Training [15][  100/  391]   Loss 0.152309   Top1 94.406250   Top5 99.960938   BatchTime 0.143517   LR 0.010000
INFO - Training [15][  120/  391]   Loss 0.149674   Top1 94.524740   Top5 99.960938   BatchTime 0.140207   LR 0.010000
INFO - Training [15][  140/  391]   Loss 0.153058   Top1 94.414062   Top5 99.966518   BatchTime 0.137752   LR 0.010000
INFO - Training [15][  160/  391]   Loss 0.152584   Top1 94.506836   Top5 99.951172   BatchTime 0.135902   LR 0.010000
INFO - Training [15][  180/  391]   Loss 0.153038   Top1 94.487847   Top5 99.952257   BatchTime 0.134446   LR 0.010000
INFO - Training [15][  200/  391]   Loss 0.152765   Top1 94.507812   Top5 99.953125   BatchTime 0.128745   LR 0.010000
INFO - Training [15][  220/  391]   Loss 0.155094   Top1 94.463778   Top5 99.953835   BatchTime 0.125443   LR 0.010000
INFO - Training [15][  240/  391]   Loss 0.154833   Top1 94.436849   Top5 99.951172   BatchTime 0.122457   LR 0.010000
INFO - Training [15][  260/  391]   Loss 0.154241   Top1 94.513221   Top5 99.954928   BatchTime 0.120641   LR 0.010000
INFO - Training [15][  280/  391]   Loss 0.156025   Top1 94.464286   Top5 99.949777   BatchTime 0.120927   LR 0.010000
INFO - Training [15][  300/  391]   Loss 0.156094   Top1 94.481771   Top5 99.947917   BatchTime 0.121091   LR 0.010000
INFO - Training [15][  320/  391]   Loss 0.156582   Top1 94.482422   Top5 99.948730   BatchTime 0.121524   LR 0.010000
INFO - Training [15][  340/  391]   Loss 0.156262   Top1 94.494485   Top5 99.949449   BatchTime 0.121641   LR 0.010000
INFO - Training [15][  360/  391]   Loss 0.156642   Top1 94.481337   Top5 99.950087   BatchTime 0.121744   LR 0.010000
INFO - Training [15][  380/  391]   Loss 0.156283   Top1 94.481908   Top5 99.948602   BatchTime 0.121792   LR 0.010000
INFO - ==> Top1: 94.496    Top5: 99.950    Loss: 0.156
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [15][   20/   79]   Loss 0.391506   Top1 88.398438   Top5 99.414062   BatchTime 0.152037
INFO - Validation [15][   40/   79]   Loss 0.398570   Top1 88.281250   Top5 99.414062   BatchTime 0.108592
INFO - Validation [15][   60/   79]   Loss 0.387138   Top1 88.515625   Top5 99.531250   BatchTime 0.091293
INFO - ==> Top1: 88.460    Top5: 99.590    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 88.650   Top5: 99.480] Sparsity : 0.839
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 88.510   Top5: 99.470] Sparsity : 0.837
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  16
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [16][   20/  391]   Loss 0.151040   Top1 94.570312   Top5 100.000000   BatchTime 0.186211   LR 0.010000
INFO - Training [16][   40/  391]   Loss 0.138240   Top1 95.019531   Top5 100.000000   BatchTime 0.131086   LR 0.010000
INFO - Training [16][   60/  391]   Loss 0.143470   Top1 94.843750   Top5 99.973958   BatchTime 0.130473   LR 0.010000
INFO - Training [16][   80/  391]   Loss 0.147762   Top1 94.863281   Top5 99.941406   BatchTime 0.128981   LR 0.010000
INFO - Training [16][  100/  391]   Loss 0.147446   Top1 94.773438   Top5 99.921875   BatchTime 0.127875   LR 0.010000
INFO - Training [16][  120/  391]   Loss 0.147497   Top1 94.804688   Top5 99.934896   BatchTime 0.127247   LR 0.010000
INFO - Training [16][  140/  391]   Loss 0.148803   Top1 94.720982   Top5 99.938616   BatchTime 0.126749   LR 0.010000
INFO - Training [16][  160/  391]   Loss 0.147855   Top1 94.770508   Top5 99.936523   BatchTime 0.126378   LR 0.010000
INFO - Training [16][  180/  391]   Loss 0.150189   Top1 94.696181   Top5 99.926215   BatchTime 0.126041   LR 0.010000
INFO - Training [16][  200/  391]   Loss 0.149827   Top1 94.667969   Top5 99.933594   BatchTime 0.125724   LR 0.010000
INFO - Training [16][  220/  391]   Loss 0.149804   Top1 94.634233   Top5 99.936080   BatchTime 0.125491   LR 0.010000
INFO - Training [16][  240/  391]   Loss 0.149911   Top1 94.645182   Top5 99.938151   BatchTime 0.125498   LR 0.010000
INFO - Training [16][  260/  391]   Loss 0.150403   Top1 94.633413   Top5 99.942909   BatchTime 0.121722   LR 0.010000
INFO - Training [16][  280/  391]   Loss 0.152297   Top1 94.531250   Top5 99.938616   BatchTime 0.119733   LR 0.010000
INFO - Training [16][  300/  391]   Loss 0.153017   Top1 94.500000   Top5 99.937500   BatchTime 0.117764   LR 0.010000
INFO - Training [16][  320/  391]   Loss 0.153468   Top1 94.501953   Top5 99.941406   BatchTime 0.116712   LR 0.010000
INFO - Training [16][  340/  391]   Loss 0.154503   Top1 94.478401   Top5 99.942555   BatchTime 0.117118   LR 0.010000
INFO - Training [16][  360/  391]   Loss 0.155953   Top1 94.431424   Top5 99.945747   BatchTime 0.117461   LR 0.010000
INFO - Training [16][  380/  391]   Loss 0.156842   Top1 94.412007   Top5 99.944490   BatchTime 0.117752   LR 0.010000
INFO - ==> Top1: 94.396    Top5: 99.938    Loss: 0.158
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [16][   20/   79]   Loss 0.382702   Top1 88.164062   Top5 99.531250   BatchTime 0.152141
INFO - Validation [16][   40/   79]   Loss 0.396774   Top1 88.125000   Top5 99.394531   BatchTime 0.108515
INFO - Validation [16][   60/   79]   Loss 0.383967   Top1 88.424479   Top5 99.440104   BatchTime 0.093909
INFO - ==> Top1: 88.220    Top5: 99.480    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 88.650   Top5: 99.480] Sparsity : 0.839
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 88.510   Top5: 99.470] Sparsity : 0.837
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  17
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [17][   20/  391]   Loss 0.165564   Top1 94.492188   Top5 99.804688   BatchTime 0.209644   LR 0.010000
INFO - Training [17][   40/  391]   Loss 0.184555   Top1 93.378906   Top5 99.843750   BatchTime 0.148213   LR 0.010000
INFO - Training [17][   60/  391]   Loss 0.182680   Top1 93.489583   Top5 99.843750   BatchTime 0.128556   LR 0.010000
INFO - Training [17][   80/  391]   Loss 0.185727   Top1 93.398438   Top5 99.873047   BatchTime 0.118889   LR 0.010000
INFO - Training [17][  100/  391]   Loss 0.185403   Top1 93.375000   Top5 99.890625   BatchTime 0.110675   LR 0.010000
INFO - Training [17][  120/  391]   Loss 0.188133   Top1 93.235677   Top5 99.895833   BatchTime 0.113820   LR 0.010000
INFO - Training [17][  140/  391]   Loss 0.187778   Top1 93.275670   Top5 99.910714   BatchTime 0.115195   LR 0.010000
INFO - Training [17][  160/  391]   Loss 0.189407   Top1 93.251953   Top5 99.902344   BatchTime 0.116318   LR 0.010000
INFO - Training [17][  180/  391]   Loss 0.189924   Top1 93.190104   Top5 99.904514   BatchTime 0.117068   LR 0.010000
INFO - Training [17][  200/  391]   Loss 0.190387   Top1 93.222656   Top5 99.906250   BatchTime 0.117728   LR 0.010000
INFO - Training [17][  220/  391]   Loss 0.190098   Top1 93.231534   Top5 99.900568   BatchTime 0.118235   LR 0.010000
INFO - Training [17][  240/  391]   Loss 0.190759   Top1 93.248698   Top5 99.899089   BatchTime 0.118625   LR 0.010000
INFO - Training [17][  260/  391]   Loss 0.191395   Top1 93.272236   Top5 99.900841   BatchTime 0.118984   LR 0.010000
INFO - Training [17][  280/  391]   Loss 0.190260   Top1 93.281250   Top5 99.905134   BatchTime 0.119311   LR 0.010000
INFO - Training [17][  300/  391]   Loss 0.190508   Top1 93.278646   Top5 99.908854   BatchTime 0.119311   LR 0.010000
INFO - Training [17][  320/  391]   Loss 0.190576   Top1 93.312988   Top5 99.902344   BatchTime 0.116750   LR 0.010000
INFO - Training [17][  340/  391]   Loss 0.189332   Top1 93.340993   Top5 99.901195   BatchTime 0.115048   LR 0.010000
INFO - Training [17][  360/  391]   Loss 0.189230   Top1 93.344184   Top5 99.900174   BatchTime 0.113195   LR 0.010000
INFO - Training [17][  380/  391]   Loss 0.189060   Top1 93.363487   Top5 99.891036   BatchTime 0.112238   LR 0.010000
INFO - ==> Top1: 93.376    Top5: 99.886    Loss: 0.189
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [17][   20/   79]   Loss 0.406813   Top1 87.460938   Top5 99.492188   BatchTime 0.152844
INFO - Validation [17][   40/   79]   Loss 0.415871   Top1 87.636719   Top5 99.316406   BatchTime 0.108145
INFO - Validation [17][   60/   79]   Loss 0.398538   Top1 88.138021   Top5 99.388021   BatchTime 0.093627
INFO - ==> Top1: 88.020    Top5: 99.420    Loss: 0.398
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 88.650   Top5: 99.480] Sparsity : 0.839
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 88.510   Top5: 99.470] Sparsity : 0.837
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  18
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [18][   20/  391]   Loss 0.179443   Top1 93.828125   Top5 99.882812   BatchTime 0.213215   LR 0.010000
INFO - Training [18][   40/  391]   Loss 0.189029   Top1 93.359375   Top5 99.902344   BatchTime 0.168166   LR 0.010000
INFO - Training [18][   60/  391]   Loss 0.173049   Top1 93.906250   Top5 99.934896   BatchTime 0.153551   LR 0.010000
INFO - Training [18][   80/  391]   Loss 0.174971   Top1 93.828125   Top5 99.941406   BatchTime 0.146051   LR 0.010000
INFO - Training [18][  100/  391]   Loss 0.172921   Top1 93.843750   Top5 99.929688   BatchTime 0.135080   LR 0.010000
INFO - Training [18][  120/  391]   Loss 0.172164   Top1 93.880208   Top5 99.921875   BatchTime 0.127231   LR 0.010000
INFO - Training [18][  140/  391]   Loss 0.172572   Top1 93.856027   Top5 99.927455   BatchTime 0.121842   LR 0.010000
INFO - Training [18][  160/  391]   Loss 0.173483   Top1 93.901367   Top5 99.926758   BatchTime 0.115991   LR 0.010000
INFO - Training [18][  180/  391]   Loss 0.174621   Top1 93.823785   Top5 99.921875   BatchTime 0.117706   LR 0.010000
INFO - Training [18][  200/  391]   Loss 0.173908   Top1 93.828125   Top5 99.925781   BatchTime 0.118275   LR 0.010000
INFO - Training [18][  220/  391]   Loss 0.175674   Top1 93.778409   Top5 99.918324   BatchTime 0.118800   LR 0.010000
INFO - Training [18][  240/  391]   Loss 0.177672   Top1 93.750000   Top5 99.912109   BatchTime 0.119200   LR 0.010000
INFO - Training [18][  260/  391]   Loss 0.178311   Top1 93.713942   Top5 99.912861   BatchTime 0.119493   LR 0.010000
INFO - Training [18][  280/  391]   Loss 0.179369   Top1 93.694196   Top5 99.910714   BatchTime 0.119770   LR 0.010000
INFO - Training [18][  300/  391]   Loss 0.178756   Top1 93.708333   Top5 99.908854   BatchTime 0.120066   LR 0.010000
INFO - Training [18][  320/  391]   Loss 0.178624   Top1 93.688965   Top5 99.904785   BatchTime 0.120210   LR 0.010000
INFO - Training [18][  340/  391]   Loss 0.179114   Top1 93.676471   Top5 99.901195   BatchTime 0.120392   LR 0.010000
INFO - Training [18][  360/  391]   Loss 0.179355   Top1 93.689236   Top5 99.893663   BatchTime 0.120320   LR 0.010000
INFO - Training [18][  380/  391]   Loss 0.179016   Top1 93.686266   Top5 99.897204   BatchTime 0.117847   LR 0.010000
INFO - ==> Top1: 93.676    Top5: 99.894    Loss: 0.179
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [18][   20/   79]   Loss 0.409737   Top1 87.812500   Top5 99.335938   BatchTime 0.121375
INFO - Validation [18][   40/   79]   Loss 0.400281   Top1 88.085938   Top5 99.492188   BatchTime 0.073894
INFO - Validation [18][   60/   79]   Loss 0.385401   Top1 88.333333   Top5 99.544271   BatchTime 0.069176
INFO - ==> Top1: 88.380    Top5: 99.570    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 88.650   Top5: 99.480] Sparsity : 0.839
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 88.510   Top5: 99.470] Sparsity : 0.837
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  19
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [19][   20/  391]   Loss 0.189562   Top1 93.320312   Top5 99.882812   BatchTime 0.215013   LR 0.010000
INFO - Training [19][   40/  391]   Loss 0.172840   Top1 94.101562   Top5 99.921875   BatchTime 0.169607   LR 0.010000
INFO - Training [19][   60/  391]   Loss 0.169421   Top1 94.218750   Top5 99.921875   BatchTime 0.154695   LR 0.010000
INFO - Training [19][   80/  391]   Loss 0.168815   Top1 94.199219   Top5 99.931641   BatchTime 0.147078   LR 0.010000
INFO - Training [19][  100/  391]   Loss 0.169417   Top1 94.156250   Top5 99.937500   BatchTime 0.142317   LR 0.010000
INFO - Training [19][  120/  391]   Loss 0.168513   Top1 94.218750   Top5 99.941406   BatchTime 0.139039   LR 0.010000
INFO - Training [19][  140/  391]   Loss 0.167526   Top1 94.274554   Top5 99.944196   BatchTime 0.136799   LR 0.010000
INFO - Training [19][  160/  391]   Loss 0.167848   Top1 94.213867   Top5 99.936523   BatchTime 0.133633   LR 0.010000
INFO - Training [19][  180/  391]   Loss 0.166085   Top1 94.266493   Top5 99.939236   BatchTime 0.128131   LR 0.010000
INFO - Training [19][  200/  391]   Loss 0.165234   Top1 94.289062   Top5 99.941406   BatchTime 0.124227   LR 0.010000
INFO - Training [19][  220/  391]   Loss 0.163720   Top1 94.357244   Top5 99.939631   BatchTime 0.120662   LR 0.010000
INFO - Training [19][  240/  391]   Loss 0.164325   Top1 94.322917   Top5 99.931641   BatchTime 0.120501   LR 0.010000
INFO - Training [19][  260/  391]   Loss 0.165382   Top1 94.305889   Top5 99.933894   BatchTime 0.120747   LR 0.010000
INFO - Training [19][  280/  391]   Loss 0.164924   Top1 94.313616   Top5 99.933036   BatchTime 0.120980   LR 0.010000
INFO - Training [19][  300/  391]   Loss 0.165826   Top1 94.247396   Top5 99.932292   BatchTime 0.121201   LR 0.010000
INFO - Training [19][  320/  391]   Loss 0.166023   Top1 94.204102   Top5 99.936523   BatchTime 0.121370   LR 0.010000
INFO - Training [19][  340/  391]   Loss 0.166828   Top1 94.161305   Top5 99.935662   BatchTime 0.121459   LR 0.010000
INFO - Training [19][  360/  391]   Loss 0.166854   Top1 94.134115   Top5 99.930556   BatchTime 0.121544   LR 0.010000
INFO - Training [19][  380/  391]   Loss 0.167004   Top1 94.140625   Top5 99.932155   BatchTime 0.121591   LR 0.010000
INFO - ==> Top1: 94.114    Top5: 99.926    Loss: 0.168
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [19][   20/   79]   Loss 0.420085   Top1 88.398438   Top5 99.453125   BatchTime 0.143478
INFO - Validation [19][   40/   79]   Loss 0.412547   Top1 88.300781   Top5 99.472656   BatchTime 0.085347
INFO - Validation [19][   60/   79]   Loss 0.401815   Top1 88.658854   Top5 99.466146   BatchTime 0.065756
INFO - ==> Top1: 88.760    Top5: 99.540    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 2 ==> Epoch [19][Top1: 88.760   Top5: 99.540] Sparsity : 0.861
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 88.650   Top5: 99.480] Sparsity : 0.839
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  20
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [20][   20/  391]   Loss 0.161034   Top1 94.531250   Top5 99.882812   BatchTime 0.182605   LR 0.010000
INFO - Training [20][   40/  391]   Loss 0.151589   Top1 94.941406   Top5 99.921875   BatchTime 0.153167   LR 0.010000
INFO - Training [20][   60/  391]   Loss 0.144683   Top1 95.065104   Top5 99.921875   BatchTime 0.143264   LR 0.010000
INFO - Training [20][   80/  391]   Loss 0.150461   Top1 94.824219   Top5 99.912109   BatchTime 0.138289   LR 0.010000
INFO - Training [20][  100/  391]   Loss 0.153430   Top1 94.656250   Top5 99.921875   BatchTime 0.135275   LR 0.010000
INFO - Training [20][  120/  391]   Loss 0.154691   Top1 94.544271   Top5 99.915365   BatchTime 0.133494   LR 0.010000
INFO - Training [20][  140/  391]   Loss 0.155904   Top1 94.453125   Top5 99.921875   BatchTime 0.132054   LR 0.010000
INFO - Training [20][  160/  391]   Loss 0.155614   Top1 94.414062   Top5 99.921875   BatchTime 0.131046   LR 0.010000
INFO - Training [20][  180/  391]   Loss 0.153948   Top1 94.496528   Top5 99.921875   BatchTime 0.130005   LR 0.010000
INFO - Training [20][  200/  391]   Loss 0.155517   Top1 94.488281   Top5 99.921875   BatchTime 0.129406   LR 0.010000
INFO - Training [20][  220/  391]   Loss 0.156629   Top1 94.403409   Top5 99.918324   BatchTime 0.128491   LR 0.010000
INFO - Training [20][  240/  391]   Loss 0.156614   Top1 94.414062   Top5 99.921875   BatchTime 0.124449   LR 0.010000
INFO - Training [20][  260/  391]   Loss 0.156787   Top1 94.414062   Top5 99.918870   BatchTime 0.121673   LR 0.010000
INFO - Training [20][  280/  391]   Loss 0.157612   Top1 94.430804   Top5 99.919085   BatchTime 0.118986   LR 0.010000
INFO - Training [20][  300/  391]   Loss 0.157377   Top1 94.445312   Top5 99.921875   BatchTime 0.118271   LR 0.010000
INFO - Training [20][  320/  391]   Loss 0.156780   Top1 94.470215   Top5 99.921875   BatchTime 0.118632   LR 0.010000
INFO - Training [20][  340/  391]   Loss 0.156246   Top1 94.510570   Top5 99.924173   BatchTime 0.118961   LR 0.010000
INFO - Training [20][  360/  391]   Loss 0.156641   Top1 94.511719   Top5 99.924045   BatchTime 0.119151   LR 0.010000
INFO - Training [20][  380/  391]   Loss 0.156694   Top1 94.518914   Top5 99.925987   BatchTime 0.119273   LR 0.010000
INFO - ==> Top1: 94.538    Top5: 99.926    Loss: 0.156
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [20][   20/   79]   Loss 0.370160   Top1 89.296875   Top5 99.414062   BatchTime 0.151613
INFO - Validation [20][   40/   79]   Loss 0.376745   Top1 89.121094   Top5 99.531250   BatchTime 0.108969
INFO - Validation [20][   60/   79]   Loss 0.369512   Top1 89.361979   Top5 99.531250   BatchTime 0.094006
INFO - ==> Top1: 89.270    Top5: 99.560    Loss: 0.364
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 88.760   Top5: 99.540] Sparsity : 0.861
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  21
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [21][   20/  391]   Loss 0.129793   Top1 95.390625   Top5 99.960938   BatchTime 0.174039   LR 0.010000
INFO - Training [21][   40/  391]   Loss 0.127024   Top1 95.605469   Top5 99.960938   BatchTime 0.133801   LR 0.010000
INFO - Training [21][   60/  391]   Loss 0.130258   Top1 95.559896   Top5 99.960938   BatchTime 0.118364   LR 0.010000
INFO - Training [21][   80/  391]   Loss 0.132804   Top1 95.292969   Top5 99.951172   BatchTime 0.110955   LR 0.010000
INFO - Training [21][  100/  391]   Loss 0.136115   Top1 95.187500   Top5 99.937500   BatchTime 0.113596   LR 0.010000
INFO - Training [21][  120/  391]   Loss 0.140008   Top1 94.941406   Top5 99.934896   BatchTime 0.115383   LR 0.010000
INFO - Training [21][  140/  391]   Loss 0.138490   Top1 95.027902   Top5 99.944196   BatchTime 0.116483   LR 0.010000
INFO - Training [21][  160/  391]   Loss 0.140492   Top1 94.985352   Top5 99.946289   BatchTime 0.117362   LR 0.010000
INFO - Training [21][  180/  391]   Loss 0.140189   Top1 95.030382   Top5 99.952257   BatchTime 0.118084   LR 0.010000
INFO - Training [21][  200/  391]   Loss 0.140282   Top1 95.039062   Top5 99.949219   BatchTime 0.118652   LR 0.010000
INFO - Training [21][  220/  391]   Loss 0.142573   Top1 94.957386   Top5 99.953835   BatchTime 0.119064   LR 0.010000
INFO - Training [21][  240/  391]   Loss 0.141675   Top1 95.003255   Top5 99.954427   BatchTime 0.119410   LR 0.010000
INFO - Training [21][  260/  391]   Loss 0.143191   Top1 94.951923   Top5 99.948918   BatchTime 0.119672   LR 0.010000
INFO - Training [21][  280/  391]   Loss 0.146067   Top1 94.857701   Top5 99.944196   BatchTime 0.119424   LR 0.010000
INFO - Training [21][  300/  391]   Loss 0.149223   Top1 94.736979   Top5 99.940104   BatchTime 0.116908   LR 0.010000
INFO - Training [21][  320/  391]   Loss 0.151380   Top1 94.675293   Top5 99.936523   BatchTime 0.115225   LR 0.010000
INFO - Training [21][  340/  391]   Loss 0.153182   Top1 94.634651   Top5 99.937960   BatchTime 0.113276   LR 0.010000
INFO - Training [21][  360/  391]   Loss 0.155723   Top1 94.531250   Top5 99.932726   BatchTime 0.113036   LR 0.010000
INFO - Training [21][  380/  391]   Loss 0.157394   Top1 94.488076   Top5 99.934211   BatchTime 0.113539   LR 0.010000
INFO - ==> Top1: 94.462    Top5: 99.932    Loss: 0.158
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [21][   20/   79]   Loss 0.402496   Top1 88.476562   Top5 99.257812   BatchTime 0.150972
INFO - Validation [21][   40/   79]   Loss 0.411139   Top1 87.968750   Top5 99.394531   BatchTime 0.107969
INFO - Validation [21][   60/   79]   Loss 0.409352   Top1 88.046875   Top5 99.427083   BatchTime 0.092657
INFO - ==> Top1: 88.310    Top5: 99.440    Loss: 0.399
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 88.760   Top5: 99.540] Sparsity : 0.861
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  22
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [22][   20/  391]   Loss 0.156517   Top1 94.570312   Top5 99.921875   BatchTime 0.211634   LR 0.010000
INFO - Training [22][   40/  391]   Loss 0.162272   Top1 94.589844   Top5 99.941406   BatchTime 0.167461   LR 0.010000
INFO - Training [22][   60/  391]   Loss 0.164933   Top1 94.388021   Top5 99.921875   BatchTime 0.152408   LR 0.010000
INFO - Training [22][   80/  391]   Loss 0.174006   Top1 93.916016   Top5 99.902344   BatchTime 0.135744   LR 0.010000
INFO - Training [22][  100/  391]   Loss 0.172008   Top1 93.976562   Top5 99.890625   BatchTime 0.128498   LR 0.010000
INFO - Training [22][  120/  391]   Loss 0.173944   Top1 93.919271   Top5 99.902344   BatchTime 0.122049   LR 0.010000
INFO - Training [22][  140/  391]   Loss 0.175390   Top1 93.861607   Top5 99.910714   BatchTime 0.119472   LR 0.010000
INFO - Training [22][  160/  391]   Loss 0.176589   Top1 93.842773   Top5 99.921875   BatchTime 0.119899   LR 0.010000
INFO - Training [22][  180/  391]   Loss 0.177826   Top1 93.780382   Top5 99.921875   BatchTime 0.120294   LR 0.010000
INFO - Training [22][  200/  391]   Loss 0.178742   Top1 93.742188   Top5 99.906250   BatchTime 0.120573   LR 0.010000
INFO - Training [22][  220/  391]   Loss 0.178946   Top1 93.700284   Top5 99.914773   BatchTime 0.120834   LR 0.010000
INFO - Training [22][  240/  391]   Loss 0.177902   Top1 93.730469   Top5 99.921875   BatchTime 0.121246   LR 0.010000
INFO - Training [22][  260/  391]   Loss 0.178157   Top1 93.719952   Top5 99.924880   BatchTime 0.121463   LR 0.010000
INFO - Training [22][  280/  391]   Loss 0.178110   Top1 93.738839   Top5 99.919085   BatchTime 0.121612   LR 0.010000
INFO - Training [22][  300/  391]   Loss 0.179035   Top1 93.677083   Top5 99.921875   BatchTime 0.121701   LR 0.010000
INFO - Training [22][  320/  391]   Loss 0.177622   Top1 93.713379   Top5 99.926758   BatchTime 0.121783   LR 0.010000
INFO - Training [22][  340/  391]   Loss 0.176886   Top1 93.717831   Top5 99.928768   BatchTime 0.120378   LR 0.010000
INFO - Training [22][  360/  391]   Loss 0.175959   Top1 93.760851   Top5 99.928385   BatchTime 0.118355   LR 0.010000
INFO - Training [22][  380/  391]   Loss 0.176963   Top1 93.727385   Top5 99.928043   BatchTime 0.116980   LR 0.010000
INFO - ==> Top1: 93.712    Top5: 99.928    Loss: 0.177
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [22][   20/   79]   Loss 0.413975   Top1 87.695312   Top5 99.570312   BatchTime 0.160197
INFO - Validation [22][   40/   79]   Loss 0.413445   Top1 88.105469   Top5 99.414062   BatchTime 0.111756
INFO - Validation [22][   60/   79]   Loss 0.404573   Top1 88.138021   Top5 99.440104   BatchTime 0.095600
INFO - ==> Top1: 88.130    Top5: 99.450    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 88.760   Top5: 99.540] Sparsity : 0.861
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  23
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [23][   20/  391]   Loss 0.175104   Top1 93.554688   Top5 99.882812   BatchTime 0.212497   LR 0.010000
INFO - Training [23][   40/  391]   Loss 0.171690   Top1 93.457031   Top5 99.902344   BatchTime 0.167053   LR 0.010000
INFO - Training [23][   60/  391]   Loss 0.168678   Top1 93.697917   Top5 99.921875   BatchTime 0.152380   LR 0.010000
INFO - Training [23][   80/  391]   Loss 0.166080   Top1 93.828125   Top5 99.931641   BatchTime 0.144936   LR 0.010000
INFO - Training [23][  100/  391]   Loss 0.165147   Top1 93.882812   Top5 99.945312   BatchTime 0.140570   LR 0.010000
INFO - Training [23][  120/  391]   Loss 0.161097   Top1 94.127604   Top5 99.954427   BatchTime 0.136844   LR 0.010000
INFO - Training [23][  140/  391]   Loss 0.162328   Top1 94.246652   Top5 99.944196   BatchTime 0.128398   LR 0.010000
INFO - Training [23][  160/  391]   Loss 0.165112   Top1 94.223633   Top5 99.936523   BatchTime 0.123812   LR 0.010000
INFO - Training [23][  180/  391]   Loss 0.164891   Top1 94.227431   Top5 99.939236   BatchTime 0.119585   LR 0.010000
INFO - Training [23][  200/  391]   Loss 0.166150   Top1 94.199219   Top5 99.929688   BatchTime 0.118282   LR 0.010000
INFO - Training [23][  220/  391]   Loss 0.166692   Top1 94.193892   Top5 99.928977   BatchTime 0.118819   LR 0.010000
INFO - Training [23][  240/  391]   Loss 0.168402   Top1 94.114583   Top5 99.921875   BatchTime 0.119238   LR 0.010000
INFO - Training [23][  260/  391]   Loss 0.170670   Top1 94.035457   Top5 99.912861   BatchTime 0.119627   LR 0.010000
INFO - Training [23][  280/  391]   Loss 0.170791   Top1 94.009487   Top5 99.916295   BatchTime 0.119906   LR 0.010000
INFO - Training [23][  300/  391]   Loss 0.170091   Top1 94.023438   Top5 99.916667   BatchTime 0.120254   LR 0.010000
INFO - Training [23][  320/  391]   Loss 0.169368   Top1 94.074707   Top5 99.919434   BatchTime 0.120440   LR 0.010000
INFO - Training [23][  340/  391]   Loss 0.168437   Top1 94.110754   Top5 99.921875   BatchTime 0.120594   LR 0.010000
INFO - Training [23][  360/  391]   Loss 0.168579   Top1 94.103733   Top5 99.926215   BatchTime 0.120696   LR 0.010000
INFO - Training [23][  380/  391]   Loss 0.167891   Top1 94.142681   Top5 99.928043   BatchTime 0.120764   LR 0.010000
INFO - ==> Top1: 94.136    Top5: 99.930    Loss: 0.168
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [23][   20/   79]   Loss 0.402443   Top1 88.671875   Top5 99.375000   BatchTime 0.134719
INFO - Validation [23][   40/   79]   Loss 0.405776   Top1 88.750000   Top5 99.394531   BatchTime 0.088648
INFO - Validation [23][   60/   79]   Loss 0.400743   Top1 88.841146   Top5 99.414062   BatchTime 0.068089
INFO - ==> Top1: 88.860    Top5: 99.480    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 88.860   Top5: 99.480] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  24
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [24][   20/  391]   Loss 0.161655   Top1 94.453125   Top5 99.960938   BatchTime 0.222544   LR 0.010000
INFO - Training [24][   40/  391]   Loss 0.161528   Top1 94.453125   Top5 99.941406   BatchTime 0.173168   LR 0.010000
INFO - Training [24][   60/  391]   Loss 0.158468   Top1 94.531250   Top5 99.947917   BatchTime 0.156653   LR 0.010000
INFO - Training [24][   80/  391]   Loss 0.157183   Top1 94.492188   Top5 99.951172   BatchTime 0.148270   LR 0.010000
INFO - Training [24][  100/  391]   Loss 0.156509   Top1 94.476562   Top5 99.945312   BatchTime 0.143325   LR 0.010000
INFO - Training [24][  120/  391]   Loss 0.155594   Top1 94.518229   Top5 99.934896   BatchTime 0.140110   LR 0.010000
INFO - Training [24][  140/  391]   Loss 0.153790   Top1 94.547991   Top5 99.933036   BatchTime 0.138203   LR 0.010000
INFO - Training [24][  160/  391]   Loss 0.153531   Top1 94.580078   Top5 99.936523   BatchTime 0.136350   LR 0.010000
INFO - Training [24][  180/  391]   Loss 0.155206   Top1 94.518229   Top5 99.943576   BatchTime 0.134891   LR 0.010000
INFO - Training [24][  200/  391]   Loss 0.157973   Top1 94.414062   Top5 99.941406   BatchTime 0.129935   LR 0.010000
INFO - Training [24][  220/  391]   Loss 0.157224   Top1 94.410511   Top5 99.943182   BatchTime 0.126515   LR 0.010000
INFO - Training [24][  240/  391]   Loss 0.156746   Top1 94.423828   Top5 99.941406   BatchTime 0.123395   LR 0.010000
INFO - Training [24][  260/  391]   Loss 0.155558   Top1 94.465144   Top5 99.945913   BatchTime 0.121638   LR 0.010000
INFO - Training [24][  280/  391]   Loss 0.154521   Top1 94.503348   Top5 99.946987   BatchTime 0.121780   LR 0.010000
INFO - Training [24][  300/  391]   Loss 0.155241   Top1 94.486979   Top5 99.947917   BatchTime 0.121872   LR 0.010000
INFO - Training [24][  320/  391]   Loss 0.155911   Top1 94.470215   Top5 99.943848   BatchTime 0.122005   LR 0.010000
INFO - Training [24][  340/  391]   Loss 0.155828   Top1 94.473805   Top5 99.944853   BatchTime 0.122092   LR 0.010000
INFO - Training [24][  360/  391]   Loss 0.156705   Top1 94.442274   Top5 99.941406   BatchTime 0.122197   LR 0.010000
INFO - Training [24][  380/  391]   Loss 0.157272   Top1 94.405839   Top5 99.938322   BatchTime 0.122233   LR 0.010000
INFO - ==> Top1: 94.402    Top5: 99.940    Loss: 0.157
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [24][   20/   79]   Loss 0.389959   Top1 88.437500   Top5 99.453125   BatchTime 0.151506
INFO - Validation [24][   40/   79]   Loss 0.375927   Top1 88.964844   Top5 99.472656   BatchTime 0.108094
INFO - Validation [24][   60/   79]   Loss 0.370934   Top1 88.997396   Top5 99.531250   BatchTime 0.092229
INFO - ==> Top1: 89.040    Top5: 99.570    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.040   Top5: 99.570] Sparsity : 0.873
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 88.890   Top5: 99.530] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  25
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [25][   20/  391]   Loss 0.139991   Top1 94.765625   Top5 99.960938   BatchTime 0.185013   LR 0.010000
INFO - Training [25][   40/  391]   Loss 0.137457   Top1 95.039062   Top5 99.941406   BatchTime 0.136923   LR 0.010000
INFO - Training [25][   60/  391]   Loss 0.137901   Top1 95.065104   Top5 99.960938   BatchTime 0.132341   LR 0.010000
INFO - Training [25][   80/  391]   Loss 0.142503   Top1 94.990234   Top5 99.960938   BatchTime 0.130131   LR 0.010000
INFO - Training [25][  100/  391]   Loss 0.147532   Top1 94.781250   Top5 99.953125   BatchTime 0.129056   LR 0.010000
INFO - Training [25][  120/  391]   Loss 0.150733   Top1 94.720052   Top5 99.941406   BatchTime 0.128107   LR 0.010000
INFO - Training [25][  140/  391]   Loss 0.149444   Top1 94.715402   Top5 99.944196   BatchTime 0.127415   LR 0.010000
INFO - Training [25][  160/  391]   Loss 0.149867   Top1 94.711914   Top5 99.941406   BatchTime 0.126961   LR 0.010000
INFO - Training [25][  180/  391]   Loss 0.150338   Top1 94.735243   Top5 99.943576   BatchTime 0.126625   LR 0.010000
INFO - Training [25][  200/  391]   Loss 0.151085   Top1 94.730469   Top5 99.929688   BatchTime 0.126262   LR 0.010000
INFO - Training [25][  220/  391]   Loss 0.151616   Top1 94.673295   Top5 99.928977   BatchTime 0.125759   LR 0.010000
INFO - Training [25][  240/  391]   Loss 0.151736   Top1 94.707031   Top5 99.918620   BatchTime 0.125166   LR 0.010000
INFO - Training [25][  260/  391]   Loss 0.150832   Top1 94.699519   Top5 99.924880   BatchTime 0.121467   LR 0.010000
INFO - Training [25][  280/  391]   Loss 0.150835   Top1 94.709821   Top5 99.927455   BatchTime 0.119421   LR 0.010000
INFO - Training [25][  300/  391]   Loss 0.150962   Top1 94.705729   Top5 99.932292   BatchTime 0.117298   LR 0.010000
INFO - Training [25][  320/  391]   Loss 0.151978   Top1 94.655762   Top5 99.926758   BatchTime 0.116323   LR 0.010000
INFO - Training [25][  340/  391]   Loss 0.150792   Top1 94.696691   Top5 99.931066   BatchTime 0.116732   LR 0.010000
INFO - Training [25][  360/  391]   Loss 0.151250   Top1 94.676649   Top5 99.932726   BatchTime 0.117071   LR 0.010000
INFO - Training [25][  380/  391]   Loss 0.151021   Top1 94.689556   Top5 99.932155   BatchTime 0.117391   LR 0.010000
INFO - ==> Top1: 94.702    Top5: 99.930    Loss: 0.151
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [25][   20/   79]   Loss 0.391677   Top1 88.398438   Top5 99.492188   BatchTime 0.153640
INFO - Validation [25][   40/   79]   Loss 0.390050   Top1 88.515625   Top5 99.550781   BatchTime 0.109591
INFO - Validation [25][   60/   79]   Loss 0.384193   Top1 88.750000   Top5 99.531250   BatchTime 0.094494
INFO - ==> Top1: 88.960    Top5: 99.570    Loss: 0.377
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.040   Top5: 99.570] Sparsity : 0.873
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.960   Top5: 99.570] Sparsity : 0.874
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  26
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [26][   20/  391]   Loss 0.145641   Top1 94.687500   Top5 99.960938   BatchTime 0.209969   LR 0.010000
INFO - Training [26][   40/  391]   Loss 0.141514   Top1 94.960938   Top5 99.960938   BatchTime 0.149488   LR 0.010000
INFO - Training [26][   60/  391]   Loss 0.137360   Top1 95.052083   Top5 99.960938   BatchTime 0.130508   LR 0.010000
INFO - Training [26][   80/  391]   Loss 0.138592   Top1 95.107422   Top5 99.951172   BatchTime 0.120424   LR 0.010000
INFO - Training [26][  100/  391]   Loss 0.140959   Top1 95.015625   Top5 99.937500   BatchTime 0.114844   LR 0.010000
INFO - Training [26][  120/  391]   Loss 0.140243   Top1 95.039062   Top5 99.941406   BatchTime 0.116442   LR 0.010000
INFO - Training [26][  140/  391]   Loss 0.141275   Top1 95.039062   Top5 99.938616   BatchTime 0.117468   LR 0.010000
INFO - Training [26][  160/  391]   Loss 0.144731   Top1 94.902344   Top5 99.926758   BatchTime 0.118142   LR 0.010000
INFO - Training [26][  180/  391]   Loss 0.145766   Top1 94.861111   Top5 99.930556   BatchTime 0.118794   LR 0.010000
INFO - Training [26][  200/  391]   Loss 0.144288   Top1 94.910156   Top5 99.937500   BatchTime 0.119747   LR 0.010000
INFO - Training [26][  220/  391]   Loss 0.145990   Top1 94.833097   Top5 99.939631   BatchTime 0.120093   LR 0.010000
INFO - Training [26][  240/  391]   Loss 0.147216   Top1 94.807943   Top5 99.944661   BatchTime 0.120321   LR 0.010000
INFO - Training [26][  260/  391]   Loss 0.146760   Top1 94.822716   Top5 99.945913   BatchTime 0.120515   LR 0.010000
INFO - Training [26][  280/  391]   Loss 0.146945   Top1 94.813058   Top5 99.946987   BatchTime 0.120679   LR 0.010000
INFO - Training [26][  300/  391]   Loss 0.145892   Top1 94.864583   Top5 99.947917   BatchTime 0.119880   LR 0.010000
INFO - Training [26][  320/  391]   Loss 0.146953   Top1 94.802246   Top5 99.946289   BatchTime 0.117790   LR 0.010000
INFO - Training [26][  340/  391]   Loss 0.147432   Top1 94.765625   Top5 99.947151   BatchTime 0.116201   LR 0.010000
INFO - Training [26][  360/  391]   Loss 0.148304   Top1 94.759115   Top5 99.945747   BatchTime 0.113863   LR 0.010000
INFO - Training [26][  380/  391]   Loss 0.148529   Top1 94.771793   Top5 99.946546   BatchTime 0.114036   LR 0.010000
INFO - ==> Top1: 94.774    Top5: 99.948    Loss: 0.149
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [26][   20/   79]   Loss 0.378543   Top1 89.023438   Top5 99.570312   BatchTime 0.152417
INFO - Validation [26][   40/   79]   Loss 0.375951   Top1 89.003906   Top5 99.589844   BatchTime 0.108902
INFO - Validation [26][   60/   79]   Loss 0.372750   Top1 88.945312   Top5 99.570312   BatchTime 0.093415
INFO - ==> Top1: 88.890    Top5: 99.590    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.040   Top5: 99.570] Sparsity : 0.873
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 88.960   Top5: 99.570] Sparsity : 0.874
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  27
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [27][   20/  391]   Loss 0.148282   Top1 94.726562   Top5 99.921875   BatchTime 0.213015   LR 0.010000
INFO - Training [27][   40/  391]   Loss 0.139292   Top1 94.921875   Top5 99.960938   BatchTime 0.167856   LR 0.010000
INFO - Training [27][   60/  391]   Loss 0.144212   Top1 94.947917   Top5 99.960938   BatchTime 0.152935   LR 0.010000
INFO - Training [27][   80/  391]   Loss 0.139359   Top1 95.087891   Top5 99.960938   BatchTime 0.143842   LR 0.010000
INFO - Training [27][  100/  391]   Loss 0.141103   Top1 95.031250   Top5 99.953125   BatchTime 0.131009   LR 0.010000
INFO - Training [27][  120/  391]   Loss 0.141127   Top1 94.967448   Top5 99.947917   BatchTime 0.124266   LR 0.010000
INFO - Training [27][  140/  391]   Loss 0.140431   Top1 94.921875   Top5 99.944196   BatchTime 0.118839   LR 0.010000
INFO - Training [27][  160/  391]   Loss 0.139277   Top1 94.951172   Top5 99.941406   BatchTime 0.118411   LR 0.010000
INFO - Training [27][  180/  391]   Loss 0.142176   Top1 94.839410   Top5 99.939236   BatchTime 0.118957   LR 0.010000
INFO - Training [27][  200/  391]   Loss 0.143449   Top1 94.789062   Top5 99.941406   BatchTime 0.119376   LR 0.010000
INFO - Training [27][  220/  391]   Loss 0.143443   Top1 94.783381   Top5 99.946733   BatchTime 0.119866   LR 0.010000
INFO - Training [27][  240/  391]   Loss 0.143206   Top1 94.801432   Top5 99.951172   BatchTime 0.120193   LR 0.010000
INFO - Training [27][  260/  391]   Loss 0.143017   Top1 94.828726   Top5 99.951923   BatchTime 0.120421   LR 0.010000
INFO - Training [27][  280/  391]   Loss 0.143651   Top1 94.840960   Top5 99.952567   BatchTime 0.120591   LR 0.010000
INFO - Training [27][  300/  391]   Loss 0.144629   Top1 94.794271   Top5 99.947917   BatchTime 0.120784   LR 0.010000
INFO - Training [27][  320/  391]   Loss 0.144800   Top1 94.785156   Top5 99.946289   BatchTime 0.120920   LR 0.010000
INFO - Training [27][  340/  391]   Loss 0.144189   Top1 94.832261   Top5 99.944853   BatchTime 0.120946   LR 0.010000
INFO - Training [27][  360/  391]   Loss 0.145334   Top1 94.822049   Top5 99.947917   BatchTime 0.119285   LR 0.010000
INFO - Training [27][  380/  391]   Loss 0.146058   Top1 94.814967   Top5 99.950658   BatchTime 0.117430   LR 0.010000
INFO - ==> Top1: 94.816    Top5: 99.948    Loss: 0.146
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [27][   20/   79]   Loss 0.381579   Top1 88.359375   Top5 99.492188   BatchTime 0.158798
INFO - Validation [27][   40/   79]   Loss 0.369250   Top1 88.730469   Top5 99.511719   BatchTime 0.110735
INFO - Validation [27][   60/   79]   Loss 0.368182   Top1 88.932292   Top5 99.531250   BatchTime 0.095207
INFO - ==> Top1: 89.010    Top5: 99.600    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.040   Top5: 99.570] Sparsity : 0.873
INFO - Scoreboard best 3 ==> Epoch [27][Top1: 89.010   Top5: 99.600] Sparsity : 0.875
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  28
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [28][   20/  391]   Loss 0.133690   Top1 95.117188   Top5 99.882812   BatchTime 0.212890   LR 0.010000
INFO - Training [28][   40/  391]   Loss 0.133912   Top1 95.332031   Top5 99.921875   BatchTime 0.168245   LR 0.010000
INFO - Training [28][   60/  391]   Loss 0.143058   Top1 94.882812   Top5 99.908854   BatchTime 0.153487   LR 0.010000
INFO - Training [28][   80/  391]   Loss 0.143667   Top1 94.902344   Top5 99.902344   BatchTime 0.145934   LR 0.010000
INFO - Training [28][  100/  391]   Loss 0.143889   Top1 94.906250   Top5 99.921875   BatchTime 0.141462   LR 0.010000
INFO - Training [28][  120/  391]   Loss 0.145014   Top1 94.915365   Top5 99.915365   BatchTime 0.138390   LR 0.010000
INFO - Training [28][  140/  391]   Loss 0.142739   Top1 94.921875   Top5 99.921875   BatchTime 0.133058   LR 0.010000
INFO - Training [28][  160/  391]   Loss 0.143206   Top1 94.926758   Top5 99.916992   BatchTime 0.127264   LR 0.010000
INFO - Training [28][  180/  391]   Loss 0.142379   Top1 94.921875   Top5 99.926215   BatchTime 0.123632   LR 0.010000
INFO - Training [28][  200/  391]   Loss 0.142277   Top1 94.941406   Top5 99.933594   BatchTime 0.118981   LR 0.010000
INFO - Training [28][  220/  391]   Loss 0.141461   Top1 95.000000   Top5 99.932528   BatchTime 0.119241   LR 0.010000
INFO - Training [28][  240/  391]   Loss 0.140955   Top1 95.035807   Top5 99.938151   BatchTime 0.119604   LR 0.010000
INFO - Training [28][  260/  391]   Loss 0.139657   Top1 95.087139   Top5 99.942909   BatchTime 0.120304   LR 0.010000
INFO - Training [28][  280/  391]   Loss 0.140200   Top1 95.061384   Top5 99.944196   BatchTime 0.120604   LR 0.010000
INFO - Training [28][  300/  391]   Loss 0.139581   Top1 95.070312   Top5 99.947917   BatchTime 0.120777   LR 0.010000
INFO - Training [28][  320/  391]   Loss 0.139592   Top1 95.068359   Top5 99.951172   BatchTime 0.120986   LR 0.010000
INFO - Training [28][  340/  391]   Loss 0.140104   Top1 95.071232   Top5 99.947151   BatchTime 0.121160   LR 0.010000
INFO - Training [28][  360/  391]   Loss 0.140640   Top1 95.052083   Top5 99.943576   BatchTime 0.121253   LR 0.010000
INFO - Training [28][  380/  391]   Loss 0.141830   Top1 95.010280   Top5 99.942434   BatchTime 0.121294   LR 0.010000
INFO - ==> Top1: 94.988    Top5: 99.944    Loss: 0.142
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [28][   20/   79]   Loss 0.380183   Top1 89.023438   Top5 99.453125   BatchTime 0.122254
INFO - Validation [28][   40/   79]   Loss 0.363974   Top1 89.511719   Top5 99.589844   BatchTime 0.081755
INFO - Validation [28][   60/   79]   Loss 0.361686   Top1 89.687500   Top5 99.570312   BatchTime 0.068620
INFO - ==> Top1: 89.560    Top5: 99.580    Loss: 0.361
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 89.560   Top5: 99.580] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 89.040   Top5: 99.570] Sparsity : 0.873
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  29
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [29][   20/  391]   Loss 0.137462   Top1 94.726562   Top5 99.960938   BatchTime 0.222381   LR 0.010000
INFO - Training [29][   40/  391]   Loss 0.132560   Top1 95.136719   Top5 99.960938   BatchTime 0.173161   LR 0.010000
INFO - Training [29][   60/  391]   Loss 0.129457   Top1 95.260417   Top5 99.973958   BatchTime 0.156767   LR 0.010000
INFO - Training [29][   80/  391]   Loss 0.132355   Top1 95.048828   Top5 99.970703   BatchTime 0.148592   LR 0.010000
INFO - Training [29][  100/  391]   Loss 0.133839   Top1 94.921875   Top5 99.953125   BatchTime 0.143721   LR 0.010000
INFO - Training [29][  120/  391]   Loss 0.131598   Top1 95.091146   Top5 99.960938   BatchTime 0.140465   LR 0.010000
INFO - Training [29][  140/  391]   Loss 0.132065   Top1 95.128348   Top5 99.966518   BatchTime 0.138046   LR 0.010000
INFO - Training [29][  160/  391]   Loss 0.132937   Top1 95.107422   Top5 99.960938   BatchTime 0.136124   LR 0.010000
INFO - Training [29][  180/  391]   Loss 0.132366   Top1 95.138889   Top5 99.965278   BatchTime 0.134699   LR 0.010000
INFO - Training [29][  200/  391]   Loss 0.134095   Top1 95.093750   Top5 99.964844   BatchTime 0.132559   LR 0.010000
INFO - Training [29][  220/  391]   Loss 0.134939   Top1 95.106534   Top5 99.964489   BatchTime 0.128160   LR 0.010000
INFO - Training [29][  240/  391]   Loss 0.134141   Top1 95.097656   Top5 99.967448   BatchTime 0.124809   LR 0.010000
INFO - Training [29][  260/  391]   Loss 0.133543   Top1 95.144231   Top5 99.963942   BatchTime 0.121647   LR 0.010000
INFO - Training [29][  280/  391]   Loss 0.134217   Top1 95.142299   Top5 99.966518   BatchTime 0.120840   LR 0.010000
INFO - Training [29][  300/  391]   Loss 0.134052   Top1 95.143229   Top5 99.968750   BatchTime 0.121005   LR 0.010000
INFO - Training [29][  320/  391]   Loss 0.134711   Top1 95.151367   Top5 99.968262   BatchTime 0.121151   LR 0.010000
INFO - Training [29][  340/  391]   Loss 0.134482   Top1 95.147059   Top5 99.963235   BatchTime 0.121275   LR 0.010000
INFO - Training [29][  360/  391]   Loss 0.135416   Top1 95.123698   Top5 99.963108   BatchTime 0.121346   LR 0.010000
INFO - Training [29][  380/  391]   Loss 0.135616   Top1 95.131579   Top5 99.960938   BatchTime 0.121436   LR 0.010000
INFO - ==> Top1: 95.124    Top5: 99.960    Loss: 0.136
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [29][   20/   79]   Loss 0.393200   Top1 88.906250   Top5 99.531250   BatchTime 0.152408
INFO - Validation [29][   40/   79]   Loss 0.382230   Top1 89.179688   Top5 99.550781   BatchTime 0.108533
INFO - Validation [29][   60/   79]   Loss 0.376352   Top1 89.062500   Top5 99.557292   BatchTime 0.093616
INFO - ==> Top1: 89.030    Top5: 99.590    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 89.560   Top5: 99.580] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 89.040   Top5: 99.570] Sparsity : 0.873
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  30
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [30][   20/  391]   Loss 0.127861   Top1 95.546875   Top5 99.921875   BatchTime 0.194005   LR 0.001000
INFO - Training [30][   40/  391]   Loss 0.121808   Top1 95.878906   Top5 99.960938   BatchTime 0.139069   LR 0.001000
INFO - Training [30][   60/  391]   Loss 0.115455   Top1 95.885417   Top5 99.960938   BatchTime 0.127559   LR 0.001000
INFO - Training [30][   80/  391]   Loss 0.116934   Top1 95.791016   Top5 99.970703   BatchTime 0.126641   LR 0.001000
INFO - Training [30][  100/  391]   Loss 0.117637   Top1 95.789062   Top5 99.976562   BatchTime 0.125929   LR 0.001000
INFO - Training [30][  120/  391]   Loss 0.115993   Top1 95.885417   Top5 99.967448   BatchTime 0.125593   LR 0.001000
INFO - Training [30][  140/  391]   Loss 0.114815   Top1 95.948661   Top5 99.972098   BatchTime 0.125276   LR 0.001000
INFO - Training [30][  160/  391]   Loss 0.114777   Top1 95.981445   Top5 99.975586   BatchTime 0.124997   LR 0.001000
INFO - Training [30][  180/  391]   Loss 0.114075   Top1 96.019965   Top5 99.973958   BatchTime 0.124896   LR 0.001000
INFO - Training [30][  200/  391]   Loss 0.115065   Top1 96.023438   Top5 99.968750   BatchTime 0.124688   LR 0.001000
INFO - Training [30][  220/  391]   Loss 0.116255   Top1 95.987216   Top5 99.968040   BatchTime 0.124521   LR 0.001000
INFO - Training [30][  240/  391]   Loss 0.114585   Top1 96.031901   Top5 99.970703   BatchTime 0.124364   LR 0.001000
INFO - Training [30][  260/  391]   Loss 0.114988   Top1 96.030649   Top5 99.972957   BatchTime 0.122451   LR 0.001000
INFO - Training [30][  280/  391]   Loss 0.114490   Top1 96.071429   Top5 99.974888   BatchTime 0.120206   LR 0.001000
INFO - Training [30][  300/  391]   Loss 0.113941   Top1 96.085938   Top5 99.971354   BatchTime 0.118488   LR 0.001000
INFO - Training [30][  320/  391]   Loss 0.113912   Top1 96.074219   Top5 99.973145   BatchTime 0.116865   LR 0.001000
INFO - Training [30][  340/  391]   Loss 0.114482   Top1 96.052390   Top5 99.974724   BatchTime 0.117329   LR 0.001000
INFO - Training [30][  360/  391]   Loss 0.114675   Top1 96.041667   Top5 99.973958   BatchTime 0.117683   LR 0.001000
INFO - Training [30][  380/  391]   Loss 0.114447   Top1 96.052632   Top5 99.973273   BatchTime 0.117974   LR 0.001000
INFO - ==> Top1: 96.060    Top5: 99.972    Loss: 0.114
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [30][   20/   79]   Loss 0.364482   Top1 89.531250   Top5 99.570312   BatchTime 0.152866
INFO - Validation [30][   40/   79]   Loss 0.356685   Top1 89.726562   Top5 99.550781   BatchTime 0.108617
INFO - Validation [30][   60/   79]   Loss 0.354831   Top1 89.609375   Top5 99.583333   BatchTime 0.093697
INFO - ==> Top1: 89.710    Top5: 99.620    Loss: 0.352
INFO - Scoreboard best 1 ==> Epoch [30][Top1: 89.710   Top5: 99.620] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [28][Top1: 89.560   Top5: 99.580] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 89.270   Top5: 99.560] Sparsity : 0.863
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  31
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [31][   20/  391]   Loss 0.109446   Top1 96.132812   Top5 100.000000   BatchTime 0.211039   LR 0.001000
INFO - Training [31][   40/  391]   Loss 0.108619   Top1 96.171875   Top5 100.000000   BatchTime 0.154923   LR 0.001000
INFO - Training [31][   60/  391]   Loss 0.108229   Top1 96.041667   Top5 99.960938   BatchTime 0.132078   LR 0.001000
INFO - Training [31][   80/  391]   Loss 0.108757   Top1 96.113281   Top5 99.970703   BatchTime 0.122900   LR 0.001000
INFO - Training [31][  100/  391]   Loss 0.110095   Top1 96.070312   Top5 99.968750   BatchTime 0.114345   LR 0.001000
INFO - Training [31][  120/  391]   Loss 0.109768   Top1 96.048177   Top5 99.973958   BatchTime 0.116279   LR 0.001000
INFO - Training [31][  140/  391]   Loss 0.109733   Top1 96.060268   Top5 99.972098   BatchTime 0.117367   LR 0.001000
INFO - Training [31][  160/  391]   Loss 0.108836   Top1 96.162109   Top5 99.970703   BatchTime 0.118136   LR 0.001000
INFO - Training [31][  180/  391]   Loss 0.108694   Top1 96.171875   Top5 99.969618   BatchTime 0.118751   LR 0.001000
INFO - Training [31][  200/  391]   Loss 0.108405   Top1 96.187500   Top5 99.968750   BatchTime 0.119248   LR 0.001000
INFO - Training [31][  220/  391]   Loss 0.107489   Top1 96.239347   Top5 99.964489   BatchTime 0.119697   LR 0.001000
INFO - Training [31][  240/  391]   Loss 0.106468   Top1 96.263021   Top5 99.967448   BatchTime 0.120078   LR 0.001000
INFO - Training [31][  260/  391]   Loss 0.106436   Top1 96.271034   Top5 99.966947   BatchTime 0.120331   LR 0.001000
INFO - Training [31][  280/  391]   Loss 0.105145   Top1 96.289062   Top5 99.969308   BatchTime 0.120498   LR 0.001000
INFO - Training [31][  300/  391]   Loss 0.104804   Top1 96.291667   Top5 99.971354   BatchTime 0.120697   LR 0.001000
INFO - Training [31][  320/  391]   Loss 0.104055   Top1 96.328125   Top5 99.973145   BatchTime 0.118118   LR 0.001000
INFO - Training [31][  340/  391]   Loss 0.104195   Top1 96.335018   Top5 99.974724   BatchTime 0.116472   LR 0.001000
INFO - Training [31][  360/  391]   Loss 0.104244   Top1 96.315104   Top5 99.976128   BatchTime 0.114753   LR 0.001000
INFO - Training [31][  380/  391]   Loss 0.104753   Top1 96.295230   Top5 99.977385   BatchTime 0.114067   LR 0.001000
INFO - ==> Top1: 96.290    Top5: 99.978    Loss: 0.105
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [31][   20/   79]   Loss 0.349019   Top1 89.960938   Top5 99.765625   BatchTime 0.152989
INFO - Validation [31][   40/   79]   Loss 0.345743   Top1 90.156250   Top5 99.726562   BatchTime 0.109233
INFO - Validation [31][   60/   79]   Loss 0.345610   Top1 90.104167   Top5 99.700521   BatchTime 0.094551
INFO - ==> Top1: 89.950    Top5: 99.690    Loss: 0.346
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 89.950   Top5: 99.690] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [30][Top1: 89.710   Top5: 99.620] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [28][Top1: 89.560   Top5: 99.580] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  32
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [32][   20/  391]   Loss 0.105546   Top1 96.093750   Top5 99.960938   BatchTime 0.210492   LR 0.001000
INFO - Training [32][   40/  391]   Loss 0.107682   Top1 96.250000   Top5 99.980469   BatchTime 0.166963   LR 0.001000
INFO - Training [32][   60/  391]   Loss 0.107248   Top1 96.367188   Top5 99.973958   BatchTime 0.152265   LR 0.001000
INFO - Training [32][   80/  391]   Loss 0.105989   Top1 96.318359   Top5 99.980469   BatchTime 0.145049   LR 0.001000
INFO - Training [32][  100/  391]   Loss 0.107768   Top1 96.171875   Top5 99.984375   BatchTime 0.133870   LR 0.001000
INFO - Training [32][  120/  391]   Loss 0.107183   Top1 96.282552   Top5 99.980469   BatchTime 0.126557   LR 0.001000
INFO - Training [32][  140/  391]   Loss 0.104583   Top1 96.411830   Top5 99.983259   BatchTime 0.121788   LR 0.001000
INFO - Training [32][  160/  391]   Loss 0.103679   Top1 96.391602   Top5 99.985352   BatchTime 0.118036   LR 0.001000
INFO - Training [32][  180/  391]   Loss 0.103553   Top1 96.393229   Top5 99.986979   BatchTime 0.118690   LR 0.001000
INFO - Training [32][  200/  391]   Loss 0.104231   Top1 96.378906   Top5 99.988281   BatchTime 0.119124   LR 0.001000
INFO - Training [32][  220/  391]   Loss 0.105193   Top1 96.352983   Top5 99.989347   BatchTime 0.119545   LR 0.001000
INFO - Training [32][  240/  391]   Loss 0.104175   Top1 96.389974   Top5 99.990234   BatchTime 0.119893   LR 0.001000
INFO - Training [32][  260/  391]   Loss 0.104625   Top1 96.373197   Top5 99.987981   BatchTime 0.120208   LR 0.001000
INFO - Training [32][  280/  391]   Loss 0.104213   Top1 96.375558   Top5 99.986049   BatchTime 0.120444   LR 0.001000
INFO - Training [32][  300/  391]   Loss 0.103933   Top1 96.359375   Top5 99.986979   BatchTime 0.120656   LR 0.001000
INFO - Training [32][  320/  391]   Loss 0.103897   Top1 96.337891   Top5 99.987793   BatchTime 0.120809   LR 0.001000
INFO - Training [32][  340/  391]   Loss 0.104736   Top1 96.300551   Top5 99.986213   BatchTime 0.120895   LR 0.001000
INFO - Training [32][  360/  391]   Loss 0.105165   Top1 96.293403   Top5 99.980469   BatchTime 0.120478   LR 0.001000
INFO - Training [32][  380/  391]   Loss 0.105280   Top1 96.299342   Top5 99.981497   BatchTime 0.118285   LR 0.001000
INFO - ==> Top1: 96.304    Top5: 99.982    Loss: 0.105
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [32][   20/   79]   Loss 0.350771   Top1 90.117188   Top5 99.609375   BatchTime 0.122557
INFO - Validation [32][   40/   79]   Loss 0.349200   Top1 90.214844   Top5 99.648438   BatchTime 0.091062
INFO - Validation [32][   60/   79]   Loss 0.349903   Top1 90.390625   Top5 99.635417   BatchTime 0.083356
INFO - ==> Top1: 90.170    Top5: 99.660    Loss: 0.350
INFO - Scoreboard best 1 ==> Epoch [32][Top1: 90.170   Top5: 99.660] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [31][Top1: 89.950   Top5: 99.690] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [30][Top1: 89.710   Top5: 99.620] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  33
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [33][   20/  391]   Loss 0.109524   Top1 96.015625   Top5 99.960938   BatchTime 0.212647   LR 0.001000
INFO - Training [33][   40/  391]   Loss 0.106057   Top1 96.250000   Top5 99.921875   BatchTime 0.168122   LR 0.001000
INFO - Training [33][   60/  391]   Loss 0.102638   Top1 96.471354   Top5 99.934896   BatchTime 0.153300   LR 0.001000
INFO - Training [33][   80/  391]   Loss 0.101650   Top1 96.513672   Top5 99.941406   BatchTime 0.146172   LR 0.001000
INFO - Training [33][  100/  391]   Loss 0.101590   Top1 96.531250   Top5 99.953125   BatchTime 0.141396   LR 0.001000
INFO - Training [33][  120/  391]   Loss 0.100731   Top1 96.549479   Top5 99.960938   BatchTime 0.138474   LR 0.001000
INFO - Training [33][  140/  391]   Loss 0.101513   Top1 96.540179   Top5 99.966518   BatchTime 0.136220   LR 0.001000
INFO - Training [33][  160/  391]   Loss 0.101472   Top1 96.557617   Top5 99.965820   BatchTime 0.129590   LR 0.001000
INFO - Training [33][  180/  391]   Loss 0.103188   Top1 96.453993   Top5 99.969618   BatchTime 0.125454   LR 0.001000
INFO - Training [33][  200/  391]   Loss 0.103466   Top1 96.453125   Top5 99.964844   BatchTime 0.122036   LR 0.001000
INFO - Training [33][  220/  391]   Loss 0.103406   Top1 96.459517   Top5 99.960938   BatchTime 0.120199   LR 0.001000
INFO - Training [33][  240/  391]   Loss 0.101642   Top1 96.542969   Top5 99.964193   BatchTime 0.120512   LR 0.001000
INFO - Training [33][  260/  391]   Loss 0.100913   Top1 96.547476   Top5 99.966947   BatchTime 0.120761   LR 0.001000
INFO - Training [33][  280/  391]   Loss 0.101370   Top1 96.523438   Top5 99.969308   BatchTime 0.120990   LR 0.001000
INFO - Training [33][  300/  391]   Loss 0.101388   Top1 96.510417   Top5 99.971354   BatchTime 0.121167   LR 0.001000
INFO - Training [33][  320/  391]   Loss 0.100906   Top1 96.516113   Top5 99.973145   BatchTime 0.121320   LR 0.001000
INFO - Training [33][  340/  391]   Loss 0.101186   Top1 96.514246   Top5 99.972426   BatchTime 0.121519   LR 0.001000
INFO - Training [33][  360/  391]   Loss 0.101094   Top1 96.501736   Top5 99.973958   BatchTime 0.121564   LR 0.001000
INFO - Training [33][  380/  391]   Loss 0.101486   Top1 96.488487   Top5 99.975329   BatchTime 0.121635   LR 0.001000
INFO - ==> Top1: 96.468    Top5: 99.976    Loss: 0.102
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [33][   20/   79]   Loss 0.351900   Top1 89.804688   Top5 99.648438   BatchTime 0.123641
INFO - Validation [33][   40/   79]   Loss 0.347637   Top1 90.019531   Top5 99.648438   BatchTime 0.075113
INFO - Validation [33][   60/   79]   Loss 0.345545   Top1 90.234375   Top5 99.648438   BatchTime 0.061235
INFO - ==> Top1: 90.130    Top5: 99.680    Loss: 0.345
INFO - Scoreboard best 1 ==> Epoch [32][Top1: 90.170   Top5: 99.660] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.130   Top5: 99.680] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [31][Top1: 89.950   Top5: 99.690] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  34
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [34][   20/  391]   Loss 0.096302   Top1 96.679688   Top5 99.960938   BatchTime 0.212946   LR 0.001000
INFO - Training [34][   40/  391]   Loss 0.098808   Top1 96.621094   Top5 99.980469   BatchTime 0.168642   LR 0.001000
INFO - Training [34][   60/  391]   Loss 0.097683   Top1 96.679688   Top5 99.973958   BatchTime 0.153494   LR 0.001000
INFO - Training [34][   80/  391]   Loss 0.094957   Top1 96.679688   Top5 99.980469   BatchTime 0.145978   LR 0.001000
INFO - Training [34][  100/  391]   Loss 0.092720   Top1 96.789062   Top5 99.984375   BatchTime 0.141579   LR 0.001000
INFO - Training [34][  120/  391]   Loss 0.093580   Top1 96.744792   Top5 99.980469   BatchTime 0.138585   LR 0.001000
INFO - Training [34][  140/  391]   Loss 0.094175   Top1 96.735491   Top5 99.977679   BatchTime 0.136527   LR 0.001000
INFO - Training [34][  160/  391]   Loss 0.093858   Top1 96.757812   Top5 99.980469   BatchTime 0.134828   LR 0.001000
INFO - Training [34][  180/  391]   Loss 0.094238   Top1 96.744792   Top5 99.982639   BatchTime 0.133520   LR 0.001000
INFO - Training [34][  200/  391]   Loss 0.094513   Top1 96.730469   Top5 99.980469   BatchTime 0.132493   LR 0.001000
INFO - Training [34][  220/  391]   Loss 0.095057   Top1 96.669034   Top5 99.978693   BatchTime 0.128387   LR 0.001000
INFO - Training [34][  240/  391]   Loss 0.094836   Top1 96.666667   Top5 99.980469   BatchTime 0.125192   LR 0.001000
INFO - Training [34][  260/  391]   Loss 0.095445   Top1 96.679688   Top5 99.978966   BatchTime 0.122571   LR 0.001000
INFO - Training [34][  280/  391]   Loss 0.096274   Top1 96.629464   Top5 99.977679   BatchTime 0.120195   LR 0.001000
INFO - Training [34][  300/  391]   Loss 0.096946   Top1 96.591146   Top5 99.971354   BatchTime 0.120374   LR 0.001000
INFO - Training [34][  320/  391]   Loss 0.097066   Top1 96.579590   Top5 99.970703   BatchTime 0.120579   LR 0.001000
INFO - Training [34][  340/  391]   Loss 0.097615   Top1 96.551011   Top5 99.972426   BatchTime 0.120767   LR 0.001000
INFO - Training [34][  360/  391]   Loss 0.097956   Top1 96.562500   Top5 99.973958   BatchTime 0.120914   LR 0.001000
INFO - Training [34][  380/  391]   Loss 0.097682   Top1 96.566612   Top5 99.975329   BatchTime 0.121031   LR 0.001000
INFO - ==> Top1: 96.568    Top5: 99.974    Loss: 0.098
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [34][   20/   79]   Loss 0.357516   Top1 89.570312   Top5 99.531250   BatchTime 0.151962
INFO - Validation [34][   40/   79]   Loss 0.350148   Top1 90.058594   Top5 99.589844   BatchTime 0.108256
INFO - Validation [34][   60/   79]   Loss 0.351813   Top1 90.104167   Top5 99.622396   BatchTime 0.093881
INFO - ==> Top1: 90.000    Top5: 99.660    Loss: 0.351
INFO - Scoreboard best 1 ==> Epoch [32][Top1: 90.170   Top5: 99.660] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.130   Top5: 99.680] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [34][Top1: 90.000   Top5: 99.660] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  35
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [35][   20/  391]   Loss 0.088195   Top1 96.796875   Top5 99.960938   BatchTime 0.191234   LR 0.001000
INFO - Training [35][   40/  391]   Loss 0.086516   Top1 96.933594   Top5 99.980469   BatchTime 0.143350   LR 0.001000
INFO - Training [35][   60/  391]   Loss 0.087601   Top1 96.875000   Top5 99.986979   BatchTime 0.121450   LR 0.001000
INFO - Training [35][   80/  391]   Loss 0.089981   Top1 96.875000   Top5 99.980469   BatchTime 0.123928   LR 0.001000
INFO - Training [35][  100/  391]   Loss 0.091524   Top1 96.796875   Top5 99.984375   BatchTime 0.123985   LR 0.001000
INFO - Training [35][  120/  391]   Loss 0.094483   Top1 96.660156   Top5 99.986979   BatchTime 0.123974   LR 0.001000
INFO - Training [35][  140/  391]   Loss 0.092148   Top1 96.735491   Top5 99.988839   BatchTime 0.124111   LR 0.001000
INFO - Training [35][  160/  391]   Loss 0.090867   Top1 96.796875   Top5 99.990234   BatchTime 0.124024   LR 0.001000
INFO - Training [35][  180/  391]   Loss 0.091763   Top1 96.770833   Top5 99.982639   BatchTime 0.124028   LR 0.001000
INFO - Training [35][  200/  391]   Loss 0.093151   Top1 96.710938   Top5 99.984375   BatchTime 0.123995   LR 0.001000
INFO - Training [35][  220/  391]   Loss 0.093125   Top1 96.693892   Top5 99.982244   BatchTime 0.123904   LR 0.001000
INFO - Training [35][  240/  391]   Loss 0.093454   Top1 96.682943   Top5 99.980469   BatchTime 0.123855   LR 0.001000
INFO - Training [35][  260/  391]   Loss 0.092871   Top1 96.715745   Top5 99.981971   BatchTime 0.123669   LR 0.001000
INFO - Training [35][  280/  391]   Loss 0.092988   Top1 96.690848   Top5 99.980469   BatchTime 0.120347   LR 0.001000
INFO - Training [35][  300/  391]   Loss 0.093314   Top1 96.666667   Top5 99.979167   BatchTime 0.118731   LR 0.001000
INFO - Training [35][  320/  391]   Loss 0.094078   Top1 96.647949   Top5 99.978027   BatchTime 0.117172   LR 0.001000
INFO - Training [35][  340/  391]   Loss 0.094646   Top1 96.626838   Top5 99.979320   BatchTime 0.116934   LR 0.001000
INFO - Training [35][  360/  391]   Loss 0.095390   Top1 96.625434   Top5 99.980469   BatchTime 0.117306   LR 0.001000
INFO - Training [35][  380/  391]   Loss 0.095771   Top1 96.591283   Top5 99.981497   BatchTime 0.117633   LR 0.001000
INFO - ==> Top1: 96.584    Top5: 99.982    Loss: 0.096
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [35][   20/   79]   Loss 0.352319   Top1 90.312500   Top5 99.609375   BatchTime 0.153483
INFO - Validation [35][   40/   79]   Loss 0.346846   Top1 90.312500   Top5 99.589844   BatchTime 0.108309
INFO - Validation [35][   60/   79]   Loss 0.345341   Top1 90.390625   Top5 99.609375   BatchTime 0.093625
INFO - ==> Top1: 90.290    Top5: 99.640    Loss: 0.344
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.170   Top5: 99.660] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.130   Top5: 99.680] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  36
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [36][   20/  391]   Loss 0.105759   Top1 96.406250   Top5 100.000000   BatchTime 0.212741   LR 0.001000
INFO - Training [36][   40/  391]   Loss 0.100663   Top1 96.367188   Top5 100.000000   BatchTime 0.168775   LR 0.001000
INFO - Training [36][   60/  391]   Loss 0.095980   Top1 96.627604   Top5 100.000000   BatchTime 0.137113   LR 0.001000
INFO - Training [36][   80/  391]   Loss 0.094520   Top1 96.718750   Top5 100.000000   BatchTime 0.126008   LR 0.001000
INFO - Training [36][  100/  391]   Loss 0.092860   Top1 96.804688   Top5 100.000000   BatchTime 0.118449   LR 0.001000
INFO - Training [36][  120/  391]   Loss 0.093254   Top1 96.751302   Top5 100.000000   BatchTime 0.115708   LR 0.001000
INFO - Training [36][  140/  391]   Loss 0.093944   Top1 96.679688   Top5 99.988839   BatchTime 0.116872   LR 0.001000
INFO - Training [36][  160/  391]   Loss 0.094150   Top1 96.704102   Top5 99.990234   BatchTime 0.117640   LR 0.001000
INFO - Training [36][  180/  391]   Loss 0.094216   Top1 96.679688   Top5 99.986979   BatchTime 0.118274   LR 0.001000
INFO - Training [36][  200/  391]   Loss 0.093369   Top1 96.734375   Top5 99.988281   BatchTime 0.118768   LR 0.001000
INFO - Training [36][  220/  391]   Loss 0.093039   Top1 96.736506   Top5 99.989347   BatchTime 0.119231   LR 0.001000
INFO - Training [36][  240/  391]   Loss 0.093891   Top1 96.718750   Top5 99.986979   BatchTime 0.119573   LR 0.001000
INFO - Training [36][  260/  391]   Loss 0.094945   Top1 96.685697   Top5 99.984976   BatchTime 0.119860   LR 0.001000
INFO - Training [36][  280/  391]   Loss 0.094530   Top1 96.693638   Top5 99.986049   BatchTime 0.120061   LR 0.001000
INFO - Training [36][  300/  391]   Loss 0.094326   Top1 96.708333   Top5 99.984375   BatchTime 0.120256   LR 0.001000
INFO - Training [36][  320/  391]   Loss 0.094774   Top1 96.679688   Top5 99.985352   BatchTime 0.119049   LR 0.001000
INFO - Training [36][  340/  391]   Loss 0.094524   Top1 96.647518   Top5 99.983915   BatchTime 0.116857   LR 0.001000
INFO - Training [36][  360/  391]   Loss 0.094640   Top1 96.649306   Top5 99.984809   BatchTime 0.115305   LR 0.001000
INFO - Training [36][  380/  391]   Loss 0.095049   Top1 96.624178   Top5 99.985609   BatchTime 0.113415   LR 0.001000
INFO - ==> Top1: 96.618    Top5: 99.986    Loss: 0.095
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [36][   20/   79]   Loss 0.356162   Top1 90.078125   Top5 99.687500   BatchTime 0.154786
INFO - Validation [36][   40/   79]   Loss 0.360294   Top1 89.882812   Top5 99.648438   BatchTime 0.108820
INFO - Validation [36][   60/   79]   Loss 0.359538   Top1 90.039062   Top5 99.661458   BatchTime 0.093500
INFO - ==> Top1: 90.010    Top5: 99.680    Loss: 0.357
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.170   Top5: 99.660] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.130   Top5: 99.680] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  37
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [37][   20/  391]   Loss 0.093524   Top1 96.914062   Top5 100.000000   BatchTime 0.213387   LR 0.001000
INFO - Training [37][   40/  391]   Loss 0.089834   Top1 97.109375   Top5 99.980469   BatchTime 0.168483   LR 0.001000
INFO - Training [37][   60/  391]   Loss 0.092157   Top1 96.940104   Top5 99.986979   BatchTime 0.154766   LR 0.001000
INFO - Training [37][   80/  391]   Loss 0.091489   Top1 96.962891   Top5 99.990234   BatchTime 0.146870   LR 0.001000
INFO - Training [37][  100/  391]   Loss 0.091308   Top1 96.898438   Top5 99.992188   BatchTime 0.140945   LR 0.001000
INFO - Training [37][  120/  391]   Loss 0.091946   Top1 96.809896   Top5 99.993490   BatchTime 0.131183   LR 0.001000
INFO - Training [37][  140/  391]   Loss 0.091928   Top1 96.830357   Top5 99.988839   BatchTime 0.125308   LR 0.001000
INFO - Training [37][  160/  391]   Loss 0.092150   Top1 96.811523   Top5 99.990234   BatchTime 0.120558   LR 0.001000
INFO - Training [37][  180/  391]   Loss 0.092451   Top1 96.766493   Top5 99.991319   BatchTime 0.119571   LR 0.001000
INFO - Training [37][  200/  391]   Loss 0.091683   Top1 96.800781   Top5 99.992188   BatchTime 0.119977   LR 0.001000
INFO - Training [37][  220/  391]   Loss 0.091087   Top1 96.839489   Top5 99.992898   BatchTime 0.120333   LR 0.001000
INFO - Training [37][  240/  391]   Loss 0.091709   Top1 96.793620   Top5 99.990234   BatchTime 0.120607   LR 0.001000
INFO - Training [37][  260/  391]   Loss 0.091192   Top1 96.823918   Top5 99.990986   BatchTime 0.120875   LR 0.001000
INFO - Training [37][  280/  391]   Loss 0.091024   Top1 96.799665   Top5 99.991629   BatchTime 0.121083   LR 0.001000
INFO - Training [37][  300/  391]   Loss 0.091876   Top1 96.770833   Top5 99.989583   BatchTime 0.121228   LR 0.001000
INFO - Training [37][  320/  391]   Loss 0.092033   Top1 96.779785   Top5 99.990234   BatchTime 0.121349   LR 0.001000
INFO - Training [37][  340/  391]   Loss 0.091791   Top1 96.785386   Top5 99.988511   BatchTime 0.121435   LR 0.001000
INFO - Training [37][  360/  391]   Loss 0.091902   Top1 96.788194   Top5 99.984809   BatchTime 0.121462   LR 0.001000
INFO - Training [37][  380/  391]   Loss 0.092282   Top1 96.786595   Top5 99.985609   BatchTime 0.119895   LR 0.001000
INFO - ==> Top1: 96.770    Top5: 99.986    Loss: 0.093
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [37][   20/   79]   Loss 0.359214   Top1 90.117188   Top5 99.609375   BatchTime 0.121094
INFO - Validation [37][   40/   79]   Loss 0.354890   Top1 90.253906   Top5 99.707031   BatchTime 0.074056
INFO - Validation [37][   60/   79]   Loss 0.356999   Top1 90.325521   Top5 99.700521   BatchTime 0.064924
INFO - ==> Top1: 90.230    Top5: 99.710    Loss: 0.355
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 90.230   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.170   Top5: 99.660] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  38
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [38][   20/  391]   Loss 0.090404   Top1 96.875000   Top5 100.000000   BatchTime 0.214278   LR 0.001000
INFO - Training [38][   40/  391]   Loss 0.085803   Top1 96.992188   Top5 100.000000   BatchTime 0.168703   LR 0.001000
INFO - Training [38][   60/  391]   Loss 0.086103   Top1 96.901042   Top5 100.000000   BatchTime 0.153616   LR 0.001000
INFO - Training [38][   80/  391]   Loss 0.088929   Top1 96.796875   Top5 100.000000   BatchTime 0.146141   LR 0.001000
INFO - Training [38][  100/  391]   Loss 0.088281   Top1 96.851562   Top5 100.000000   BatchTime 0.141556   LR 0.001000
INFO - Training [38][  120/  391]   Loss 0.090516   Top1 96.777344   Top5 100.000000   BatchTime 0.138429   LR 0.001000
INFO - Training [38][  140/  391]   Loss 0.091992   Top1 96.763393   Top5 99.988839   BatchTime 0.136216   LR 0.001000
INFO - Training [38][  160/  391]   Loss 0.092048   Top1 96.757812   Top5 99.990234   BatchTime 0.134116   LR 0.001000
INFO - Training [38][  180/  391]   Loss 0.091189   Top1 96.814236   Top5 99.991319   BatchTime 0.128282   LR 0.001000
INFO - Training [38][  200/  391]   Loss 0.090899   Top1 96.847656   Top5 99.992188   BatchTime 0.124457   LR 0.001000
INFO - Training [38][  220/  391]   Loss 0.091993   Top1 96.803977   Top5 99.985795   BatchTime 0.120855   LR 0.001000
INFO - Training [38][  240/  391]   Loss 0.092511   Top1 96.777344   Top5 99.983724   BatchTime 0.119654   LR 0.001000
INFO - Training [38][  260/  391]   Loss 0.092715   Top1 96.751803   Top5 99.978966   BatchTime 0.119910   LR 0.001000
INFO - Training [38][  280/  391]   Loss 0.092603   Top1 96.746652   Top5 99.980469   BatchTime 0.120198   LR 0.001000
INFO - Training [38][  300/  391]   Loss 0.093335   Top1 96.723958   Top5 99.981771   BatchTime 0.120383   LR 0.001000
INFO - Training [38][  320/  391]   Loss 0.093488   Top1 96.723633   Top5 99.982910   BatchTime 0.120591   LR 0.001000
INFO - Training [38][  340/  391]   Loss 0.093199   Top1 96.730239   Top5 99.983915   BatchTime 0.120728   LR 0.001000
INFO - Training [38][  360/  391]   Loss 0.092396   Top1 96.753472   Top5 99.982639   BatchTime 0.120909   LR 0.001000
INFO - Training [38][  380/  391]   Loss 0.092688   Top1 96.749589   Top5 99.983553   BatchTime 0.120963   LR 0.001000
INFO - ==> Top1: 96.744    Top5: 99.984    Loss: 0.093
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [38][   20/   79]   Loss 0.352645   Top1 90.546875   Top5 99.609375   BatchTime 0.152557
INFO - Validation [38][   40/   79]   Loss 0.354836   Top1 90.332031   Top5 99.648438   BatchTime 0.093208
INFO - Validation [38][   60/   79]   Loss 0.353765   Top1 90.455729   Top5 99.635417   BatchTime 0.070954
INFO - ==> Top1: 90.270    Top5: 99.660    Loss: 0.352
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Scoreboard best 2 ==> Epoch [38][Top1: 90.270   Top5: 99.660] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 90.230   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  39
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [39][   20/  391]   Loss 0.085360   Top1 97.226562   Top5 99.960938   BatchTime 0.173523   LR 0.001000
INFO - Training [39][   40/  391]   Loss 0.084484   Top1 97.304688   Top5 99.960938   BatchTime 0.151459   LR 0.001000
INFO - Training [39][   60/  391]   Loss 0.085246   Top1 97.239583   Top5 99.960938   BatchTime 0.141690   LR 0.001000
INFO - Training [39][   80/  391]   Loss 0.087582   Top1 97.070312   Top5 99.970703   BatchTime 0.137373   LR 0.001000
INFO - Training [39][  100/  391]   Loss 0.089118   Top1 96.984375   Top5 99.976562   BatchTime 0.134614   LR 0.001000
INFO - Training [39][  120/  391]   Loss 0.088862   Top1 96.914062   Top5 99.980469   BatchTime 0.133493   LR 0.001000
INFO - Training [39][  140/  391]   Loss 0.089736   Top1 96.863839   Top5 99.977679   BatchTime 0.132252   LR 0.001000
INFO - Training [39][  160/  391]   Loss 0.089953   Top1 96.870117   Top5 99.980469   BatchTime 0.131141   LR 0.001000
INFO - Training [39][  180/  391]   Loss 0.090013   Top1 96.875000   Top5 99.982639   BatchTime 0.130295   LR 0.001000
INFO - Training [39][  200/  391]   Loss 0.088088   Top1 96.957031   Top5 99.980469   BatchTime 0.129591   LR 0.001000
INFO - Training [39][  220/  391]   Loss 0.088220   Top1 96.938920   Top5 99.982244   BatchTime 0.129070   LR 0.001000
INFO - Training [39][  240/  391]   Loss 0.087806   Top1 96.930339   Top5 99.980469   BatchTime 0.124676   LR 0.001000
INFO - Training [39][  260/  391]   Loss 0.089274   Top1 96.865986   Top5 99.978966   BatchTime 0.122269   LR 0.001000
INFO - Training [39][  280/  391]   Loss 0.088839   Top1 96.866629   Top5 99.980469   BatchTime 0.119790   LR 0.001000
INFO - Training [39][  300/  391]   Loss 0.089613   Top1 96.843750   Top5 99.981771   BatchTime 0.118718   LR 0.001000
INFO - Training [39][  320/  391]   Loss 0.090307   Top1 96.821289   Top5 99.982910   BatchTime 0.119075   LR 0.001000
INFO - Training [39][  340/  391]   Loss 0.090612   Top1 96.812960   Top5 99.981618   BatchTime 0.119320   LR 0.001000
INFO - Training [39][  360/  391]   Loss 0.090793   Top1 96.818576   Top5 99.982639   BatchTime 0.119555   LR 0.001000
INFO - Training [39][  380/  391]   Loss 0.091147   Top1 96.809211   Top5 99.981497   BatchTime 0.119733   LR 0.001000
INFO - ==> Top1: 96.798    Top5: 99.982    Loss: 0.091
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [39][   20/   79]   Loss 0.357582   Top1 90.312500   Top5 99.687500   BatchTime 0.152516
INFO - Validation [39][   40/   79]   Loss 0.355102   Top1 90.390625   Top5 99.648438   BatchTime 0.108138
INFO - Validation [39][   60/   79]   Loss 0.353377   Top1 90.468750   Top5 99.661458   BatchTime 0.093266
INFO - ==> Top1: 90.340    Top5: 99.710    Loss: 0.353
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 90.270   Top5: 99.660] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  40
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [40][   20/  391]   Loss 0.083581   Top1 97.109375   Top5 100.000000   BatchTime 0.172611   LR 0.001000
INFO - Training [40][   40/  391]   Loss 0.084078   Top1 97.207031   Top5 100.000000   BatchTime 0.132660   LR 0.001000
INFO - Training [40][   60/  391]   Loss 0.085324   Top1 97.122396   Top5 100.000000   BatchTime 0.119496   LR 0.001000
INFO - Training [40][   80/  391]   Loss 0.088496   Top1 97.050781   Top5 99.980469   BatchTime 0.115331   LR 0.001000
INFO - Training [40][  100/  391]   Loss 0.091346   Top1 96.890625   Top5 99.976562   BatchTime 0.117066   LR 0.001000
INFO - Training [40][  120/  391]   Loss 0.090227   Top1 96.946615   Top5 99.980469   BatchTime 0.118073   LR 0.001000
INFO - Training [40][  140/  391]   Loss 0.089372   Top1 96.914062   Top5 99.983259   BatchTime 0.118801   LR 0.001000
INFO - Training [40][  160/  391]   Loss 0.089938   Top1 96.875000   Top5 99.985352   BatchTime 0.119495   LR 0.001000
INFO - Training [40][  180/  391]   Loss 0.089595   Top1 96.853299   Top5 99.982639   BatchTime 0.119993   LR 0.001000
INFO - Training [40][  200/  391]   Loss 0.090797   Top1 96.812500   Top5 99.980469   BatchTime 0.120385   LR 0.001000
INFO - Training [40][  220/  391]   Loss 0.089785   Top1 96.843040   Top5 99.982244   BatchTime 0.120696   LR 0.001000
INFO - Training [40][  240/  391]   Loss 0.090481   Top1 96.819661   Top5 99.983724   BatchTime 0.120823   LR 0.001000
INFO - Training [40][  260/  391]   Loss 0.090735   Top1 96.808894   Top5 99.984976   BatchTime 0.121049   LR 0.001000
INFO - Training [40][  280/  391]   Loss 0.090088   Top1 96.855469   Top5 99.980469   BatchTime 0.119801   LR 0.001000
INFO - Training [40][  300/  391]   Loss 0.090074   Top1 96.841146   Top5 99.979167   BatchTime 0.117582   LR 0.001000
INFO - Training [40][  320/  391]   Loss 0.090082   Top1 96.857910   Top5 99.978027   BatchTime 0.116105   LR 0.001000
INFO - Training [40][  340/  391]   Loss 0.090875   Top1 96.829044   Top5 99.979320   BatchTime 0.113636   LR 0.001000
INFO - Training [40][  360/  391]   Loss 0.090553   Top1 96.829427   Top5 99.980469   BatchTime 0.114117   LR 0.001000
INFO - Training [40][  380/  391]   Loss 0.090188   Top1 96.831826   Top5 99.981497   BatchTime 0.114620   LR 0.001000
INFO - ==> Top1: 96.836    Top5: 99.982    Loss: 0.090
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [40][   20/   79]   Loss 0.350591   Top1 90.625000   Top5 99.687500   BatchTime 0.152298
INFO - Validation [40][   40/   79]   Loss 0.356104   Top1 90.292969   Top5 99.667969   BatchTime 0.107316
INFO - Validation [40][   60/   79]   Loss 0.353915   Top1 90.442708   Top5 99.674479   BatchTime 0.092588
INFO - ==> Top1: 90.340    Top5: 99.670    Loss: 0.353
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  41
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [41][   20/  391]   Loss 0.087883   Top1 96.601562   Top5 100.000000   BatchTime 0.213749   LR 0.001000
INFO - Training [41][   40/  391]   Loss 0.086633   Top1 96.777344   Top5 100.000000   BatchTime 0.168144   LR 0.001000
INFO - Training [41][   60/  391]   Loss 0.088102   Top1 96.770833   Top5 100.000000   BatchTime 0.150165   LR 0.001000
INFO - Training [41][   80/  391]   Loss 0.088608   Top1 96.796875   Top5 99.990234   BatchTime 0.133175   LR 0.001000
INFO - Training [41][  100/  391]   Loss 0.091441   Top1 96.726562   Top5 99.992188   BatchTime 0.124540   LR 0.001000
INFO - Training [41][  120/  391]   Loss 0.090435   Top1 96.796875   Top5 99.993490   BatchTime 0.117389   LR 0.001000
INFO - Training [41][  140/  391]   Loss 0.089056   Top1 96.863839   Top5 99.994420   BatchTime 0.116866   LR 0.001000
INFO - Training [41][  160/  391]   Loss 0.088374   Top1 96.884766   Top5 99.995117   BatchTime 0.117670   LR 0.001000
INFO - Training [41][  180/  391]   Loss 0.088924   Top1 96.840278   Top5 99.995660   BatchTime 0.118711   LR 0.001000
INFO - Training [41][  200/  391]   Loss 0.088875   Top1 96.820312   Top5 99.996094   BatchTime 0.119212   LR 0.001000
INFO - Training [41][  220/  391]   Loss 0.090004   Top1 96.779119   Top5 99.989347   BatchTime 0.119585   LR 0.001000
INFO - Training [41][  240/  391]   Loss 0.089980   Top1 96.809896   Top5 99.990234   BatchTime 0.119951   LR 0.001000
INFO - Training [41][  260/  391]   Loss 0.090131   Top1 96.805889   Top5 99.990986   BatchTime 0.120215   LR 0.001000
INFO - Training [41][  280/  391]   Loss 0.089999   Top1 96.802455   Top5 99.991629   BatchTime 0.120373   LR 0.001000
INFO - Training [41][  300/  391]   Loss 0.089962   Top1 96.799479   Top5 99.992188   BatchTime 0.120520   LR 0.001000
INFO - Training [41][  320/  391]   Loss 0.090131   Top1 96.806641   Top5 99.990234   BatchTime 0.120631   LR 0.001000
INFO - Training [41][  340/  391]   Loss 0.090004   Top1 96.817555   Top5 99.988511   BatchTime 0.119082   LR 0.001000
INFO - Training [41][  360/  391]   Loss 0.090304   Top1 96.803385   Top5 99.989149   BatchTime 0.117239   LR 0.001000
INFO - Training [41][  380/  391]   Loss 0.090492   Top1 96.796875   Top5 99.989720   BatchTime 0.115829   LR 0.001000
INFO - ==> Top1: 96.792    Top5: 99.990    Loss: 0.091
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [41][   20/   79]   Loss 0.361503   Top1 90.273438   Top5 99.726562   BatchTime 0.130445
INFO - Validation [41][   40/   79]   Loss 0.358446   Top1 90.175781   Top5 99.589844   BatchTime 0.078417
INFO - Validation [41][   60/   79]   Loss 0.352458   Top1 90.429688   Top5 99.609375   BatchTime 0.061032
INFO - ==> Top1: 90.230    Top5: 99.640    Loss: 0.352
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  42
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [42][   20/  391]   Loss 0.088919   Top1 97.031250   Top5 99.960938   BatchTime 0.161730   LR 0.001000
INFO - Training [42][   40/  391]   Loss 0.082611   Top1 97.128906   Top5 99.980469   BatchTime 0.118329   LR 0.001000
INFO - Training [42][   60/  391]   Loss 0.086281   Top1 97.044271   Top5 99.986979   BatchTime 0.105759   LR 0.001000
INFO - Training [42][   80/  391]   Loss 0.086400   Top1 97.050781   Top5 99.990234   BatchTime 0.098700   LR 0.001000
INFO - Training [42][  100/  391]   Loss 0.087038   Top1 96.968750   Top5 99.992188   BatchTime 0.094364   LR 0.001000
INFO - Training [42][  120/  391]   Loss 0.087851   Top1 96.959635   Top5 99.986979   BatchTime 0.091560   LR 0.001000
INFO - Training [42][  140/  391]   Loss 0.085940   Top1 97.036830   Top5 99.988839   BatchTime 0.089643   LR 0.001000
INFO - Training [42][  160/  391]   Loss 0.086374   Top1 96.958008   Top5 99.990234   BatchTime 0.088002   LR 0.001000
INFO - Training [42][  180/  391]   Loss 0.087530   Top1 96.909722   Top5 99.991319   BatchTime 0.087943   LR 0.001000
INFO - Training [42][  200/  391]   Loss 0.088120   Top1 96.894531   Top5 99.992188   BatchTime 0.087038   LR 0.001000
INFO - Training [42][  220/  391]   Loss 0.087913   Top1 96.906960   Top5 99.992898   BatchTime 0.087132   LR 0.001000
INFO - Training [42][  240/  391]   Loss 0.088534   Top1 96.875000   Top5 99.993490   BatchTime 0.086977   LR 0.001000
INFO - Training [42][  260/  391]   Loss 0.087530   Top1 96.902043   Top5 99.990986   BatchTime 0.088324   LR 0.001000
INFO - Training [42][  280/  391]   Loss 0.087410   Top1 96.875000   Top5 99.988839   BatchTime 0.090876   LR 0.001000
INFO - Training [42][  300/  391]   Loss 0.087920   Top1 96.856771   Top5 99.989583   BatchTime 0.093085   LR 0.001000
INFO - Training [42][  320/  391]   Loss 0.087332   Top1 96.877441   Top5 99.987793   BatchTime 0.095011   LR 0.001000
INFO - Training [42][  340/  391]   Loss 0.087812   Top1 96.865809   Top5 99.986213   BatchTime 0.096722   LR 0.001000
INFO - Training [42][  360/  391]   Loss 0.087995   Top1 96.857639   Top5 99.984809   BatchTime 0.098213   LR 0.001000
INFO - Training [42][  380/  391]   Loss 0.088567   Top1 96.840049   Top5 99.983553   BatchTime 0.099545   LR 0.001000
INFO - ==> Top1: 96.838    Top5: 99.984    Loss: 0.089
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [42][   20/   79]   Loss 0.363554   Top1 90.351562   Top5 99.609375   BatchTime 0.151899
INFO - Validation [42][   40/   79]   Loss 0.358971   Top1 90.195312   Top5 99.589844   BatchTime 0.108731
INFO - Validation [42][   60/   79]   Loss 0.355096   Top1 90.338542   Top5 99.609375   BatchTime 0.092386
INFO - ==> Top1: 90.190    Top5: 99.640    Loss: 0.353
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  43
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [43][   20/  391]   Loss 0.088833   Top1 96.835938   Top5 100.000000   BatchTime 0.188989   LR 0.001000
INFO - Training [43][   40/  391]   Loss 0.087062   Top1 96.816406   Top5 99.980469   BatchTime 0.131769   LR 0.001000
INFO - Training [43][   60/  391]   Loss 0.086998   Top1 96.901042   Top5 99.986979   BatchTime 0.127309   LR 0.001000
INFO - Training [43][   80/  391]   Loss 0.086996   Top1 96.875000   Top5 99.990234   BatchTime 0.126664   LR 0.001000
INFO - Training [43][  100/  391]   Loss 0.086560   Top1 96.843750   Top5 99.992188   BatchTime 0.126090   LR 0.001000
INFO - Training [43][  120/  391]   Loss 0.087650   Top1 96.809896   Top5 99.993490   BatchTime 0.125743   LR 0.001000
INFO - Training [43][  140/  391]   Loss 0.086909   Top1 96.886161   Top5 99.994420   BatchTime 0.125418   LR 0.001000
INFO - Training [43][  160/  391]   Loss 0.086922   Top1 96.914062   Top5 99.990234   BatchTime 0.124555   LR 0.001000
INFO - Training [43][  180/  391]   Loss 0.087188   Top1 96.948785   Top5 99.991319   BatchTime 0.124398   LR 0.001000
INFO - Training [43][  200/  391]   Loss 0.086856   Top1 96.976562   Top5 99.992188   BatchTime 0.124247   LR 0.001000
INFO - Training [43][  220/  391]   Loss 0.086508   Top1 96.995739   Top5 99.989347   BatchTime 0.124215   LR 0.001000
INFO - Training [43][  240/  391]   Loss 0.086276   Top1 97.011719   Top5 99.986979   BatchTime 0.124487   LR 0.001000
INFO - Training [43][  260/  391]   Loss 0.086222   Top1 97.040264   Top5 99.987981   BatchTime 0.121592   LR 0.001000
INFO - Training [43][  280/  391]   Loss 0.087108   Top1 96.981027   Top5 99.988839   BatchTime 0.119279   LR 0.001000
INFO - Training [43][  300/  391]   Loss 0.087238   Top1 96.971354   Top5 99.984375   BatchTime 0.117265   LR 0.001000
INFO - Training [43][  320/  391]   Loss 0.087200   Top1 96.965332   Top5 99.985352   BatchTime 0.114797   LR 0.001000
INFO - Training [43][  340/  391]   Loss 0.086971   Top1 96.976103   Top5 99.981618   BatchTime 0.115745   LR 0.001000
INFO - Training [43][  360/  391]   Loss 0.087777   Top1 96.963976   Top5 99.978299   BatchTime 0.116184   LR 0.001000
INFO - Training [43][  380/  391]   Loss 0.087880   Top1 96.946957   Top5 99.977385   BatchTime 0.116535   LR 0.001000
INFO - ==> Top1: 96.938    Top5: 99.978    Loss: 0.088
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [43][   20/   79]   Loss 0.371235   Top1 89.843750   Top5 99.687500   BatchTime 0.152553
INFO - Validation [43][   40/   79]   Loss 0.359440   Top1 90.078125   Top5 99.628906   BatchTime 0.107992
INFO - Validation [43][   60/   79]   Loss 0.359699   Top1 90.091146   Top5 99.622396   BatchTime 0.092779
INFO - ==> Top1: 90.020    Top5: 99.660    Loss: 0.359
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [35][Top1: 90.290   Top5: 99.640] Sparsity : 0.876
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  44
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [44][   20/  391]   Loss 0.093624   Top1 96.406250   Top5 100.000000   BatchTime 0.213648   LR 0.001000
INFO - Training [44][   40/  391]   Loss 0.095027   Top1 96.757812   Top5 100.000000   BatchTime 0.161221   LR 0.001000
INFO - Training [44][   60/  391]   Loss 0.096786   Top1 96.653646   Top5 100.000000   BatchTime 0.134454   LR 0.001000
INFO - Training [44][   80/  391]   Loss 0.095539   Top1 96.621094   Top5 100.000000   BatchTime 0.123404   LR 0.001000
INFO - Training [44][  100/  391]   Loss 0.093494   Top1 96.726562   Top5 100.000000   BatchTime 0.115704   LR 0.001000
INFO - Training [44][  120/  391]   Loss 0.093125   Top1 96.686198   Top5 99.993490   BatchTime 0.116122   LR 0.001000
INFO - Training [44][  140/  391]   Loss 0.090399   Top1 96.796875   Top5 99.988839   BatchTime 0.117249   LR 0.001000
INFO - Training [44][  160/  391]   Loss 0.089942   Top1 96.826172   Top5 99.985352   BatchTime 0.118081   LR 0.001000
INFO - Training [44][  180/  391]   Loss 0.090887   Top1 96.809896   Top5 99.986979   BatchTime 0.118698   LR 0.001000
INFO - Training [44][  200/  391]   Loss 0.090324   Top1 96.824219   Top5 99.988281   BatchTime 0.119289   LR 0.001000
INFO - Training [44][  220/  391]   Loss 0.089510   Top1 96.875000   Top5 99.989347   BatchTime 0.119682   LR 0.001000
INFO - Training [44][  240/  391]   Loss 0.088369   Top1 96.933594   Top5 99.990234   BatchTime 0.119996   LR 0.001000
INFO - Training [44][  260/  391]   Loss 0.087822   Top1 96.938101   Top5 99.990986   BatchTime 0.120239   LR 0.001000
INFO - Training [44][  280/  391]   Loss 0.088532   Top1 96.905692   Top5 99.991629   BatchTime 0.120473   LR 0.001000
INFO - Training [44][  300/  391]   Loss 0.087929   Top1 96.942708   Top5 99.992188   BatchTime 0.120614   LR 0.001000
INFO - Training [44][  320/  391]   Loss 0.088073   Top1 96.926270   Top5 99.990234   BatchTime 0.118667   LR 0.001000
INFO - Training [44][  340/  391]   Loss 0.088120   Top1 96.911765   Top5 99.988511   BatchTime 0.116951   LR 0.001000
INFO - Training [44][  360/  391]   Loss 0.087922   Top1 96.920573   Top5 99.986979   BatchTime 0.115484   LR 0.001000
INFO - Training [44][  380/  391]   Loss 0.087991   Top1 96.916118   Top5 99.985609   BatchTime 0.113779   LR 0.001000
INFO - ==> Top1: 96.930    Top5: 99.986    Loss: 0.088
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [44][   20/   79]   Loss 0.356201   Top1 90.156250   Top5 99.726562   BatchTime 0.151498
INFO - Validation [44][   40/   79]   Loss 0.356061   Top1 90.390625   Top5 99.648438   BatchTime 0.107849
INFO - Validation [44][   60/   79]   Loss 0.356076   Top1 90.455729   Top5 99.635417   BatchTime 0.093022
INFO - ==> Top1: 90.350    Top5: 99.670    Loss: 0.357
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  45
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [45][   20/  391]   Loss 0.075433   Top1 97.109375   Top5 100.000000   BatchTime 0.214153   LR 0.001000
INFO - Training [45][   40/  391]   Loss 0.085519   Top1 97.031250   Top5 99.980469   BatchTime 0.168815   LR 0.001000
INFO - Training [45][   60/  391]   Loss 0.085606   Top1 96.888021   Top5 99.986979   BatchTime 0.153466   LR 0.001000
INFO - Training [45][   80/  391]   Loss 0.082481   Top1 97.070312   Top5 99.990234   BatchTime 0.145996   LR 0.001000
INFO - Training [45][  100/  391]   Loss 0.084530   Top1 97.023438   Top5 99.992188   BatchTime 0.137821   LR 0.001000
INFO - Training [45][  120/  391]   Loss 0.082887   Top1 97.063802   Top5 99.993490   BatchTime 0.128710   LR 0.001000
INFO - Training [45][  140/  391]   Loss 0.083213   Top1 97.053571   Top5 99.988839   BatchTime 0.124002   LR 0.001000
INFO - Training [45][  160/  391]   Loss 0.083494   Top1 97.060547   Top5 99.990234   BatchTime 0.118413   LR 0.001000
INFO - Training [45][  180/  391]   Loss 0.083327   Top1 97.078993   Top5 99.986979   BatchTime 0.118998   LR 0.001000
INFO - Training [45][  200/  391]   Loss 0.083439   Top1 97.062500   Top5 99.984375   BatchTime 0.119506   LR 0.001000
INFO - Training [45][  220/  391]   Loss 0.083321   Top1 97.070312   Top5 99.985795   BatchTime 0.119909   LR 0.001000
INFO - Training [45][  240/  391]   Loss 0.083016   Top1 97.063802   Top5 99.986979   BatchTime 0.120213   LR 0.001000
INFO - Training [45][  260/  391]   Loss 0.082909   Top1 97.070312   Top5 99.984976   BatchTime 0.120554   LR 0.001000
INFO - Training [45][  280/  391]   Loss 0.083252   Top1 97.053571   Top5 99.983259   BatchTime 0.121056   LR 0.001000
INFO - Training [45][  300/  391]   Loss 0.084068   Top1 97.028646   Top5 99.984375   BatchTime 0.120860   LR 0.001000
INFO - Training [45][  320/  391]   Loss 0.084376   Top1 97.009277   Top5 99.985352   BatchTime 0.120971   LR 0.001000
INFO - Training [45][  340/  391]   Loss 0.084080   Top1 97.022059   Top5 99.986213   BatchTime 0.121093   LR 0.001000
INFO - Training [45][  360/  391]   Loss 0.084636   Top1 96.996528   Top5 99.982639   BatchTime 0.121182   LR 0.001000
INFO - Training [45][  380/  391]   Loss 0.084571   Top1 96.996299   Top5 99.983553   BatchTime 0.119228   LR 0.001000
INFO - ==> Top1: 96.998    Top5: 99.984    Loss: 0.085
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [45][   20/   79]   Loss 0.359745   Top1 89.726562   Top5 99.726562   BatchTime 0.123128
INFO - Validation [45][   40/   79]   Loss 0.362516   Top1 90.078125   Top5 99.628906   BatchTime 0.074817
INFO - Validation [45][   60/   79]   Loss 0.361617   Top1 90.221354   Top5 99.674479   BatchTime 0.065149
INFO - ==> Top1: 90.210    Top5: 99.700    Loss: 0.359
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  46
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [46][   20/  391]   Loss 0.076485   Top1 97.343750   Top5 100.000000   BatchTime 0.215094   LR 0.001000
INFO - Training [46][   40/  391]   Loss 0.081050   Top1 97.246094   Top5 100.000000   BatchTime 0.169428   LR 0.001000
INFO - Training [46][   60/  391]   Loss 0.080799   Top1 97.213542   Top5 99.973958   BatchTime 0.154164   LR 0.001000
INFO - Training [46][   80/  391]   Loss 0.080726   Top1 97.216797   Top5 99.980469   BatchTime 0.146477   LR 0.001000
INFO - Training [46][  100/  391]   Loss 0.080232   Top1 97.203125   Top5 99.976562   BatchTime 0.141811   LR 0.001000
INFO - Training [46][  120/  391]   Loss 0.080271   Top1 97.161458   Top5 99.973958   BatchTime 0.138718   LR 0.001000
INFO - Training [46][  140/  391]   Loss 0.081624   Top1 97.109375   Top5 99.977679   BatchTime 0.136552   LR 0.001000
INFO - Training [46][  160/  391]   Loss 0.081302   Top1 97.153320   Top5 99.980469   BatchTime 0.134403   LR 0.001000
INFO - Training [46][  180/  391]   Loss 0.083094   Top1 97.070312   Top5 99.982639   BatchTime 0.128660   LR 0.001000
INFO - Training [46][  200/  391]   Loss 0.082105   Top1 97.105469   Top5 99.984375   BatchTime 0.124915   LR 0.001000
INFO - Training [46][  220/  391]   Loss 0.083270   Top1 97.084517   Top5 99.985795   BatchTime 0.121570   LR 0.001000
INFO - Training [46][  240/  391]   Loss 0.082770   Top1 97.119141   Top5 99.986979   BatchTime 0.121144   LR 0.001000
INFO - Training [46][  260/  391]   Loss 0.082641   Top1 97.130409   Top5 99.984976   BatchTime 0.121318   LR 0.001000
INFO - Training [46][  280/  391]   Loss 0.081082   Top1 97.173549   Top5 99.983259   BatchTime 0.121484   LR 0.001000
INFO - Training [46][  300/  391]   Loss 0.081975   Top1 97.145833   Top5 99.984375   BatchTime 0.121640   LR 0.001000
INFO - Training [46][  320/  391]   Loss 0.083179   Top1 97.082520   Top5 99.985352   BatchTime 0.121789   LR 0.001000
INFO - Training [46][  340/  391]   Loss 0.083129   Top1 97.095588   Top5 99.983915   BatchTime 0.121899   LR 0.001000
INFO - Training [46][  360/  391]   Loss 0.083259   Top1 97.083333   Top5 99.984809   BatchTime 0.122001   LR 0.001000
INFO - Training [46][  380/  391]   Loss 0.083647   Top1 97.051809   Top5 99.983553   BatchTime 0.122015   LR 0.001000
INFO - ==> Top1: 97.060    Top5: 99.984    Loss: 0.084
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [46][   20/   79]   Loss 0.373913   Top1 90.117188   Top5 99.570312   BatchTime 0.150284
INFO - Validation [46][   40/   79]   Loss 0.364658   Top1 90.332031   Top5 99.531250   BatchTime 0.089998
INFO - Validation [46][   60/   79]   Loss 0.360388   Top1 90.403646   Top5 99.557292   BatchTime 0.068786
INFO - ==> Top1: 90.240    Top5: 99.610    Loss: 0.358
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  47
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [47][   20/  391]   Loss 0.081653   Top1 96.953125   Top5 100.000000   BatchTime 0.174049   LR 0.001000
INFO - Training [47][   40/  391]   Loss 0.081289   Top1 97.128906   Top5 99.960938   BatchTime 0.149879   LR 0.001000
INFO - Training [47][   60/  391]   Loss 0.082417   Top1 97.109375   Top5 99.973958   BatchTime 0.141260   LR 0.001000
INFO - Training [47][   80/  391]   Loss 0.084031   Top1 97.050781   Top5 99.980469   BatchTime 0.136787   LR 0.001000
INFO - Training [47][  100/  391]   Loss 0.085851   Top1 96.960938   Top5 99.984375   BatchTime 0.134207   LR 0.001000
INFO - Training [47][  120/  391]   Loss 0.086445   Top1 96.946615   Top5 99.986979   BatchTime 0.132399   LR 0.001000
INFO - Training [47][  140/  391]   Loss 0.086449   Top1 96.930804   Top5 99.988839   BatchTime 0.131210   LR 0.001000
INFO - Training [47][  160/  391]   Loss 0.088112   Top1 96.889648   Top5 99.985352   BatchTime 0.130187   LR 0.001000
INFO - Training [47][  180/  391]   Loss 0.088222   Top1 96.883681   Top5 99.986979   BatchTime 0.129385   LR 0.001000
INFO - Training [47][  200/  391]   Loss 0.088783   Top1 96.843750   Top5 99.988281   BatchTime 0.128770   LR 0.001000
INFO - Training [47][  220/  391]   Loss 0.088914   Top1 96.864347   Top5 99.989347   BatchTime 0.128471   LR 0.001000
INFO - Training [47][  240/  391]   Loss 0.087647   Top1 96.917318   Top5 99.990234   BatchTime 0.124197   LR 0.001000
INFO - Training [47][  260/  391]   Loss 0.087132   Top1 96.932091   Top5 99.990986   BatchTime 0.121742   LR 0.001000
INFO - Training [47][  280/  391]   Loss 0.087119   Top1 96.911272   Top5 99.991629   BatchTime 0.119356   LR 0.001000
INFO - Training [47][  300/  391]   Loss 0.087168   Top1 96.901042   Top5 99.992188   BatchTime 0.117926   LR 0.001000
INFO - Training [47][  320/  391]   Loss 0.086796   Top1 96.909180   Top5 99.992676   BatchTime 0.118265   LR 0.001000
INFO - Training [47][  340/  391]   Loss 0.087286   Top1 96.879596   Top5 99.993107   BatchTime 0.118990   LR 0.001000
INFO - Training [47][  360/  391]   Loss 0.087185   Top1 96.892361   Top5 99.993490   BatchTime 0.119297   LR 0.001000
INFO - Training [47][  380/  391]   Loss 0.087836   Top1 96.860609   Top5 99.991776   BatchTime 0.119498   LR 0.001000
INFO - ==> Top1: 96.856    Top5: 99.992    Loss: 0.088
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [47][   20/   79]   Loss 0.353532   Top1 89.804688   Top5 99.570312   BatchTime 0.147817
INFO - Validation [47][   40/   79]   Loss 0.361356   Top1 89.746094   Top5 99.609375   BatchTime 0.104933
INFO - Validation [47][   60/   79]   Loss 0.357013   Top1 90.000000   Top5 99.622396   BatchTime 0.091715
INFO - ==> Top1: 90.000    Top5: 99.650    Loss: 0.356
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  48
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [48][   20/  391]   Loss 0.091923   Top1 96.835938   Top5 100.000000   BatchTime 0.179124   LR 0.001000
INFO - Training [48][   40/  391]   Loss 0.091559   Top1 96.796875   Top5 100.000000   BatchTime 0.134205   LR 0.001000
INFO - Training [48][   60/  391]   Loss 0.084872   Top1 96.992188   Top5 100.000000   BatchTime 0.120203   LR 0.001000
INFO - Training [48][   80/  391]   Loss 0.085529   Top1 97.001953   Top5 100.000000   BatchTime 0.111667   LR 0.001000
INFO - Training [48][  100/  391]   Loss 0.084327   Top1 97.070312   Top5 100.000000   BatchTime 0.113929   LR 0.001000
INFO - Training [48][  120/  391]   Loss 0.085086   Top1 97.057292   Top5 100.000000   BatchTime 0.115607   LR 0.001000
INFO - Training [48][  140/  391]   Loss 0.086785   Top1 96.992188   Top5 99.983259   BatchTime 0.116800   LR 0.001000
INFO - Training [48][  160/  391]   Loss 0.085444   Top1 97.041016   Top5 99.985352   BatchTime 0.117638   LR 0.001000
INFO - Training [48][  180/  391]   Loss 0.085848   Top1 97.035590   Top5 99.982639   BatchTime 0.118389   LR 0.001000
INFO - Training [48][  200/  391]   Loss 0.086350   Top1 97.007812   Top5 99.984375   BatchTime 0.118904   LR 0.001000
INFO - Training [48][  220/  391]   Loss 0.086641   Top1 97.002841   Top5 99.985795   BatchTime 0.119380   LR 0.001000
INFO - Training [48][  240/  391]   Loss 0.085669   Top1 97.041016   Top5 99.983724   BatchTime 0.119675   LR 0.001000
INFO - Training [48][  260/  391]   Loss 0.085920   Top1 97.016226   Top5 99.984976   BatchTime 0.119925   LR 0.001000
INFO - Training [48][  280/  391]   Loss 0.086303   Top1 96.997768   Top5 99.983259   BatchTime 0.119764   LR 0.001000
INFO - Training [48][  300/  391]   Loss 0.086007   Top1 97.023438   Top5 99.984375   BatchTime 0.117171   LR 0.001000
INFO - Training [48][  320/  391]   Loss 0.085728   Top1 97.014160   Top5 99.985352   BatchTime 0.115450   LR 0.001000
INFO - Training [48][  340/  391]   Loss 0.085689   Top1 97.028952   Top5 99.986213   BatchTime 0.113615   LR 0.001000
INFO - Training [48][  360/  391]   Loss 0.085748   Top1 97.013889   Top5 99.986979   BatchTime 0.113049   LR 0.001000
INFO - Training [48][  380/  391]   Loss 0.085412   Top1 97.035362   Top5 99.987664   BatchTime 0.113575   LR 0.001000
INFO - ==> Top1: 97.016    Top5: 99.988    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [48][   20/   79]   Loss 0.366816   Top1 89.921875   Top5 99.765625   BatchTime 0.153122
INFO - Validation [48][   40/   79]   Loss 0.364731   Top1 90.253906   Top5 99.628906   BatchTime 0.109208
INFO - Validation [48][   60/   79]   Loss 0.362306   Top1 90.312500   Top5 99.622396   BatchTime 0.094311
INFO - ==> Top1: 90.230    Top5: 99.660    Loss: 0.362
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  49
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [49][   20/  391]   Loss 0.096703   Top1 96.484375   Top5 99.921875   BatchTime 0.213081   LR 0.001000
INFO - Training [49][   40/  391]   Loss 0.085029   Top1 96.914062   Top5 99.960938   BatchTime 0.168458   LR 0.001000
INFO - Training [49][   60/  391]   Loss 0.084500   Top1 97.018229   Top5 99.973958   BatchTime 0.153352   LR 0.001000
INFO - Training [49][   80/  391]   Loss 0.087647   Top1 96.923828   Top5 99.970703   BatchTime 0.135933   LR 0.001000
INFO - Training [49][  100/  391]   Loss 0.085846   Top1 96.968750   Top5 99.976562   BatchTime 0.126865   LR 0.001000
INFO - Training [49][  120/  391]   Loss 0.086672   Top1 96.966146   Top5 99.973958   BatchTime 0.120721   LR 0.001000
INFO - Training [49][  140/  391]   Loss 0.085733   Top1 96.930804   Top5 99.977679   BatchTime 0.117354   LR 0.001000
INFO - Training [49][  160/  391]   Loss 0.086448   Top1 96.865234   Top5 99.975586   BatchTime 0.118124   LR 0.001000
INFO - Training [49][  180/  391]   Loss 0.086185   Top1 96.879340   Top5 99.978299   BatchTime 0.118767   LR 0.001000
INFO - Training [49][  200/  391]   Loss 0.087009   Top1 96.847656   Top5 99.980469   BatchTime 0.119311   LR 0.001000
INFO - Training [49][  220/  391]   Loss 0.086610   Top1 96.864347   Top5 99.978693   BatchTime 0.119708   LR 0.001000
INFO - Training [49][  240/  391]   Loss 0.085667   Top1 96.901042   Top5 99.980469   BatchTime 0.120082   LR 0.001000
INFO - Training [49][  260/  391]   Loss 0.084785   Top1 96.944111   Top5 99.981971   BatchTime 0.120331   LR 0.001000
INFO - Training [49][  280/  391]   Loss 0.085338   Top1 96.955915   Top5 99.977679   BatchTime 0.120538   LR 0.001000
INFO - Training [49][  300/  391]   Loss 0.085359   Top1 96.960938   Top5 99.979167   BatchTime 0.120714   LR 0.001000
INFO - Training [49][  320/  391]   Loss 0.085499   Top1 96.975098   Top5 99.980469   BatchTime 0.120919   LR 0.001000
INFO - Training [49][  340/  391]   Loss 0.085285   Top1 96.976103   Top5 99.979320   BatchTime 0.120029   LR 0.001000
INFO - Training [49][  360/  391]   Loss 0.085449   Top1 96.996528   Top5 99.980469   BatchTime 0.117714   LR 0.001000
INFO - Training [49][  380/  391]   Loss 0.085460   Top1 97.000411   Top5 99.973273   BatchTime 0.116084   LR 0.001000
INFO - ==> Top1: 97.008    Top5: 99.974    Loss: 0.085
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [49][   20/   79]   Loss 0.360541   Top1 90.195312   Top5 99.570312   BatchTime 0.161738
INFO - Validation [49][   40/   79]   Loss 0.359773   Top1 90.234375   Top5 99.609375   BatchTime 0.113415
INFO - Validation [49][   60/   79]   Loss 0.358034   Top1 90.325521   Top5 99.622396   BatchTime 0.096682
INFO - ==> Top1: 90.140    Top5: 99.670    Loss: 0.359
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  50
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [50][   20/  391]   Loss 0.076459   Top1 97.617188   Top5 100.000000   BatchTime 0.219290   LR 0.001000
INFO - Training [50][   40/  391]   Loss 0.076378   Top1 97.441406   Top5 100.000000   BatchTime 0.171101   LR 0.001000
INFO - Training [50][   60/  391]   Loss 0.079867   Top1 97.200521   Top5 100.000000   BatchTime 0.155404   LR 0.001000
INFO - Training [50][   80/  391]   Loss 0.084396   Top1 97.031250   Top5 100.000000   BatchTime 0.147413   LR 0.001000
INFO - Training [50][  100/  391]   Loss 0.081261   Top1 97.132812   Top5 100.000000   BatchTime 0.142526   LR 0.001000
INFO - Training [50][  120/  391]   Loss 0.083016   Top1 97.115885   Top5 100.000000   BatchTime 0.139306   LR 0.001000
INFO - Training [50][  140/  391]   Loss 0.083176   Top1 97.031250   Top5 100.000000   BatchTime 0.131264   LR 0.001000
INFO - Training [50][  160/  391]   Loss 0.083437   Top1 97.011719   Top5 100.000000   BatchTime 0.126258   LR 0.001000
INFO - Training [50][  180/  391]   Loss 0.085013   Top1 96.948785   Top5 100.000000   BatchTime 0.122119   LR 0.001000
INFO - Training [50][  200/  391]   Loss 0.084582   Top1 96.964844   Top5 100.000000   BatchTime 0.119964   LR 0.001000
INFO - Training [50][  220/  391]   Loss 0.084289   Top1 96.995739   Top5 100.000000   BatchTime 0.120270   LR 0.001000
INFO - Training [50][  240/  391]   Loss 0.084243   Top1 96.988932   Top5 100.000000   BatchTime 0.120549   LR 0.001000
INFO - Training [50][  260/  391]   Loss 0.083973   Top1 97.007212   Top5 100.000000   BatchTime 0.120814   LR 0.001000
INFO - Training [50][  280/  391]   Loss 0.083150   Top1 97.045201   Top5 100.000000   BatchTime 0.121016   LR 0.001000
INFO - Training [50][  300/  391]   Loss 0.083045   Top1 97.044271   Top5 100.000000   BatchTime 0.121187   LR 0.001000
INFO - Training [50][  320/  391]   Loss 0.083710   Top1 97.028809   Top5 99.995117   BatchTime 0.121322   LR 0.001000
INFO - Training [50][  340/  391]   Loss 0.083368   Top1 97.028952   Top5 99.995404   BatchTime 0.121399   LR 0.001000
INFO - Training [50][  360/  391]   Loss 0.083195   Top1 97.044271   Top5 99.995660   BatchTime 0.121495   LR 0.001000
INFO - Training [50][  380/  391]   Loss 0.082512   Top1 97.064145   Top5 99.995888   BatchTime 0.121537   LR 0.001000
INFO - ==> Top1: 97.022    Top5: 99.992    Loss: 0.084
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [50][   20/   79]   Loss 0.367500   Top1 90.273438   Top5 99.570312   BatchTime 0.138191
INFO - Validation [50][   40/   79]   Loss 0.367596   Top1 89.941406   Top5 99.589844   BatchTime 0.092577
INFO - Validation [50][   60/   79]   Loss 0.364651   Top1 90.182292   Top5 99.596354   BatchTime 0.071684
INFO - ==> Top1: 90.070    Top5: 99.630    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  51
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [51][   20/  391]   Loss 0.078538   Top1 97.070312   Top5 100.000000   BatchTime 0.221697   LR 0.001000
INFO - Training [51][   40/  391]   Loss 0.085137   Top1 96.757812   Top5 100.000000   BatchTime 0.172725   LR 0.001000
INFO - Training [51][   60/  391]   Loss 0.080572   Top1 96.992188   Top5 100.000000   BatchTime 0.156407   LR 0.001000
INFO - Training [51][   80/  391]   Loss 0.080905   Top1 97.031250   Top5 100.000000   BatchTime 0.148306   LR 0.001000
INFO - Training [51][  100/  391]   Loss 0.080430   Top1 97.078125   Top5 100.000000   BatchTime 0.143523   LR 0.001000
INFO - Training [51][  120/  391]   Loss 0.080056   Top1 97.044271   Top5 100.000000   BatchTime 0.140242   LR 0.001000
INFO - Training [51][  140/  391]   Loss 0.079765   Top1 97.064732   Top5 100.000000   BatchTime 0.137780   LR 0.001000
INFO - Training [51][  160/  391]   Loss 0.079493   Top1 97.060547   Top5 100.000000   BatchTime 0.135948   LR 0.001000
INFO - Training [51][  180/  391]   Loss 0.079534   Top1 97.096354   Top5 99.995660   BatchTime 0.134487   LR 0.001000
INFO - Training [51][  200/  391]   Loss 0.079461   Top1 97.109375   Top5 99.992188   BatchTime 0.130167   LR 0.001000
INFO - Training [51][  220/  391]   Loss 0.080152   Top1 97.077415   Top5 99.992898   BatchTime 0.126537   LR 0.001000
INFO - Training [51][  240/  391]   Loss 0.080478   Top1 97.083333   Top5 99.993490   BatchTime 0.123284   LR 0.001000
INFO - Training [51][  260/  391]   Loss 0.080132   Top1 97.097356   Top5 99.993990   BatchTime 0.120373   LR 0.001000
INFO - Training [51][  280/  391]   Loss 0.080771   Top1 97.070312   Top5 99.994420   BatchTime 0.120438   LR 0.001000
INFO - Training [51][  300/  391]   Loss 0.081167   Top1 97.059896   Top5 99.994792   BatchTime 0.120657   LR 0.001000
INFO - Training [51][  320/  391]   Loss 0.080976   Top1 97.072754   Top5 99.995117   BatchTime 0.120826   LR 0.001000
INFO - Training [51][  340/  391]   Loss 0.080846   Top1 97.084099   Top5 99.995404   BatchTime 0.120973   LR 0.001000
INFO - Training [51][  360/  391]   Loss 0.080994   Top1 97.085503   Top5 99.995660   BatchTime 0.121114   LR 0.001000
INFO - Training [51][  380/  391]   Loss 0.080839   Top1 97.086760   Top5 99.995888   BatchTime 0.121204   LR 0.001000
INFO - ==> Top1: 97.098    Top5: 99.996    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [51][   20/   79]   Loss 0.370864   Top1 89.687500   Top5 99.531250   BatchTime 0.153361
INFO - Validation [51][   40/   79]   Loss 0.365743   Top1 90.058594   Top5 99.531250   BatchTime 0.109229
INFO - Validation [51][   60/   79]   Loss 0.365395   Top1 90.130208   Top5 99.570312   BatchTime 0.094463
INFO - ==> Top1: 90.080    Top5: 99.590    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  52
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [52][   20/  391]   Loss 0.079609   Top1 97.031250   Top5 100.000000   BatchTime 0.189938   LR 0.001000
INFO - Training [52][   40/  391]   Loss 0.084622   Top1 96.894531   Top5 99.980469   BatchTime 0.133975   LR 0.001000
INFO - Training [52][   60/  391]   Loss 0.085585   Top1 96.835938   Top5 99.973958   BatchTime 0.131909   LR 0.001000
INFO - Training [52][   80/  391]   Loss 0.084359   Top1 96.914062   Top5 99.980469   BatchTime 0.130833   LR 0.001000
INFO - Training [52][  100/  391]   Loss 0.084536   Top1 96.984375   Top5 99.976562   BatchTime 0.129329   LR 0.001000
INFO - Training [52][  120/  391]   Loss 0.085474   Top1 96.992188   Top5 99.973958   BatchTime 0.128368   LR 0.001000
INFO - Training [52][  140/  391]   Loss 0.083919   Top1 97.047991   Top5 99.977679   BatchTime 0.127739   LR 0.001000
INFO - Training [52][  160/  391]   Loss 0.084065   Top1 97.055664   Top5 99.980469   BatchTime 0.127193   LR 0.001000
INFO - Training [52][  180/  391]   Loss 0.083905   Top1 97.083333   Top5 99.982639   BatchTime 0.126783   LR 0.001000
INFO - Training [52][  200/  391]   Loss 0.083516   Top1 97.117188   Top5 99.984375   BatchTime 0.126402   LR 0.001000
INFO - Training [52][  220/  391]   Loss 0.083743   Top1 97.105824   Top5 99.985795   BatchTime 0.126170   LR 0.001000
INFO - Training [52][  240/  391]   Loss 0.084216   Top1 97.096354   Top5 99.986979   BatchTime 0.125638   LR 0.001000
INFO - Training [52][  260/  391]   Loss 0.083833   Top1 97.112380   Top5 99.987981   BatchTime 0.122031   LR 0.001000
INFO - Training [52][  280/  391]   Loss 0.083190   Top1 97.120536   Top5 99.988839   BatchTime 0.119935   LR 0.001000
INFO - Training [52][  300/  391]   Loss 0.083193   Top1 97.096354   Top5 99.981771   BatchTime 0.117727   LR 0.001000
INFO - Training [52][  320/  391]   Loss 0.083277   Top1 97.116699   Top5 99.982910   BatchTime 0.116748   LR 0.001000
INFO - Training [52][  340/  391]   Loss 0.083156   Top1 97.130055   Top5 99.983915   BatchTime 0.117192   LR 0.001000
INFO - Training [52][  360/  391]   Loss 0.083283   Top1 97.109375   Top5 99.982639   BatchTime 0.117563   LR 0.001000
INFO - Training [52][  380/  391]   Loss 0.083619   Top1 97.099095   Top5 99.983553   BatchTime 0.117815   LR 0.001000
INFO - ==> Top1: 97.090    Top5: 99.984    Loss: 0.084
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [52][   20/   79]   Loss 0.366611   Top1 89.921875   Top5 99.648438   BatchTime 0.150228
INFO - Validation [52][   40/   79]   Loss 0.372902   Top1 90.000000   Top5 99.609375   BatchTime 0.106768
INFO - Validation [52][   60/   79]   Loss 0.370837   Top1 90.078125   Top5 99.609375   BatchTime 0.092729
INFO - ==> Top1: 90.090    Top5: 99.630    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.340   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  53
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [53][   20/  391]   Loss 0.083028   Top1 97.109375   Top5 99.960938   BatchTime 0.211694   LR 0.001000
INFO - Training [53][   40/  391]   Loss 0.073131   Top1 97.539062   Top5 99.980469   BatchTime 0.151786   LR 0.001000
INFO - Training [53][   60/  391]   Loss 0.076445   Top1 97.369792   Top5 99.973958   BatchTime 0.131263   LR 0.001000
INFO - Training [53][   80/  391]   Loss 0.077577   Top1 97.314453   Top5 99.970703   BatchTime 0.121614   LR 0.001000
INFO - Training [53][  100/  391]   Loss 0.080431   Top1 97.242188   Top5 99.976562   BatchTime 0.115716   LR 0.001000
INFO - Training [53][  120/  391]   Loss 0.080029   Top1 97.239583   Top5 99.980469   BatchTime 0.117025   LR 0.001000
INFO - Training [53][  140/  391]   Loss 0.082679   Top1 97.131696   Top5 99.983259   BatchTime 0.117884   LR 0.001000
INFO - Training [53][  160/  391]   Loss 0.084028   Top1 97.080078   Top5 99.985352   BatchTime 0.118540   LR 0.001000
INFO - Training [53][  180/  391]   Loss 0.083482   Top1 97.100694   Top5 99.982639   BatchTime 0.119135   LR 0.001000
INFO - Training [53][  200/  391]   Loss 0.082179   Top1 97.164062   Top5 99.984375   BatchTime 0.119540   LR 0.001000
INFO - Training [53][  220/  391]   Loss 0.081650   Top1 97.169744   Top5 99.985795   BatchTime 0.119931   LR 0.001000
INFO - Training [53][  240/  391]   Loss 0.081315   Top1 97.171224   Top5 99.986979   BatchTime 0.120222   LR 0.001000
INFO - Training [53][  260/  391]   Loss 0.081010   Top1 97.169471   Top5 99.987981   BatchTime 0.120377   LR 0.001000
INFO - Training [53][  280/  391]   Loss 0.080778   Top1 97.207031   Top5 99.986049   BatchTime 0.120541   LR 0.001000
INFO - Training [53][  300/  391]   Loss 0.081667   Top1 97.177083   Top5 99.984375   BatchTime 0.120015   LR 0.001000
INFO - Training [53][  320/  391]   Loss 0.081609   Top1 97.177734   Top5 99.985352   BatchTime 0.117548   LR 0.001000
INFO - Training [53][  340/  391]   Loss 0.081107   Top1 97.180607   Top5 99.983915   BatchTime 0.115855   LR 0.001000
INFO - Training [53][  360/  391]   Loss 0.080747   Top1 97.191840   Top5 99.984809   BatchTime 0.113867   LR 0.001000
INFO - Training [53][  380/  391]   Loss 0.081260   Top1 97.164885   Top5 99.985609   BatchTime 0.113598   LR 0.001000
INFO - ==> Top1: 97.164    Top5: 99.984    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [53][   20/   79]   Loss 0.364635   Top1 90.039062   Top5 99.648438   BatchTime 0.153942
INFO - Validation [53][   40/   79]   Loss 0.364740   Top1 90.429688   Top5 99.648438   BatchTime 0.108584
INFO - Validation [53][   60/   79]   Loss 0.364427   Top1 90.429688   Top5 99.661458   BatchTime 0.094018
INFO - ==> Top1: 90.380    Top5: 99.670    Loss: 0.363
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  54
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [54][   20/  391]   Loss 0.080588   Top1 96.992188   Top5 100.000000   BatchTime 0.212400   LR 0.001000
INFO - Training [54][   40/  391]   Loss 0.079865   Top1 97.148438   Top5 100.000000   BatchTime 0.167697   LR 0.001000
INFO - Training [54][   60/  391]   Loss 0.080120   Top1 97.148438   Top5 100.000000   BatchTime 0.152676   LR 0.001000
INFO - Training [54][   80/  391]   Loss 0.083412   Top1 97.089844   Top5 100.000000   BatchTime 0.145170   LR 0.001000
INFO - Training [54][  100/  391]   Loss 0.084680   Top1 97.023438   Top5 99.992188   BatchTime 0.133374   LR 0.001000
INFO - Training [54][  120/  391]   Loss 0.084088   Top1 97.063802   Top5 99.993490   BatchTime 0.126467   LR 0.001000
INFO - Training [54][  140/  391]   Loss 0.084985   Top1 97.008929   Top5 99.994420   BatchTime 0.120473   LR 0.001000
INFO - Training [54][  160/  391]   Loss 0.083445   Top1 97.060547   Top5 99.995117   BatchTime 0.117602   LR 0.001000
INFO - Training [54][  180/  391]   Loss 0.084168   Top1 97.009549   Top5 99.991319   BatchTime 0.118346   LR 0.001000
INFO - Training [54][  200/  391]   Loss 0.084073   Top1 97.035156   Top5 99.992188   BatchTime 0.118871   LR 0.001000
INFO - Training [54][  220/  391]   Loss 0.082989   Top1 97.066761   Top5 99.992898   BatchTime 0.119336   LR 0.001000
INFO - Training [54][  240/  391]   Loss 0.082351   Top1 97.145182   Top5 99.990234   BatchTime 0.119645   LR 0.001000
INFO - Training [54][  260/  391]   Loss 0.081722   Top1 97.184495   Top5 99.990986   BatchTime 0.119888   LR 0.001000
INFO - Training [54][  280/  391]   Loss 0.082040   Top1 97.179129   Top5 99.986049   BatchTime 0.120119   LR 0.001000
INFO - Training [54][  300/  391]   Loss 0.081810   Top1 97.197917   Top5 99.984375   BatchTime 0.120354   LR 0.001000
INFO - Training [54][  320/  391]   Loss 0.081598   Top1 97.192383   Top5 99.982910   BatchTime 0.120490   LR 0.001000
INFO - Training [54][  340/  391]   Loss 0.081478   Top1 97.189798   Top5 99.983915   BatchTime 0.120607   LR 0.001000
INFO - Training [54][  360/  391]   Loss 0.081773   Top1 97.159288   Top5 99.984809   BatchTime 0.119684   LR 0.001000
INFO - Training [54][  380/  391]   Loss 0.082160   Top1 97.148438   Top5 99.983553   BatchTime 0.117554   LR 0.001000
INFO - ==> Top1: 97.160    Top5: 99.984    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [54][   20/   79]   Loss 0.373476   Top1 89.765625   Top5 99.609375   BatchTime 0.124093
INFO - Validation [54][   40/   79]   Loss 0.369234   Top1 90.039062   Top5 99.589844   BatchTime 0.089550
INFO - Validation [54][   60/   79]   Loss 0.367942   Top1 90.221354   Top5 99.609375   BatchTime 0.080545
INFO - ==> Top1: 90.200    Top5: 99.640    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  55
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [55][   20/  391]   Loss 0.079677   Top1 97.031250   Top5 100.000000   BatchTime 0.214921   LR 0.001000
INFO - Training [55][   40/  391]   Loss 0.078681   Top1 97.187500   Top5 100.000000   BatchTime 0.169171   LR 0.001000
INFO - Training [55][   60/  391]   Loss 0.076761   Top1 97.278646   Top5 100.000000   BatchTime 0.153823   LR 0.001000
INFO - Training [55][   80/  391]   Loss 0.078777   Top1 97.265625   Top5 99.990234   BatchTime 0.146273   LR 0.001000
INFO - Training [55][  100/  391]   Loss 0.078726   Top1 97.289062   Top5 99.992188   BatchTime 0.141531   LR 0.001000
INFO - Training [55][  120/  391]   Loss 0.078184   Top1 97.298177   Top5 99.986979   BatchTime 0.138482   LR 0.001000
INFO - Training [55][  140/  391]   Loss 0.078643   Top1 97.293527   Top5 99.977679   BatchTime 0.136232   LR 0.001000
INFO - Training [55][  160/  391]   Loss 0.077429   Top1 97.324219   Top5 99.980469   BatchTime 0.129925   LR 0.001000
INFO - Training [55][  180/  391]   Loss 0.077751   Top1 97.291667   Top5 99.982639   BatchTime 0.125483   LR 0.001000
INFO - Training [55][  200/  391]   Loss 0.077571   Top1 97.292969   Top5 99.984375   BatchTime 0.122159   LR 0.001000
INFO - Training [55][  220/  391]   Loss 0.078423   Top1 97.262074   Top5 99.982244   BatchTime 0.119388   LR 0.001000
INFO - Training [55][  240/  391]   Loss 0.078776   Top1 97.268880   Top5 99.983724   BatchTime 0.119758   LR 0.001000
INFO - Training [55][  260/  391]   Loss 0.079189   Top1 97.277644   Top5 99.981971   BatchTime 0.120120   LR 0.001000
INFO - Training [55][  280/  391]   Loss 0.079474   Top1 97.254464   Top5 99.980469   BatchTime 0.120332   LR 0.001000
INFO - Training [55][  300/  391]   Loss 0.079323   Top1 97.260417   Top5 99.981771   BatchTime 0.120532   LR 0.001000
INFO - Training [55][  320/  391]   Loss 0.079834   Top1 97.238770   Top5 99.982910   BatchTime 0.120752   LR 0.001000
INFO - Training [55][  340/  391]   Loss 0.080750   Top1 97.198989   Top5 99.979320   BatchTime 0.120890   LR 0.001000
INFO - Training [55][  360/  391]   Loss 0.081208   Top1 97.176649   Top5 99.980469   BatchTime 0.121055   LR 0.001000
INFO - Training [55][  380/  391]   Loss 0.080936   Top1 97.189556   Top5 99.979441   BatchTime 0.121147   LR 0.001000
INFO - ==> Top1: 97.210    Top5: 99.980    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [55][   20/   79]   Loss 0.366910   Top1 89.921875   Top5 99.726562   BatchTime 0.120767
INFO - Validation [55][   40/   79]   Loss 0.364559   Top1 90.039062   Top5 99.628906   BatchTime 0.073649
INFO - Validation [55][   60/   79]   Loss 0.366381   Top1 90.039062   Top5 99.635417   BatchTime 0.057870
INFO - ==> Top1: 90.010    Top5: 99.670    Loss: 0.363
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  56
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [56][   20/  391]   Loss 0.086385   Top1 96.757812   Top5 100.000000   BatchTime 0.202011   LR 0.001000
INFO - Training [56][   40/  391]   Loss 0.085340   Top1 96.894531   Top5 100.000000   BatchTime 0.163664   LR 0.001000
INFO - Training [56][   60/  391]   Loss 0.080652   Top1 97.252604   Top5 99.986979   BatchTime 0.150367   LR 0.001000
INFO - Training [56][   80/  391]   Loss 0.079576   Top1 97.333984   Top5 99.990234   BatchTime 0.143734   LR 0.001000
INFO - Training [56][  100/  391]   Loss 0.081230   Top1 97.250000   Top5 99.992188   BatchTime 0.139675   LR 0.001000
INFO - Training [56][  120/  391]   Loss 0.078467   Top1 97.343750   Top5 99.993490   BatchTime 0.137138   LR 0.001000
INFO - Training [56][  140/  391]   Loss 0.078660   Top1 97.327009   Top5 99.994420   BatchTime 0.135190   LR 0.001000
INFO - Training [56][  160/  391]   Loss 0.080266   Top1 97.285156   Top5 99.995117   BatchTime 0.133744   LR 0.001000
INFO - Training [56][  180/  391]   Loss 0.079767   Top1 97.296007   Top5 99.995660   BatchTime 0.133040   LR 0.001000
INFO - Training [56][  200/  391]   Loss 0.079406   Top1 97.277344   Top5 99.996094   BatchTime 0.132054   LR 0.001000
INFO - Training [56][  220/  391]   Loss 0.079456   Top1 97.254972   Top5 99.992898   BatchTime 0.128994   LR 0.001000
INFO - Training [56][  240/  391]   Loss 0.079768   Top1 97.223307   Top5 99.993490   BatchTime 0.125427   LR 0.001000
INFO - Training [56][  260/  391]   Loss 0.079942   Top1 97.175481   Top5 99.993990   BatchTime 0.123038   LR 0.001000
INFO - Training [56][  280/  391]   Loss 0.080691   Top1 97.148438   Top5 99.991629   BatchTime 0.119698   LR 0.001000
INFO - Training [56][  300/  391]   Loss 0.079933   Top1 97.166667   Top5 99.989583   BatchTime 0.120086   LR 0.001000
INFO - Training [56][  320/  391]   Loss 0.079972   Top1 97.160645   Top5 99.987793   BatchTime 0.120332   LR 0.001000
INFO - Training [56][  340/  391]   Loss 0.080036   Top1 97.141544   Top5 99.988511   BatchTime 0.120463   LR 0.001000
INFO - Training [56][  360/  391]   Loss 0.079985   Top1 97.135417   Top5 99.986979   BatchTime 0.120629   LR 0.001000
INFO - Training [56][  380/  391]   Loss 0.080183   Top1 97.119655   Top5 99.987664   BatchTime 0.120739   LR 0.001000
INFO - ==> Top1: 97.100    Top5: 99.986    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [56][   20/   79]   Loss 0.356798   Top1 90.429688   Top5 99.648438   BatchTime 0.154269
INFO - Validation [56][   40/   79]   Loss 0.361063   Top1 90.371094   Top5 99.589844   BatchTime 0.109949
INFO - Validation [56][   60/   79]   Loss 0.362246   Top1 90.377604   Top5 99.583333   BatchTime 0.095730
INFO - ==> Top1: 90.310    Top5: 99.620    Loss: 0.363
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  57
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [57][   20/  391]   Loss 0.075534   Top1 97.343750   Top5 100.000000   BatchTime 0.186455   LR 0.001000
INFO - Training [57][   40/  391]   Loss 0.077845   Top1 97.187500   Top5 100.000000   BatchTime 0.141293   LR 0.001000
INFO - Training [57][   60/  391]   Loss 0.074886   Top1 97.369792   Top5 100.000000   BatchTime 0.119005   LR 0.001000
INFO - Training [57][   80/  391]   Loss 0.075977   Top1 97.294922   Top5 100.000000   BatchTime 0.120663   LR 0.001000
INFO - Training [57][  100/  391]   Loss 0.076447   Top1 97.195312   Top5 100.000000   BatchTime 0.121340   LR 0.001000
INFO - Training [57][  120/  391]   Loss 0.078069   Top1 97.115885   Top5 100.000000   BatchTime 0.121729   LR 0.001000
INFO - Training [57][  140/  391]   Loss 0.078640   Top1 97.087054   Top5 99.994420   BatchTime 0.122146   LR 0.001000
INFO - Training [57][  160/  391]   Loss 0.079243   Top1 97.080078   Top5 99.995117   BatchTime 0.122340   LR 0.001000
INFO - Training [57][  180/  391]   Loss 0.080465   Top1 97.026910   Top5 99.995660   BatchTime 0.122283   LR 0.001000
INFO - Training [57][  200/  391]   Loss 0.080890   Top1 97.007812   Top5 99.992188   BatchTime 0.122425   LR 0.001000
INFO - Training [57][  220/  391]   Loss 0.079984   Top1 97.056108   Top5 99.992898   BatchTime 0.122490   LR 0.001000
INFO - Training [57][  240/  391]   Loss 0.081696   Top1 97.014974   Top5 99.993490   BatchTime 0.122461   LR 0.001000
INFO - Training [57][  260/  391]   Loss 0.080731   Top1 97.061298   Top5 99.993990   BatchTime 0.122427   LR 0.001000
INFO - Training [57][  280/  391]   Loss 0.080624   Top1 97.059152   Top5 99.994420   BatchTime 0.119762   LR 0.001000
INFO - Training [57][  300/  391]   Loss 0.080829   Top1 97.046875   Top5 99.992188   BatchTime 0.117861   LR 0.001000
INFO - Training [57][  320/  391]   Loss 0.080303   Top1 97.080078   Top5 99.990234   BatchTime 0.116058   LR 0.001000
INFO - Training [57][  340/  391]   Loss 0.079995   Top1 97.102482   Top5 99.990809   BatchTime 0.115004   LR 0.001000
INFO - Training [57][  360/  391]   Loss 0.079935   Top1 97.102865   Top5 99.991319   BatchTime 0.115486   LR 0.001000
INFO - Training [57][  380/  391]   Loss 0.080196   Top1 97.092928   Top5 99.989720   BatchTime 0.115855   LR 0.001000
INFO - ==> Top1: 97.074    Top5: 99.990    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [57][   20/   79]   Loss 0.369119   Top1 89.882812   Top5 99.687500   BatchTime 0.151996
INFO - Validation [57][   40/   79]   Loss 0.369860   Top1 90.019531   Top5 99.589844   BatchTime 0.108727
INFO - Validation [57][   60/   79]   Loss 0.371471   Top1 90.039062   Top5 99.609375   BatchTime 0.093901
INFO - ==> Top1: 90.070    Top5: 99.630    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  58
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [58][   20/  391]   Loss 0.077827   Top1 97.031250   Top5 100.000000   BatchTime 0.214238   LR 0.001000
INFO - Training [58][   40/  391]   Loss 0.076910   Top1 97.148438   Top5 100.000000   BatchTime 0.168608   LR 0.001000
INFO - Training [58][   60/  391]   Loss 0.075661   Top1 97.317708   Top5 100.000000   BatchTime 0.141453   LR 0.001000
INFO - Training [58][   80/  391]   Loss 0.075841   Top1 97.294922   Top5 100.000000   BatchTime 0.129135   LR 0.001000
INFO - Training [58][  100/  391]   Loss 0.075193   Top1 97.296875   Top5 100.000000   BatchTime 0.121391   LR 0.001000
INFO - Training [58][  120/  391]   Loss 0.077596   Top1 97.226562   Top5 100.000000   BatchTime 0.117768   LR 0.001000
INFO - Training [58][  140/  391]   Loss 0.077833   Top1 97.226562   Top5 99.994420   BatchTime 0.118664   LR 0.001000
INFO - Training [58][  160/  391]   Loss 0.078058   Top1 97.197266   Top5 99.995117   BatchTime 0.119268   LR 0.001000
INFO - Training [58][  180/  391]   Loss 0.076751   Top1 97.252604   Top5 99.995660   BatchTime 0.119731   LR 0.001000
INFO - Training [58][  200/  391]   Loss 0.075432   Top1 97.312500   Top5 99.996094   BatchTime 0.120026   LR 0.001000
INFO - Training [58][  220/  391]   Loss 0.075300   Top1 97.333097   Top5 99.992898   BatchTime 0.120444   LR 0.001000
INFO - Training [58][  240/  391]   Loss 0.074866   Top1 97.347005   Top5 99.993490   BatchTime 0.121019   LR 0.001000
INFO - Training [58][  260/  391]   Loss 0.074972   Top1 97.352764   Top5 99.993990   BatchTime 0.121191   LR 0.001000
INFO - Training [58][  280/  391]   Loss 0.075264   Top1 97.313058   Top5 99.994420   BatchTime 0.121318   LR 0.001000
INFO - Training [58][  300/  391]   Loss 0.074780   Top1 97.338542   Top5 99.994792   BatchTime 0.121422   LR 0.001000
INFO - Training [58][  320/  391]   Loss 0.075465   Top1 97.316895   Top5 99.995117   BatchTime 0.120476   LR 0.001000
INFO - Training [58][  340/  391]   Loss 0.075558   Top1 97.323070   Top5 99.993107   BatchTime 0.118214   LR 0.001000
INFO - Training [58][  360/  391]   Loss 0.075645   Top1 97.326389   Top5 99.993490   BatchTime 0.116430   LR 0.001000
INFO - Training [58][  380/  391]   Loss 0.076635   Top1 97.296464   Top5 99.989720   BatchTime 0.114433   LR 0.001000
INFO - ==> Top1: 97.290    Top5: 99.990    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [58][   20/   79]   Loss 0.370688   Top1 90.039062   Top5 99.687500   BatchTime 0.151238
INFO - Validation [58][   40/   79]   Loss 0.369292   Top1 90.019531   Top5 99.707031   BatchTime 0.107632
INFO - Validation [58][   60/   79]   Loss 0.368196   Top1 90.260417   Top5 99.687500   BatchTime 0.092564
INFO - ==> Top1: 90.180    Top5: 99.710    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  59
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [59][   20/  391]   Loss 0.066471   Top1 97.500000   Top5 100.000000   BatchTime 0.216530   LR 0.001000
INFO - Training [59][   40/  391]   Loss 0.080351   Top1 97.109375   Top5 99.960938   BatchTime 0.169658   LR 0.001000
INFO - Training [59][   60/  391]   Loss 0.078638   Top1 97.122396   Top5 99.973958   BatchTime 0.154229   LR 0.001000
INFO - Training [59][   80/  391]   Loss 0.076976   Top1 97.207031   Top5 99.980469   BatchTime 0.146330   LR 0.001000
INFO - Training [59][  100/  391]   Loss 0.075852   Top1 97.226562   Top5 99.984375   BatchTime 0.141737   LR 0.001000
INFO - Training [59][  120/  391]   Loss 0.073954   Top1 97.343750   Top5 99.986979   BatchTime 0.130842   LR 0.001000
INFO - Training [59][  140/  391]   Loss 0.072796   Top1 97.405134   Top5 99.983259   BatchTime 0.125168   LR 0.001000
INFO - Training [59][  160/  391]   Loss 0.073998   Top1 97.358398   Top5 99.985352   BatchTime 0.120592   LR 0.001000
INFO - Training [59][  180/  391]   Loss 0.075214   Top1 97.282986   Top5 99.982639   BatchTime 0.118895   LR 0.001000
INFO - Training [59][  200/  391]   Loss 0.075857   Top1 97.253906   Top5 99.984375   BatchTime 0.119432   LR 0.001000
INFO - Training [59][  220/  391]   Loss 0.076366   Top1 97.251420   Top5 99.985795   BatchTime 0.119738   LR 0.001000
INFO - Training [59][  240/  391]   Loss 0.076308   Top1 97.255859   Top5 99.986979   BatchTime 0.120086   LR 0.001000
INFO - Training [59][  260/  391]   Loss 0.076907   Top1 97.238582   Top5 99.984976   BatchTime 0.120341   LR 0.001000
INFO - Training [59][  280/  391]   Loss 0.077241   Top1 97.246094   Top5 99.986049   BatchTime 0.120571   LR 0.001000
INFO - Training [59][  300/  391]   Loss 0.077611   Top1 97.226562   Top5 99.986979   BatchTime 0.120737   LR 0.001000
INFO - Training [59][  320/  391]   Loss 0.078049   Top1 97.219238   Top5 99.987793   BatchTime 0.120867   LR 0.001000
INFO - Training [59][  340/  391]   Loss 0.078993   Top1 97.192096   Top5 99.988511   BatchTime 0.120956   LR 0.001000
INFO - Training [59][  360/  391]   Loss 0.079628   Top1 97.167969   Top5 99.989149   BatchTime 0.120918   LR 0.001000
INFO - Training [59][  380/  391]   Loss 0.079273   Top1 97.193668   Top5 99.987664   BatchTime 0.119800   LR 0.001000
INFO - ==> Top1: 97.204    Top5: 99.988    Loss: 0.079
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [59][   20/   79]   Loss 0.364848   Top1 89.726562   Top5 99.609375   BatchTime 0.125477
INFO - Validation [59][   40/   79]   Loss 0.365135   Top1 90.097656   Top5 99.570312   BatchTime 0.075955
INFO - Validation [59][   60/   79]   Loss 0.366719   Top1 90.169271   Top5 99.596354   BatchTime 0.059355
INFO - ==> Top1: 90.140    Top5: 99.630    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  60
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [60][   20/  391]   Loss 0.074524   Top1 97.187500   Top5 99.960938   BatchTime 0.214886   LR 0.000100
INFO - Training [60][   40/  391]   Loss 0.078994   Top1 97.207031   Top5 99.980469   BatchTime 0.169387   LR 0.000100
INFO - Training [60][   60/  391]   Loss 0.077680   Top1 97.239583   Top5 99.986979   BatchTime 0.154161   LR 0.000100
INFO - Training [60][   80/  391]   Loss 0.074144   Top1 97.333984   Top5 99.990234   BatchTime 0.146517   LR 0.000100
INFO - Training [60][  100/  391]   Loss 0.075882   Top1 97.218750   Top5 99.984375   BatchTime 0.141875   LR 0.000100
INFO - Training [60][  120/  391]   Loss 0.074721   Top1 97.291667   Top5 99.986979   BatchTime 0.138778   LR 0.000100
INFO - Training [60][  140/  391]   Loss 0.074645   Top1 97.276786   Top5 99.988839   BatchTime 0.136646   LR 0.000100
INFO - Training [60][  160/  391]   Loss 0.076731   Top1 97.192383   Top5 99.990234   BatchTime 0.135016   LR 0.000100
INFO - Training [60][  180/  391]   Loss 0.075606   Top1 97.239583   Top5 99.991319   BatchTime 0.130984   LR 0.000100
INFO - Training [60][  200/  391]   Loss 0.076164   Top1 97.226562   Top5 99.992188   BatchTime 0.126473   LR 0.000100
INFO - Training [60][  220/  391]   Loss 0.076283   Top1 97.247869   Top5 99.992898   BatchTime 0.123531   LR 0.000100
INFO - Training [60][  240/  391]   Loss 0.075516   Top1 97.246094   Top5 99.993490   BatchTime 0.119562   LR 0.000100
INFO - Training [60][  260/  391]   Loss 0.076902   Top1 97.205529   Top5 99.990986   BatchTime 0.119859   LR 0.000100
INFO - Training [60][  280/  391]   Loss 0.076546   Top1 97.237723   Top5 99.988839   BatchTime 0.120413   LR 0.000100
INFO - Training [60][  300/  391]   Loss 0.076433   Top1 97.234375   Top5 99.989583   BatchTime 0.120628   LR 0.000100
INFO - Training [60][  320/  391]   Loss 0.076688   Top1 97.221680   Top5 99.987793   BatchTime 0.120806   LR 0.000100
INFO - Training [60][  340/  391]   Loss 0.076286   Top1 97.251838   Top5 99.988511   BatchTime 0.120931   LR 0.000100
INFO - Training [60][  360/  391]   Loss 0.076420   Top1 97.254774   Top5 99.986979   BatchTime 0.121018   LR 0.000100
INFO - Training [60][  380/  391]   Loss 0.075802   Top1 97.284128   Top5 99.987664   BatchTime 0.121129   LR 0.000100
INFO - ==> Top1: 97.292    Top5: 99.988    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [60][   20/   79]   Loss 0.366869   Top1 90.117188   Top5 99.687500   BatchTime 0.150177
INFO - Validation [60][   40/   79]   Loss 0.364181   Top1 90.312500   Top5 99.648438   BatchTime 0.107470
INFO - Validation [60][   60/   79]   Loss 0.364542   Top1 90.429688   Top5 99.674479   BatchTime 0.081945
INFO - ==> Top1: 90.330    Top5: 99.680    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.340   Top5: 99.710] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  61
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [61][   20/  391]   Loss 0.079834   Top1 97.226562   Top5 100.000000   BatchTime 0.176736   LR 0.000100
INFO - Training [61][   40/  391]   Loss 0.081425   Top1 97.226562   Top5 100.000000   BatchTime 0.142815   LR 0.000100
INFO - Training [61][   60/  391]   Loss 0.079346   Top1 97.252604   Top5 99.986979   BatchTime 0.136450   LR 0.000100
INFO - Training [61][   80/  391]   Loss 0.077441   Top1 97.333984   Top5 99.990234   BatchTime 0.133244   LR 0.000100
INFO - Training [61][  100/  391]   Loss 0.076200   Top1 97.367188   Top5 99.992188   BatchTime 0.131234   LR 0.000100
INFO - Training [61][  120/  391]   Loss 0.077267   Top1 97.356771   Top5 99.980469   BatchTime 0.129865   LR 0.000100
INFO - Training [61][  140/  391]   Loss 0.077765   Top1 97.287946   Top5 99.983259   BatchTime 0.129014   LR 0.000100
INFO - Training [61][  160/  391]   Loss 0.076643   Top1 97.304688   Top5 99.985352   BatchTime 0.128403   LR 0.000100
INFO - Training [61][  180/  391]   Loss 0.077061   Top1 97.304688   Top5 99.986979   BatchTime 0.127769   LR 0.000100
INFO - Training [61][  200/  391]   Loss 0.077800   Top1 97.265625   Top5 99.984375   BatchTime 0.127306   LR 0.000100
INFO - Training [61][  220/  391]   Loss 0.078077   Top1 97.247869   Top5 99.982244   BatchTime 0.126895   LR 0.000100
INFO - Training [61][  240/  391]   Loss 0.076981   Top1 97.291667   Top5 99.983724   BatchTime 0.124596   LR 0.000100
INFO - Training [61][  260/  391]   Loss 0.076837   Top1 97.283654   Top5 99.984976   BatchTime 0.121740   LR 0.000100
INFO - Training [61][  280/  391]   Loss 0.076586   Top1 97.273996   Top5 99.986049   BatchTime 0.119780   LR 0.000100
INFO - Training [61][  300/  391]   Loss 0.075965   Top1 97.296875   Top5 99.986979   BatchTime 0.117034   LR 0.000100
INFO - Training [61][  320/  391]   Loss 0.075721   Top1 97.312012   Top5 99.987793   BatchTime 0.117550   LR 0.000100
INFO - Training [61][  340/  391]   Loss 0.076207   Top1 97.295496   Top5 99.986213   BatchTime 0.117908   LR 0.000100
INFO - Training [61][  360/  391]   Loss 0.075960   Top1 97.306858   Top5 99.986979   BatchTime 0.118198   LR 0.000100
INFO - Training [61][  380/  391]   Loss 0.075862   Top1 97.312911   Top5 99.987664   BatchTime 0.118466   LR 0.000100
INFO - ==> Top1: 97.292    Top5: 99.988    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [61][   20/   79]   Loss 0.377340   Top1 90.078125   Top5 99.765625   BatchTime 0.152937
INFO - Validation [61][   40/   79]   Loss 0.370227   Top1 90.234375   Top5 99.667969   BatchTime 0.104781
INFO - Validation [61][   60/   79]   Loss 0.367857   Top1 90.455729   Top5 99.648438   BatchTime 0.091427
INFO - ==> Top1: 90.360    Top5: 99.680    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 90.360   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  62
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [62][   20/  391]   Loss 0.083601   Top1 97.187500   Top5 100.000000   BatchTime 0.205803   LR 0.000100
INFO - Training [62][   40/  391]   Loss 0.078893   Top1 97.089844   Top5 100.000000   BatchTime 0.143034   LR 0.000100
INFO - Training [62][   60/  391]   Loss 0.076946   Top1 97.265625   Top5 100.000000   BatchTime 0.125958   LR 0.000100
INFO - Training [62][   80/  391]   Loss 0.078615   Top1 97.226562   Top5 99.990234   BatchTime 0.115266   LR 0.000100
INFO - Training [62][  100/  391]   Loss 0.080371   Top1 97.078125   Top5 99.992188   BatchTime 0.114777   LR 0.000100
INFO - Training [62][  120/  391]   Loss 0.079955   Top1 97.096354   Top5 99.993490   BatchTime 0.116198   LR 0.000100
INFO - Training [62][  140/  391]   Loss 0.079849   Top1 97.131696   Top5 99.994420   BatchTime 0.117283   LR 0.000100
INFO - Training [62][  160/  391]   Loss 0.078415   Top1 97.167969   Top5 99.995117   BatchTime 0.118213   LR 0.000100
INFO - Training [62][  180/  391]   Loss 0.078671   Top1 97.191840   Top5 99.995660   BatchTime 0.118869   LR 0.000100
INFO - Training [62][  200/  391]   Loss 0.079712   Top1 97.140625   Top5 99.996094   BatchTime 0.119331   LR 0.000100
INFO - Training [62][  220/  391]   Loss 0.078794   Top1 97.208807   Top5 99.996449   BatchTime 0.119650   LR 0.000100
INFO - Training [62][  240/  391]   Loss 0.078905   Top1 97.220052   Top5 99.993490   BatchTime 0.119893   LR 0.000100
INFO - Training [62][  260/  391]   Loss 0.078119   Top1 97.265625   Top5 99.990986   BatchTime 0.120131   LR 0.000100
INFO - Training [62][  280/  391]   Loss 0.077475   Top1 97.268415   Top5 99.991629   BatchTime 0.120310   LR 0.000100
INFO - Training [62][  300/  391]   Loss 0.077461   Top1 97.270833   Top5 99.992188   BatchTime 0.118535   LR 0.000100
INFO - Training [62][  320/  391]   Loss 0.077052   Top1 97.277832   Top5 99.992676   BatchTime 0.116703   LR 0.000100
INFO - Training [62][  340/  391]   Loss 0.076773   Top1 97.293199   Top5 99.990809   BatchTime 0.115198   LR 0.000100
INFO - Training [62][  360/  391]   Loss 0.076339   Top1 97.324219   Top5 99.991319   BatchTime 0.113930   LR 0.000100
INFO - Training [62][  380/  391]   Loss 0.075686   Top1 97.358141   Top5 99.991776   BatchTime 0.114318   LR 0.000100
INFO - ==> Top1: 97.364    Top5: 99.988    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [62][   20/   79]   Loss 0.367436   Top1 90.195312   Top5 99.648438   BatchTime 0.151706
INFO - Validation [62][   40/   79]   Loss 0.364089   Top1 90.292969   Top5 99.589844   BatchTime 0.108354
INFO - Validation [62][   60/   79]   Loss 0.365051   Top1 90.455729   Top5 99.609375   BatchTime 0.093513
INFO - ==> Top1: 90.300    Top5: 99.630    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [61][Top1: 90.360   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 90.350   Top5: 99.670] Sparsity : 0.877
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  63
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [63][   20/  391]   Loss 0.068330   Top1 97.695312   Top5 100.000000   BatchTime 0.212865   LR 0.000100
INFO - Training [63][   40/  391]   Loss 0.071333   Top1 97.597656   Top5 99.980469   BatchTime 0.167796   LR 0.000100
INFO - Training [63][   60/  391]   Loss 0.075845   Top1 97.460938   Top5 99.973958   BatchTime 0.152826   LR 0.000100
INFO - Training [63][   80/  391]   Loss 0.076643   Top1 97.353516   Top5 99.980469   BatchTime 0.139835   LR 0.000100
INFO - Training [63][  100/  391]   Loss 0.075724   Top1 97.320312   Top5 99.984375   BatchTime 0.128859   LR 0.000100
INFO - Training [63][  120/  391]   Loss 0.074604   Top1 97.389323   Top5 99.986979   BatchTime 0.123308   LR 0.000100
INFO - Training [63][  140/  391]   Loss 0.074167   Top1 97.377232   Top5 99.988839   BatchTime 0.116681   LR 0.000100
INFO - Training [63][  160/  391]   Loss 0.073307   Top1 97.436523   Top5 99.990234   BatchTime 0.118063   LR 0.000100
INFO - Training [63][  180/  391]   Loss 0.073274   Top1 97.434896   Top5 99.986979   BatchTime 0.118621   LR 0.000100
INFO - Training [63][  200/  391]   Loss 0.074449   Top1 97.359375   Top5 99.988281   BatchTime 0.119212   LR 0.000100
INFO - Training [63][  220/  391]   Loss 0.074834   Top1 97.336648   Top5 99.989347   BatchTime 0.119624   LR 0.000100
INFO - Training [63][  240/  391]   Loss 0.074291   Top1 97.360026   Top5 99.990234   BatchTime 0.119949   LR 0.000100
INFO - Training [63][  260/  391]   Loss 0.074072   Top1 97.370793   Top5 99.987981   BatchTime 0.120260   LR 0.000100
INFO - Training [63][  280/  391]   Loss 0.074716   Top1 97.338170   Top5 99.988839   BatchTime 0.120476   LR 0.000100
INFO - Training [63][  300/  391]   Loss 0.074003   Top1 97.375000   Top5 99.989583   BatchTime 0.120605   LR 0.000100
INFO - Training [63][  320/  391]   Loss 0.073483   Top1 97.399902   Top5 99.990234   BatchTime 0.120760   LR 0.000100
INFO - Training [63][  340/  391]   Loss 0.074249   Top1 97.373621   Top5 99.990809   BatchTime 0.120824   LR 0.000100
INFO - Training [63][  360/  391]   Loss 0.074238   Top1 97.380642   Top5 99.989149   BatchTime 0.118059   LR 0.000100
INFO - Training [63][  380/  391]   Loss 0.074642   Top1 97.378701   Top5 99.989720   BatchTime 0.116555   LR 0.000100
INFO - ==> Top1: 97.378    Top5: 99.988    Loss: 0.075
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [63][   20/   79]   Loss 0.371427   Top1 90.390625   Top5 99.648438   BatchTime 0.167798
INFO - Validation [63][   40/   79]   Loss 0.367565   Top1 90.410156   Top5 99.648438   BatchTime 0.116606
INFO - Validation [63][   60/   79]   Loss 0.365488   Top1 90.559896   Top5 99.648438   BatchTime 0.099231
INFO - ==> Top1: 90.490    Top5: 99.680    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.360   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  64
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [64][   20/  391]   Loss 0.074602   Top1 97.460938   Top5 99.960938   BatchTime 0.214900   LR 0.000100
INFO - Training [64][   40/  391]   Loss 0.075274   Top1 97.265625   Top5 99.980469   BatchTime 0.169416   LR 0.000100
INFO - Training [64][   60/  391]   Loss 0.074346   Top1 97.291667   Top5 99.986979   BatchTime 0.154045   LR 0.000100
INFO - Training [64][   80/  391]   Loss 0.074908   Top1 97.236328   Top5 99.980469   BatchTime 0.146564   LR 0.000100
INFO - Training [64][  100/  391]   Loss 0.073116   Top1 97.328125   Top5 99.984375   BatchTime 0.140647   LR 0.000100
INFO - Training [64][  120/  391]   Loss 0.073776   Top1 97.317708   Top5 99.980469   BatchTime 0.137715   LR 0.000100
INFO - Training [64][  140/  391]   Loss 0.074055   Top1 97.287946   Top5 99.983259   BatchTime 0.130768   LR 0.000100
INFO - Training [64][  160/  391]   Loss 0.074929   Top1 97.231445   Top5 99.985352   BatchTime 0.125787   LR 0.000100
INFO - Training [64][  180/  391]   Loss 0.075223   Top1 97.226562   Top5 99.986979   BatchTime 0.121684   LR 0.000100
INFO - Training [64][  200/  391]   Loss 0.074862   Top1 97.253906   Top5 99.988281   BatchTime 0.117569   LR 0.000100
INFO - Training [64][  220/  391]   Loss 0.075373   Top1 97.233665   Top5 99.989347   BatchTime 0.118832   LR 0.000100
INFO - Training [64][  240/  391]   Loss 0.076366   Top1 97.233073   Top5 99.983724   BatchTime 0.119172   LR 0.000100
INFO - Training [64][  260/  391]   Loss 0.077293   Top1 97.190505   Top5 99.984976   BatchTime 0.119473   LR 0.000100
INFO - Training [64][  280/  391]   Loss 0.076729   Top1 97.215402   Top5 99.983259   BatchTime 0.119831   LR 0.000100
INFO - Training [64][  300/  391]   Loss 0.077072   Top1 97.210938   Top5 99.981771   BatchTime 0.120190   LR 0.000100
INFO - Training [64][  320/  391]   Loss 0.077466   Top1 97.207031   Top5 99.982910   BatchTime 0.120357   LR 0.000100
INFO - Training [64][  340/  391]   Loss 0.077460   Top1 97.221967   Top5 99.979320   BatchTime 0.120481   LR 0.000100
INFO - Training [64][  360/  391]   Loss 0.076991   Top1 97.254774   Top5 99.980469   BatchTime 0.120613   LR 0.000100
INFO - Training [64][  380/  391]   Loss 0.077387   Top1 97.236842   Top5 99.981497   BatchTime 0.120693   LR 0.000100
INFO - ==> Top1: 97.242    Top5: 99.982    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [64][   20/   79]   Loss 0.362082   Top1 89.726562   Top5 99.687500   BatchTime 0.132819
INFO - Validation [64][   40/   79]   Loss 0.365371   Top1 89.921875   Top5 99.628906   BatchTime 0.087653
INFO - Validation [64][   60/   79]   Loss 0.364521   Top1 90.104167   Top5 99.648438   BatchTime 0.071372
INFO - ==> Top1: 89.990    Top5: 99.670    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.360   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  65
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [65][   20/  391]   Loss 0.063634   Top1 97.968750   Top5 99.960938   BatchTime 0.225127   LR 0.000100
INFO - Training [65][   40/  391]   Loss 0.062766   Top1 97.968750   Top5 99.980469   BatchTime 0.174377   LR 0.000100
INFO - Training [65][   60/  391]   Loss 0.064626   Top1 97.786458   Top5 99.986979   BatchTime 0.157632   LR 0.000100
INFO - Training [65][   80/  391]   Loss 0.068066   Top1 97.666016   Top5 99.990234   BatchTime 0.149115   LR 0.000100
INFO - Training [65][  100/  391]   Loss 0.070988   Top1 97.609375   Top5 99.992188   BatchTime 0.144025   LR 0.000100
INFO - Training [65][  120/  391]   Loss 0.070448   Top1 97.630208   Top5 99.993490   BatchTime 0.140621   LR 0.000100
INFO - Training [65][  140/  391]   Loss 0.070671   Top1 97.617188   Top5 99.994420   BatchTime 0.138195   LR 0.000100
INFO - Training [65][  160/  391]   Loss 0.071316   Top1 97.607422   Top5 99.995117   BatchTime 0.136345   LR 0.000100
INFO - Training [65][  180/  391]   Loss 0.071789   Top1 97.560764   Top5 99.995660   BatchTime 0.134821   LR 0.000100
INFO - Training [65][  200/  391]   Loss 0.071794   Top1 97.550781   Top5 99.996094   BatchTime 0.131563   LR 0.000100
INFO - Training [65][  220/  391]   Loss 0.072428   Top1 97.531960   Top5 99.996449   BatchTime 0.127097   LR 0.000100
INFO - Training [65][  240/  391]   Loss 0.071645   Top1 97.548828   Top5 99.990234   BatchTime 0.124014   LR 0.000100
INFO - Training [65][  260/  391]   Loss 0.071887   Top1 97.530048   Top5 99.990986   BatchTime 0.120611   LR 0.000100
INFO - Training [65][  280/  391]   Loss 0.071686   Top1 97.547433   Top5 99.991629   BatchTime 0.120937   LR 0.000100
INFO - Training [65][  300/  391]   Loss 0.072271   Top1 97.510417   Top5 99.992188   BatchTime 0.121157   LR 0.000100
INFO - Training [65][  320/  391]   Loss 0.072194   Top1 97.495117   Top5 99.992676   BatchTime 0.121290   LR 0.000100
INFO - Training [65][  340/  391]   Loss 0.072464   Top1 97.465533   Top5 99.993107   BatchTime 0.121411   LR 0.000100
INFO - Training [65][  360/  391]   Loss 0.072250   Top1 97.480469   Top5 99.993490   BatchTime 0.121496   LR 0.000100
INFO - Training [65][  380/  391]   Loss 0.072363   Top1 97.481497   Top5 99.993832   BatchTime 0.121539   LR 0.000100
INFO - ==> Top1: 97.504    Top5: 99.994    Loss: 0.072
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [65][   20/   79]   Loss 0.365614   Top1 90.312500   Top5 99.609375   BatchTime 0.152122
INFO - Validation [65][   40/   79]   Loss 0.364285   Top1 90.312500   Top5 99.589844   BatchTime 0.108756
INFO - Validation [65][   60/   79]   Loss 0.365365   Top1 90.312500   Top5 99.609375   BatchTime 0.094338
INFO - ==> Top1: 90.220    Top5: 99.640    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.360   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  66
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [66][   20/  391]   Loss 0.076693   Top1 96.992188   Top5 100.000000   BatchTime 0.182950   LR 0.000100
INFO - Training [66][   40/  391]   Loss 0.072007   Top1 97.441406   Top5 100.000000   BatchTime 0.133383   LR 0.000100
INFO - Training [66][   60/  391]   Loss 0.072279   Top1 97.408854   Top5 100.000000   BatchTime 0.129710   LR 0.000100
INFO - Training [66][   80/  391]   Loss 0.072444   Top1 97.441406   Top5 100.000000   BatchTime 0.128169   LR 0.000100
INFO - Training [66][  100/  391]   Loss 0.072406   Top1 97.460938   Top5 100.000000   BatchTime 0.127398   LR 0.000100
INFO - Training [66][  120/  391]   Loss 0.073713   Top1 97.402344   Top5 100.000000   BatchTime 0.126896   LR 0.000100
INFO - Training [66][  140/  391]   Loss 0.075045   Top1 97.360491   Top5 99.994420   BatchTime 0.126481   LR 0.000100
INFO - Training [66][  160/  391]   Loss 0.074432   Top1 97.431641   Top5 99.995117   BatchTime 0.126155   LR 0.000100
INFO - Training [66][  180/  391]   Loss 0.074064   Top1 97.473958   Top5 99.995660   BatchTime 0.125928   LR 0.000100
INFO - Training [66][  200/  391]   Loss 0.073916   Top1 97.484375   Top5 99.996094   BatchTime 0.125600   LR 0.000100
INFO - Training [66][  220/  391]   Loss 0.074398   Top1 97.482244   Top5 99.996449   BatchTime 0.125344   LR 0.000100
INFO - Training [66][  240/  391]   Loss 0.074142   Top1 97.470703   Top5 99.996745   BatchTime 0.125150   LR 0.000100
INFO - Training [66][  260/  391]   Loss 0.074446   Top1 97.451923   Top5 99.993990   BatchTime 0.122024   LR 0.000100
INFO - Training [66][  280/  391]   Loss 0.074731   Top1 97.444196   Top5 99.994420   BatchTime 0.119789   LR 0.000100
INFO - Training [66][  300/  391]   Loss 0.075339   Top1 97.427083   Top5 99.992188   BatchTime 0.117873   LR 0.000100
INFO - Training [66][  320/  391]   Loss 0.075586   Top1 97.395020   Top5 99.992676   BatchTime 0.115729   LR 0.000100
INFO - Training [66][  340/  391]   Loss 0.075101   Top1 97.424173   Top5 99.993107   BatchTime 0.116300   LR 0.000100
INFO - Training [66][  360/  391]   Loss 0.075540   Top1 97.419705   Top5 99.993490   BatchTime 0.116676   LR 0.000100
INFO - Training [66][  380/  391]   Loss 0.074956   Top1 97.432155   Top5 99.993832   BatchTime 0.116980   LR 0.000100
INFO - ==> Top1: 97.436    Top5: 99.994    Loss: 0.075
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [66][   20/   79]   Loss 0.370079   Top1 89.843750   Top5 99.570312   BatchTime 0.152558
INFO - Validation [66][   40/   79]   Loss 0.368316   Top1 90.058594   Top5 99.589844   BatchTime 0.107675
INFO - Validation [66][   60/   79]   Loss 0.367389   Top1 90.117188   Top5 99.622396   BatchTime 0.093075
INFO - ==> Top1: 90.000    Top5: 99.660    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.360   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  67
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [67][   20/  391]   Loss 0.075332   Top1 97.343750   Top5 100.000000   BatchTime 0.215342   LR 0.000100
INFO - Training [67][   40/  391]   Loss 0.071412   Top1 97.460938   Top5 100.000000   BatchTime 0.157594   LR 0.000100
INFO - Training [67][   60/  391]   Loss 0.072041   Top1 97.421875   Top5 100.000000   BatchTime 0.133313   LR 0.000100
INFO - Training [67][   80/  391]   Loss 0.072901   Top1 97.353516   Top5 99.990234   BatchTime 0.123696   LR 0.000100
INFO - Training [67][  100/  391]   Loss 0.072296   Top1 97.367188   Top5 99.992188   BatchTime 0.114058   LR 0.000100
INFO - Training [67][  120/  391]   Loss 0.072462   Top1 97.434896   Top5 99.993490   BatchTime 0.116720   LR 0.000100
INFO - Training [67][  140/  391]   Loss 0.074004   Top1 97.354911   Top5 99.994420   BatchTime 0.117709   LR 0.000100
INFO - Training [67][  160/  391]   Loss 0.074857   Top1 97.353516   Top5 99.995117   BatchTime 0.118516   LR 0.000100
INFO - Training [67][  180/  391]   Loss 0.075468   Top1 97.343750   Top5 99.995660   BatchTime 0.119182   LR 0.000100
INFO - Training [67][  200/  391]   Loss 0.076400   Top1 97.265625   Top5 99.996094   BatchTime 0.119750   LR 0.000100
INFO - Training [67][  220/  391]   Loss 0.075899   Top1 97.269176   Top5 99.996449   BatchTime 0.120104   LR 0.000100
INFO - Training [67][  240/  391]   Loss 0.075684   Top1 97.275391   Top5 99.996745   BatchTime 0.120383   LR 0.000100
INFO - Training [67][  260/  391]   Loss 0.074854   Top1 97.301683   Top5 99.996995   BatchTime 0.120586   LR 0.000100
INFO - Training [67][  280/  391]   Loss 0.074881   Top1 97.301897   Top5 99.997210   BatchTime 0.120753   LR 0.000100
INFO - Training [67][  300/  391]   Loss 0.074407   Top1 97.333333   Top5 99.997396   BatchTime 0.121062   LR 0.000100
INFO - Training [67][  320/  391]   Loss 0.074065   Top1 97.353516   Top5 99.997559   BatchTime 0.118274   LR 0.000100
INFO - Training [67][  340/  391]   Loss 0.074519   Top1 97.325368   Top5 99.997702   BatchTime 0.116682   LR 0.000100
INFO - Training [67][  360/  391]   Loss 0.074211   Top1 97.354601   Top5 99.997830   BatchTime 0.114756   LR 0.000100
INFO - Training [67][  380/  391]   Loss 0.073492   Top1 97.386924   Top5 99.997944   BatchTime 0.113827   LR 0.000100
INFO - ==> Top1: 97.384    Top5: 99.998    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [67][   20/   79]   Loss 0.374528   Top1 89.687500   Top5 99.609375   BatchTime 0.152483
INFO - Validation [67][   40/   79]   Loss 0.369809   Top1 90.058594   Top5 99.609375   BatchTime 0.109034
INFO - Validation [67][   60/   79]   Loss 0.367990   Top1 90.299479   Top5 99.661458   BatchTime 0.094416
INFO - ==> Top1: 90.240    Top5: 99.670    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [61][Top1: 90.360   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  68
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [68][   20/  391]   Loss 0.082483   Top1 97.226562   Top5 100.000000   BatchTime 0.214588   LR 0.000100
INFO - Training [68][   40/  391]   Loss 0.070745   Top1 97.695312   Top5 100.000000   BatchTime 0.168816   LR 0.000100
INFO - Training [68][   60/  391]   Loss 0.073373   Top1 97.486979   Top5 100.000000   BatchTime 0.153420   LR 0.000100
INFO - Training [68][   80/  391]   Loss 0.074170   Top1 97.539062   Top5 99.990234   BatchTime 0.145646   LR 0.000100
INFO - Training [68][  100/  391]   Loss 0.074201   Top1 97.515625   Top5 99.992188   BatchTime 0.134730   LR 0.000100
INFO - Training [68][  120/  391]   Loss 0.074865   Top1 97.454427   Top5 99.993490   BatchTime 0.127147   LR 0.000100
INFO - Training [68][  140/  391]   Loss 0.074141   Top1 97.466518   Top5 99.994420   BatchTime 0.121907   LR 0.000100
INFO - Training [68][  160/  391]   Loss 0.073852   Top1 97.460938   Top5 99.990234   BatchTime 0.116739   LR 0.000100
INFO - Training [68][  180/  391]   Loss 0.073274   Top1 97.452257   Top5 99.991319   BatchTime 0.118033   LR 0.000100
INFO - Training [68][  200/  391]   Loss 0.073479   Top1 97.496094   Top5 99.988281   BatchTime 0.118585   LR 0.000100
INFO - Training [68][  220/  391]   Loss 0.074971   Top1 97.460938   Top5 99.989347   BatchTime 0.119052   LR 0.000100
INFO - Training [68][  240/  391]   Loss 0.074367   Top1 97.464193   Top5 99.986979   BatchTime 0.119365   LR 0.000100
INFO - Training [68][  260/  391]   Loss 0.074877   Top1 97.439904   Top5 99.984976   BatchTime 0.119708   LR 0.000100
INFO - Training [68][  280/  391]   Loss 0.074861   Top1 97.413504   Top5 99.986049   BatchTime 0.119959   LR 0.000100
INFO - Training [68][  300/  391]   Loss 0.074604   Top1 97.416667   Top5 99.984375   BatchTime 0.120198   LR 0.000100
INFO - Training [68][  320/  391]   Loss 0.074945   Top1 97.412109   Top5 99.985352   BatchTime 0.120366   LR 0.000100
INFO - Training [68][  340/  391]   Loss 0.075297   Top1 97.417279   Top5 99.986213   BatchTime 0.120305   LR 0.000100
INFO - Training [68][  360/  391]   Loss 0.075619   Top1 97.393663   Top5 99.986979   BatchTime 0.120475   LR 0.000100
INFO - Training [68][  380/  391]   Loss 0.076118   Top1 97.356086   Top5 99.987664   BatchTime 0.117916   LR 0.000100
INFO - ==> Top1: 97.350    Top5: 99.988    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [68][   20/   79]   Loss 0.366031   Top1 90.117188   Top5 99.648438   BatchTime 0.124072
INFO - Validation [68][   40/   79]   Loss 0.367396   Top1 90.292969   Top5 99.628906   BatchTime 0.082868
INFO - Validation [68][   60/   79]   Loss 0.364459   Top1 90.481771   Top5 99.661458   BatchTime 0.077106
INFO - ==> Top1: 90.400    Top5: 99.680    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [53][Top1: 90.380   Top5: 99.670] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  69
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [69][   20/  391]   Loss 0.066389   Top1 97.773438   Top5 100.000000   BatchTime 0.213831   LR 0.000100
INFO - Training [69][   40/  391]   Loss 0.071200   Top1 97.480469   Top5 99.980469   BatchTime 0.169039   LR 0.000100
INFO - Training [69][   60/  391]   Loss 0.073547   Top1 97.447917   Top5 99.973958   BatchTime 0.153966   LR 0.000100
INFO - Training [69][   80/  391]   Loss 0.072548   Top1 97.421875   Top5 99.980469   BatchTime 0.147544   LR 0.000100
INFO - Training [69][  100/  391]   Loss 0.069831   Top1 97.523438   Top5 99.984375   BatchTime 0.142858   LR 0.000100
INFO - Training [69][  120/  391]   Loss 0.071841   Top1 97.434896   Top5 99.986979   BatchTime 0.139587   LR 0.000100
INFO - Training [69][  140/  391]   Loss 0.071337   Top1 97.455357   Top5 99.988839   BatchTime 0.137258   LR 0.000100
INFO - Training [69][  160/  391]   Loss 0.071191   Top1 97.475586   Top5 99.990234   BatchTime 0.131286   LR 0.000100
INFO - Training [69][  180/  391]   Loss 0.071359   Top1 97.478299   Top5 99.986979   BatchTime 0.126370   LR 0.000100
INFO - Training [69][  200/  391]   Loss 0.070395   Top1 97.531250   Top5 99.988281   BatchTime 0.123113   LR 0.000100
INFO - Training [69][  220/  391]   Loss 0.070471   Top1 97.531960   Top5 99.989347   BatchTime 0.119532   LR 0.000100
INFO - Training [69][  240/  391]   Loss 0.071482   Top1 97.490234   Top5 99.983724   BatchTime 0.119902   LR 0.000100
INFO - Training [69][  260/  391]   Loss 0.072352   Top1 97.436899   Top5 99.984976   BatchTime 0.120189   LR 0.000100
INFO - Training [69][  280/  391]   Loss 0.072576   Top1 97.438616   Top5 99.986049   BatchTime 0.120400   LR 0.000100
INFO - Training [69][  300/  391]   Loss 0.072444   Top1 97.453125   Top5 99.984375   BatchTime 0.120552   LR 0.000100
INFO - Training [69][  320/  391]   Loss 0.072902   Top1 97.446289   Top5 99.982910   BatchTime 0.120750   LR 0.000100
INFO - Training [69][  340/  391]   Loss 0.072765   Top1 97.449449   Top5 99.983915   BatchTime 0.120873   LR 0.000100
INFO - Training [69][  360/  391]   Loss 0.073321   Top1 97.426215   Top5 99.984809   BatchTime 0.121023   LR 0.000100
INFO - Training [69][  380/  391]   Loss 0.073487   Top1 97.423931   Top5 99.985609   BatchTime 0.121083   LR 0.000100
INFO - ==> Top1: 97.408    Top5: 99.986    Loss: 0.074
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [69][   20/   79]   Loss 0.365811   Top1 90.703125   Top5 99.570312   BatchTime 0.125250
INFO - Validation [69][   40/   79]   Loss 0.366646   Top1 90.546875   Top5 99.570312   BatchTime 0.076187
INFO - Validation [69][   60/   79]   Loss 0.365159   Top1 90.520833   Top5 99.622396   BatchTime 0.059484
INFO - ==> Top1: 90.390    Top5: 99.660    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [69][Top1: 90.390   Top5: 99.660] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  70
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [70][   20/  391]   Loss 0.068246   Top1 97.812500   Top5 99.960938   BatchTime 0.188658   LR 0.000010
INFO - Training [70][   40/  391]   Loss 0.070892   Top1 97.656250   Top5 99.960938   BatchTime 0.156125   LR 0.000010
INFO - Training [70][   60/  391]   Loss 0.073056   Top1 97.591146   Top5 99.973958   BatchTime 0.145410   LR 0.000010
INFO - Training [70][   80/  391]   Loss 0.074263   Top1 97.412109   Top5 99.980469   BatchTime 0.140035   LR 0.000010
INFO - Training [70][  100/  391]   Loss 0.077193   Top1 97.281250   Top5 99.984375   BatchTime 0.136851   LR 0.000010
INFO - Training [70][  120/  391]   Loss 0.075871   Top1 97.298177   Top5 99.986979   BatchTime 0.134494   LR 0.000010
INFO - Training [70][  140/  391]   Loss 0.077015   Top1 97.254464   Top5 99.988839   BatchTime 0.132896   LR 0.000010
INFO - Training [70][  160/  391]   Loss 0.075716   Top1 97.275391   Top5 99.990234   BatchTime 0.131592   LR 0.000010
INFO - Training [70][  180/  391]   Loss 0.075167   Top1 97.304688   Top5 99.986979   BatchTime 0.130627   LR 0.000010
INFO - Training [70][  200/  391]   Loss 0.074919   Top1 97.292969   Top5 99.988281   BatchTime 0.129856   LR 0.000010
INFO - Training [70][  220/  391]   Loss 0.074363   Top1 97.329545   Top5 99.989347   BatchTime 0.128123   LR 0.000010
INFO - Training [70][  240/  391]   Loss 0.073525   Top1 97.360026   Top5 99.990234   BatchTime 0.124531   LR 0.000010
INFO - Training [70][  260/  391]   Loss 0.073846   Top1 97.352764   Top5 99.990986   BatchTime 0.122257   LR 0.000010
INFO - Training [70][  280/  391]   Loss 0.073437   Top1 97.391183   Top5 99.991629   BatchTime 0.119220   LR 0.000010
INFO - Training [70][  300/  391]   Loss 0.073749   Top1 97.367188   Top5 99.992188   BatchTime 0.119351   LR 0.000010
INFO - Training [70][  320/  391]   Loss 0.074029   Top1 97.375488   Top5 99.990234   BatchTime 0.119631   LR 0.000010
INFO - Training [70][  340/  391]   Loss 0.073531   Top1 97.405790   Top5 99.990809   BatchTime 0.119845   LR 0.000010
INFO - Training [70][  360/  391]   Loss 0.073284   Top1 97.415365   Top5 99.991319   BatchTime 0.120004   LR 0.000010
INFO - Training [70][  380/  391]   Loss 0.073186   Top1 97.401316   Top5 99.991776   BatchTime 0.120165   LR 0.000010
INFO - ==> Top1: 97.410    Top5: 99.992    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [70][   20/   79]   Loss 0.367893   Top1 90.195312   Top5 99.726562   BatchTime 0.154526
INFO - Validation [70][   40/   79]   Loss 0.365119   Top1 90.097656   Top5 99.707031   BatchTime 0.110404
INFO - Validation [70][   60/   79]   Loss 0.362123   Top1 90.286458   Top5 99.687500   BatchTime 0.095144
INFO - ==> Top1: 90.220    Top5: 99.710    Loss: 0.362
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [69][Top1: 90.390   Top5: 99.660] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  71
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [71][   20/  391]   Loss 0.064564   Top1 97.851562   Top5 100.000000   BatchTime 0.178206   LR 0.000010
INFO - Training [71][   40/  391]   Loss 0.069827   Top1 97.597656   Top5 99.980469   BatchTime 0.134421   LR 0.000010
INFO - Training [71][   60/  391]   Loss 0.072033   Top1 97.526042   Top5 99.973958   BatchTime 0.118440   LR 0.000010
INFO - Training [71][   80/  391]   Loss 0.071980   Top1 97.509766   Top5 99.970703   BatchTime 0.117045   LR 0.000010
INFO - Training [71][  100/  391]   Loss 0.071783   Top1 97.492188   Top5 99.976562   BatchTime 0.118323   LR 0.000010
INFO - Training [71][  120/  391]   Loss 0.071290   Top1 97.526042   Top5 99.980469   BatchTime 0.119132   LR 0.000010
INFO - Training [71][  140/  391]   Loss 0.072717   Top1 97.477679   Top5 99.977679   BatchTime 0.120380   LR 0.000010
INFO - Training [71][  160/  391]   Loss 0.071809   Top1 97.514648   Top5 99.970703   BatchTime 0.120815   LR 0.000010
INFO - Training [71][  180/  391]   Loss 0.073137   Top1 97.456597   Top5 99.969618   BatchTime 0.121161   LR 0.000010
INFO - Training [71][  200/  391]   Loss 0.072860   Top1 97.468750   Top5 99.972656   BatchTime 0.121395   LR 0.000010
INFO - Training [71][  220/  391]   Loss 0.072515   Top1 97.482244   Top5 99.975142   BatchTime 0.121589   LR 0.000010
INFO - Training [71][  240/  391]   Loss 0.072244   Top1 97.486979   Top5 99.973958   BatchTime 0.121719   LR 0.000010
INFO - Training [71][  260/  391]   Loss 0.072395   Top1 97.478966   Top5 99.975962   BatchTime 0.121796   LR 0.000010
INFO - Training [71][  280/  391]   Loss 0.072682   Top1 97.441406   Top5 99.977679   BatchTime 0.119192   LR 0.000010
INFO - Training [71][  300/  391]   Loss 0.073609   Top1 97.408854   Top5 99.979167   BatchTime 0.117256   LR 0.000010
INFO - Training [71][  320/  391]   Loss 0.073114   Top1 97.431641   Top5 99.980469   BatchTime 0.115568   LR 0.000010
INFO - Training [71][  340/  391]   Loss 0.072376   Top1 97.456342   Top5 99.981618   BatchTime 0.113818   LR 0.000010
INFO - Training [71][  360/  391]   Loss 0.072728   Top1 97.439236   Top5 99.982639   BatchTime 0.114245   LR 0.000010
INFO - Training [71][  380/  391]   Loss 0.072779   Top1 97.442434   Top5 99.981497   BatchTime 0.114734   LR 0.000010
INFO - ==> Top1: 97.434    Top5: 99.980    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [71][   20/   79]   Loss 0.364855   Top1 90.351562   Top5 99.687500   BatchTime 0.153425
INFO - Validation [71][   40/   79]   Loss 0.366131   Top1 90.273438   Top5 99.648438   BatchTime 0.109303
INFO - Validation [71][   60/   79]   Loss 0.364009   Top1 90.481771   Top5 99.635417   BatchTime 0.094652
INFO - ==> Top1: 90.370    Top5: 99.650    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [69][Top1: 90.390   Top5: 99.660] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  72
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [72][   20/  391]   Loss 0.065265   Top1 97.734375   Top5 100.000000   BatchTime 0.212627   LR 0.000010
INFO - Training [72][   40/  391]   Loss 0.074620   Top1 97.519531   Top5 100.000000   BatchTime 0.167913   LR 0.000010
INFO - Training [72][   60/  391]   Loss 0.075056   Top1 97.500000   Top5 100.000000   BatchTime 0.145629   LR 0.000010
INFO - Training [72][   80/  391]   Loss 0.078012   Top1 97.392578   Top5 100.000000   BatchTime 0.130959   LR 0.000010
INFO - Training [72][  100/  391]   Loss 0.073885   Top1 97.523438   Top5 100.000000   BatchTime 0.122924   LR 0.000010
INFO - Training [72][  120/  391]   Loss 0.073159   Top1 97.493490   Top5 100.000000   BatchTime 0.115051   LR 0.000010
INFO - Training [72][  140/  391]   Loss 0.072025   Top1 97.539062   Top5 100.000000   BatchTime 0.116416   LR 0.000010
INFO - Training [72][  160/  391]   Loss 0.072339   Top1 97.509766   Top5 99.995117   BatchTime 0.117342   LR 0.000010
INFO - Training [72][  180/  391]   Loss 0.073962   Top1 97.430556   Top5 99.995660   BatchTime 0.118017   LR 0.000010
INFO - Training [72][  200/  391]   Loss 0.073243   Top1 97.472656   Top5 99.992188   BatchTime 0.118593   LR 0.000010
INFO - Training [72][  220/  391]   Loss 0.073282   Top1 97.450284   Top5 99.992898   BatchTime 0.119028   LR 0.000010
INFO - Training [72][  240/  391]   Loss 0.073259   Top1 97.438151   Top5 99.990234   BatchTime 0.119334   LR 0.000010
INFO - Training [72][  260/  391]   Loss 0.073094   Top1 97.436899   Top5 99.990986   BatchTime 0.119665   LR 0.000010
INFO - Training [72][  280/  391]   Loss 0.073543   Top1 97.438616   Top5 99.991629   BatchTime 0.119886   LR 0.000010
INFO - Training [72][  300/  391]   Loss 0.073832   Top1 97.416667   Top5 99.992188   BatchTime 0.120067   LR 0.000010
INFO - Training [72][  320/  391]   Loss 0.074017   Top1 97.421875   Top5 99.992676   BatchTime 0.120472   LR 0.000010
INFO - Training [72][  340/  391]   Loss 0.073251   Top1 97.447151   Top5 99.993107   BatchTime 0.117748   LR 0.000010
INFO - Training [72][  360/  391]   Loss 0.073506   Top1 97.417535   Top5 99.991319   BatchTime 0.116208   LR 0.000010
INFO - Training [72][  380/  391]   Loss 0.073983   Top1 97.380757   Top5 99.989720   BatchTime 0.114594   LR 0.000010
INFO - ==> Top1: 97.376    Top5: 99.990    Loss: 0.074
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [72][   20/   79]   Loss 0.368347   Top1 90.156250   Top5 99.648438   BatchTime 0.160681
INFO - Validation [72][   40/   79]   Loss 0.368371   Top1 90.234375   Top5 99.648438   BatchTime 0.113591
INFO - Validation [72][   60/   79]   Loss 0.365473   Top1 90.494792   Top5 99.661458   BatchTime 0.096963
INFO - ==> Top1: 90.490    Top5: 99.660    Loss: 0.364
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  73
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [73][   20/  391]   Loss 0.073312   Top1 97.382812   Top5 99.960938   BatchTime 0.214366   LR 0.000010
INFO - Training [73][   40/  391]   Loss 0.072810   Top1 97.285156   Top5 99.980469   BatchTime 0.169077   LR 0.000010
INFO - Training [73][   60/  391]   Loss 0.074876   Top1 97.174479   Top5 99.973958   BatchTime 0.153839   LR 0.000010
INFO - Training [73][   80/  391]   Loss 0.073663   Top1 97.187500   Top5 99.980469   BatchTime 0.146191   LR 0.000010
INFO - Training [73][  100/  391]   Loss 0.072142   Top1 97.328125   Top5 99.984375   BatchTime 0.141511   LR 0.000010
INFO - Training [73][  120/  391]   Loss 0.074051   Top1 97.311198   Top5 99.986979   BatchTime 0.133828   LR 0.000010
INFO - Training [73][  140/  391]   Loss 0.074326   Top1 97.299107   Top5 99.988839   BatchTime 0.127454   LR 0.000010
INFO - Training [73][  160/  391]   Loss 0.073873   Top1 97.333984   Top5 99.990234   BatchTime 0.123233   LR 0.000010
INFO - Training [73][  180/  391]   Loss 0.073889   Top1 97.300347   Top5 99.991319   BatchTime 0.119589   LR 0.000010
INFO - Training [73][  200/  391]   Loss 0.073537   Top1 97.304688   Top5 99.992188   BatchTime 0.120366   LR 0.000010
INFO - Training [73][  220/  391]   Loss 0.073582   Top1 97.304688   Top5 99.992898   BatchTime 0.120717   LR 0.000010
INFO - Training [73][  240/  391]   Loss 0.073624   Top1 97.314453   Top5 99.993490   BatchTime 0.121045   LR 0.000010
INFO - Training [73][  260/  391]   Loss 0.073999   Top1 97.313702   Top5 99.993990   BatchTime 0.121298   LR 0.000010
INFO - Training [73][  280/  391]   Loss 0.073912   Top1 97.335379   Top5 99.994420   BatchTime 0.121462   LR 0.000010
INFO - Training [73][  300/  391]   Loss 0.074283   Top1 97.312500   Top5 99.992188   BatchTime 0.121606   LR 0.000010
INFO - Training [73][  320/  391]   Loss 0.073386   Top1 97.348633   Top5 99.990234   BatchTime 0.121702   LR 0.000010
INFO - Training [73][  340/  391]   Loss 0.073344   Top1 97.352941   Top5 99.990809   BatchTime 0.121744   LR 0.000010
INFO - Training [73][  360/  391]   Loss 0.073040   Top1 97.354601   Top5 99.991319   BatchTime 0.121755   LR 0.000010
INFO - Training [73][  380/  391]   Loss 0.073339   Top1 97.345806   Top5 99.991776   BatchTime 0.121348   LR 0.000010
INFO - ==> Top1: 97.348    Top5: 99.988    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [73][   20/   79]   Loss 0.368906   Top1 90.312500   Top5 99.609375   BatchTime 0.125295
INFO - Validation [73][   40/   79]   Loss 0.370699   Top1 90.175781   Top5 99.609375   BatchTime 0.075804
INFO - Validation [73][   60/   79]   Loss 0.368913   Top1 90.377604   Top5 99.635417   BatchTime 0.059267
INFO - ==> Top1: 90.280    Top5: 99.660    Loss: 0.368
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  74
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [74][   20/  391]   Loss 0.067519   Top1 97.421875   Top5 100.000000   BatchTime 0.216430   LR 0.000010
INFO - Training [74][   40/  391]   Loss 0.065142   Top1 97.519531   Top5 100.000000   BatchTime 0.170346   LR 0.000010
INFO - Training [74][   60/  391]   Loss 0.068555   Top1 97.408854   Top5 100.000000   BatchTime 0.154615   LR 0.000010
INFO - Training [74][   80/  391]   Loss 0.071111   Top1 97.333984   Top5 99.990234   BatchTime 0.146966   LR 0.000010
INFO - Training [74][  100/  391]   Loss 0.072499   Top1 97.210938   Top5 99.992188   BatchTime 0.142315   LR 0.000010
INFO - Training [74][  120/  391]   Loss 0.073184   Top1 97.180990   Top5 99.993490   BatchTime 0.139247   LR 0.000010
INFO - Training [74][  140/  391]   Loss 0.072968   Top1 97.209821   Top5 99.994420   BatchTime 0.136878   LR 0.000010
INFO - Training [74][  160/  391]   Loss 0.074293   Top1 97.163086   Top5 99.995117   BatchTime 0.135094   LR 0.000010
INFO - Training [74][  180/  391]   Loss 0.073629   Top1 97.178819   Top5 99.995660   BatchTime 0.132418   LR 0.000010
INFO - Training [74][  200/  391]   Loss 0.073832   Top1 97.226562   Top5 99.996094   BatchTime 0.127676   LR 0.000010
INFO - Training [74][  220/  391]   Loss 0.073915   Top1 97.230114   Top5 99.996449   BatchTime 0.124476   LR 0.000010
INFO - Training [74][  240/  391]   Loss 0.073849   Top1 97.294922   Top5 99.996745   BatchTime 0.120746   LR 0.000010
INFO - Training [74][  260/  391]   Loss 0.074372   Top1 97.280649   Top5 99.996995   BatchTime 0.120576   LR 0.000010
INFO - Training [74][  280/  391]   Loss 0.074776   Top1 97.285156   Top5 99.997210   BatchTime 0.120775   LR 0.000010
INFO - Training [74][  300/  391]   Loss 0.075367   Top1 97.276042   Top5 99.997396   BatchTime 0.120955   LR 0.000010
INFO - Training [74][  320/  391]   Loss 0.075153   Top1 97.299805   Top5 99.995117   BatchTime 0.121115   LR 0.000010
INFO - Training [74][  340/  391]   Loss 0.075423   Top1 97.313879   Top5 99.995404   BatchTime 0.121247   LR 0.000010
INFO - Training [74][  360/  391]   Loss 0.075356   Top1 97.309028   Top5 99.995660   BatchTime 0.121305   LR 0.000010
INFO - Training [74][  380/  391]   Loss 0.075715   Top1 97.290296   Top5 99.995888   BatchTime 0.121412   LR 0.000010
INFO - ==> Top1: 97.296    Top5: 99.996    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [74][   20/   79]   Loss 0.364850   Top1 90.273438   Top5 99.648438   BatchTime 0.151472
INFO - Validation [74][   40/   79]   Loss 0.365287   Top1 90.253906   Top5 99.687500   BatchTime 0.108121
INFO - Validation [74][   60/   79]   Loss 0.363955   Top1 90.429688   Top5 99.674479   BatchTime 0.084915
INFO - ==> Top1: 90.330    Top5: 99.720    Loss: 0.364
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  75
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [75][   20/  391]   Loss 0.073027   Top1 97.695312   Top5 100.000000   BatchTime 0.180462   LR 0.000010
INFO - Training [75][   40/  391]   Loss 0.075634   Top1 97.421875   Top5 100.000000   BatchTime 0.138011   LR 0.000010
INFO - Training [75][   60/  391]   Loss 0.074038   Top1 97.486979   Top5 99.986979   BatchTime 0.133282   LR 0.000010
INFO - Training [75][   80/  391]   Loss 0.071399   Top1 97.480469   Top5 99.980469   BatchTime 0.130733   LR 0.000010
INFO - Training [75][  100/  391]   Loss 0.073676   Top1 97.359375   Top5 99.976562   BatchTime 0.129443   LR 0.000010
INFO - Training [75][  120/  391]   Loss 0.071823   Top1 97.486979   Top5 99.980469   BatchTime 0.128449   LR 0.000010
INFO - Training [75][  140/  391]   Loss 0.072292   Top1 97.477679   Top5 99.977679   BatchTime 0.127739   LR 0.000010
INFO - Training [75][  160/  391]   Loss 0.073246   Top1 97.446289   Top5 99.970703   BatchTime 0.127217   LR 0.000010
INFO - Training [75][  180/  391]   Loss 0.072787   Top1 97.469618   Top5 99.969618   BatchTime 0.126787   LR 0.000010
INFO - Training [75][  200/  391]   Loss 0.073547   Top1 97.449219   Top5 99.972656   BatchTime 0.126327   LR 0.000010
INFO - Training [75][  220/  391]   Loss 0.072800   Top1 97.450284   Top5 99.975142   BatchTime 0.126042   LR 0.000010
INFO - Training [75][  240/  391]   Loss 0.072410   Top1 97.451172   Top5 99.977214   BatchTime 0.124464   LR 0.000010
INFO - Training [75][  260/  391]   Loss 0.072538   Top1 97.445913   Top5 99.975962   BatchTime 0.121355   LR 0.000010
INFO - Training [75][  280/  391]   Loss 0.072628   Top1 97.446987   Top5 99.977679   BatchTime 0.119868   LR 0.000010
INFO - Training [75][  300/  391]   Loss 0.073012   Top1 97.450521   Top5 99.979167   BatchTime 0.117007   LR 0.000010
INFO - Training [75][  320/  391]   Loss 0.073251   Top1 97.441406   Top5 99.980469   BatchTime 0.117558   LR 0.000010
INFO - Training [75][  340/  391]   Loss 0.074139   Top1 97.408088   Top5 99.981618   BatchTime 0.117875   LR 0.000010
INFO - Training [75][  360/  391]   Loss 0.074233   Top1 97.400174   Top5 99.982639   BatchTime 0.117827   LR 0.000010
INFO - Training [75][  380/  391]   Loss 0.074505   Top1 97.403372   Top5 99.983553   BatchTime 0.118109   LR 0.000010
INFO - ==> Top1: 97.384    Top5: 99.984    Loss: 0.075
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [75][   20/   79]   Loss 0.371147   Top1 90.039062   Top5 99.648438   BatchTime 0.151151
INFO - Validation [75][   40/   79]   Loss 0.369753   Top1 90.156250   Top5 99.687500   BatchTime 0.107996
INFO - Validation [75][   60/   79]   Loss 0.368067   Top1 90.338542   Top5 99.700521   BatchTime 0.093788
INFO - ==> Top1: 90.210    Top5: 99.730    Loss: 0.368
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  76
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [76][   20/  391]   Loss 0.068462   Top1 97.578125   Top5 100.000000   BatchTime 0.201786   LR 0.000010
INFO - Training [76][   40/  391]   Loss 0.069916   Top1 97.597656   Top5 100.000000   BatchTime 0.142209   LR 0.000010
INFO - Training [76][   60/  391]   Loss 0.070874   Top1 97.486979   Top5 100.000000   BatchTime 0.123800   LR 0.000010
INFO - Training [76][   80/  391]   Loss 0.070630   Top1 97.490234   Top5 100.000000   BatchTime 0.114108   LR 0.000010
INFO - Training [76][  100/  391]   Loss 0.070233   Top1 97.515625   Top5 100.000000   BatchTime 0.114249   LR 0.000010
INFO - Training [76][  120/  391]   Loss 0.067936   Top1 97.597656   Top5 100.000000   BatchTime 0.115872   LR 0.000010
INFO - Training [76][  140/  391]   Loss 0.071307   Top1 97.516741   Top5 100.000000   BatchTime 0.116986   LR 0.000010
INFO - Training [76][  160/  391]   Loss 0.073063   Top1 97.426758   Top5 100.000000   BatchTime 0.117813   LR 0.000010
INFO - Training [76][  180/  391]   Loss 0.072627   Top1 97.469618   Top5 99.995660   BatchTime 0.118369   LR 0.000010
INFO - Training [76][  200/  391]   Loss 0.072667   Top1 97.457031   Top5 99.996094   BatchTime 0.118874   LR 0.000010
INFO - Training [76][  220/  391]   Loss 0.072946   Top1 97.446733   Top5 99.996449   BatchTime 0.119196   LR 0.000010
INFO - Training [76][  240/  391]   Loss 0.072616   Top1 97.473958   Top5 99.993490   BatchTime 0.119543   LR 0.000010
INFO - Training [76][  260/  391]   Loss 0.073603   Top1 97.424880   Top5 99.993990   BatchTime 0.119823   LR 0.000010
INFO - Training [76][  280/  391]   Loss 0.073451   Top1 97.446987   Top5 99.994420   BatchTime 0.120052   LR 0.000010
INFO - Training [76][  300/  391]   Loss 0.073495   Top1 97.440104   Top5 99.994792   BatchTime 0.118059   LR 0.000010
INFO - Training [76][  320/  391]   Loss 0.073248   Top1 97.441406   Top5 99.995117   BatchTime 0.116163   LR 0.000010
INFO - Training [76][  340/  391]   Loss 0.073498   Top1 97.417279   Top5 99.993107   BatchTime 0.114620   LR 0.000010
INFO - Training [76][  360/  391]   Loss 0.073519   Top1 97.426215   Top5 99.993490   BatchTime 0.112482   LR 0.000010
INFO - Training [76][  380/  391]   Loss 0.073595   Top1 97.432155   Top5 99.993832   BatchTime 0.113311   LR 0.000010
INFO - ==> Top1: 97.430    Top5: 99.994    Loss: 0.074
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [76][   20/   79]   Loss 0.371057   Top1 89.921875   Top5 99.687500   BatchTime 0.154054
INFO - Validation [76][   40/   79]   Loss 0.368162   Top1 90.117188   Top5 99.667969   BatchTime 0.108550
INFO - Validation [76][   60/   79]   Loss 0.366815   Top1 90.273438   Top5 99.687500   BatchTime 0.094246
INFO - ==> Top1: 90.350    Top5: 99.710    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  77
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [77][   20/  391]   Loss 0.064306   Top1 97.656250   Top5 100.000000   BatchTime 0.211824   LR 0.000010
INFO - Training [77][   40/  391]   Loss 0.073031   Top1 97.285156   Top5 100.000000   BatchTime 0.167155   LR 0.000010
INFO - Training [77][   60/  391]   Loss 0.068878   Top1 97.421875   Top5 100.000000   BatchTime 0.152365   LR 0.000010
INFO - Training [77][   80/  391]   Loss 0.072965   Top1 97.275391   Top5 100.000000   BatchTime 0.140949   LR 0.000010
INFO - Training [77][  100/  391]   Loss 0.074861   Top1 97.289062   Top5 100.000000   BatchTime 0.130096   LR 0.000010
INFO - Training [77][  120/  391]   Loss 0.074064   Top1 97.350260   Top5 100.000000   BatchTime 0.124480   LR 0.000010
INFO - Training [77][  140/  391]   Loss 0.074087   Top1 97.338170   Top5 99.994420   BatchTime 0.117558   LR 0.000010
INFO - Training [77][  160/  391]   Loss 0.072876   Top1 97.402344   Top5 99.995117   BatchTime 0.118652   LR 0.000010
INFO - Training [77][  180/  391]   Loss 0.072764   Top1 97.382812   Top5 99.991319   BatchTime 0.119194   LR 0.000010
INFO - Training [77][  200/  391]   Loss 0.072919   Top1 97.398438   Top5 99.988281   BatchTime 0.119604   LR 0.000010
INFO - Training [77][  220/  391]   Loss 0.072634   Top1 97.386364   Top5 99.989347   BatchTime 0.119923   LR 0.000010
INFO - Training [77][  240/  391]   Loss 0.072115   Top1 97.408854   Top5 99.990234   BatchTime 0.120356   LR 0.000010
INFO - Training [77][  260/  391]   Loss 0.072213   Top1 97.397837   Top5 99.990986   BatchTime 0.120550   LR 0.000010
INFO - Training [77][  280/  391]   Loss 0.072260   Top1 97.413504   Top5 99.988839   BatchTime 0.120784   LR 0.000010
INFO - Training [77][  300/  391]   Loss 0.072691   Top1 97.398438   Top5 99.989583   BatchTime 0.120929   LR 0.000010
INFO - Training [77][  320/  391]   Loss 0.072576   Top1 97.404785   Top5 99.987793   BatchTime 0.121032   LR 0.000010
INFO - Training [77][  340/  391]   Loss 0.072904   Top1 97.392004   Top5 99.986213   BatchTime 0.121457   LR 0.000010
INFO - Training [77][  360/  391]   Loss 0.073103   Top1 97.371962   Top5 99.986979   BatchTime 0.118706   LR 0.000010
INFO - Training [77][  380/  391]   Loss 0.073124   Top1 97.374589   Top5 99.987664   BatchTime 0.117193   LR 0.000010
INFO - ==> Top1: 97.380    Top5: 99.988    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [77][   20/   79]   Loss 0.367768   Top1 90.117188   Top5 99.609375   BatchTime 0.166942
INFO - Validation [77][   40/   79]   Loss 0.366770   Top1 90.078125   Top5 99.589844   BatchTime 0.115933
INFO - Validation [77][   60/   79]   Loss 0.368747   Top1 90.286458   Top5 99.635417   BatchTime 0.098239
INFO - ==> Top1: 90.220    Top5: 99.630    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  78
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [78][   20/  391]   Loss 0.069541   Top1 97.695312   Top5 100.000000   BatchTime 0.214222   LR 0.000010
INFO - Training [78][   40/  391]   Loss 0.071253   Top1 97.714844   Top5 100.000000   BatchTime 0.168670   LR 0.000010
INFO - Training [78][   60/  391]   Loss 0.070669   Top1 97.747396   Top5 99.986979   BatchTime 0.153647   LR 0.000010
INFO - Training [78][   80/  391]   Loss 0.072791   Top1 97.607422   Top5 99.990234   BatchTime 0.146097   LR 0.000010
INFO - Training [78][  100/  391]   Loss 0.073258   Top1 97.554688   Top5 99.992188   BatchTime 0.141407   LR 0.000010
INFO - Training [78][  120/  391]   Loss 0.073151   Top1 97.532552   Top5 99.993490   BatchTime 0.138403   LR 0.000010
INFO - Training [78][  140/  391]   Loss 0.073089   Top1 97.544643   Top5 99.994420   BatchTime 0.131963   LR 0.000010
INFO - Training [78][  160/  391]   Loss 0.072160   Top1 97.578125   Top5 99.995117   BatchTime 0.126306   LR 0.000010
INFO - Training [78][  180/  391]   Loss 0.071982   Top1 97.591146   Top5 99.995660   BatchTime 0.122603   LR 0.000010
INFO - Training [78][  200/  391]   Loss 0.071488   Top1 97.593750   Top5 99.996094   BatchTime 0.119271   LR 0.000010
INFO - Training [78][  220/  391]   Loss 0.071803   Top1 97.567472   Top5 99.996449   BatchTime 0.119775   LR 0.000010
INFO - Training [78][  240/  391]   Loss 0.071648   Top1 97.555339   Top5 99.993490   BatchTime 0.120091   LR 0.000010
INFO - Training [78][  260/  391]   Loss 0.071730   Top1 97.530048   Top5 99.993990   BatchTime 0.120309   LR 0.000010
INFO - Training [78][  280/  391]   Loss 0.071569   Top1 97.519531   Top5 99.991629   BatchTime 0.120560   LR 0.000010
INFO - Training [78][  300/  391]   Loss 0.071698   Top1 97.502604   Top5 99.992188   BatchTime 0.120722   LR 0.000010
INFO - Training [78][  320/  391]   Loss 0.071313   Top1 97.502441   Top5 99.992676   BatchTime 0.120951   LR 0.000010
INFO - Training [78][  340/  391]   Loss 0.072193   Top1 97.465533   Top5 99.993107   BatchTime 0.121044   LR 0.000010
INFO - Training [78][  360/  391]   Loss 0.073012   Top1 97.447917   Top5 99.991319   BatchTime 0.121117   LR 0.000010
INFO - Training [78][  380/  391]   Loss 0.073163   Top1 97.434211   Top5 99.989720   BatchTime 0.121179   LR 0.000010
INFO - Validation [78][   20/   79]   Loss 0.368562   Top1 90.000000   Top5 99.648438   BatchTime 0.137847
INFO - Validation [78][   40/   79]   Loss 0.367727   Top1 90.117188   Top5 99.589844   BatchTime 0.090229
INFO - Validation [78][   60/   79]   Loss 0.370525   Top1 90.195312   Top5 99.609375   BatchTime 0.072238
INFO - ==> Top1: 90.210    Top5: 99.630    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  79
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [79][   20/  391]   Loss 0.068987   Top1 97.500000   Top5 100.000000   BatchTime 0.218820   LR 0.000010
INFO - Training [79][   40/  391]   Loss 0.071234   Top1 97.441406   Top5 100.000000   BatchTime 0.171198   LR 0.000010
INFO - Training [79][   60/  391]   Loss 0.072166   Top1 97.356771   Top5 100.000000   BatchTime 0.155624   LR 0.000010
INFO - Training [79][   80/  391]   Loss 0.070210   Top1 97.490234   Top5 100.000000   BatchTime 0.147538   LR 0.000010
INFO - Training [79][  100/  391]   Loss 0.069864   Top1 97.468750   Top5 100.000000   BatchTime 0.142833   LR 0.000010
INFO - Training [79][  120/  391]   Loss 0.070827   Top1 97.389323   Top5 100.000000   BatchTime 0.139738   LR 0.000010
INFO - Training [79][  140/  391]   Loss 0.072118   Top1 97.371652   Top5 99.988839   BatchTime 0.137408   LR 0.000010
INFO - Training [79][  160/  391]   Loss 0.072481   Top1 97.368164   Top5 99.990234   BatchTime 0.135508   LR 0.000010
INFO - Training [79][  180/  391]   Loss 0.073525   Top1 97.335069   Top5 99.991319   BatchTime 0.134172   LR 0.000010
INFO - Training [79][  200/  391]   Loss 0.071449   Top1 97.414062   Top5 99.992188   BatchTime 0.131195   LR 0.000010
INFO - Training [79][  220/  391]   Loss 0.072951   Top1 97.343750   Top5 99.992898   BatchTime 0.126774   LR 0.000010
INFO - Training [79][  240/  391]   Loss 0.073773   Top1 97.320964   Top5 99.990234   BatchTime 0.124154   LR 0.000010
INFO - Training [79][  260/  391]   Loss 0.074641   Top1 97.289663   Top5 99.984976   BatchTime 0.120333   LR 0.000010
INFO - Training [79][  280/  391]   Loss 0.073963   Top1 97.338170   Top5 99.986049   BatchTime 0.120478   LR 0.000010
INFO - Training [79][  300/  391]   Loss 0.073950   Top1 97.320312   Top5 99.986979   BatchTime 0.120696   LR 0.000010
INFO - Training [79][  320/  391]   Loss 0.073487   Top1 97.377930   Top5 99.987793   BatchTime 0.120868   LR 0.000010
INFO - Training [79][  340/  391]   Loss 0.073427   Top1 97.369026   Top5 99.988511   BatchTime 0.120977   LR 0.000010
INFO - Training [79][  360/  391]   Loss 0.073627   Top1 97.363281   Top5 99.989149   BatchTime 0.121104   LR 0.000010
INFO - Training [79][  380/  391]   Loss 0.073628   Top1 97.372533   Top5 99.989720   BatchTime 0.121207   LR 0.000010
INFO - ==> Top1: 97.382    Top5: 99.990    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [79][   20/   79]   Loss 0.372531   Top1 90.312500   Top5 99.648438   BatchTime 0.151945
INFO - Validation [79][   40/   79]   Loss 0.369579   Top1 90.292969   Top5 99.628906   BatchTime 0.108434
INFO - Validation [79][   60/   79]   Loss 0.367160   Top1 90.377604   Top5 99.661458   BatchTime 0.093917
INFO - ==> Top1: 90.330    Top5: 99.690    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [63][Top1: 90.490   Top5: 99.680] Sparsity : 0.878
INFO - Scoreboard best 2 ==> Epoch [72][Top1: 90.490   Top5: 99.660] Sparsity : 0.878
INFO - Scoreboard best 3 ==> Epoch [68][Top1: 90.400   Top5: 99.680] Sparsity : 0.878
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_25_epoch80_20221104-022811/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 0.372531   Top1 90.312500   Top5 99.648438   BatchTime 0.140334
INFO - Validation [   40/   79]   Loss 0.369579   Top1 90.292969   Top5 99.628906   BatchTime 0.090834
INFO - Validation [   60/   79]   Loss 0.367160   Top1 90.377604   Top5 99.661458   BatchTime 0.069310
INFO - ==> Top1: 90.330    Top5: 99.690    Loss: 0.365
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/pruned_model/MobileNetv2_cifar10_a8w8_25_epoch80_checkpoint.pth.tar
INFO - Program completed successfully ... exiting ...
INFO - If you have any questions or suggestions, please visit: github.com/zhutmost/lsq-net