INFO - Log file for this run: /home/ilena7440/LSQ/out/ResNet18_imagenet_a8w8_30_epoch80_20221107-152605/ResNet18_imagenet_a8w8_30_epoch80_20221107-152605.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/ResNet18_imagenet_a8w8_30_epoch80_20221107-152605/tb_runs
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
INFO - Created `resnet18` model for `imagenet` dataset
          Use pre-trained model = True
*******************pre-trained****************
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
/home/ilena7440/LSQ/quan/quantizer/lsq.py:128: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 0.0001
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Validation [   20/  391]   Loss 10.426115   Top1 0.000000   Top5 0.000000   BatchTime 1.091849
INFO - Validation [   40/  391]   Loss 9.851932   Top1 0.058594   Top5 0.390625   BatchTime 0.622297
INFO - Validation [   60/  391]   Loss 10.078457   Top1 0.039062   Top5 0.299479   BatchTime 0.465876
INFO - Validation [   80/  391]   Loss 9.897195   Top1 0.029297   Top5 0.234375   BatchTime 0.390108
INFO - Validation [  100/  391]   Loss 9.835794   Top1 0.023438   Top5 0.234375   BatchTime 0.342080
INFO - Validation [  120/  391]   Loss 9.867029   Top1 0.019531   Top5 0.208333   BatchTime 0.308523
INFO - Validation [  140/  391]   Loss 9.910343   Top1 0.033482   Top5 0.251116   BatchTime 0.283879
INFO - Validation [  160/  391]   Loss 9.933602   Top1 0.219727   Top5 0.517578   BatchTime 0.264855
INFO - Validation [  180/  391]   Loss 9.767149   Top1 0.199653   Top5 0.594618   BatchTime 0.253745
INFO - Validation [  200/  391]   Loss 9.629207   Top1 0.261719   Top5 0.882812   BatchTime 0.244661
INFO - Validation [  220/  391]   Loss 9.559465   Top1 0.255682   Top5 0.948153   BatchTime 0.234255
INFO - Validation [  240/  391]   Loss 9.480269   Top1 0.341797   Top5 1.103516   BatchTime 0.226003
INFO - Validation [  260/  391]   Loss 9.385702   Top1 0.357572   Top5 1.259014   BatchTime 0.218840
INFO - Validation [  280/  391]   Loss 9.332992   Top1 0.357143   Top5 1.283482   BatchTime 0.212785
INFO - Validation [  300/  391]   Loss 9.263727   Top1 0.390625   Top5 1.419271   BatchTime 0.207503
INFO - Validation [  320/  391]   Loss 9.216885   Top1 0.427246   Top5 1.542969   BatchTime 0.202833
INFO - Validation [  340/  391]   Loss 9.160497   Top1 0.418199   Top5 1.592371   BatchTime 0.198286
INFO - Validation [  360/  391]   Loss 9.129761   Top1 0.444878   Top5 1.740451   BatchTime 0.193454
INFO - Validation [  380/  391]   Loss 9.136724   Top1 0.425576   Top5 1.679688   BatchTime 0.189227
INFO - ==> Top1: 0.414    Top5: 1.678    Loss: 9.133
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.414   Top5: 1.678] Sparsity : 0.058
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.2599, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0666, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0226, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0788, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0176, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0898, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0175, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0898, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0318, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0088, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0324, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0140, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0324, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0221, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0058, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0218, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0053, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0218, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0125, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0057, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 1.623476   Top1 62.421875   Top5 83.320312   BatchTime 0.694397   LR 0.010000
INFO - Training [0][   40/10010]   Loss 1.762317   Top1 60.175781   Top5 80.957031   BatchTime 0.511024   LR 0.010000
INFO - Training [0][   60/10010]   Loss 1.860251   Top1 58.255208   Top5 79.492188   BatchTime 0.450877   LR 0.010000
INFO - Training [0][   80/10010]   Loss 1.910840   Top1 56.748047   Top5 78.857422   BatchTime 0.419848   LR 0.010000
INFO - Training [0][  100/10010]   Loss 1.943714   Top1 55.992188   Top5 78.367188   BatchTime 0.402068   LR 0.010000
INFO - Training [0][  120/10010]   Loss 1.964394   Top1 55.475260   Top5 78.040365   BatchTime 0.390214   LR 0.010000
INFO - Training [0][  140/10010]   Loss 1.995165   Top1 54.888393   Top5 77.589286   BatchTime 0.381893   LR 0.010000
INFO - Training [0][  160/10010]   Loss 2.043444   Top1 54.174805   Top5 76.987305   BatchTime 0.374568   LR 0.010000
INFO - Training [0][  180/10010]   Loss 2.086510   Top1 53.346354   Top5 76.514757   BatchTime 0.368830   LR 0.010000
INFO - Training [0][  200/10010]   Loss 2.121077   Top1 52.632812   Top5 76.003906   BatchTime 0.364258   LR 0.010000
INFO - Training [0][  220/10010]   Loss 2.177209   Top1 51.615767   Top5 75.046165   BatchTime 0.360939   LR 0.010000
INFO - Training [0][  240/10010]   Loss 2.277046   Top1 49.817708   Top5 73.391927   BatchTime 0.358430   LR 0.010000
INFO - Training [0][  260/10010]   Loss 2.404391   Top1 47.701322   Top5 71.183894   BatchTime 0.355848   LR 0.010000
INFO - Training [0][  280/10010]   Loss 2.549811   Top1 45.404576   Top5 68.643973   BatchTime 0.353911   LR 0.010000
INFO - Training [0][  300/10010]   Loss 2.680073   Top1 43.338542   Top5 66.361979   BatchTime 0.352255   LR 0.010000
INFO - Training [0][  320/10010]   Loss 2.796985   Top1 41.562500   Top5 64.323730   BatchTime 0.350524   LR 0.010000
INFO - Training [0][  340/10010]   Loss 2.903629   Top1 39.965533   Top5 62.424173   BatchTime 0.349098   LR 0.010000
INFO - Training [0][  360/10010]   Loss 2.995888   Top1 38.470052   Top5 60.785590   BatchTime 0.347756   LR 0.010000
INFO - Training [0][  380/10010]   Loss 3.076262   Top1 37.164885   Top5 59.342105   BatchTime 0.346729   LR 0.010000
INFO - Training [0][  400/10010]   Loss 3.146610   Top1 36.021484   Top5 58.080078   BatchTime 0.345903   LR 0.010000
INFO - Training [0][  420/10010]   Loss 3.206475   Top1 35.055804   Top5 56.992188   BatchTime 0.345007   LR 0.010000
INFO - Training [0][  440/10010]   Loss 3.259128   Top1 34.245384   Top5 56.090199   BatchTime 0.344307   LR 0.010000
INFO - Training [0][  460/10010]   Loss 3.304978   Top1 33.524117   Top5 55.287024   BatchTime 0.343752   LR 0.010000
INFO - Training [0][  480/10010]   Loss 3.343710   Top1 32.835286   Top5 54.578451   BatchTime 0.342925   LR 0.010000
INFO - Training [0][  500/10010]   Loss 3.378465   Top1 32.214062   Top5 53.987500   BatchTime 0.342509   LR 0.010000
INFO - Training [0][  520/10010]   Loss 3.411629   Top1 31.649639   Top5 53.368389   BatchTime 0.341842   LR 0.010000
INFO - Training [0][  540/10010]   Loss 3.446106   Top1 31.074942   Top5 52.738715   BatchTime 0.341099   LR 0.010000
INFO - Training [0][  560/10010]   Loss 3.474771   Top1 30.620815   Top5 52.248884   BatchTime 0.340741   LR 0.010000
INFO - Training [0][  580/10010]   Loss 3.499527   Top1 30.211476   Top5 51.773976   BatchTime 0.340374   LR 0.010000
INFO - Training [0][  600/10010]   Loss 3.520519   Top1 29.854167   Top5 51.402344   BatchTime 0.339872   LR 0.010000
INFO - Training [0][  620/10010]   Loss 3.538902   Top1 29.552671   Top5 51.098790   BatchTime 0.339502   LR 0.010000
INFO - Training [0][  640/10010]   Loss 3.555139   Top1 29.252930   Top5 50.810547   BatchTime 0.339170   LR 0.010000
INFO - Training [0][  660/10010]   Loss 3.568744   Top1 28.972538   Top5 50.545691   BatchTime 0.338801   LR 0.010000
INFO - Training [0][  680/10010]   Loss 3.580808   Top1 28.724724   Top5 50.349265   BatchTime 0.338661   LR 0.010000
INFO - Training [0][  700/10010]   Loss 3.593273   Top1 28.491071   Top5 50.140625   BatchTime 0.338338   LR 0.010000
INFO - Training [0][  720/10010]   Loss 3.603734   Top1 28.300781   Top5 49.979384   BatchTime 0.337982   LR 0.010000
INFO - Training [0][  740/10010]   Loss 3.611231   Top1 28.097551   Top5 49.831081   BatchTime 0.337606   LR 0.010000
INFO - Training [0][  760/10010]   Loss 3.621377   Top1 27.929688   Top5 49.664885   BatchTime 0.337358   LR 0.010000
INFO - Training [0][  780/10010]   Loss 3.629303   Top1 27.809495   Top5 49.515224   BatchTime 0.336955   LR 0.010000
INFO - Training [0][  800/10010]   Loss 3.638452   Top1 27.659180   Top5 49.375000   BatchTime 0.336757   LR 0.010000
INFO - Training [0][  820/10010]   Loss 3.645443   Top1 27.517149   Top5 49.275915   BatchTime 0.336525   LR 0.010000
INFO - Training [0][  840/10010]   Loss 3.652062   Top1 27.384673   Top5 49.154576   BatchTime 0.336334   LR 0.010000
INFO - Training [0][  860/10010]   Loss 3.657467   Top1 27.280160   Top5 49.083394   BatchTime 0.336064   LR 0.010000
INFO - Training [0][  880/10010]   Loss 3.661993   Top1 27.191939   Top5 49.025213   BatchTime 0.335740   LR 0.010000
INFO - Training [0][  900/10010]   Loss 3.666351   Top1 27.088542   Top5 48.944444   BatchTime 0.335501   LR 0.010000
INFO - Training [0][  920/10010]   Loss 3.671938   Top1 26.976053   Top5 48.842561   BatchTime 0.335393   LR 0.010000
INFO - Training [0][  940/10010]   Loss 3.676938   Top1 26.852560   Top5 48.744182   BatchTime 0.335178   LR 0.010000
INFO - Training [0][  960/10010]   Loss 3.682766   Top1 26.727702   Top5 48.642578   BatchTime 0.334969   LR 0.010000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd0eca21ca0>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 57, in train
    outputs = model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/model/resnet.py", line 217, in forward
    return self._forward_impl(x)
  File "/home/ilena7440/LSQ/model/resnet.py", line 200, in _forward_impl
    x = self.conv1(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/func.py", line 23, in forward
    quantized_act = self.quan_a_fn(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/quantizer/lsq.py", line 260, in forward
    x = round_pass(x)
  File "/home/ilena7440/LSQ/quan/quantizer/lsq.py", line 25, in round_pass
    return (y - y_grad).detach() + y_grad
KeyboardInterrupt