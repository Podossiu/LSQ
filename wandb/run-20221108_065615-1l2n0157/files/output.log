INFO - Log file for this run: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_10_epoch80_20221108-065616/MobileNetv2_imagenet_a8w8_10_epoch80_20221108-065616.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_10_epoch80_20221108-065616/tb_runs
********************pre-trained*****************
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
255
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/LSQ/quan/quantizer/lsq.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.005
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.005
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
INFO - Validation [   20/  391]   Loss 27.494899   Top1 0.000000   Top5 0.117188   BatchTime 0.898582
INFO - Validation [   40/  391]   Loss 28.453143   Top1 0.000000   Top5 0.058594   BatchTime 0.525793
INFO - Validation [   60/  391]   Loss 27.431588   Top1 0.000000   Top5 0.338542   BatchTime 0.397776
INFO - Validation [   80/  391]   Loss 24.830360   Top1 0.000000   Top5 0.302734   BatchTime 0.334804
INFO - Validation [  100/  391]   Loss 23.232639   Top1 0.000000   Top5 0.242188   BatchTime 0.297607
INFO - Validation [  120/  391]   Loss 23.381565   Top1 0.305990   Top5 0.559896   BatchTime 0.276184
INFO - Validation [  140/  391]   Loss 24.094356   Top1 0.262277   Top5 0.479911   BatchTime 0.266636
INFO - Validation [  160/  391]   Loss 24.122167   Top1 0.229492   Top5 0.419922   BatchTime 0.258837
INFO - Validation [  180/  391]   Loss 23.957889   Top1 0.203993   Top5 0.373264   BatchTime 0.246007
INFO - Validation [  200/  391]   Loss 24.037360   Top1 0.183594   Top5 0.335938   BatchTime 0.235586
INFO - Validation [  220/  391]   Loss 24.155474   Top1 0.170455   Top5 0.482955   BatchTime 0.227851
INFO - Validation [  240/  391]   Loss 24.176308   Top1 0.156250   Top5 0.445964   BatchTime 0.221918
INFO - Validation [  260/  391]   Loss 24.185363   Top1 0.144231   Top5 0.411659   BatchTime 0.216535
INFO - Validation [  280/  391]   Loss 24.248383   Top1 0.133929   Top5 0.465960   BatchTime 0.212056
INFO - Validation [  300/  391]   Loss 24.188895   Top1 0.125000   Top5 0.606771   BatchTime 0.208400
INFO - Validation [  320/  391]   Loss 24.236692   Top1 0.117188   Top5 0.568848   BatchTime 0.205021
INFO - Validation [  340/  391]   Loss 24.114940   Top1 0.110294   Top5 0.569853   BatchTime 0.201565
INFO - Validation [  360/  391]   Loss 24.022330   Top1 0.104167   Top5 0.538194   BatchTime 0.197189
INFO - Validation [  380/  391]   Loss 24.205605   Top1 0.098684   Top5 0.509868   BatchTime 0.193172
INFO - ==> Top1: 0.096    Top5: 0.496    Loss: 24.301
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.096   Top5: 0.496] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.0418, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0624, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0610, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0296, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0329, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0609, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0207, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0220, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0871, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0175, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0351, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0728, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0206, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0135, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0847, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0161, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0112, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0934, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0163, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0374, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0619, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0165, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0094, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0723, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0106, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0088, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0764, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0083, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0094, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0794, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0148, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0243, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0478, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0082, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0093, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0565, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0065, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0102, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0647, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0065, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0276, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0359, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0108, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0098, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0455, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0084, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0111, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0580, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0036, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0219, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0308, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 5.708885   Top1 6.835938   Top5 17.695312   BatchTime 0.707113   LR 0.005000
INFO - Training [0][   40/10010]   Loss 5.441391   Top1 8.574219   Top5 21.894531   BatchTime 0.530395   LR 0.005000
INFO - Training [0][   60/10010]   Loss 5.171504   Top1 10.625000   Top5 25.989583   BatchTime 0.469193   LR 0.005000
INFO - Training [0][   80/10010]   Loss 4.895905   Top1 12.968750   Top5 30.253906   BatchTime 0.436382   LR 0.005000
INFO - Training [0][  100/10010]   Loss 4.660897   Top1 15.437500   Top5 33.929688   BatchTime 0.429487   LR 0.005000
INFO - Training [0][  120/10010]   Loss 4.472403   Top1 17.584635   Top5 36.966146   BatchTime 0.415854   LR 0.005000
INFO - Training [0][  140/10010]   Loss 4.309908   Top1 19.414062   Top5 39.620536   BatchTime 0.405491   LR 0.005000
INFO - Training [0][  160/10010]   Loss 4.175325   Top1 21.269531   Top5 41.845703   BatchTime 0.397281   LR 0.005000
INFO - Training [0][  180/10010]   Loss 4.061338   Top1 22.730035   Top5 43.732639   BatchTime 0.390975   LR 0.005000
INFO - Training [0][  200/10010]   Loss 3.965935   Top1 23.953125   Top5 45.332031   BatchTime 0.385035   LR 0.005000
INFO - Training [0][  220/10010]   Loss 3.866856   Top1 25.259233   Top5 47.006392   BatchTime 0.380878   LR 0.005000
INFO - Training [0][  240/10010]   Loss 3.779810   Top1 26.572266   Top5 48.372396   BatchTime 0.377505   LR 0.005000
INFO - Training [0][  260/10010]   Loss 3.703377   Top1 27.734375   Top5 49.669471   BatchTime 0.374744   LR 0.005000
INFO - Training [0][  280/10010]   Loss 3.644518   Top1 28.543527   Top5 50.703125   BatchTime 0.372418   LR 0.005000
INFO - Training [0][  300/10010]   Loss 3.571738   Top1 29.507812   Top5 51.901042   BatchTime 0.370547   LR 0.005000
INFO - Training [0][  320/10010]   Loss 3.502615   Top1 30.458984   Top5 53.073730   BatchTime 0.368742   LR 0.005000
INFO - Training [0][  340/10010]   Loss 3.448297   Top1 31.314338   Top5 53.993566   BatchTime 0.367244   LR 0.005000
INFO - Training [0][  360/10010]   Loss 3.394729   Top1 32.107205   Top5 54.858941   BatchTime 0.365606   LR 0.005000
INFO - Training [0][  380/10010]   Loss 3.348226   Top1 32.791941   Top5 55.649671   BatchTime 0.364756   LR 0.005000
INFO - Training [0][  400/10010]   Loss 3.302605   Top1 33.486328   Top5 56.378906   BatchTime 0.363638   LR 0.005000
INFO - Training [0][  420/10010]   Loss 3.254995   Top1 34.235491   Top5 57.139137   BatchTime 0.362512   LR 0.005000
INFO - Training [0][  440/10010]   Loss 3.214181   Top1 34.840199   Top5 57.855114   BatchTime 0.361466   LR 0.005000
INFO - Training [0][  460/10010]   Loss 3.173472   Top1 35.421196   Top5 58.541101   BatchTime 0.360529   LR 0.005000
INFO - Training [0][  480/10010]   Loss 3.135572   Top1 35.970052   Top5 59.156901   BatchTime 0.359652   LR 0.005000
INFO - Training [0][  500/10010]   Loss 3.097733   Top1 36.550000   Top5 59.759375   BatchTime 0.358707   LR 0.005000
INFO - Training [0][  520/10010]   Loss 3.066443   Top1 37.050781   Top5 60.253906   BatchTime 0.358141   LR 0.005000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4b56ef8d30>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Error in sys.excepthook:
Traceback (most recent call last):
  File "/usr/lib/python3.8/linecache.py", line 74, in checkcache
    stat = os.stat(fullname)
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 62, in train
    outputs = model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/model/mobilenet.py", line 142, in forward
    x = self.features(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/model/mobilenet.py", line 103, in forward
    return x + self.conv(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/func.py", line 22, in forward
    quantized_weight = self.quan_w_fn(self.weight)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/quantizer/lsq.py", line 132, in forward
    quant_x = (x.abs() - p_scale) / s
KeyboardInterrupt