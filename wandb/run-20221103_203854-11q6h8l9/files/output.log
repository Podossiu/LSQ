
Files already downloaded and verified
INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854.log
2022-11-03 20:38:54.948172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-03 20:38:55.090956: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-03 20:38:55.474123: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-03 20:38:55.474175: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-03 20:38:55.474182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/tb_runs
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
Files already downloaded and verified
hello
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `cifar10` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): QuanConv2d(
          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): IdentityQuan()
          (quan_a_fn): IdentityQuan()
        )
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv): Sequential(
      (0): QuanConv2d(
        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
        (quan_w_fn): SLsqQuan()
        (quan_a_fn): LsqQuan()
      )
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): QuanLinear(
      in_features=1280, out_features=10, bias=True
      (quan_w_fn): IdentityQuan()
      (quan_a_fn): IdentityQuan()
    )
  )
)
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 2.545371   Top1 10.429688   Top5 49.101562   BatchTime 0.166882
INFO - Validation [   40/   79]   Loss 2.549466   Top1 10.175781   Top5 49.941406   BatchTime 0.100428
INFO - Validation [   60/   79]   Loss 2.541519   Top1 10.117188   Top5 50.377604   BatchTime 0.077954
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.546
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - >>>>>>>> Epoch   0
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [0][   20/  391]   Loss 1.609825   Top1 68.593750   Top5 96.289062   BatchTime 0.204312   LR 0.010000
INFO - Training [0][   40/  391]   Loss 1.340406   Top1 70.078125   Top5 96.914062   BatchTime 0.151237   LR 0.010000
INFO - Training [0][   60/  391]   Loss 1.158603   Top1 71.640625   Top5 97.317708   BatchTime 0.133839   LR 0.010000
INFO - Training [0][   80/  391]   Loss 1.021951   Top1 73.320312   Top5 97.753906   BatchTime 0.124630   LR 0.010000
INFO - Training [0][  100/  391]   Loss 0.926273   Top1 74.929688   Top5 98.007812   BatchTime 0.119537   LR 0.010000
INFO - Training [0][  120/  391]   Loss 0.853287   Top1 76.100260   Top5 98.248698   BatchTime 0.115893   LR 0.010000
INFO - Training [0][  140/  391]   Loss 0.798571   Top1 77.198661   Top5 98.398438   BatchTime 0.113504   LR 0.010000
INFO - Training [0][  160/  391]   Loss 0.756398   Top1 78.076172   Top5 98.520508   BatchTime 0.111657   LR 0.010000
INFO - Training [0][  180/  391]   Loss 0.716463   Top1 78.971354   Top5 98.632812   BatchTime 0.110233   LR 0.010000
INFO - Training [0][  200/  391]   Loss 0.690449   Top1 79.394531   Top5 98.718750   BatchTime 0.109106   LR 0.010000
INFO - Training [0][  220/  391]   Loss 0.667362   Top1 79.825994   Top5 98.785511   BatchTime 0.108136   LR 0.010000
INFO - Training [0][  240/  391]   Loss 0.645425   Top1 80.322266   Top5 98.844401   BatchTime 0.107267   LR 0.010000
INFO - Training [0][  260/  391]   Loss 0.625117   Top1 80.766226   Top5 98.924279   BatchTime 0.106575   LR 0.010000
INFO - Training [0][  280/  391]   Loss 0.608360   Top1 81.130022   Top5 98.978795   BatchTime 0.106002   LR 0.010000
INFO - Training [0][  300/  391]   Loss 0.593001   Top1 81.500000   Top5 99.020833   BatchTime 0.105446   LR 0.010000
INFO - Training [0][  320/  391]   Loss 0.578693   Top1 81.857910   Top5 99.060059   BatchTime 0.105001   LR 0.010000
INFO - Training [0][  340/  391]   Loss 0.565754   Top1 82.173713   Top5 99.085478   BatchTime 0.104486   LR 0.010000
INFO - Training [0][  360/  391]   Loss 0.552444   Top1 82.567274   Top5 99.127604   BatchTime 0.103976   LR 0.010000
INFO - Training [0][  380/  391]   Loss 0.541470   Top1 82.845395   Top5 99.148849   BatchTime 0.103848   LR 0.010000
INFO - ==> Top1: 82.974    Top5: 99.162    Loss: 0.536
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [0][   20/   79]   Loss 0.457257   Top1 84.296875   Top5 99.375000   BatchTime 0.124818
INFO - Validation [0][   40/   79]   Loss 0.460853   Top1 84.414062   Top5 99.121094   BatchTime 0.079361
INFO - Validation [0][   60/   79]   Loss 0.457164   Top1 84.583333   Top5 99.166667   BatchTime 0.064395
INFO - ==> Top1: 84.440    Top5: 99.290    Loss: 0.455
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 84.440   Top5: 99.290] Sparsity : 0.312
INFO - Scoreboard best 2 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   1
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [1][   20/  391]   Loss 0.305028   Top1 89.804688   Top5 99.843750   BatchTime 0.192667   LR 0.010000
INFO - Training [1][   40/  391]   Loss 0.297974   Top1 89.628906   Top5 99.746094   BatchTime 0.145423   LR 0.010000
INFO - Training [1][   60/  391]   Loss 0.299732   Top1 89.466146   Top5 99.817708   BatchTime 0.129528   LR 0.010000
INFO - Training [1][   80/  391]   Loss 0.299293   Top1 89.541016   Top5 99.824219   BatchTime 0.121586   LR 0.010000
INFO - Training [1][  100/  391]   Loss 0.293634   Top1 89.789062   Top5 99.820312   BatchTime 0.116932   LR 0.010000
INFO - Training [1][  120/  391]   Loss 0.291124   Top1 89.778646   Top5 99.804688   BatchTime 0.113832   LR 0.010000
INFO - Training [1][  140/  391]   Loss 0.287359   Top1 89.871652   Top5 99.810268   BatchTime 0.111635   LR 0.010000
INFO - Training [1][  160/  391]   Loss 0.285582   Top1 89.936523   Top5 99.819336   BatchTime 0.109945   LR 0.010000
INFO - Training [1][  180/  391]   Loss 0.284921   Top1 89.939236   Top5 99.813368   BatchTime 0.108660   LR 0.010000
INFO - Training [1][  200/  391]   Loss 0.284304   Top1 90.007812   Top5 99.816406   BatchTime 0.107654   LR 0.010000
INFO - Training [1][  220/  391]   Loss 0.284694   Top1 90.003551   Top5 99.801136   BatchTime 0.106845   LR 0.010000
INFO - Training [1][  240/  391]   Loss 0.282457   Top1 90.091146   Top5 99.811198   BatchTime 0.106136   LR 0.010000
INFO - Training [1][  260/  391]   Loss 0.280942   Top1 90.141226   Top5 99.804688   BatchTime 0.105576   LR 0.010000
INFO - Training [1][  280/  391]   Loss 0.277678   Top1 90.270647   Top5 99.807478   BatchTime 0.105057   LR 0.010000
INFO - Training [1][  300/  391]   Loss 0.274737   Top1 90.328125   Top5 99.812500   BatchTime 0.104631   LR 0.010000
INFO - Training [1][  320/  391]   Loss 0.272120   Top1 90.427246   Top5 99.807129   BatchTime 0.104211   LR 0.010000
INFO - Training [1][  340/  391]   Loss 0.270783   Top1 90.475643   Top5 99.806985   BatchTime 0.103826   LR 0.010000
INFO - Training [1][  360/  391]   Loss 0.268988   Top1 90.536024   Top5 99.811198   BatchTime 0.103399   LR 0.010000
INFO - Training [1][  380/  391]   Loss 0.267148   Top1 90.592105   Top5 99.814967   BatchTime 0.103051   LR 0.010000
INFO - ==> Top1: 90.632    Top5: 99.816    Loss: 0.266
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [1][   20/   79]   Loss 0.435289   Top1 85.859375   Top5 99.375000   BatchTime 0.125052
INFO - Validation [1][   40/   79]   Loss 0.435762   Top1 85.976562   Top5 99.179688   BatchTime 0.079699
INFO - Validation [1][   60/   79]   Loss 0.425537   Top1 86.315104   Top5 99.283854   BatchTime 0.064433
INFO - ==> Top1: 86.580    Top5: 99.380    Loss: 0.419
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 86.580   Top5: 99.380] Sparsity : 0.553
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 84.440   Top5: 99.290] Sparsity : 0.312
INFO - Scoreboard best 3 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   2
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [2][   20/  391]   Loss 0.217454   Top1 92.187500   Top5 99.843750   BatchTime 0.195974   LR 0.010000
INFO - Training [2][   40/  391]   Loss 0.221698   Top1 92.011719   Top5 99.765625   BatchTime 0.147492   LR 0.010000
INFO - Training [2][   60/  391]   Loss 0.215159   Top1 92.265625   Top5 99.778646   BatchTime 0.131528   LR 0.010000
INFO - Training [2][   80/  391]   Loss 0.208888   Top1 92.587891   Top5 99.794922   BatchTime 0.123635   LR 0.010000
INFO - Training [2][  100/  391]   Loss 0.206257   Top1 92.679688   Top5 99.820312   BatchTime 0.118728   LR 0.010000
INFO - Training [2][  120/  391]   Loss 0.205782   Top1 92.610677   Top5 99.837240   BatchTime 0.115453   LR 0.010000
INFO - Training [2][  140/  391]   Loss 0.205449   Top1 92.594866   Top5 99.854911   BatchTime 0.113300   LR 0.010000
INFO - Training [2][  160/  391]   Loss 0.206820   Top1 92.558594   Top5 99.853516   BatchTime 0.111610   LR 0.010000
INFO - Training [2][  180/  391]   Loss 0.207663   Top1 92.560764   Top5 99.869792   BatchTime 0.110319   LR 0.010000
INFO - Training [2][  200/  391]   Loss 0.207259   Top1 92.566406   Top5 99.878906   BatchTime 0.109159   LR 0.010000
INFO - Training [2][  220/  391]   Loss 0.208546   Top1 92.549716   Top5 99.886364   BatchTime 0.108185   LR 0.010000
INFO - Training [2][  240/  391]   Loss 0.208643   Top1 92.535807   Top5 99.892578   BatchTime 0.107435   LR 0.010000
INFO - Training [2][  260/  391]   Loss 0.208516   Top1 92.569111   Top5 99.894832   BatchTime 0.106806   LR 0.010000
INFO - Training [2][  280/  391]   Loss 0.208900   Top1 92.586496   Top5 99.899554   BatchTime 0.106350   LR 0.010000
INFO - Training [2][  300/  391]   Loss 0.208549   Top1 92.609375   Top5 99.890625   BatchTime 0.105883   LR 0.010000
INFO - Training [2][  320/  391]   Loss 0.207273   Top1 92.658691   Top5 99.892578   BatchTime 0.105432   LR 0.010000
INFO - Training [2][  340/  391]   Loss 0.206439   Top1 92.690717   Top5 99.894301   BatchTime 0.104968   LR 0.010000
INFO - Training [2][  360/  391]   Loss 0.205420   Top1 92.719184   Top5 99.891493   BatchTime 0.104484   LR 0.010000
INFO - Training [2][  380/  391]   Loss 0.205028   Top1 92.722039   Top5 99.891036   BatchTime 0.104065   LR 0.010000
INFO - ==> Top1: 92.750    Top5: 99.892    Loss: 0.204
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [2][   20/   79]   Loss 0.408738   Top1 87.421875   Top5 99.609375   BatchTime 0.121557
INFO - Validation [2][   40/   79]   Loss 0.412832   Top1 87.636719   Top5 99.492188   BatchTime 0.078680
INFO - Validation [2][   60/   79]   Loss 0.407369   Top1 87.656250   Top5 99.492188   BatchTime 0.063809
INFO - ==> Top1: 87.490    Top5: 99.540    Loss: 0.407
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 87.490   Top5: 99.540] Sparsity : 0.573
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 86.580   Top5: 99.380] Sparsity : 0.553
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 84.440   Top5: 99.290] Sparsity : 0.312
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   3
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [3][   20/  391]   Loss 0.154155   Top1 94.609375   Top5 99.960938   BatchTime 0.184925   LR 0.010000
INFO - Training [3][   40/  391]   Loss 0.144200   Top1 94.882812   Top5 99.980469   BatchTime 0.142064   LR 0.010000
INFO - Training [3][   60/  391]   Loss 0.152493   Top1 94.765625   Top5 99.973958   BatchTime 0.128352   LR 0.010000
INFO - Training [3][   80/  391]   Loss 0.154526   Top1 94.638672   Top5 99.970703   BatchTime 0.121244   LR 0.010000
INFO - Training [3][  100/  391]   Loss 0.160125   Top1 94.507812   Top5 99.968750   BatchTime 0.116977   LR 0.010000
INFO - Training [3][  120/  391]   Loss 0.163823   Top1 94.388021   Top5 99.967448   BatchTime 0.114039   LR 0.010000
INFO - Training [3][  140/  391]   Loss 0.163247   Top1 94.397321   Top5 99.960938   BatchTime 0.111865   LR 0.010000
INFO - Training [3][  160/  391]   Loss 0.164331   Top1 94.331055   Top5 99.956055   BatchTime 0.110246   LR 0.010000
INFO - Training [3][  180/  391]   Loss 0.164981   Top1 94.270833   Top5 99.960938   BatchTime 0.109111   LR 0.010000
INFO - Training [3][  200/  391]   Loss 0.164800   Top1 94.226562   Top5 99.957031   BatchTime 0.108133   LR 0.010000
INFO - Training [3][  220/  391]   Loss 0.164995   Top1 94.204545   Top5 99.960938   BatchTime 0.107268   LR 0.010000
INFO - Training [3][  240/  391]   Loss 0.164126   Top1 94.212240   Top5 99.960938   BatchTime 0.106569   LR 0.010000
INFO - Training [3][  260/  391]   Loss 0.164969   Top1 94.182692   Top5 99.963942   BatchTime 0.105997   LR 0.010000
INFO - Training [3][  280/  391]   Loss 0.165653   Top1 94.160156   Top5 99.958147   BatchTime 0.105426   LR 0.010000
INFO - Training [3][  300/  391]   Loss 0.164622   Top1 94.205729   Top5 99.955729   BatchTime 0.105012   LR 0.010000
INFO - Training [3][  320/  391]   Loss 0.164245   Top1 94.223633   Top5 99.956055   BatchTime 0.104682   LR 0.010000
INFO - Training [3][  340/  391]   Loss 0.163466   Top1 94.230239   Top5 99.956342   BatchTime 0.104279   LR 0.010000
INFO - Training [3][  360/  391]   Loss 0.163210   Top1 94.214410   Top5 99.958767   BatchTime 0.103855   LR 0.010000
INFO - Training [3][  380/  391]   Loss 0.162270   Top1 94.245477   Top5 99.960938   BatchTime 0.103445   LR 0.010000
INFO - ==> Top1: 94.250    Top5: 99.962    Loss: 0.162
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [3][   20/   79]   Loss 0.427533   Top1 88.125000   Top5 99.492188   BatchTime 0.121673
INFO - Validation [3][   40/   79]   Loss 0.420574   Top1 88.125000   Top5 99.492188   BatchTime 0.078248
INFO - Validation [3][   60/   79]   Loss 0.410509   Top1 88.359375   Top5 99.544271   BatchTime 0.063645
INFO - ==> Top1: 88.260    Top5: 99.600    Loss: 0.411
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 88.260   Top5: 99.600] Sparsity : 0.581
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 87.490   Top5: 99.540] Sparsity : 0.573
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 86.580   Top5: 99.380] Sparsity : 0.553
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   4
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [4][   20/  391]   Loss 0.136950   Top1 94.804688   Top5 100.000000   BatchTime 0.183420   LR 0.010000
INFO - Training [4][   40/  391]   Loss 0.145553   Top1 94.531250   Top5 99.980469   BatchTime 0.141196   LR 0.010000
INFO - Training [4][   60/  391]   Loss 0.141464   Top1 94.635417   Top5 99.986979   BatchTime 0.127084   LR 0.010000
INFO - Training [4][   80/  391]   Loss 0.138899   Top1 94.765625   Top5 99.980469   BatchTime 0.120156   LR 0.010000
INFO - Training [4][  100/  391]   Loss 0.142200   Top1 94.656250   Top5 99.976562   BatchTime 0.115699   LR 0.010000
INFO - Training [4][  120/  391]   Loss 0.142015   Top1 94.733073   Top5 99.973958   BatchTime 0.112918   LR 0.010000
INFO - Training [4][  140/  391]   Loss 0.142733   Top1 94.760045   Top5 99.972098   BatchTime 0.110938   LR 0.010000
INFO - Training [4][  160/  391]   Loss 0.140332   Top1 94.907227   Top5 99.970703   BatchTime 0.109465   LR 0.010000
INFO - Training [4][  180/  391]   Loss 0.139920   Top1 94.895833   Top5 99.969618   BatchTime 0.108355   LR 0.010000
INFO - Training [4][  200/  391]   Loss 0.139004   Top1 94.941406   Top5 99.968750   BatchTime 0.107499   LR 0.010000
INFO - Training [4][  220/  391]   Loss 0.138855   Top1 94.946733   Top5 99.971591   BatchTime 0.106744   LR 0.010000
INFO - Training [4][  240/  391]   Loss 0.136963   Top1 95.029297   Top5 99.970703   BatchTime 0.106150   LR 0.010000
INFO - Training [4][  260/  391]   Loss 0.136576   Top1 95.090144   Top5 99.969952   BatchTime 0.105587   LR 0.010000
INFO - Training [4][  280/  391]   Loss 0.137435   Top1 95.058594   Top5 99.960938   BatchTime 0.105127   LR 0.010000
INFO - Training [4][  300/  391]   Loss 0.137054   Top1 95.075521   Top5 99.963542   BatchTime 0.104710   LR 0.010000
INFO - Training [4][  320/  391]   Loss 0.136556   Top1 95.085449   Top5 99.963379   BatchTime 0.104432   LR 0.010000
INFO - Training [4][  340/  391]   Loss 0.136064   Top1 95.103401   Top5 99.963235   BatchTime 0.104092   LR 0.010000
INFO - Training [4][  360/  391]   Loss 0.136149   Top1 95.075955   Top5 99.965278   BatchTime 0.103695   LR 0.010000
INFO - Training [4][  380/  391]   Loss 0.136097   Top1 95.065789   Top5 99.967105   BatchTime 0.103330   LR 0.010000
INFO - ==> Top1: 95.068    Top5: 99.968    Loss: 0.136
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [4][   20/   79]   Loss 0.381071   Top1 88.593750   Top5 99.531250   BatchTime 0.121831
INFO - Validation [4][   40/   79]   Loss 0.399101   Top1 88.457031   Top5 99.492188   BatchTime 0.075307
INFO - Validation [4][   60/   79]   Loss 0.388080   Top1 88.723958   Top5 99.518229   BatchTime 0.062040
INFO - ==> Top1: 88.690    Top5: 99.570    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.690   Top5: 99.570] Sparsity : 0.587
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 88.260   Top5: 99.600] Sparsity : 0.581
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 87.490   Top5: 99.540] Sparsity : 0.573
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   5
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [5][   20/  391]   Loss 0.120600   Top1 95.898438   Top5 100.000000   BatchTime 0.183364   LR 0.010000
INFO - Training [5][   40/  391]   Loss 0.119465   Top1 95.703125   Top5 100.000000   BatchTime 0.141476   LR 0.010000
INFO - Training [5][   60/  391]   Loss 0.115450   Top1 95.768229   Top5 100.000000   BatchTime 0.127454   LR 0.010000
INFO - Training [5][   80/  391]   Loss 0.114381   Top1 95.830078   Top5 99.990234   BatchTime 0.120473   LR 0.010000
INFO - Training [5][  100/  391]   Loss 0.113956   Top1 95.953125   Top5 99.984375   BatchTime 0.116086   LR 0.010000
INFO - Training [5][  120/  391]   Loss 0.116976   Top1 95.898438   Top5 99.980469   BatchTime 0.113368   LR 0.010000
INFO - Training [5][  140/  391]   Loss 0.114201   Top1 95.993304   Top5 99.983259   BatchTime 0.111465   LR 0.010000
INFO - Training [5][  160/  391]   Loss 0.114655   Top1 95.981445   Top5 99.985352   BatchTime 0.110027   LR 0.010000
INFO - Training [5][  180/  391]   Loss 0.114268   Top1 95.933160   Top5 99.986979   BatchTime 0.108808   LR 0.010000
INFO - Training [5][  200/  391]   Loss 0.113606   Top1 95.992188   Top5 99.980469   BatchTime 0.107935   LR 0.010000
INFO - Training [5][  220/  391]   Loss 0.114611   Top1 95.955256   Top5 99.978693   BatchTime 0.107180   LR 0.010000
INFO - Training [5][  240/  391]   Loss 0.113927   Top1 95.973307   Top5 99.980469   BatchTime 0.106498   LR 0.010000
INFO - Training [5][  260/  391]   Loss 0.113028   Top1 96.009615   Top5 99.981971   BatchTime 0.105976   LR 0.010000
INFO - Training [5][  280/  391]   Loss 0.112743   Top1 95.993304   Top5 99.980469   BatchTime 0.105461   LR 0.010000
INFO - Training [5][  300/  391]   Loss 0.112446   Top1 95.994792   Top5 99.981771   BatchTime 0.105114   LR 0.010000
INFO - Training [5][  320/  391]   Loss 0.111841   Top1 96.025391   Top5 99.982910   BatchTime 0.104754   LR 0.010000
INFO - Training [5][  340/  391]   Loss 0.113025   Top1 95.985754   Top5 99.979320   BatchTime 0.104338   LR 0.010000
INFO - Training [5][  360/  391]   Loss 0.112022   Top1 96.032986   Top5 99.978299   BatchTime 0.103929   LR 0.010000
INFO - Training [5][  380/  391]   Loss 0.113326   Top1 95.976562   Top5 99.979441   BatchTime 0.103540   LR 0.010000
INFO - ==> Top1: 95.960    Top5: 99.980    Loss: 0.114
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [5][   20/   79]   Loss 0.383690   Top1 88.750000   Top5 99.765625   BatchTime 0.117914
INFO - Validation [5][   40/   79]   Loss 0.399325   Top1 88.632812   Top5 99.511719   BatchTime 0.071796
INFO - Validation [5][   60/   79]   Loss 0.395717   Top1 88.880208   Top5 99.531250   BatchTime 0.059597
INFO - ==> Top1: 88.750    Top5: 99.570    Loss: 0.397
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.750   Top5: 99.570] Sparsity : 0.592
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.690   Top5: 99.570] Sparsity : 0.587
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 88.260   Top5: 99.600] Sparsity : 0.581
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   6
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [6][   20/  391]   Loss 0.109599   Top1 96.210938   Top5 99.960938   BatchTime 0.181189   LR 0.010000
INFO - Training [6][   40/  391]   Loss 0.106346   Top1 96.269531   Top5 99.980469   BatchTime 0.140796   LR 0.010000
INFO - Training [6][   60/  391]   Loss 0.103157   Top1 96.341146   Top5 99.986979   BatchTime 0.125846   LR 0.010000
INFO - Training [6][   80/  391]   Loss 0.100952   Top1 96.455078   Top5 99.970703   BatchTime 0.119790   LR 0.010000
INFO - Training [6][  100/  391]   Loss 0.102008   Top1 96.375000   Top5 99.968750   BatchTime 0.115968   LR 0.010000
INFO - Training [6][  120/  391]   Loss 0.101597   Top1 96.399740   Top5 99.973958   BatchTime 0.113432   LR 0.010000
INFO - Training [6][  140/  391]   Loss 0.101968   Top1 96.411830   Top5 99.972098   BatchTime 0.111430   LR 0.010000
INFO - Training [6][  160/  391]   Loss 0.101052   Top1 96.406250   Top5 99.975586   BatchTime 0.109936   LR 0.010000
INFO - Training [6][  180/  391]   Loss 0.101732   Top1 96.371528   Top5 99.973958   BatchTime 0.108785   LR 0.010000
INFO - Training [6][  200/  391]   Loss 0.101653   Top1 96.390625   Top5 99.976562   BatchTime 0.107739   LR 0.010000
INFO - Training [6][  220/  391]   Loss 0.100596   Top1 96.424006   Top5 99.978693   BatchTime 0.106878   LR 0.010000
INFO - Training [6][  240/  391]   Loss 0.099729   Top1 96.425781   Top5 99.980469   BatchTime 0.106271   LR 0.010000
INFO - Training [6][  260/  391]   Loss 0.099865   Top1 96.418269   Top5 99.981971   BatchTime 0.105701   LR 0.010000
INFO - Training [6][  280/  391]   Loss 0.099118   Top1 96.459263   Top5 99.983259   BatchTime 0.105252   LR 0.010000
INFO - Training [6][  300/  391]   Loss 0.099108   Top1 96.442708   Top5 99.984375   BatchTime 0.104919   LR 0.010000
INFO - Training [6][  320/  391]   Loss 0.098990   Top1 96.445312   Top5 99.985352   BatchTime 0.104504   LR 0.010000
INFO - Training [6][  340/  391]   Loss 0.099284   Top1 96.429228   Top5 99.986213   BatchTime 0.104104   LR 0.010000
INFO - Training [6][  360/  391]   Loss 0.099027   Top1 96.430122   Top5 99.986979   BatchTime 0.103710   LR 0.010000
INFO - Training [6][  380/  391]   Loss 0.099010   Top1 96.430921   Top5 99.985609   BatchTime 0.103351   LR 0.010000
INFO - ==> Top1: 96.436    Top5: 99.986    Loss: 0.099
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [6][   20/   79]   Loss 0.383507   Top1 88.789062   Top5 99.570312   BatchTime 0.119504
INFO - Validation [6][   40/   79]   Loss 0.400020   Top1 89.140625   Top5 99.531250   BatchTime 0.072435
INFO - Validation [6][   60/   79]   Loss 0.394653   Top1 89.322917   Top5 99.570312   BatchTime 0.058621
INFO - ==> Top1: 89.330    Top5: 99.590    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 89.330   Top5: 99.590] Sparsity : 0.606
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.750   Top5: 99.570] Sparsity : 0.592
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.690   Top5: 99.570] Sparsity : 0.587
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   7
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [7][   20/  391]   Loss 0.079856   Top1 97.187500   Top5 100.000000   BatchTime 0.179439   LR 0.010000
INFO - Training [7][   40/  391]   Loss 0.086817   Top1 96.894531   Top5 100.000000   BatchTime 0.139244   LR 0.010000
INFO - Training [7][   60/  391]   Loss 0.088994   Top1 96.848958   Top5 100.000000   BatchTime 0.126045   LR 0.010000
INFO - Training [7][   80/  391]   Loss 0.087271   Top1 96.904297   Top5 100.000000   BatchTime 0.120711   LR 0.010000
INFO - Training [7][  100/  391]   Loss 0.087127   Top1 96.898438   Top5 100.000000   BatchTime 0.116524   LR 0.010000
INFO - Training [7][  120/  391]   Loss 0.087545   Top1 96.927083   Top5 99.993490   BatchTime 0.113739   LR 0.010000
INFO - Training [7][  140/  391]   Loss 0.089137   Top1 96.886161   Top5 99.994420   BatchTime 0.111812   LR 0.010000
INFO - Training [7][  160/  391]   Loss 0.088432   Top1 96.865234   Top5 99.995117   BatchTime 0.110285   LR 0.010000
INFO - Training [7][  180/  391]   Loss 0.089088   Top1 96.857639   Top5 99.995660   BatchTime 0.109202   LR 0.010000
INFO - Training [7][  200/  391]   Loss 0.090723   Top1 96.820312   Top5 99.992188   BatchTime 0.108282   LR 0.010000
INFO - Training [7][  220/  391]   Loss 0.091658   Top1 96.775568   Top5 99.992898   BatchTime 0.107567   LR 0.010000
INFO - Training [7][  240/  391]   Loss 0.092157   Top1 96.767578   Top5 99.993490   BatchTime 0.106880   LR 0.010000
INFO - Training [7][  260/  391]   Loss 0.092745   Top1 96.775841   Top5 99.990986   BatchTime 0.106343   LR 0.010000
INFO - Training [7][  280/  391]   Loss 0.094118   Top1 96.715960   Top5 99.991629   BatchTime 0.105850   LR 0.010000
INFO - Training [7][  300/  391]   Loss 0.094635   Top1 96.682292   Top5 99.992188   BatchTime 0.105416   LR 0.010000
INFO - Training [7][  320/  391]   Loss 0.094553   Top1 96.691895   Top5 99.990234   BatchTime 0.105025   LR 0.010000
INFO - Training [7][  340/  391]   Loss 0.094831   Top1 96.700368   Top5 99.988511   BatchTime 0.104655   LR 0.010000
INFO - Training [7][  360/  391]   Loss 0.094062   Top1 96.723090   Top5 99.989149   BatchTime 0.104239   LR 0.010000
INFO - Training [7][  380/  391]   Loss 0.093911   Top1 96.731086   Top5 99.989720   BatchTime 0.103834   LR 0.010000
INFO - ==> Top1: 96.712    Top5: 99.990    Loss: 0.094
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [7][   20/   79]   Loss 0.388258   Top1 89.531250   Top5 99.453125   BatchTime 0.121397
INFO - Validation [7][   40/   79]   Loss 0.394273   Top1 89.453125   Top5 99.375000   BatchTime 0.073738
INFO - Validation [7][   60/   79]   Loss 0.394130   Top1 89.531250   Top5 99.453125   BatchTime 0.060899
INFO - ==> Top1: 89.470    Top5: 99.540    Loss: 0.392
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 89.470   Top5: 99.540] Sparsity : 0.618
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 89.330   Top5: 99.590] Sparsity : 0.606
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 88.750   Top5: 99.570] Sparsity : 0.592
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   8
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [8][   20/  391]   Loss 0.081521   Top1 97.343750   Top5 100.000000   BatchTime 0.177062   LR 0.010000
INFO - Training [8][   40/  391]   Loss 0.081875   Top1 97.285156   Top5 100.000000   BatchTime 0.138389   LR 0.010000
INFO - Training [8][   60/  391]   Loss 0.083566   Top1 97.187500   Top5 99.973958   BatchTime 0.125575   LR 0.010000
INFO - Training [8][   80/  391]   Loss 0.082940   Top1 97.167969   Top5 99.980469   BatchTime 0.119041   LR 0.010000
INFO - Training [8][  100/  391]   Loss 0.083013   Top1 97.164062   Top5 99.984375   BatchTime 0.115261   LR 0.010000
INFO - Training [8][  120/  391]   Loss 0.083931   Top1 97.102865   Top5 99.986979   BatchTime 0.112622   LR 0.010000
INFO - Training [8][  140/  391]   Loss 0.084754   Top1 97.087054   Top5 99.983259   BatchTime 0.110628   LR 0.010000
INFO - Training [8][  160/  391]   Loss 0.085469   Top1 97.036133   Top5 99.985352   BatchTime 0.109295   LR 0.010000
INFO - Training [8][  180/  391]   Loss 0.084975   Top1 97.026910   Top5 99.986979   BatchTime 0.108311   LR 0.010000
INFO - Training [8][  200/  391]   Loss 0.085258   Top1 97.003906   Top5 99.988281   BatchTime 0.107425   LR 0.010000
INFO - Training [8][  220/  391]   Loss 0.085127   Top1 97.002841   Top5 99.989347   BatchTime 0.106664   LR 0.010000
INFO - Training [8][  240/  391]   Loss 0.085286   Top1 96.998698   Top5 99.990234   BatchTime 0.106140   LR 0.010000
INFO - Training [8][  260/  391]   Loss 0.085223   Top1 97.034255   Top5 99.987981   BatchTime 0.105658   LR 0.010000
INFO - Training [8][  280/  391]   Loss 0.085577   Top1 97.022879   Top5 99.988839   BatchTime 0.105241   LR 0.010000
INFO - Training [8][  300/  391]   Loss 0.085705   Top1 97.028646   Top5 99.989583   BatchTime 0.104875   LR 0.010000
INFO - Training [8][  320/  391]   Loss 0.087213   Top1 96.984863   Top5 99.987793   BatchTime 0.104574   LR 0.010000
INFO - Training [8][  340/  391]   Loss 0.086432   Top1 97.003676   Top5 99.988511   BatchTime 0.104267   LR 0.010000
INFO - Training [8][  360/  391]   Loss 0.087440   Top1 96.948785   Top5 99.989149   BatchTime 0.103876   LR 0.010000
INFO - Training [8][  380/  391]   Loss 0.087393   Top1 96.961349   Top5 99.989720   BatchTime 0.103516   LR 0.010000
INFO - ==> Top1: 96.968    Top5: 99.990    Loss: 0.087
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [8][   20/   79]   Loss 0.373863   Top1 90.078125   Top5 99.570312   BatchTime 0.119255
INFO - Validation [8][   40/   79]   Loss 0.401041   Top1 89.511719   Top5 99.453125   BatchTime 0.072612
INFO - Validation [8][   60/   79]   Loss 0.390465   Top1 89.635417   Top5 99.505208   BatchTime 0.058489
INFO - ==> Top1: 89.600    Top5: 99.560    Loss: 0.388
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.600   Top5: 99.560] Sparsity : 0.629
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.470   Top5: 99.540] Sparsity : 0.618
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 89.330   Top5: 99.590] Sparsity : 0.606
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   9
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [9][   20/  391]   Loss 0.073297   Top1 97.226562   Top5 100.000000   BatchTime 0.165492   LR 0.010000
INFO - Training [9][   40/  391]   Loss 0.078217   Top1 97.265625   Top5 100.000000   BatchTime 0.135439   LR 0.010000
INFO - Training [9][   60/  391]   Loss 0.075167   Top1 97.317708   Top5 100.000000   BatchTime 0.123842   LR 0.010000
INFO - Training [9][   80/  391]   Loss 0.076060   Top1 97.294922   Top5 99.990234   BatchTime 0.117875   LR 0.010000
INFO - Training [9][  100/  391]   Loss 0.075794   Top1 97.281250   Top5 99.984375   BatchTime 0.115001   LR 0.010000
INFO - Training [9][  120/  391]   Loss 0.077671   Top1 97.167969   Top5 99.980469   BatchTime 0.112487   LR 0.010000
INFO - Training [9][  140/  391]   Loss 0.076921   Top1 97.248884   Top5 99.983259   BatchTime 0.110558   LR 0.010000
INFO - Training [9][  160/  391]   Loss 0.075887   Top1 97.255859   Top5 99.985352   BatchTime 0.109333   LR 0.010000
INFO - Training [9][  180/  391]   Loss 0.076067   Top1 97.248264   Top5 99.986979   BatchTime 0.108263   LR 0.010000
INFO - Training [9][  200/  391]   Loss 0.075309   Top1 97.265625   Top5 99.988281   BatchTime 0.107505   LR 0.010000
INFO - Training [9][  220/  391]   Loss 0.075562   Top1 97.258523   Top5 99.989347   BatchTime 0.106814   LR 0.010000
INFO - Training [9][  240/  391]   Loss 0.074863   Top1 97.298177   Top5 99.990234   BatchTime 0.106234   LR 0.010000
INFO - Training [9][  260/  391]   Loss 0.076089   Top1 97.265625   Top5 99.990986   BatchTime 0.105746   LR 0.010000
INFO - Training [9][  280/  391]   Loss 0.076453   Top1 97.262835   Top5 99.991629   BatchTime 0.105277   LR 0.010000
INFO - Training [9][  300/  391]   Loss 0.075801   Top1 97.296875   Top5 99.992188   BatchTime 0.104972   LR 0.010000
INFO - Training [9][  320/  391]   Loss 0.077295   Top1 97.258301   Top5 99.992676   BatchTime 0.104606   LR 0.010000
INFO - Training [9][  340/  391]   Loss 0.077810   Top1 97.228860   Top5 99.993107   BatchTime 0.104210   LR 0.010000
INFO - Training [9][  360/  391]   Loss 0.078210   Top1 97.220052   Top5 99.993490   BatchTime 0.103844   LR 0.010000
INFO - Training [9][  380/  391]   Loss 0.078733   Top1 97.177220   Top5 99.991776   BatchTime 0.103455   LR 0.010000
INFO - ==> Top1: 97.160    Top5: 99.992    Loss: 0.079
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [9][   20/   79]   Loss 0.358979   Top1 89.960938   Top5 99.765625   BatchTime 0.119315
INFO - Validation [9][   40/   79]   Loss 0.381805   Top1 89.687500   Top5 99.531250   BatchTime 0.072535
INFO - Validation [9][   60/   79]   Loss 0.386671   Top1 89.648438   Top5 99.583333   BatchTime 0.058228
INFO - ==> Top1: 89.640    Top5: 99.620    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.640   Top5: 99.620] Sparsity : 0.672
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.600   Top5: 99.560] Sparsity : 0.629
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.470   Top5: 99.540] Sparsity : 0.618
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  10
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [10][   20/  391]   Loss 0.082076   Top1 96.992188   Top5 100.000000   BatchTime 0.172301   LR 0.010000
INFO - Training [10][   40/  391]   Loss 0.074084   Top1 97.519531   Top5 100.000000   BatchTime 0.138107   LR 0.010000
INFO - Training [10][   60/  391]   Loss 0.074461   Top1 97.473958   Top5 100.000000   BatchTime 0.125455   LR 0.010000
INFO - Training [10][   80/  391]   Loss 0.071641   Top1 97.578125   Top5 100.000000   BatchTime 0.119024   LR 0.010000
INFO - Training [10][  100/  391]   Loss 0.072943   Top1 97.531250   Top5 100.000000   BatchTime 0.115216   LR 0.010000
INFO - Training [10][  120/  391]   Loss 0.073592   Top1 97.571615   Top5 100.000000   BatchTime 0.112565   LR 0.010000
INFO - Training [10][  140/  391]   Loss 0.076191   Top1 97.460938   Top5 100.000000   BatchTime 0.110739   LR 0.010000
INFO - Training [10][  160/  391]   Loss 0.076333   Top1 97.421875   Top5 100.000000   BatchTime 0.109462   LR 0.010000
INFO - Training [10][  180/  391]   Loss 0.075761   Top1 97.434896   Top5 99.995660   BatchTime 0.108297   LR 0.010000
INFO - Training [10][  200/  391]   Loss 0.076579   Top1 97.378906   Top5 99.996094   BatchTime 0.107366   LR 0.010000
INFO - Training [10][  220/  391]   Loss 0.079077   Top1 97.276278   Top5 99.996449   BatchTime 0.106621   LR 0.010000
INFO - Training [10][  240/  391]   Loss 0.078496   Top1 97.294922   Top5 99.996745   BatchTime 0.105971   LR 0.010000
INFO - Training [10][  260/  391]   Loss 0.078853   Top1 97.277644   Top5 99.996995   BatchTime 0.105577   LR 0.010000
INFO - Training [10][  280/  391]   Loss 0.079847   Top1 97.234933   Top5 99.997210   BatchTime 0.105152   LR 0.010000
INFO - Training [10][  300/  391]   Loss 0.080912   Top1 97.203125   Top5 99.997396   BatchTime 0.104841   LR 0.010000
INFO - Training [10][  320/  391]   Loss 0.081442   Top1 97.167969   Top5 99.995117   BatchTime 0.104511   LR 0.010000
INFO - Training [10][  340/  391]   Loss 0.082339   Top1 97.125460   Top5 99.995404   BatchTime 0.104161   LR 0.010000
INFO - Training [10][  360/  391]   Loss 0.082908   Top1 97.092014   Top5 99.995660   BatchTime 0.103806   LR 0.010000
INFO - Training [10][  380/  391]   Loss 0.083601   Top1 97.068257   Top5 99.995888   BatchTime 0.103465   LR 0.010000
INFO - ==> Top1: 97.022    Top5: 99.996    Loss: 0.085
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [10][   20/   79]   Loss 0.380946   Top1 89.570312   Top5 99.726562   BatchTime 0.117401
INFO - Validation [10][   40/   79]   Loss 0.394408   Top1 89.453125   Top5 99.492188   BatchTime 0.071505
INFO - Validation [10][   60/   79]   Loss 0.390821   Top1 89.661458   Top5 99.505208   BatchTime 0.056568
INFO - ==> Top1: 89.740    Top5: 99.550    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 89.740   Top5: 99.550] Sparsity : 0.688
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 89.640   Top5: 99.620] Sparsity : 0.672
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 89.600   Top5: 99.560] Sparsity : 0.629
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  11
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [11][   20/  391]   Loss 0.081434   Top1 97.031250   Top5 100.000000   BatchTime 0.169518   LR 0.010000
INFO - Training [11][   40/  391]   Loss 0.077464   Top1 97.207031   Top5 100.000000   BatchTime 0.137005   LR 0.010000
INFO - Training [11][   60/  391]   Loss 0.077390   Top1 97.135417   Top5 100.000000   BatchTime 0.124815   LR 0.010000
INFO - Training [11][   80/  391]   Loss 0.081064   Top1 96.962891   Top5 100.000000   BatchTime 0.118701   LR 0.010000
INFO - Training [11][  100/  391]   Loss 0.080229   Top1 96.968750   Top5 100.000000   BatchTime 0.114801   LR 0.010000
INFO - Training [11][  120/  391]   Loss 0.081080   Top1 96.933594   Top5 100.000000   BatchTime 0.112429   LR 0.010000
INFO - Training [11][  140/  391]   Loss 0.081521   Top1 96.930804   Top5 100.000000   BatchTime 0.110701   LR 0.010000
INFO - Training [11][  160/  391]   Loss 0.082504   Top1 96.889648   Top5 100.000000   BatchTime 0.110201   LR 0.010000
INFO - Training [11][  180/  391]   Loss 0.082051   Top1 96.927083   Top5 99.995660   BatchTime 0.109030   LR 0.010000
INFO - Training [11][  200/  391]   Loss 0.081702   Top1 96.933594   Top5 99.992188   BatchTime 0.108264   LR 0.010000
INFO - Training [11][  220/  391]   Loss 0.081722   Top1 96.960227   Top5 99.992898   BatchTime 0.107517   LR 0.010000
INFO - Training [11][  240/  391]   Loss 0.082622   Top1 96.959635   Top5 99.993490   BatchTime 0.106912   LR 0.010000
INFO - Training [11][  260/  391]   Loss 0.082539   Top1 96.995192   Top5 99.993990   BatchTime 0.106374   LR 0.010000
INFO - Training [11][  280/  391]   Loss 0.082474   Top1 96.992188   Top5 99.994420   BatchTime 0.105998   LR 0.010000
INFO - Training [11][  300/  391]   Loss 0.081945   Top1 97.036458   Top5 99.994792   BatchTime 0.105588   LR 0.010000
INFO - Training [11][  320/  391]   Loss 0.081817   Top1 97.045898   Top5 99.995117   BatchTime 0.105249   LR 0.010000
INFO - Training [11][  340/  391]   Loss 0.081770   Top1 97.068015   Top5 99.995404   BatchTime 0.104940   LR 0.010000
INFO - Training [11][  360/  391]   Loss 0.082391   Top1 97.042101   Top5 99.993490   BatchTime 0.104540   LR 0.010000
INFO - Training [11][  380/  391]   Loss 0.083289   Top1 97.023026   Top5 99.993832   BatchTime 0.104145   LR 0.010000
INFO - ==> Top1: 97.026    Top5: 99.992    Loss: 0.083
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [11][   20/   79]   Loss 0.389760   Top1 89.882812   Top5 99.609375   BatchTime 0.120544
INFO - Validation [11][   40/   79]   Loss 0.394084   Top1 89.863281   Top5 99.472656   BatchTime 0.073049
INFO - Validation [11][   60/   79]   Loss 0.387710   Top1 90.000000   Top5 99.505208   BatchTime 0.058199
INFO - ==> Top1: 89.760    Top5: 99.540    Loss: 0.389
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.760   Top5: 99.540] Sparsity : 0.692
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 89.740   Top5: 99.550] Sparsity : 0.688
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 89.640   Top5: 99.620] Sparsity : 0.672
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  12
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [12][   20/  391]   Loss 0.071940   Top1 97.500000   Top5 100.000000   BatchTime 0.166598   LR 0.010000
INFO - Training [12][   40/  391]   Loss 0.074878   Top1 97.480469   Top5 100.000000   BatchTime 0.135071   LR 0.010000
INFO - Training [12][   60/  391]   Loss 0.073861   Top1 97.500000   Top5 100.000000   BatchTime 0.123648   LR 0.010000
INFO - Training [12][   80/  391]   Loss 0.072009   Top1 97.558594   Top5 100.000000   BatchTime 0.118107   LR 0.010000
INFO - Training [12][  100/  391]   Loss 0.071592   Top1 97.570312   Top5 100.000000   BatchTime 0.114800   LR 0.010000
INFO - Training [12][  120/  391]   Loss 0.074040   Top1 97.454427   Top5 100.000000   BatchTime 0.112523   LR 0.010000
INFO - Training [12][  140/  391]   Loss 0.074869   Top1 97.416295   Top5 99.994420   BatchTime 0.110840   LR 0.010000
INFO - Training [12][  160/  391]   Loss 0.074117   Top1 97.392578   Top5 99.995117   BatchTime 0.109556   LR 0.010000
INFO - Training [12][  180/  391]   Loss 0.074194   Top1 97.374132   Top5 99.995660   BatchTime 0.108483   LR 0.010000
INFO - Training [12][  200/  391]   Loss 0.074632   Top1 97.367188   Top5 99.996094   BatchTime 0.107713   LR 0.010000
INFO - Training [12][  220/  391]   Loss 0.075594   Top1 97.301136   Top5 99.996449   BatchTime 0.107048   LR 0.010000
INFO - Training [12][  240/  391]   Loss 0.075449   Top1 97.317708   Top5 99.996745   BatchTime 0.106449   LR 0.010000
INFO - Training [12][  260/  391]   Loss 0.075703   Top1 97.301683   Top5 99.996995   BatchTime 0.106111   LR 0.010000
INFO - Training [12][  280/  391]   Loss 0.077219   Top1 97.254464   Top5 99.991629   BatchTime 0.105600   LR 0.010000
INFO - Training [12][  300/  391]   Loss 0.077379   Top1 97.257812   Top5 99.992188   BatchTime 0.105166   LR 0.010000
INFO - Training [12][  320/  391]   Loss 0.077549   Top1 97.260742   Top5 99.992676   BatchTime 0.104788   LR 0.010000
INFO - Training [12][  340/  391]   Loss 0.077700   Top1 97.263327   Top5 99.993107   BatchTime 0.104423   LR 0.010000
INFO - Training [12][  360/  391]   Loss 0.078109   Top1 97.263455   Top5 99.991319   BatchTime 0.104045   LR 0.010000
INFO - Training [12][  380/  391]   Loss 0.078297   Top1 97.253289   Top5 99.989720   BatchTime 0.103683   LR 0.010000
INFO - ==> Top1: 97.252    Top5: 99.988    Loss: 0.078
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [12][   20/   79]   Loss 0.392813   Top1 89.648438   Top5 99.531250   BatchTime 0.122876
INFO - Validation [12][   40/   79]   Loss 0.404980   Top1 89.414062   Top5 99.414062   BatchTime 0.074302
INFO - Validation [12][   60/   79]   Loss 0.401910   Top1 89.752604   Top5 99.414062   BatchTime 0.058090
INFO - ==> Top1: 89.720    Top5: 99.460    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.760   Top5: 99.540] Sparsity : 0.692
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 89.740   Top5: 99.550] Sparsity : 0.688
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.720   Top5: 99.460] Sparsity : 0.695
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  13
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [13][   20/  391]   Loss 0.062299   Top1 97.968750   Top5 100.000000   BatchTime 0.166882   LR 0.010000
INFO - Training [13][   40/  391]   Loss 0.064047   Top1 97.949219   Top5 100.000000   BatchTime 0.134196   LR 0.010000
INFO - Training [13][   60/  391]   Loss 0.064759   Top1 97.877604   Top5 99.986979   BatchTime 0.123031   LR 0.010000
INFO - Training [13][   80/  391]   Loss 0.069342   Top1 97.705078   Top5 99.990234   BatchTime 0.117415   LR 0.010000
INFO - Training [13][  100/  391]   Loss 0.069115   Top1 97.703125   Top5 99.992188   BatchTime 0.114067   LR 0.010000
INFO - Training [13][  120/  391]   Loss 0.070763   Top1 97.610677   Top5 99.993490   BatchTime 0.111693   LR 0.010000
INFO - Training [13][  140/  391]   Loss 0.070705   Top1 97.594866   Top5 99.994420   BatchTime 0.109906   LR 0.010000
INFO - Training [13][  160/  391]   Loss 0.072608   Top1 97.500000   Top5 99.995117   BatchTime 0.108633   LR 0.010000
INFO - Training [13][  180/  391]   Loss 0.073061   Top1 97.465278   Top5 99.995660   BatchTime 0.107582   LR 0.010000
INFO - Training [13][  200/  391]   Loss 0.072670   Top1 97.496094   Top5 99.996094   BatchTime 0.106700   LR 0.010000
INFO - Training [13][  220/  391]   Loss 0.072290   Top1 97.485795   Top5 99.996449   BatchTime 0.106484   LR 0.010000
INFO - Training [13][  240/  391]   Loss 0.071792   Top1 97.503255   Top5 99.993490   BatchTime 0.105912   LR 0.010000
INFO - Training [13][  260/  391]   Loss 0.071689   Top1 97.493990   Top5 99.993990   BatchTime 0.105471   LR 0.010000
INFO - Training [13][  280/  391]   Loss 0.072301   Top1 97.466518   Top5 99.994420   BatchTime 0.105041   LR 0.010000
INFO - Training [13][  300/  391]   Loss 0.073306   Top1 97.450521   Top5 99.994792   BatchTime 0.104701   LR 0.010000
INFO - Training [13][  320/  391]   Loss 0.073924   Top1 97.429199   Top5 99.995117   BatchTime 0.104391   LR 0.010000
INFO - Training [13][  340/  391]   Loss 0.073997   Top1 97.410386   Top5 99.993107   BatchTime 0.104052   LR 0.010000
INFO - Training [13][  360/  391]   Loss 0.074641   Top1 97.398003   Top5 99.991319   BatchTime 0.103712   LR 0.010000
INFO - Training [13][  380/  391]   Loss 0.074748   Top1 97.409539   Top5 99.991776   BatchTime 0.103382   LR 0.010000
INFO - ==> Top1: 97.408    Top5: 99.992    Loss: 0.075
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [13][   20/   79]   Loss 0.418372   Top1 89.453125   Top5 99.492188   BatchTime 0.119842
INFO - Validation [13][   40/   79]   Loss 0.420413   Top1 89.531250   Top5 99.394531   BatchTime 0.073004
INFO - Validation [13][   60/   79]   Loss 0.412714   Top1 89.791667   Top5 99.505208   BatchTime 0.057238
INFO - ==> Top1: 89.750    Top5: 99.570    Loss: 0.407
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.760   Top5: 99.540] Sparsity : 0.692
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 89.750   Top5: 99.570] Sparsity : 0.699
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.740   Top5: 99.550] Sparsity : 0.688
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  14
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [14][   20/  391]   Loss 0.062635   Top1 97.890625   Top5 100.000000   BatchTime 0.171268   LR 0.010000
INFO - Training [14][   40/  391]   Loss 0.059824   Top1 97.988281   Top5 99.980469   BatchTime 0.135204   LR 0.010000
INFO - Training [14][   60/  391]   Loss 0.063433   Top1 97.838542   Top5 99.986979   BatchTime 0.123764   LR 0.010000
INFO - Training [14][   80/  391]   Loss 0.063817   Top1 97.792969   Top5 99.990234   BatchTime 0.118021   LR 0.010000
INFO - Training [14][  100/  391]   Loss 0.066075   Top1 97.664062   Top5 99.992188   BatchTime 0.114412   LR 0.010000
INFO - Training [14][  120/  391]   Loss 0.064968   Top1 97.688802   Top5 99.993490   BatchTime 0.112120   LR 0.010000
INFO - Training [14][  140/  391]   Loss 0.065867   Top1 97.661830   Top5 99.994420   BatchTime 0.110517   LR 0.010000
INFO - Training [14][  160/  391]   Loss 0.066472   Top1 97.631836   Top5 99.990234   BatchTime 0.109174   LR 0.010000
INFO - Training [14][  180/  391]   Loss 0.066308   Top1 97.660590   Top5 99.991319   BatchTime 0.108228   LR 0.010000
INFO - Training [14][  200/  391]   Loss 0.066388   Top1 97.664062   Top5 99.992188   BatchTime 0.107387   LR 0.010000
INFO - Training [14][  220/  391]   Loss 0.066514   Top1 97.666903   Top5 99.992898   BatchTime 0.106624   LR 0.010000
INFO - Training [14][  240/  391]   Loss 0.066106   Top1 97.682292   Top5 99.993490   BatchTime 0.106051   LR 0.010000
INFO - Training [14][  260/  391]   Loss 0.066491   Top1 97.662260   Top5 99.993990   BatchTime 0.105594   LR 0.010000
INFO - Training [14][  280/  391]   Loss 0.066426   Top1 97.672991   Top5 99.994420   BatchTime 0.105190   LR 0.010000
INFO - Training [14][  300/  391]   Loss 0.066623   Top1 97.677083   Top5 99.994792   BatchTime 0.104877   LR 0.010000
INFO - Training [14][  320/  391]   Loss 0.067195   Top1 97.644043   Top5 99.995117   BatchTime 0.104512   LR 0.010000
INFO - Training [14][  340/  391]   Loss 0.068738   Top1 97.598805   Top5 99.990809   BatchTime 0.104211   LR 0.010000
INFO - Training [14][  360/  391]   Loss 0.069834   Top1 97.565104   Top5 99.989149   BatchTime 0.103928   LR 0.010000
INFO - Training [14][  380/  391]   Loss 0.070322   Top1 97.545230   Top5 99.989720   BatchTime 0.103574   LR 0.010000
INFO - ==> Top1: 97.510    Top5: 99.990    Loss: 0.071
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [14][   20/   79]   Loss 0.375978   Top1 90.546875   Top5 99.765625   BatchTime 0.120835
INFO - Validation [14][   40/   79]   Loss 0.381341   Top1 90.253906   Top5 99.628906   BatchTime 0.073284
INFO - Validation [14][   60/   79]   Loss 0.381126   Top1 90.338542   Top5 99.648438   BatchTime 0.057434
INFO - ==> Top1: 90.170    Top5: 99.650    Loss: 0.380
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 89.760   Top5: 99.540] Sparsity : 0.692
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 89.750   Top5: 99.570] Sparsity : 0.699
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  15
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [15][   20/  391]   Loss 0.061750   Top1 97.812500   Top5 100.000000   BatchTime 0.171262   LR 0.010000
INFO - Training [15][   40/  391]   Loss 0.064652   Top1 97.617188   Top5 100.000000   BatchTime 0.132231   LR 0.010000
INFO - Training [15][   60/  391]   Loss 0.060959   Top1 97.760417   Top5 100.000000   BatchTime 0.121547   LR 0.010000
INFO - Training [15][   80/  391]   Loss 0.061087   Top1 97.812500   Top5 100.000000   BatchTime 0.116130   LR 0.010000
INFO - Training [15][  100/  391]   Loss 0.061886   Top1 97.750000   Top5 100.000000   BatchTime 0.112843   LR 0.010000
INFO - Training [15][  120/  391]   Loss 0.061451   Top1 97.766927   Top5 100.000000   BatchTime 0.110771   LR 0.010000
INFO - Training [15][  140/  391]   Loss 0.062837   Top1 97.745536   Top5 100.000000   BatchTime 0.109531   LR 0.010000
INFO - Training [15][  160/  391]   Loss 0.064017   Top1 97.690430   Top5 100.000000   BatchTime 0.108321   LR 0.010000
INFO - Training [15][  180/  391]   Loss 0.065290   Top1 97.634549   Top5 100.000000   BatchTime 0.107429   LR 0.010000
INFO - Training [15][  200/  391]   Loss 0.065403   Top1 97.628906   Top5 100.000000   BatchTime 0.106737   LR 0.010000
INFO - Training [15][  220/  391]   Loss 0.066522   Top1 97.585227   Top5 100.000000   BatchTime 0.106105   LR 0.010000
INFO - Training [15][  240/  391]   Loss 0.066190   Top1 97.604167   Top5 100.000000   BatchTime 0.105595   LR 0.010000
INFO - Training [15][  260/  391]   Loss 0.065879   Top1 97.617188   Top5 100.000000   BatchTime 0.105153   LR 0.010000
INFO - Training [15][  280/  391]   Loss 0.066153   Top1 97.603237   Top5 100.000000   BatchTime 0.104789   LR 0.010000
INFO - Training [15][  300/  391]   Loss 0.066888   Top1 97.580729   Top5 100.000000   BatchTime 0.104891   LR 0.010000
INFO - Training [15][  320/  391]   Loss 0.067118   Top1 97.580566   Top5 100.000000   BatchTime 0.104583   LR 0.010000
INFO - Training [15][  340/  391]   Loss 0.067136   Top1 97.587316   Top5 100.000000   BatchTime 0.104226   LR 0.010000
INFO - Training [15][  360/  391]   Loss 0.067770   Top1 97.562934   Top5 100.000000   BatchTime 0.103917   LR 0.010000
INFO - Training [15][  380/  391]   Loss 0.068252   Top1 97.534951   Top5 100.000000   BatchTime 0.103555   LR 0.010000
INFO - ==> Top1: 97.522    Top5: 100.000    Loss: 0.069
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [15][   20/   79]   Loss 0.409785   Top1 89.726562   Top5 99.570312   BatchTime 0.122715
INFO - Validation [15][   40/   79]   Loss 0.407366   Top1 89.589844   Top5 99.492188   BatchTime 0.074301
INFO - Validation [15][   60/   79]   Loss 0.405600   Top1 89.531250   Top5 99.531250   BatchTime 0.058149
INFO - ==> Top1: 89.630    Top5: 99.560    Loss: 0.398
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 89.760   Top5: 99.540] Sparsity : 0.692
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 89.750   Top5: 99.570] Sparsity : 0.699
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  16
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [16][   20/  391]   Loss 0.080101   Top1 96.953125   Top5 100.000000   BatchTime 0.170984   LR 0.010000
INFO - Training [16][   40/  391]   Loss 0.073627   Top1 97.187500   Top5 100.000000   BatchTime 0.132844   LR 0.010000
INFO - Training [16][   60/  391]   Loss 0.070507   Top1 97.369792   Top5 100.000000   BatchTime 0.121769   LR 0.010000
INFO - Training [16][   80/  391]   Loss 0.069756   Top1 97.460938   Top5 99.990234   BatchTime 0.116475   LR 0.010000
INFO - Training [16][  100/  391]   Loss 0.069331   Top1 97.546875   Top5 99.992188   BatchTime 0.113255   LR 0.010000
INFO - Training [16][  120/  391]   Loss 0.071050   Top1 97.473958   Top5 99.993490   BatchTime 0.111028   LR 0.010000
INFO - Training [16][  140/  391]   Loss 0.070206   Top1 97.533482   Top5 99.994420   BatchTime 0.109489   LR 0.010000
INFO - Training [16][  160/  391]   Loss 0.070492   Top1 97.500000   Top5 99.995117   BatchTime 0.108288   LR 0.010000
INFO - Training [16][  180/  391]   Loss 0.071168   Top1 97.456597   Top5 99.991319   BatchTime 0.107355   LR 0.010000
INFO - Training [16][  200/  391]   Loss 0.071671   Top1 97.425781   Top5 99.992188   BatchTime 0.106639   LR 0.010000
INFO - Training [16][  220/  391]   Loss 0.071271   Top1 97.404119   Top5 99.992898   BatchTime 0.106105   LR 0.010000
INFO - Training [16][  240/  391]   Loss 0.071329   Top1 97.382812   Top5 99.993490   BatchTime 0.105483   LR 0.010000
INFO - Training [16][  260/  391]   Loss 0.071677   Top1 97.391827   Top5 99.993990   BatchTime 0.105053   LR 0.010000
INFO - Training [16][  280/  391]   Loss 0.072152   Top1 97.377232   Top5 99.991629   BatchTime 0.104654   LR 0.010000
INFO - Training [16][  300/  391]   Loss 0.072390   Top1 97.395833   Top5 99.992188   BatchTime 0.104335   LR 0.010000
INFO - Training [16][  320/  391]   Loss 0.072291   Top1 97.409668   Top5 99.992676   BatchTime 0.104100   LR 0.010000
INFO - Training [16][  340/  391]   Loss 0.072524   Top1 97.398897   Top5 99.993107   BatchTime 0.103819   LR 0.010000
INFO - Training [16][  360/  391]   Loss 0.072481   Top1 97.393663   Top5 99.993490   BatchTime 0.103547   LR 0.010000
INFO - Training [16][  380/  391]   Loss 0.072052   Top1 97.409539   Top5 99.993832   BatchTime 0.103050   LR 0.010000
INFO - ==> Top1: 97.404    Top5: 99.994    Loss: 0.072
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [16][   20/   79]   Loss 0.398041   Top1 90.429688   Top5 99.492188   BatchTime 0.129746
INFO - Validation [16][   40/   79]   Loss 0.401730   Top1 90.214844   Top5 99.375000   BatchTime 0.077862
INFO - Validation [16][   60/   79]   Loss 0.405787   Top1 90.078125   Top5 99.466146   BatchTime 0.060539
INFO - ==> Top1: 90.080    Top5: 99.510    Loss: 0.404
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 89.760   Top5: 99.540] Sparsity : 0.692
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  17
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [17][   20/  391]   Loss 0.072374   Top1 97.695312   Top5 99.960938   BatchTime 0.174314   LR 0.010000
INFO - Training [17][   40/  391]   Loss 0.072775   Top1 97.675781   Top5 99.960938   BatchTime 0.129011   LR 0.010000
INFO - Training [17][   60/  391]   Loss 0.069456   Top1 97.721354   Top5 99.973958   BatchTime 0.119141   LR 0.010000
INFO - Training [17][   80/  391]   Loss 0.069564   Top1 97.656250   Top5 99.980469   BatchTime 0.114285   LR 0.010000
INFO - Training [17][  100/  391]   Loss 0.068627   Top1 97.648438   Top5 99.984375   BatchTime 0.111476   LR 0.010000
INFO - Training [17][  120/  391]   Loss 0.071936   Top1 97.532552   Top5 99.986979   BatchTime 0.109592   LR 0.010000
INFO - Training [17][  140/  391]   Loss 0.072144   Top1 97.516741   Top5 99.988839   BatchTime 0.108153   LR 0.010000
INFO - Training [17][  160/  391]   Loss 0.071785   Top1 97.534180   Top5 99.990234   BatchTime 0.107106   LR 0.010000
INFO - Training [17][  180/  391]   Loss 0.071522   Top1 97.517361   Top5 99.991319   BatchTime 0.106243   LR 0.010000
INFO - Training [17][  200/  391]   Loss 0.072642   Top1 97.480469   Top5 99.988281   BatchTime 0.105608   LR 0.010000
INFO - Training [17][  220/  391]   Loss 0.072698   Top1 97.468040   Top5 99.985795   BatchTime 0.105193   LR 0.010000
INFO - Training [17][  240/  391]   Loss 0.072971   Top1 97.457682   Top5 99.986979   BatchTime 0.104770   LR 0.010000
INFO - Training [17][  260/  391]   Loss 0.073314   Top1 97.457933   Top5 99.984976   BatchTime 0.104390   LR 0.010000
INFO - Training [17][  280/  391]   Loss 0.073212   Top1 97.455357   Top5 99.986049   BatchTime 0.104104   LR 0.010000
INFO - Training [17][  300/  391]   Loss 0.073106   Top1 97.460938   Top5 99.984375   BatchTime 0.103913   LR 0.010000
INFO - Training [17][  320/  391]   Loss 0.073274   Top1 97.451172   Top5 99.982910   BatchTime 0.103709   LR 0.010000
INFO - Training [17][  340/  391]   Loss 0.073026   Top1 97.447151   Top5 99.983915   BatchTime 0.103426   LR 0.010000
INFO - Training [17][  360/  391]   Loss 0.072612   Top1 97.456597   Top5 99.984809   BatchTime 0.103191   LR 0.010000
INFO - Training [17][  380/  391]   Loss 0.072622   Top1 97.450658   Top5 99.985609   BatchTime 0.103117   LR 0.010000
INFO - ==> Top1: 97.440    Top5: 99.986    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [17][   20/   79]   Loss 0.395868   Top1 89.726562   Top5 99.570312   BatchTime 0.129769
INFO - Validation [17][   40/   79]   Loss 0.399269   Top1 89.921875   Top5 99.433594   BatchTime 0.081091
INFO - Validation [17][   60/   79]   Loss 0.400072   Top1 90.052083   Top5 99.466146   BatchTime 0.062622
INFO - ==> Top1: 90.060    Top5: 99.490    Loss: 0.394
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 90.060   Top5: 99.490] Sparsity : 0.732
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  18
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [18][   20/  391]   Loss 0.064691   Top1 97.695312   Top5 99.960938   BatchTime 0.173345   LR 0.010000
INFO - Training [18][   40/  391]   Loss 0.071535   Top1 97.519531   Top5 99.980469   BatchTime 0.123241   LR 0.010000
INFO - Training [18][   60/  391]   Loss 0.068028   Top1 97.643229   Top5 99.986979   BatchTime 0.116414   LR 0.010000
INFO - Training [18][   80/  391]   Loss 0.066382   Top1 97.636719   Top5 99.990234   BatchTime 0.112400   LR 0.010000
INFO - Training [18][  100/  391]   Loss 0.065139   Top1 97.601562   Top5 99.992188   BatchTime 0.109836   LR 0.010000
INFO - Training [18][  120/  391]   Loss 0.064101   Top1 97.604167   Top5 99.993490   BatchTime 0.108235   LR 0.010000
INFO - Training [18][  140/  391]   Loss 0.065606   Top1 97.594866   Top5 99.994420   BatchTime 0.107037   LR 0.010000
INFO - Training [18][  160/  391]   Loss 0.065772   Top1 97.607422   Top5 99.995117   BatchTime 0.106113   LR 0.010000
INFO - Training [18][  180/  391]   Loss 0.065876   Top1 97.625868   Top5 99.991319   BatchTime 0.105434   LR 0.010000
INFO - Training [18][  200/  391]   Loss 0.065956   Top1 97.621094   Top5 99.992188   BatchTime 0.104877   LR 0.010000
INFO - Training [18][  220/  391]   Loss 0.067099   Top1 97.613636   Top5 99.992898   BatchTime 0.104464   LR 0.010000
INFO - Training [18][  240/  391]   Loss 0.067979   Top1 97.581380   Top5 99.993490   BatchTime 0.104069   LR 0.010000
INFO - Training [18][  260/  391]   Loss 0.067357   Top1 97.590144   Top5 99.993990   BatchTime 0.103664   LR 0.010000
INFO - Training [18][  280/  391]   Loss 0.067699   Top1 97.600446   Top5 99.991629   BatchTime 0.103382   LR 0.010000
INFO - Training [18][  300/  391]   Loss 0.067040   Top1 97.606771   Top5 99.992188   BatchTime 0.103145   LR 0.010000
INFO - Training [18][  320/  391]   Loss 0.066740   Top1 97.624512   Top5 99.992676   BatchTime 0.102890   LR 0.010000
INFO - Training [18][  340/  391]   Loss 0.067288   Top1 97.594210   Top5 99.993107   BatchTime 0.102619   LR 0.010000
INFO - Training [18][  360/  391]   Loss 0.067761   Top1 97.575955   Top5 99.993490   BatchTime 0.102356   LR 0.010000
INFO - Training [18][  380/  391]   Loss 0.067785   Top1 97.561678   Top5 99.993832   BatchTime 0.102176   LR 0.010000
INFO - ==> Top1: 97.560    Top5: 99.994    Loss: 0.068
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [18][   20/   79]   Loss 0.440090   Top1 89.140625   Top5 99.531250   BatchTime 0.127694
INFO - Validation [18][   40/   79]   Loss 0.431575   Top1 89.277344   Top5 99.414062   BatchTime 0.084527
INFO - Validation [18][   60/   79]   Loss 0.417821   Top1 89.596354   Top5 99.505208   BatchTime 0.065041
INFO - ==> Top1: 89.660    Top5: 99.530    Loss: 0.413
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 90.060   Top5: 99.490] Sparsity : 0.732
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  19
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [19][   20/  391]   Loss 0.074352   Top1 97.265625   Top5 100.000000   BatchTime 0.171858   LR 0.010000
INFO - Training [19][   40/  391]   Loss 0.064650   Top1 97.597656   Top5 100.000000   BatchTime 0.126185   LR 0.010000
INFO - Training [19][   60/  391]   Loss 0.067159   Top1 97.539062   Top5 100.000000   BatchTime 0.114715   LR 0.010000
INFO - Training [19][   80/  391]   Loss 0.065047   Top1 97.685547   Top5 100.000000   BatchTime 0.111063   LR 0.010000
INFO - Training [19][  100/  391]   Loss 0.064094   Top1 97.664062   Top5 100.000000   BatchTime 0.108812   LR 0.010000
INFO - Training [19][  120/  391]   Loss 0.063221   Top1 97.708333   Top5 100.000000   BatchTime 0.107370   LR 0.010000
INFO - Training [19][  140/  391]   Loss 0.063059   Top1 97.712054   Top5 100.000000   BatchTime 0.106529   LR 0.010000
INFO - Training [19][  160/  391]   Loss 0.062257   Top1 97.778320   Top5 100.000000   BatchTime 0.105718   LR 0.010000
INFO - Training [19][  180/  391]   Loss 0.061603   Top1 97.808160   Top5 99.995660   BatchTime 0.104978   LR 0.010000
INFO - Training [19][  200/  391]   Loss 0.060442   Top1 97.863281   Top5 99.996094   BatchTime 0.104503   LR 0.010000
INFO - Training [19][  220/  391]   Loss 0.060505   Top1 97.851562   Top5 99.992898   BatchTime 0.104059   LR 0.010000
INFO - Training [19][  240/  391]   Loss 0.061354   Top1 97.822266   Top5 99.990234   BatchTime 0.103733   LR 0.010000
INFO - Training [19][  260/  391]   Loss 0.061863   Top1 97.794471   Top5 99.990986   BatchTime 0.103429   LR 0.010000
INFO - Training [19][  280/  391]   Loss 0.061728   Top1 97.795759   Top5 99.991629   BatchTime 0.103204   LR 0.010000
INFO - Training [19][  300/  391]   Loss 0.061609   Top1 97.807292   Top5 99.992188   BatchTime 0.102994   LR 0.010000
INFO - Training [19][  320/  391]   Loss 0.062146   Top1 97.790527   Top5 99.992676   BatchTime 0.102776   LR 0.010000
INFO - Training [19][  340/  391]   Loss 0.062816   Top1 97.764246   Top5 99.993107   BatchTime 0.102513   LR 0.010000
INFO - Training [19][  360/  391]   Loss 0.063205   Top1 97.743056   Top5 99.991319   BatchTime 0.102263   LR 0.010000
INFO - Training [19][  380/  391]   Loss 0.062726   Top1 97.767270   Top5 99.991776   BatchTime 0.102037   LR 0.010000
INFO - ==> Top1: 97.758    Top5: 99.992    Loss: 0.063
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [19][   20/   79]   Loss 0.423873   Top1 89.570312   Top5 99.375000   BatchTime 0.130923
INFO - Validation [19][   40/   79]   Loss 0.426242   Top1 89.882812   Top5 99.375000   BatchTime 0.088630
INFO - Validation [19][   60/   79]   Loss 0.420598   Top1 89.986979   Top5 99.401042   BatchTime 0.069748
INFO - ==> Top1: 90.040    Top5: 99.450    Loss: 0.412
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 90.060   Top5: 99.490] Sparsity : 0.732
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  20
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [20][   20/  391]   Loss 0.059719   Top1 97.890625   Top5 100.000000   BatchTime 0.179298   LR 0.010000
INFO - Training [20][   40/  391]   Loss 0.054405   Top1 98.144531   Top5 100.000000   BatchTime 0.132371   LR 0.010000
INFO - Training [20][   60/  391]   Loss 0.051535   Top1 98.111979   Top5 100.000000   BatchTime 0.119653   LR 0.010000
INFO - Training [20][   80/  391]   Loss 0.052422   Top1 98.037109   Top5 100.000000   BatchTime 0.114782   LR 0.010000
INFO - Training [20][  100/  391]   Loss 0.053037   Top1 98.000000   Top5 100.000000   BatchTime 0.111956   LR 0.010000
INFO - Training [20][  120/  391]   Loss 0.053386   Top1 97.994792   Top5 100.000000   BatchTime 0.110005   LR 0.010000
INFO - Training [20][  140/  391]   Loss 0.056248   Top1 97.912946   Top5 100.000000   BatchTime 0.108690   LR 0.010000
INFO - Training [20][  160/  391]   Loss 0.057938   Top1 97.890625   Top5 100.000000   BatchTime 0.107571   LR 0.010000
INFO - Training [20][  180/  391]   Loss 0.058193   Top1 97.886285   Top5 100.000000   BatchTime 0.106694   LR 0.010000
INFO - Training [20][  200/  391]   Loss 0.056698   Top1 97.933594   Top5 100.000000   BatchTime 0.106008   LR 0.010000
INFO - Training [20][  220/  391]   Loss 0.057156   Top1 97.922585   Top5 100.000000   BatchTime 0.105487   LR 0.010000
INFO - Training [20][  240/  391]   Loss 0.057820   Top1 97.893880   Top5 100.000000   BatchTime 0.105066   LR 0.010000
INFO - Training [20][  260/  391]   Loss 0.059508   Top1 97.866587   Top5 100.000000   BatchTime 0.104643   LR 0.010000
INFO - Training [20][  280/  391]   Loss 0.059262   Top1 97.873884   Top5 100.000000   BatchTime 0.104278   LR 0.010000
INFO - Training [20][  300/  391]   Loss 0.058977   Top1 97.875000   Top5 99.997396   BatchTime 0.103994   LR 0.010000
INFO - Training [20][  320/  391]   Loss 0.059490   Top1 97.844238   Top5 99.997559   BatchTime 0.103725   LR 0.010000
INFO - Training [20][  340/  391]   Loss 0.059583   Top1 97.856158   Top5 99.997702   BatchTime 0.103424   LR 0.010000
INFO - Training [20][  360/  391]   Loss 0.059562   Top1 97.853733   Top5 99.997830   BatchTime 0.103167   LR 0.010000
INFO - Training [20][  380/  391]   Loss 0.059472   Top1 97.857730   Top5 99.997944   BatchTime 0.102918   LR 0.010000
INFO - ==> Top1: 97.832    Top5: 99.998    Loss: 0.060
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [20][   20/   79]   Loss 0.399882   Top1 90.000000   Top5 99.492188   BatchTime 0.130324
INFO - Validation [20][   40/   79]   Loss 0.410305   Top1 90.039062   Top5 99.433594   BatchTime 0.087675
INFO - Validation [20][   60/   79]   Loss 0.411122   Top1 90.052083   Top5 99.479167   BatchTime 0.071163
INFO - ==> Top1: 90.070    Top5: 99.550    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 90.070   Top5: 99.550] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  21
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [21][   20/  391]   Loss 0.053609   Top1 98.046875   Top5 100.000000   BatchTime 0.175843   LR 0.010000
INFO - Training [21][   40/  391]   Loss 0.051114   Top1 98.203125   Top5 99.960938   BatchTime 0.131437   LR 0.010000
INFO - Training [21][   60/  391]   Loss 0.052654   Top1 98.164062   Top5 99.973958   BatchTime 0.117419   LR 0.010000
INFO - Training [21][   80/  391]   Loss 0.053486   Top1 98.115234   Top5 99.980469   BatchTime 0.113061   LR 0.010000
INFO - Training [21][  100/  391]   Loss 0.053718   Top1 98.125000   Top5 99.984375   BatchTime 0.110467   LR 0.010000
INFO - Training [21][  120/  391]   Loss 0.054312   Top1 98.125000   Top5 99.986979   BatchTime 0.108560   LR 0.010000
INFO - Training [21][  140/  391]   Loss 0.053377   Top1 98.136161   Top5 99.988839   BatchTime 0.107395   LR 0.010000
INFO - Training [21][  160/  391]   Loss 0.054473   Top1 98.066406   Top5 99.990234   BatchTime 0.106476   LR 0.010000
INFO - Training [21][  180/  391]   Loss 0.054169   Top1 98.077257   Top5 99.991319   BatchTime 0.105823   LR 0.010000
INFO - Training [21][  200/  391]   Loss 0.054475   Top1 98.078125   Top5 99.992188   BatchTime 0.105331   LR 0.010000
INFO - Training [21][  220/  391]   Loss 0.055615   Top1 98.071733   Top5 99.992898   BatchTime 0.104947   LR 0.010000
INFO - Training [21][  240/  391]   Loss 0.055066   Top1 98.076172   Top5 99.993490   BatchTime 0.104547   LR 0.010000
INFO - Training [21][  260/  391]   Loss 0.054454   Top1 98.082933   Top5 99.993990   BatchTime 0.104218   LR 0.010000
INFO - Training [21][  280/  391]   Loss 0.055680   Top1 98.024554   Top5 99.994420   BatchTime 0.103921   LR 0.010000
INFO - Training [21][  300/  391]   Loss 0.055158   Top1 98.054688   Top5 99.989583   BatchTime 0.103662   LR 0.010000
INFO - Training [21][  320/  391]   Loss 0.055073   Top1 98.039551   Top5 99.990234   BatchTime 0.103426   LR 0.010000
INFO - Training [21][  340/  391]   Loss 0.055176   Top1 98.042279   Top5 99.990809   BatchTime 0.103175   LR 0.010000
INFO - Training [21][  360/  391]   Loss 0.056066   Top1 98.016493   Top5 99.991319   BatchTime 0.102923   LR 0.010000
INFO - Training [21][  380/  391]   Loss 0.056307   Top1 98.005757   Top5 99.991776   BatchTime 0.102687   LR 0.010000
INFO - ==> Top1: 98.000    Top5: 99.992    Loss: 0.057
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [21][   20/   79]   Loss 0.415908   Top1 90.000000   Top5 99.609375   BatchTime 0.130621
INFO - Validation [21][   40/   79]   Loss 0.420130   Top1 90.156250   Top5 99.453125   BatchTime 0.087449
INFO - Validation [21][   60/   79]   Loss 0.426967   Top1 90.130208   Top5 99.440104   BatchTime 0.073680
INFO - ==> Top1: 89.990    Top5: 99.520    Loss: 0.422
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 90.070   Top5: 99.550] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  22
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [22][   20/  391]   Loss 0.053371   Top1 98.203125   Top5 99.960938   BatchTime 0.175461   LR 0.010000
INFO - Training [22][   40/  391]   Loss 0.055950   Top1 98.105469   Top5 99.980469   BatchTime 0.131629   LR 0.010000
INFO - Training [22][   60/  391]   Loss 0.055209   Top1 98.138021   Top5 99.986979   BatchTime 0.112058   LR 0.010000
INFO - Training [22][   80/  391]   Loss 0.056334   Top1 98.144531   Top5 99.990234   BatchTime 0.111987   LR 0.010000
INFO - Training [22][  100/  391]   Loss 0.055473   Top1 98.148438   Top5 99.992188   BatchTime 0.109789   LR 0.010000
INFO - Training [22][  120/  391]   Loss 0.058495   Top1 98.040365   Top5 99.993490   BatchTime 0.108358   LR 0.010000
INFO - Training [22][  140/  391]   Loss 0.059576   Top1 97.979911   Top5 99.994420   BatchTime 0.107354   LR 0.010000
INFO - Training [22][  160/  391]   Loss 0.059560   Top1 97.998047   Top5 99.995117   BatchTime 0.106486   LR 0.010000
INFO - Training [22][  180/  391]   Loss 0.059461   Top1 98.007812   Top5 99.995660   BatchTime 0.105755   LR 0.010000
INFO - Training [22][  200/  391]   Loss 0.059756   Top1 98.003906   Top5 99.996094   BatchTime 0.105205   LR 0.010000
INFO - Training [22][  220/  391]   Loss 0.059278   Top1 97.993608   Top5 99.996449   BatchTime 0.104760   LR 0.010000
INFO - Training [22][  240/  391]   Loss 0.058703   Top1 98.017578   Top5 99.996745   BatchTime 0.104488   LR 0.010000
INFO - Training [22][  260/  391]   Loss 0.058336   Top1 98.040865   Top5 99.996995   BatchTime 0.104224   LR 0.010000
INFO - Training [22][  280/  391]   Loss 0.057799   Top1 98.052455   Top5 99.997210   BatchTime 0.103950   LR 0.010000
INFO - Training [22][  300/  391]   Loss 0.058239   Top1 98.028646   Top5 99.994792   BatchTime 0.103718   LR 0.010000
INFO - Training [22][  320/  391]   Loss 0.058095   Top1 98.029785   Top5 99.995117   BatchTime 0.103495   LR 0.010000
INFO - Training [22][  340/  391]   Loss 0.057746   Top1 98.035386   Top5 99.995404   BatchTime 0.103289   LR 0.010000
INFO - Training [22][  360/  391]   Loss 0.057471   Top1 98.038194   Top5 99.995660   BatchTime 0.103090   LR 0.010000
INFO - Training [22][  380/  391]   Loss 0.058454   Top1 97.983141   Top5 99.995888   BatchTime 0.102888   LR 0.010000
INFO - ==> Top1: 97.984    Top5: 99.996    Loss: 0.059
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [22][   20/   79]   Loss 0.406278   Top1 89.453125   Top5 99.726562   BatchTime 0.128902
INFO - Validation [22][   40/   79]   Loss 0.415356   Top1 89.687500   Top5 99.589844   BatchTime 0.087636
INFO - Validation [22][   60/   79]   Loss 0.419145   Top1 89.596354   Top5 99.557292   BatchTime 0.073373
INFO - ==> Top1: 89.740    Top5: 99.590    Loss: 0.414
INFO - Scoreboard best 1 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 90.070   Top5: 99.550] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  23
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [23][   20/  391]   Loss 0.045360   Top1 98.359375   Top5 100.000000   BatchTime 0.176604   LR 0.010000
INFO - Training [23][   40/  391]   Loss 0.052516   Top1 98.164062   Top5 100.000000   BatchTime 0.131558   LR 0.010000
INFO - Training [23][   60/  391]   Loss 0.054263   Top1 98.138021   Top5 100.000000   BatchTime 0.113338   LR 0.010000
INFO - Training [23][   80/  391]   Loss 0.052090   Top1 98.232422   Top5 100.000000   BatchTime 0.110312   LR 0.010000
INFO - Training [23][  100/  391]   Loss 0.051890   Top1 98.257812   Top5 100.000000   BatchTime 0.108401   LR 0.010000
INFO - Training [23][  120/  391]   Loss 0.052884   Top1 98.248698   Top5 100.000000   BatchTime 0.107151   LR 0.010000
INFO - Training [23][  140/  391]   Loss 0.053740   Top1 98.197545   Top5 100.000000   BatchTime 0.106030   LR 0.010000
INFO - Training [23][  160/  391]   Loss 0.054371   Top1 98.134766   Top5 100.000000   BatchTime 0.105401   LR 0.010000
INFO - Training [23][  180/  391]   Loss 0.053818   Top1 98.151042   Top5 100.000000   BatchTime 0.104800   LR 0.010000
INFO - Training [23][  200/  391]   Loss 0.054749   Top1 98.128906   Top5 99.996094   BatchTime 0.104361   LR 0.010000
INFO - Training [23][  220/  391]   Loss 0.054294   Top1 98.135653   Top5 99.996449   BatchTime 0.103991   LR 0.010000
INFO - Training [23][  240/  391]   Loss 0.053692   Top1 98.160807   Top5 99.996745   BatchTime 0.103756   LR 0.010000
INFO - Training [23][  260/  391]   Loss 0.053750   Top1 98.155048   Top5 99.996995   BatchTime 0.103588   LR 0.010000
INFO - Training [23][  280/  391]   Loss 0.054387   Top1 98.147321   Top5 99.997210   BatchTime 0.103334   LR 0.010000
INFO - Training [23][  300/  391]   Loss 0.054581   Top1 98.127604   Top5 99.997396   BatchTime 0.103170   LR 0.010000
INFO - Training [23][  320/  391]   Loss 0.054530   Top1 98.125000   Top5 99.997559   BatchTime 0.103049   LR 0.010000
INFO - Training [23][  340/  391]   Loss 0.054057   Top1 98.138787   Top5 99.997702   BatchTime 0.102859   LR 0.010000
INFO - Training [23][  360/  391]   Loss 0.054051   Top1 98.140191   Top5 99.997830   BatchTime 0.102622   LR 0.010000
INFO - Training [23][  380/  391]   Loss 0.053617   Top1 98.153783   Top5 99.995888   BatchTime 0.102413   LR 0.010000
INFO - ==> Top1: 98.144    Top5: 99.996    Loss: 0.054
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [23][   20/   79]   Loss 0.401657   Top1 90.156250   Top5 99.609375   BatchTime 0.129607
INFO - Validation [23][   40/   79]   Loss 0.416558   Top1 90.566406   Top5 99.453125   BatchTime 0.087898
INFO - Validation [23][   60/   79]   Loss 0.424034   Top1 90.455729   Top5 99.492188   BatchTime 0.073986
INFO - ==> Top1: 90.400    Top5: 99.550    Loss: 0.420
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 90.080   Top5: 99.510] Sparsity : 0.730
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  24
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [24][   20/  391]   Loss 0.051583   Top1 98.164062   Top5 100.000000   BatchTime 0.176737   LR 0.010000
INFO - Training [24][   40/  391]   Loss 0.049311   Top1 98.242188   Top5 100.000000   BatchTime 0.134222   LR 0.010000
INFO - Training [24][   60/  391]   Loss 0.051677   Top1 98.098958   Top5 100.000000   BatchTime 0.114045   LR 0.010000
INFO - Training [24][   80/  391]   Loss 0.050222   Top1 98.183594   Top5 100.000000   BatchTime 0.109306   LR 0.010000
INFO - Training [24][  100/  391]   Loss 0.050438   Top1 98.195312   Top5 100.000000   BatchTime 0.107639   LR 0.010000
INFO - Training [24][  120/  391]   Loss 0.050184   Top1 98.190104   Top5 100.000000   BatchTime 0.106428   LR 0.010000
INFO - Training [24][  140/  391]   Loss 0.049904   Top1 98.175223   Top5 100.000000   BatchTime 0.106458   LR 0.010000
INFO - Training [24][  160/  391]   Loss 0.050561   Top1 98.188477   Top5 100.000000   BatchTime 0.105662   LR 0.010000
INFO - Training [24][  180/  391]   Loss 0.050946   Top1 98.181424   Top5 100.000000   BatchTime 0.105087   LR 0.010000
INFO - Training [24][  200/  391]   Loss 0.051019   Top1 98.175781   Top5 100.000000   BatchTime 0.104637   LR 0.010000
INFO - Training [24][  220/  391]   Loss 0.050726   Top1 98.196023   Top5 100.000000   BatchTime 0.104246   LR 0.010000
INFO - Training [24][  240/  391]   Loss 0.050289   Top1 98.206380   Top5 100.000000   BatchTime 0.103919   LR 0.010000
INFO - Training [24][  260/  391]   Loss 0.050447   Top1 98.194111   Top5 100.000000   BatchTime 0.103588   LR 0.010000
INFO - Training [24][  280/  391]   Loss 0.050059   Top1 98.214286   Top5 100.000000   BatchTime 0.103354   LR 0.010000
INFO - Training [24][  300/  391]   Loss 0.050948   Top1 98.190104   Top5 99.997396   BatchTime 0.103172   LR 0.010000
INFO - Training [24][  320/  391]   Loss 0.051265   Top1 98.183594   Top5 99.997559   BatchTime 0.103011   LR 0.010000
INFO - Training [24][  340/  391]   Loss 0.051113   Top1 98.198529   Top5 99.997702   BatchTime 0.102890   LR 0.010000
INFO - Training [24][  360/  391]   Loss 0.051518   Top1 98.172743   Top5 99.997830   BatchTime 0.102626   LR 0.010000
INFO - Training [24][  380/  391]   Loss 0.051823   Top1 98.168174   Top5 99.997944   BatchTime 0.102443   LR 0.010000
INFO - ==> Top1: 98.176    Top5: 99.998    Loss: 0.052
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [24][   20/   79]   Loss 0.403979   Top1 90.390625   Top5 99.648438   BatchTime 0.130639
INFO - Validation [24][   40/   79]   Loss 0.416892   Top1 90.332031   Top5 99.492188   BatchTime 0.087778
INFO - Validation [24][   60/   79]   Loss 0.415642   Top1 90.416667   Top5 99.557292   BatchTime 0.072837
INFO - ==> Top1: 90.310    Top5: 99.620    Loss: 0.411
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.310   Top5: 99.620] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 90.170   Top5: 99.650] Sparsity : 0.709
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  25
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [25][   20/  391]   Loss 0.046295   Top1 98.398438   Top5 100.000000   BatchTime 0.178981   LR 0.010000
INFO - Training [25][   40/  391]   Loss 0.046664   Top1 98.378906   Top5 100.000000   BatchTime 0.131085   LR 0.010000
INFO - Training [25][   60/  391]   Loss 0.045168   Top1 98.385417   Top5 100.000000   BatchTime 0.114710   LR 0.010000
INFO - Training [25][   80/  391]   Loss 0.048216   Top1 98.320312   Top5 100.000000   BatchTime 0.108710   LR 0.010000
INFO - Training [25][  100/  391]   Loss 0.049114   Top1 98.257812   Top5 100.000000   BatchTime 0.107103   LR 0.010000
INFO - Training [25][  120/  391]   Loss 0.048143   Top1 98.242188   Top5 100.000000   BatchTime 0.105986   LR 0.010000
INFO - Training [25][  140/  391]   Loss 0.048898   Top1 98.236607   Top5 100.000000   BatchTime 0.105233   LR 0.010000
INFO - Training [25][  160/  391]   Loss 0.048732   Top1 98.261719   Top5 100.000000   BatchTime 0.104656   LR 0.010000
INFO - Training [25][  180/  391]   Loss 0.048646   Top1 98.233507   Top5 100.000000   BatchTime 0.104166   LR 0.010000
INFO - Training [25][  200/  391]   Loss 0.048927   Top1 98.250000   Top5 100.000000   BatchTime 0.103794   LR 0.010000
INFO - Training [25][  220/  391]   Loss 0.049562   Top1 98.238636   Top5 100.000000   BatchTime 0.103126   LR 0.010000
INFO - Training [25][  240/  391]   Loss 0.049365   Top1 98.238932   Top5 100.000000   BatchTime 0.102945   LR 0.010000
INFO - Training [25][  260/  391]   Loss 0.048487   Top1 98.266226   Top5 100.000000   BatchTime 0.102808   LR 0.010000
INFO - Training [25][  280/  391]   Loss 0.048832   Top1 98.256138   Top5 100.000000   BatchTime 0.102616   LR 0.010000
INFO - Training [25][  300/  391]   Loss 0.048888   Top1 98.250000   Top5 100.000000   BatchTime 0.102434   LR 0.010000
INFO - Training [25][  320/  391]   Loss 0.049551   Top1 98.232422   Top5 100.000000   BatchTime 0.102325   LR 0.010000
INFO - Training [25][  340/  391]   Loss 0.049600   Top1 98.249081   Top5 100.000000   BatchTime 0.102119   LR 0.010000
INFO - Training [25][  360/  391]   Loss 0.049708   Top1 98.244358   Top5 99.997830   BatchTime 0.101923   LR 0.010000
INFO - Training [25][  380/  391]   Loss 0.049830   Top1 98.250411   Top5 99.997944   BatchTime 0.101756   LR 0.010000
INFO - ==> Top1: 98.248    Top5: 99.998    Loss: 0.050
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [25][   20/   79]   Loss 0.428937   Top1 90.000000   Top5 99.531250   BatchTime 0.130412
INFO - Validation [25][   40/   79]   Loss 0.430137   Top1 89.843750   Top5 99.453125   BatchTime 0.087754
INFO - Validation [25][   60/   79]   Loss 0.421764   Top1 90.195312   Top5 99.466146   BatchTime 0.073449
INFO - ==> Top1: 90.310    Top5: 99.510    Loss: 0.417
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.310   Top5: 99.620] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.310   Top5: 99.510] Sparsity : 0.749
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  26
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [26][   20/  391]   Loss 0.044695   Top1 98.359375   Top5 100.000000   BatchTime 0.182850   LR 0.010000
INFO - Training [26][   40/  391]   Loss 0.045053   Top1 98.339844   Top5 100.000000   BatchTime 0.131371   LR 0.010000
INFO - Training [26][   60/  391]   Loss 0.047172   Top1 98.307292   Top5 100.000000   BatchTime 0.116498   LR 0.010000
INFO - Training [26][   80/  391]   Loss 0.048132   Top1 98.251953   Top5 100.000000   BatchTime 0.108147   LR 0.010000
INFO - Training [26][  100/  391]   Loss 0.047050   Top1 98.312500   Top5 100.000000   BatchTime 0.106536   LR 0.010000
INFO - Training [26][  120/  391]   Loss 0.048815   Top1 98.242188   Top5 100.000000   BatchTime 0.105558   LR 0.010000
INFO - Training [26][  140/  391]   Loss 0.049379   Top1 98.231027   Top5 100.000000   BatchTime 0.104768   LR 0.010000
INFO - Training [26][  160/  391]   Loss 0.049597   Top1 98.222656   Top5 100.000000   BatchTime 0.104232   LR 0.010000
INFO - Training [26][  180/  391]   Loss 0.050087   Top1 98.207465   Top5 100.000000   BatchTime 0.104332   LR 0.010000
INFO - Training [26][  200/  391]   Loss 0.048930   Top1 98.257812   Top5 100.000000   BatchTime 0.103915   LR 0.010000
INFO - Training [26][  220/  391]   Loss 0.048851   Top1 98.274148   Top5 100.000000   BatchTime 0.103579   LR 0.010000
INFO - Training [26][  240/  391]   Loss 0.049000   Top1 98.271484   Top5 100.000000   BatchTime 0.103289   LR 0.010000
INFO - Training [26][  260/  391]   Loss 0.049237   Top1 98.263221   Top5 100.000000   BatchTime 0.103134   LR 0.010000
INFO - Training [26][  280/  391]   Loss 0.049302   Top1 98.261719   Top5 100.000000   BatchTime 0.102860   LR 0.010000
INFO - Training [26][  300/  391]   Loss 0.049736   Top1 98.247396   Top5 100.000000   BatchTime 0.102743   LR 0.010000
INFO - Training [26][  320/  391]   Loss 0.050480   Top1 98.217773   Top5 100.000000   BatchTime 0.102653   LR 0.010000
INFO - Training [26][  340/  391]   Loss 0.050862   Top1 98.203125   Top5 100.000000   BatchTime 0.102440   LR 0.010000
INFO - Training [26][  360/  391]   Loss 0.051385   Top1 98.200955   Top5 100.000000   BatchTime 0.102209   LR 0.010000
INFO - Training [26][  380/  391]   Loss 0.051866   Top1 98.192845   Top5 99.997944   BatchTime 0.102067   LR 0.010000
INFO - ==> Top1: 98.192    Top5: 99.998    Loss: 0.052
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [26][   20/   79]   Loss 0.418364   Top1 89.531250   Top5 99.531250   BatchTime 0.130960
INFO - Validation [26][   40/   79]   Loss 0.429948   Top1 89.921875   Top5 99.433594   BatchTime 0.087187
INFO - Validation [26][   60/   79]   Loss 0.425277   Top1 90.104167   Top5 99.466146   BatchTime 0.072940
INFO - ==> Top1: 90.230    Top5: 99.530    Loss: 0.418
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.310   Top5: 99.620] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.310   Top5: 99.510] Sparsity : 0.749
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  27
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [27][   20/  391]   Loss 0.043109   Top1 98.476562   Top5 100.000000   BatchTime 0.179862   LR 0.010000
INFO - Training [27][   40/  391]   Loss 0.052689   Top1 98.183594   Top5 100.000000   BatchTime 0.129733   LR 0.010000
INFO - Training [27][   60/  391]   Loss 0.051731   Top1 98.255208   Top5 100.000000   BatchTime 0.114910   LR 0.010000
INFO - Training [27][   80/  391]   Loss 0.050754   Top1 98.261719   Top5 100.000000   BatchTime 0.105065   LR 0.010000
INFO - Training [27][  100/  391]   Loss 0.051857   Top1 98.171875   Top5 100.000000   BatchTime 0.105060   LR 0.010000
INFO - Training [27][  120/  391]   Loss 0.052445   Top1 98.111979   Top5 100.000000   BatchTime 0.104558   LR 0.010000
INFO - Training [27][  140/  391]   Loss 0.054439   Top1 98.069196   Top5 99.983259   BatchTime 0.104131   LR 0.010000
INFO - Training [27][  160/  391]   Loss 0.054633   Top1 98.071289   Top5 99.985352   BatchTime 0.103680   LR 0.010000
INFO - Training [27][  180/  391]   Loss 0.055388   Top1 98.077257   Top5 99.986979   BatchTime 0.103228   LR 0.010000
INFO - Training [27][  200/  391]   Loss 0.056639   Top1 98.000000   Top5 99.984375   BatchTime 0.102977   LR 0.010000
INFO - Training [27][  220/  391]   Loss 0.056633   Top1 98.014915   Top5 99.985795   BatchTime 0.102707   LR 0.010000
INFO - Training [27][  240/  391]   Loss 0.056874   Top1 98.011068   Top5 99.986979   BatchTime 0.102619   LR 0.010000
INFO - Training [27][  260/  391]   Loss 0.056992   Top1 98.007812   Top5 99.987981   BatchTime 0.102414   LR 0.010000
INFO - Training [27][  280/  391]   Loss 0.057914   Top1 97.977121   Top5 99.988839   BatchTime 0.102183   LR 0.010000
INFO - Training [27][  300/  391]   Loss 0.058549   Top1 97.958333   Top5 99.989583   BatchTime 0.102044   LR 0.010000
INFO - Training [27][  320/  391]   Loss 0.058584   Top1 97.949219   Top5 99.987793   BatchTime 0.101914   LR 0.010000
INFO - Training [27][  340/  391]   Loss 0.058364   Top1 97.954963   Top5 99.988511   BatchTime 0.101785   LR 0.010000
INFO - Training [27][  360/  391]   Loss 0.059195   Top1 97.914497   Top5 99.989149   BatchTime 0.101684   LR 0.010000
INFO - Training [27][  380/  391]   Loss 0.059598   Top1 97.902961   Top5 99.989720   BatchTime 0.101548   LR 0.010000
INFO - ==> Top1: 97.902    Top5: 99.990    Loss: 0.060
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [27][   20/   79]   Loss 0.420073   Top1 89.804688   Top5 99.609375   BatchTime 0.131414
INFO - Validation [27][   40/   79]   Loss 0.424118   Top1 89.863281   Top5 99.472656   BatchTime 0.088230
INFO - Validation [27][   60/   79]   Loss 0.416495   Top1 89.973958   Top5 99.505208   BatchTime 0.074173
INFO - ==> Top1: 89.940    Top5: 99.570    Loss: 0.410
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.310   Top5: 99.620] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.310   Top5: 99.510] Sparsity : 0.749
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  28
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [28][   20/  391]   Loss 0.061227   Top1 97.734375   Top5 100.000000   BatchTime 0.181684   LR 0.010000
INFO - Training [28][   40/  391]   Loss 0.054560   Top1 97.929688   Top5 100.000000   BatchTime 0.131214   LR 0.010000
INFO - Training [28][   60/  391]   Loss 0.060026   Top1 97.786458   Top5 100.000000   BatchTime 0.114999   LR 0.010000
INFO - Training [28][   80/  391]   Loss 0.059721   Top1 97.880859   Top5 100.000000   BatchTime 0.104626   LR 0.010000
INFO - Training [28][  100/  391]   Loss 0.059128   Top1 97.945312   Top5 100.000000   BatchTime 0.103781   LR 0.010000
INFO - Training [28][  120/  391]   Loss 0.062060   Top1 97.864583   Top5 99.993490   BatchTime 0.103365   LR 0.010000
INFO - Training [28][  140/  391]   Loss 0.061587   Top1 97.862723   Top5 99.994420   BatchTime 0.102932   LR 0.010000
INFO - Training [28][  160/  391]   Loss 0.063100   Top1 97.778320   Top5 99.995117   BatchTime 0.102623   LR 0.010000
INFO - Training [28][  180/  391]   Loss 0.063739   Top1 97.738715   Top5 99.995660   BatchTime 0.102411   LR 0.010000
INFO - Training [28][  200/  391]   Loss 0.064487   Top1 97.726562   Top5 99.996094   BatchTime 0.102268   LR 0.010000
INFO - Training [28][  220/  391]   Loss 0.064049   Top1 97.780540   Top5 99.992898   BatchTime 0.102502   LR 0.010000
INFO - Training [28][  240/  391]   Loss 0.064570   Top1 97.770182   Top5 99.993490   BatchTime 0.102417   LR 0.010000
INFO - Training [28][  260/  391]   Loss 0.065404   Top1 97.734375   Top5 99.993990   BatchTime 0.102262   LR 0.010000
INFO - Training [28][  280/  391]   Loss 0.066166   Top1 97.706473   Top5 99.988839   BatchTime 0.102117   LR 0.010000
INFO - Training [28][  300/  391]   Loss 0.065803   Top1 97.718750   Top5 99.989583   BatchTime 0.102059   LR 0.010000
INFO - Training [28][  320/  391]   Loss 0.066343   Top1 97.685547   Top5 99.990234   BatchTime 0.101952   LR 0.010000
INFO - Training [28][  340/  391]   Loss 0.067173   Top1 97.674632   Top5 99.988511   BatchTime 0.101750   LR 0.010000
INFO - Training [28][  360/  391]   Loss 0.067258   Top1 97.656250   Top5 99.989149   BatchTime 0.101564   LR 0.010000
INFO - Training [28][  380/  391]   Loss 0.068108   Top1 97.621299   Top5 99.989720   BatchTime 0.101437   LR 0.010000
INFO - ==> Top1: 97.610    Top5: 99.990    Loss: 0.068
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [28][   20/   79]   Loss 0.423354   Top1 89.687500   Top5 99.414062   BatchTime 0.130470
INFO - Validation [28][   40/   79]   Loss 0.433960   Top1 89.824219   Top5 99.375000   BatchTime 0.087270
INFO - Validation [28][   60/   79]   Loss 0.417551   Top1 90.026042   Top5 99.492188   BatchTime 0.073327
INFO - ==> Top1: 89.930    Top5: 99.540    Loss: 0.413
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.310   Top5: 99.620] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.310   Top5: 99.510] Sparsity : 0.749
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  29
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [29][   20/  391]   Loss 0.059287   Top1 97.695312   Top5 99.960938   BatchTime 0.177896   LR 0.010000
INFO - Training [29][   40/  391]   Loss 0.067025   Top1 97.578125   Top5 99.980469   BatchTime 0.129168   LR 0.010000
INFO - Training [29][   60/  391]   Loss 0.064367   Top1 97.708333   Top5 99.986979   BatchTime 0.116066   LR 0.010000
INFO - Training [29][   80/  391]   Loss 0.063510   Top1 97.695312   Top5 99.990234   BatchTime 0.105340   LR 0.010000
INFO - Training [29][  100/  391]   Loss 0.064691   Top1 97.687500   Top5 99.992188   BatchTime 0.103937   LR 0.010000
INFO - Training [29][  120/  391]   Loss 0.063948   Top1 97.695312   Top5 99.993490   BatchTime 0.103318   LR 0.010000
INFO - Training [29][  140/  391]   Loss 0.064632   Top1 97.672991   Top5 99.994420   BatchTime 0.102882   LR 0.010000
INFO - Training [29][  160/  391]   Loss 0.066194   Top1 97.636719   Top5 99.995117   BatchTime 0.102640   LR 0.010000
INFO - Training [29][  180/  391]   Loss 0.067667   Top1 97.591146   Top5 99.995660   BatchTime 0.102380   LR 0.010000
INFO - Training [29][  200/  391]   Loss 0.068670   Top1 97.566406   Top5 99.996094   BatchTime 0.102197   LR 0.010000
INFO - Training [29][  220/  391]   Loss 0.069139   Top1 97.560369   Top5 99.996449   BatchTime 0.102064   LR 0.010000
INFO - Training [29][  240/  391]   Loss 0.068458   Top1 97.604167   Top5 99.996745   BatchTime 0.101862   LR 0.010000
INFO - Training [29][  260/  391]   Loss 0.068562   Top1 97.587139   Top5 99.996995   BatchTime 0.101703   LR 0.010000
INFO - Training [29][  280/  391]   Loss 0.069110   Top1 97.564174   Top5 99.991629   BatchTime 0.101540   LR 0.010000
INFO - Training [29][  300/  391]   Loss 0.069031   Top1 97.578125   Top5 99.992188   BatchTime 0.101438   LR 0.010000
INFO - Training [29][  320/  391]   Loss 0.069092   Top1 97.568359   Top5 99.992676   BatchTime 0.101340   LR 0.010000
INFO - Training [29][  340/  391]   Loss 0.069182   Top1 97.573529   Top5 99.993107   BatchTime 0.101158   LR 0.010000
INFO - Training [29][  360/  391]   Loss 0.069846   Top1 97.556424   Top5 99.993490   BatchTime 0.101045   LR 0.010000
INFO - Training [29][  380/  391]   Loss 0.070343   Top1 97.549342   Top5 99.993832   BatchTime 0.100954   LR 0.010000
INFO - ==> Top1: 97.524    Top5: 99.994    Loss: 0.071
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [29][   20/   79]   Loss 0.425591   Top1 90.195312   Top5 99.570312   BatchTime 0.130282
INFO - Validation [29][   40/   79]   Loss 0.438726   Top1 89.824219   Top5 99.375000   BatchTime 0.087759
INFO - Validation [29][   60/   79]   Loss 0.423894   Top1 89.973958   Top5 99.440104   BatchTime 0.072925
INFO - ==> Top1: 90.010    Top5: 99.490    Loss: 0.421
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.310   Top5: 99.620] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.310   Top5: 99.510] Sparsity : 0.749
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  30
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [30][   20/  391]   Loss 0.067143   Top1 97.617188   Top5 100.000000   BatchTime 0.172566   LR 0.001000
INFO - Training [30][   40/  391]   Loss 0.060506   Top1 97.949219   Top5 100.000000   BatchTime 0.128129   LR 0.001000
INFO - Training [30][   60/  391]   Loss 0.059230   Top1 97.942708   Top5 100.000000   BatchTime 0.113201   LR 0.001000
INFO - Training [30][   80/  391]   Loss 0.060688   Top1 97.841797   Top5 100.000000   BatchTime 0.106331   LR 0.001000
INFO - Training [30][  100/  391]   Loss 0.062242   Top1 97.796875   Top5 100.000000   BatchTime 0.103963   LR 0.001000
INFO - Training [30][  120/  391]   Loss 0.061854   Top1 97.819010   Top5 99.993490   BatchTime 0.103487   LR 0.001000
INFO - Training [30][  140/  391]   Loss 0.060848   Top1 97.851562   Top5 99.994420   BatchTime 0.102959   LR 0.001000
INFO - Training [30][  160/  391]   Loss 0.059515   Top1 97.871094   Top5 99.995117   BatchTime 0.102637   LR 0.001000
INFO - Training [30][  180/  391]   Loss 0.058750   Top1 97.925347   Top5 99.995660   BatchTime 0.102293   LR 0.001000
INFO - Training [30][  200/  391]   Loss 0.059286   Top1 97.882812   Top5 99.996094   BatchTime 0.102022   LR 0.001000
INFO - Training [30][  220/  391]   Loss 0.059133   Top1 97.890625   Top5 99.996449   BatchTime 0.101916   LR 0.001000
INFO - Training [30][  240/  391]   Loss 0.058510   Top1 97.906901   Top5 99.996745   BatchTime 0.101852   LR 0.001000
INFO - Training [30][  260/  391]   Loss 0.058230   Top1 97.935697   Top5 99.996995   BatchTime 0.102194   LR 0.001000
INFO - Training [30][  280/  391]   Loss 0.057778   Top1 97.957589   Top5 99.997210   BatchTime 0.102071   LR 0.001000
INFO - Training [30][  300/  391]   Loss 0.057251   Top1 97.971354   Top5 99.997396   BatchTime 0.101986   LR 0.001000
INFO - Training [30][  320/  391]   Loss 0.057055   Top1 97.976074   Top5 99.997559   BatchTime 0.101947   LR 0.001000
INFO - Training [30][  340/  391]   Loss 0.057207   Top1 97.964154   Top5 99.997702   BatchTime 0.101757   LR 0.001000
INFO - Training [30][  360/  391]   Loss 0.057715   Top1 97.936198   Top5 99.997830   BatchTime 0.101589   LR 0.001000
INFO - Training [30][  380/  391]   Loss 0.057474   Top1 97.958470   Top5 99.997944   BatchTime 0.101413   LR 0.001000
INFO - ==> Top1: 97.972    Top5: 99.998    Loss: 0.057
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [30][   20/   79]   Loss 0.382452   Top1 90.937500   Top5 99.726562   BatchTime 0.132811
INFO - Validation [30][   40/   79]   Loss 0.400046   Top1 90.625000   Top5 99.589844   BatchTime 0.089070
INFO - Validation [30][   60/   79]   Loss 0.390546   Top1 90.651042   Top5 99.622396   BatchTime 0.074582
INFO - ==> Top1: 90.560    Top5: 99.640    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [30][Top1: 90.560   Top5: 99.640] Sparsity : 0.773
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 90.310   Top5: 99.620] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  31
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [31][   20/  391]   Loss 0.054912   Top1 98.203125   Top5 100.000000   BatchTime 0.177439   LR 0.001000
INFO - Training [31][   40/  391]   Loss 0.052882   Top1 98.222656   Top5 100.000000   BatchTime 0.128875   LR 0.001000
INFO - Training [31][   60/  391]   Loss 0.051548   Top1 98.190104   Top5 100.000000   BatchTime 0.113314   LR 0.001000
INFO - Training [31][   80/  391]   Loss 0.052141   Top1 98.193359   Top5 100.000000   BatchTime 0.104761   LR 0.001000
INFO - Training [31][  100/  391]   Loss 0.051799   Top1 98.187500   Top5 100.000000   BatchTime 0.102678   LR 0.001000
INFO - Training [31][  120/  391]   Loss 0.050669   Top1 98.235677   Top5 100.000000   BatchTime 0.102279   LR 0.001000
INFO - Training [31][  140/  391]   Loss 0.050855   Top1 98.242188   Top5 100.000000   BatchTime 0.101903   LR 0.001000
INFO - Training [31][  160/  391]   Loss 0.051161   Top1 98.193359   Top5 100.000000   BatchTime 0.101659   LR 0.001000
INFO - Training [31][  180/  391]   Loss 0.051043   Top1 98.224826   Top5 100.000000   BatchTime 0.101574   LR 0.001000
INFO - Training [31][  200/  391]   Loss 0.050973   Top1 98.214844   Top5 100.000000   BatchTime 0.101443   LR 0.001000
INFO - Training [31][  220/  391]   Loss 0.051040   Top1 98.213778   Top5 100.000000   BatchTime 0.101354   LR 0.001000
INFO - Training [31][  240/  391]   Loss 0.050788   Top1 98.225911   Top5 100.000000   BatchTime 0.101251   LR 0.001000
INFO - Training [31][  260/  391]   Loss 0.050233   Top1 98.239183   Top5 100.000000   BatchTime 0.101146   LR 0.001000
INFO - Training [31][  280/  391]   Loss 0.050834   Top1 98.214286   Top5 100.000000   BatchTime 0.101052   LR 0.001000
INFO - Training [31][  300/  391]   Loss 0.050612   Top1 98.223958   Top5 100.000000   BatchTime 0.101033   LR 0.001000
INFO - Training [31][  320/  391]   Loss 0.049902   Top1 98.242188   Top5 100.000000   BatchTime 0.100989   LR 0.001000
INFO - Training [31][  340/  391]   Loss 0.049571   Top1 98.253676   Top5 100.000000   BatchTime 0.100969   LR 0.001000
INFO - Training [31][  360/  391]   Loss 0.049885   Top1 98.246528   Top5 100.000000   BatchTime 0.100849   LR 0.001000
INFO - Training [31][  380/  391]   Loss 0.050142   Top1 98.225740   Top5 100.000000   BatchTime 0.100742   LR 0.001000
INFO - ==> Top1: 98.238    Top5: 100.000    Loss: 0.050
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [31][   20/   79]   Loss 0.374470   Top1 90.703125   Top5 99.687500   BatchTime 0.130497
INFO - Validation [31][   40/   79]   Loss 0.400543   Top1 90.332031   Top5 99.472656   BatchTime 0.087584
INFO - Validation [31][   60/   79]   Loss 0.390636   Top1 90.403646   Top5 99.531250   BatchTime 0.073383
INFO - ==> Top1: 90.340    Top5: 99.560    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [30][Top1: 90.560   Top5: 99.640] Sparsity : 0.773
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Scoreboard best 3 ==> Epoch [31][Top1: 90.340   Top5: 99.560] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  32
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [32][   20/  391]   Loss 0.045021   Top1 98.554688   Top5 100.000000   BatchTime 0.167046   LR 0.001000
INFO - Training [32][   40/  391]   Loss 0.045991   Top1 98.437500   Top5 100.000000   BatchTime 0.126617   LR 0.001000
INFO - Training [32][   60/  391]   Loss 0.047379   Top1 98.359375   Top5 100.000000   BatchTime 0.111671   LR 0.001000
INFO - Training [32][   80/  391]   Loss 0.048488   Top1 98.310547   Top5 100.000000   BatchTime 0.105638   LR 0.001000
INFO - Training [32][  100/  391]   Loss 0.049335   Top1 98.304688   Top5 100.000000   BatchTime 0.102443   LR 0.001000
INFO - Training [32][  120/  391]   Loss 0.048104   Top1 98.359375   Top5 99.993490   BatchTime 0.102179   LR 0.001000
INFO - Training [32][  140/  391]   Loss 0.047373   Top1 98.398438   Top5 99.994420   BatchTime 0.101997   LR 0.001000
INFO - Training [32][  160/  391]   Loss 0.048311   Top1 98.334961   Top5 99.995117   BatchTime 0.101845   LR 0.001000
INFO - Training [32][  180/  391]   Loss 0.048975   Top1 98.328993   Top5 99.995660   BatchTime 0.101633   LR 0.001000
INFO - Training [32][  200/  391]   Loss 0.049109   Top1 98.343750   Top5 99.996094   BatchTime 0.101556   LR 0.001000
INFO - Training [32][  220/  391]   Loss 0.048906   Top1 98.362926   Top5 99.996449   BatchTime 0.101420   LR 0.001000
INFO - Training [32][  240/  391]   Loss 0.048773   Top1 98.362630   Top5 99.996745   BatchTime 0.101310   LR 0.001000
INFO - Training [32][  260/  391]   Loss 0.048534   Top1 98.362380   Top5 99.996995   BatchTime 0.101272   LR 0.001000
INFO - Training [32][  280/  391]   Loss 0.047688   Top1 98.392857   Top5 99.997210   BatchTime 0.101186   LR 0.001000
INFO - Training [32][  300/  391]   Loss 0.047686   Top1 98.401042   Top5 99.997396   BatchTime 0.101134   LR 0.001000
INFO - Training [32][  320/  391]   Loss 0.047344   Top1 98.400879   Top5 99.997559   BatchTime 0.101387   LR 0.001000
INFO - Training [32][  340/  391]   Loss 0.047845   Top1 98.375460   Top5 99.993107   BatchTime 0.101279   LR 0.001000
INFO - Training [32][  360/  391]   Loss 0.048026   Top1 98.372396   Top5 99.991319   BatchTime 0.101121   LR 0.001000
INFO - Training [32][  380/  391]   Loss 0.047992   Top1 98.359375   Top5 99.991776   BatchTime 0.101028   LR 0.001000
INFO - ==> Top1: 98.372    Top5: 99.992    Loss: 0.048
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [32][   20/   79]   Loss 0.364879   Top1 90.507812   Top5 99.570312   BatchTime 0.131686
INFO - Validation [32][   40/   79]   Loss 0.389947   Top1 90.585938   Top5 99.531250   BatchTime 0.088702
INFO - Validation [32][   60/   79]   Loss 0.387309   Top1 90.572917   Top5 99.609375   BatchTime 0.073509
INFO - ==> Top1: 90.670    Top5: 99.630    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [32][Top1: 90.670   Top5: 99.630] Sparsity : 0.774
INFO - Scoreboard best 2 ==> Epoch [30][Top1: 90.560   Top5: 99.640] Sparsity : 0.773
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 90.400   Top5: 99.550] Sparsity : 0.742
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  33
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [33][   20/  391]   Loss 0.053940   Top1 98.007812   Top5 100.000000   BatchTime 0.171438   LR 0.001000
INFO - Training [33][   40/  391]   Loss 0.049708   Top1 98.261719   Top5 99.960938   BatchTime 0.128774   LR 0.001000
INFO - Training [33][   60/  391]   Loss 0.047650   Top1 98.333333   Top5 99.973958   BatchTime 0.113502   LR 0.001000
INFO - Training [33][   80/  391]   Loss 0.048417   Top1 98.320312   Top5 99.980469   BatchTime 0.105544   LR 0.001000
INFO - Training [33][  100/  391]   Loss 0.046986   Top1 98.312500   Top5 99.984375   BatchTime 0.102662   LR 0.001000
INFO - Training [33][  120/  391]   Loss 0.047391   Top1 98.261719   Top5 99.986979   BatchTime 0.102383   LR 0.001000
INFO - Training [33][  140/  391]   Loss 0.046504   Top1 98.314732   Top5 99.983259   BatchTime 0.102248   LR 0.001000
INFO - Training [33][  160/  391]   Loss 0.046249   Top1 98.359375   Top5 99.985352   BatchTime 0.102004   LR 0.001000
INFO - Training [33][  180/  391]   Loss 0.045996   Top1 98.359375   Top5 99.986979   BatchTime 0.101823   LR 0.001000
INFO - Training [33][  200/  391]   Loss 0.045873   Top1 98.378906   Top5 99.988281   BatchTime 0.101703   LR 0.001000
INFO - Training [33][  220/  391]   Loss 0.044787   Top1 98.405540   Top5 99.989347   BatchTime 0.101573   LR 0.001000
INFO - Training [33][  240/  391]   Loss 0.044780   Top1 98.398438   Top5 99.990234   BatchTime 0.101493   LR 0.001000
INFO - Training [33][  260/  391]   Loss 0.044938   Top1 98.380409   Top5 99.990986   BatchTime 0.101411   LR 0.001000
INFO - Training [33][  280/  391]   Loss 0.046034   Top1 98.339844   Top5 99.991629   BatchTime 0.101354   LR 0.001000
INFO - Training [33][  300/  391]   Loss 0.045557   Top1 98.351562   Top5 99.992188   BatchTime 0.101327   LR 0.001000
INFO - Training [33][  320/  391]   Loss 0.046122   Top1 98.342285   Top5 99.992676   BatchTime 0.101294   LR 0.001000
INFO - Training [33][  340/  391]   Loss 0.045960   Top1 98.352482   Top5 99.993107   BatchTime 0.101195   LR 0.001000
INFO - Training [33][  360/  391]   Loss 0.045965   Top1 98.352865   Top5 99.993490   BatchTime 0.101041   LR 0.001000
INFO - Training [33][  380/  391]   Loss 0.046164   Top1 98.353207   Top5 99.993832   BatchTime 0.100910   LR 0.001000
INFO - ==> Top1: 98.346    Top5: 99.994    Loss: 0.046
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [33][   20/   79]   Loss 0.364607   Top1 91.328125   Top5 99.492188   BatchTime 0.131687
INFO - Validation [33][   40/   79]   Loss 0.392297   Top1 90.683594   Top5 99.453125   BatchTime 0.088149
INFO - Validation [33][   60/   79]   Loss 0.386506   Top1 90.781250   Top5 99.531250   BatchTime 0.073941
INFO - ==> Top1: 90.700    Top5: 99.580    Loss: 0.382
INFO - Scoreboard best 1 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.670   Top5: 99.630] Sparsity : 0.774
INFO - Scoreboard best 3 ==> Epoch [30][Top1: 90.560   Top5: 99.640] Sparsity : 0.773
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  34
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [34][   20/  391]   Loss 0.044934   Top1 98.398438   Top5 100.000000   BatchTime 0.173655   LR 0.001000
INFO - Training [34][   40/  391]   Loss 0.045948   Top1 98.378906   Top5 100.000000   BatchTime 0.128879   LR 0.001000
INFO - Training [34][   60/  391]   Loss 0.043569   Top1 98.476562   Top5 100.000000   BatchTime 0.114040   LR 0.001000
INFO - Training [34][   80/  391]   Loss 0.043871   Top1 98.457031   Top5 100.000000   BatchTime 0.106691   LR 0.001000
INFO - Training [34][  100/  391]   Loss 0.044239   Top1 98.437500   Top5 100.000000   BatchTime 0.102894   LR 0.001000
INFO - Training [34][  120/  391]   Loss 0.044479   Top1 98.424479   Top5 100.000000   BatchTime 0.102762   LR 0.001000
INFO - Training [34][  140/  391]   Loss 0.044689   Top1 98.443080   Top5 100.000000   BatchTime 0.102690   LR 0.001000
INFO - Training [34][  160/  391]   Loss 0.044341   Top1 98.447266   Top5 100.000000   BatchTime 0.102500   LR 0.001000
INFO - Training [34][  180/  391]   Loss 0.043780   Top1 98.485243   Top5 100.000000   BatchTime 0.102190   LR 0.001000
INFO - Training [34][  200/  391]   Loss 0.043648   Top1 98.484375   Top5 100.000000   BatchTime 0.102107   LR 0.001000
INFO - Training [34][  220/  391]   Loss 0.044040   Top1 98.473011   Top5 100.000000   BatchTime 0.102015   LR 0.001000
INFO - Training [34][  240/  391]   Loss 0.043683   Top1 98.483073   Top5 100.000000   BatchTime 0.101940   LR 0.001000
INFO - Training [34][  260/  391]   Loss 0.043784   Top1 98.491587   Top5 100.000000   BatchTime 0.101858   LR 0.001000
INFO - Training [34][  280/  391]   Loss 0.043540   Top1 98.496094   Top5 100.000000   BatchTime 0.101746   LR 0.001000
INFO - Training [34][  300/  391]   Loss 0.043659   Top1 98.479167   Top5 100.000000   BatchTime 0.101767   LR 0.001000
INFO - Training [34][  320/  391]   Loss 0.043685   Top1 98.496094   Top5 100.000000   BatchTime 0.101673   LR 0.001000
INFO - Training [34][  340/  391]   Loss 0.043937   Top1 98.467371   Top5 100.000000   BatchTime 0.101558   LR 0.001000
INFO - Training [34][  360/  391]   Loss 0.044614   Top1 98.428819   Top5 100.000000   BatchTime 0.101439   LR 0.001000
INFO - Training [34][  380/  391]   Loss 0.044630   Top1 98.435444   Top5 100.000000   BatchTime 0.101348   LR 0.001000
INFO - ==> Top1: 98.432    Top5: 100.000    Loss: 0.045
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [34][   20/   79]   Loss 0.377163   Top1 90.585938   Top5 99.687500   BatchTime 0.138799
INFO - Validation [34][   40/   79]   Loss 0.395640   Top1 90.507812   Top5 99.550781   BatchTime 0.091523
INFO - Validation [34][   60/   79]   Loss 0.389219   Top1 90.546875   Top5 99.622396   BatchTime 0.075837
INFO - ==> Top1: 90.580    Top5: 99.650    Loss: 0.385
INFO - Scoreboard best 1 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.670   Top5: 99.630] Sparsity : 0.774
INFO - Scoreboard best 3 ==> Epoch [34][Top1: 90.580   Top5: 99.650] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  35
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [35][   20/  391]   Loss 0.036463   Top1 98.945312   Top5 100.000000   BatchTime 0.175678   LR 0.001000
INFO - Training [35][   40/  391]   Loss 0.039079   Top1 98.769531   Top5 100.000000   BatchTime 0.130429   LR 0.001000
INFO - Training [35][   60/  391]   Loss 0.039054   Top1 98.671875   Top5 100.000000   BatchTime 0.113910   LR 0.001000
INFO - Training [35][   80/  391]   Loss 0.037925   Top1 98.769531   Top5 100.000000   BatchTime 0.106740   LR 0.001000
INFO - Training [35][  100/  391]   Loss 0.039292   Top1 98.718750   Top5 99.992188   BatchTime 0.101722   LR 0.001000
INFO - Training [35][  120/  391]   Loss 0.040981   Top1 98.626302   Top5 99.993490   BatchTime 0.101496   LR 0.001000
INFO - Training [35][  140/  391]   Loss 0.041091   Top1 98.655134   Top5 99.994420   BatchTime 0.101328   LR 0.001000
INFO - Training [35][  160/  391]   Loss 0.041070   Top1 98.608398   Top5 99.995117   BatchTime 0.101334   LR 0.001000
INFO - Training [35][  180/  391]   Loss 0.040569   Top1 98.650174   Top5 99.995660   BatchTime 0.101233   LR 0.001000
INFO - Training [35][  200/  391]   Loss 0.040106   Top1 98.671875   Top5 99.996094   BatchTime 0.101178   LR 0.001000
INFO - Training [35][  220/  391]   Loss 0.040277   Top1 98.654119   Top5 99.996449   BatchTime 0.101150   LR 0.001000
INFO - Training [35][  240/  391]   Loss 0.040796   Top1 98.616536   Top5 99.996745   BatchTime 0.101111   LR 0.001000
INFO - Training [35][  260/  391]   Loss 0.040822   Top1 98.617788   Top5 99.996995   BatchTime 0.100999   LR 0.001000
INFO - Training [35][  280/  391]   Loss 0.040901   Top1 98.630022   Top5 99.997210   BatchTime 0.100945   LR 0.001000
INFO - Training [35][  300/  391]   Loss 0.040773   Top1 98.638021   Top5 99.997396   BatchTime 0.100917   LR 0.001000
INFO - Training [35][  320/  391]   Loss 0.040862   Top1 98.645020   Top5 99.997559   BatchTime 0.100882   LR 0.001000
INFO - Training [35][  340/  391]   Loss 0.040651   Top1 98.658088   Top5 99.997702   BatchTime 0.100761   LR 0.001000
INFO - Training [35][  360/  391]   Loss 0.041276   Top1 98.628472   Top5 99.997830   BatchTime 0.100691   LR 0.001000
INFO - Training [35][  380/  391]   Loss 0.041185   Top1 98.628701   Top5 99.997944   BatchTime 0.100625   LR 0.001000
INFO - ==> Top1: 98.628    Top5: 99.998    Loss: 0.041
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [35][   20/   79]   Loss 0.379110   Top1 90.781250   Top5 99.687500   BatchTime 0.130237
INFO - Validation [35][   40/   79]   Loss 0.399720   Top1 90.625000   Top5 99.550781   BatchTime 0.086708
INFO - Validation [35][   60/   79]   Loss 0.388504   Top1 90.898438   Top5 99.622396   BatchTime 0.073077
INFO - ==> Top1: 90.830    Top5: 99.660    Loss: 0.382
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.670   Top5: 99.630] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  36
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [36][   20/  391]   Loss 0.043593   Top1 98.320312   Top5 100.000000   BatchTime 0.174359   LR 0.001000
INFO - Training [36][   40/  391]   Loss 0.040606   Top1 98.554688   Top5 100.000000   BatchTime 0.127802   LR 0.001000
INFO - Training [36][   60/  391]   Loss 0.038144   Top1 98.684896   Top5 100.000000   BatchTime 0.112517   LR 0.001000
INFO - Training [36][   80/  391]   Loss 0.039035   Top1 98.632812   Top5 100.000000   BatchTime 0.106481   LR 0.001000
INFO - Training [36][  100/  391]   Loss 0.038312   Top1 98.710938   Top5 100.000000   BatchTime 0.099862   LR 0.001000
INFO - Training [36][  120/  391]   Loss 0.038117   Top1 98.717448   Top5 100.000000   BatchTime 0.099934   LR 0.001000
INFO - Training [36][  140/  391]   Loss 0.040326   Top1 98.632812   Top5 100.000000   BatchTime 0.100036   LR 0.001000
INFO - Training [36][  160/  391]   Loss 0.041021   Top1 98.608398   Top5 100.000000   BatchTime 0.100121   LR 0.001000
INFO - Training [36][  180/  391]   Loss 0.040793   Top1 98.637153   Top5 100.000000   BatchTime 0.100116   LR 0.001000
INFO - Training [36][  200/  391]   Loss 0.041720   Top1 98.582031   Top5 100.000000   BatchTime 0.100287   LR 0.001000
INFO - Training [36][  220/  391]   Loss 0.041217   Top1 98.597301   Top5 100.000000   BatchTime 0.100285   LR 0.001000
INFO - Training [36][  240/  391]   Loss 0.040983   Top1 98.606771   Top5 100.000000   BatchTime 0.100226   LR 0.001000
INFO - Training [36][  260/  391]   Loss 0.041517   Top1 98.590745   Top5 100.000000   BatchTime 0.100269   LR 0.001000
INFO - Training [36][  280/  391]   Loss 0.041254   Top1 98.613281   Top5 100.000000   BatchTime 0.100224   LR 0.001000
INFO - Training [36][  300/  391]   Loss 0.041966   Top1 98.583333   Top5 100.000000   BatchTime 0.100239   LR 0.001000
INFO - Training [36][  320/  391]   Loss 0.042318   Top1 98.566895   Top5 100.000000   BatchTime 0.100263   LR 0.001000
INFO - Training [36][  340/  391]   Loss 0.042558   Top1 98.554688   Top5 100.000000   BatchTime 0.100224   LR 0.001000
INFO - Training [36][  360/  391]   Loss 0.042431   Top1 98.563368   Top5 100.000000   BatchTime 0.100104   LR 0.001000
INFO - Training [36][  380/  391]   Loss 0.042616   Top1 98.546464   Top5 100.000000   BatchTime 0.100038   LR 0.001000
INFO - ==> Top1: 98.548    Top5: 100.000    Loss: 0.043
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [36][   20/   79]   Loss 0.378340   Top1 90.742188   Top5 99.570312   BatchTime 0.130733
INFO - Validation [36][   40/   79]   Loss 0.399663   Top1 90.566406   Top5 99.511719   BatchTime 0.087855
INFO - Validation [36][   60/   79]   Loss 0.390419   Top1 90.781250   Top5 99.596354   BatchTime 0.073372
INFO - ==> Top1: 90.660    Top5: 99.630    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.670   Top5: 99.630] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  37
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [37][   20/  391]   Loss 0.054746   Top1 97.890625   Top5 100.000000   BatchTime 0.186911   LR 0.001000
INFO - Training [37][   40/  391]   Loss 0.044870   Top1 98.378906   Top5 100.000000   BatchTime 0.133298   LR 0.001000
INFO - Training [37][   60/  391]   Loss 0.043301   Top1 98.528646   Top5 100.000000   BatchTime 0.116946   LR 0.001000
INFO - Training [37][   80/  391]   Loss 0.044732   Top1 98.476562   Top5 100.000000   BatchTime 0.108764   LR 0.001000
INFO - Training [37][  100/  391]   Loss 0.043134   Top1 98.539062   Top5 100.000000   BatchTime 0.103082   LR 0.001000
INFO - Training [37][  120/  391]   Loss 0.042325   Top1 98.567708   Top5 100.000000   BatchTime 0.102102   LR 0.001000
INFO - Training [37][  140/  391]   Loss 0.041649   Top1 98.582589   Top5 100.000000   BatchTime 0.101875   LR 0.001000
INFO - Training [37][  160/  391]   Loss 0.042170   Top1 98.549805   Top5 100.000000   BatchTime 0.101608   LR 0.001000
INFO - Training [37][  180/  391]   Loss 0.041677   Top1 98.554688   Top5 100.000000   BatchTime 0.101419   LR 0.001000
INFO - Training [37][  200/  391]   Loss 0.041517   Top1 98.550781   Top5 100.000000   BatchTime 0.101328   LR 0.001000
INFO - Training [37][  220/  391]   Loss 0.042208   Top1 98.536932   Top5 99.996449   BatchTime 0.101312   LR 0.001000
INFO - Training [37][  240/  391]   Loss 0.042282   Top1 98.548177   Top5 99.996745   BatchTime 0.101235   LR 0.001000
INFO - Training [37][  260/  391]   Loss 0.042100   Top1 98.557692   Top5 99.996995   BatchTime 0.101206   LR 0.001000
INFO - Training [37][  280/  391]   Loss 0.042137   Top1 98.554688   Top5 99.997210   BatchTime 0.101114   LR 0.001000
INFO - Training [37][  300/  391]   Loss 0.042357   Top1 98.546875   Top5 99.997396   BatchTime 0.101062   LR 0.001000
INFO - Training [37][  320/  391]   Loss 0.042212   Top1 98.552246   Top5 99.997559   BatchTime 0.100974   LR 0.001000
INFO - Training [37][  340/  391]   Loss 0.041837   Top1 98.566176   Top5 99.997702   BatchTime 0.100923   LR 0.001000
INFO - Training [37][  360/  391]   Loss 0.041662   Top1 98.567708   Top5 99.997830   BatchTime 0.100792   LR 0.001000
INFO - Training [37][  380/  391]   Loss 0.041847   Top1 98.558799   Top5 99.997944   BatchTime 0.100709   LR 0.001000
INFO - ==> Top1: 98.554    Top5: 99.996    Loss: 0.042
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [37][   20/   79]   Loss 0.390978   Top1 90.703125   Top5 99.687500   BatchTime 0.132683
INFO - Validation [37][   40/   79]   Loss 0.408093   Top1 90.507812   Top5 99.511719   BatchTime 0.088655
INFO - Validation [37][   60/   79]   Loss 0.399444   Top1 90.468750   Top5 99.622396   BatchTime 0.074261
INFO - ==> Top1: 90.520    Top5: 99.670    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.670   Top5: 99.630] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  38
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [38][   20/  391]   Loss 0.040888   Top1 98.789062   Top5 100.000000   BatchTime 0.184995   LR 0.001000
INFO - Training [38][   40/  391]   Loss 0.037948   Top1 98.808594   Top5 100.000000   BatchTime 0.131350   LR 0.001000
INFO - Training [38][   60/  391]   Loss 0.038624   Top1 98.723958   Top5 100.000000   BatchTime 0.114766   LR 0.001000
INFO - Training [38][   80/  391]   Loss 0.041190   Top1 98.632812   Top5 100.000000   BatchTime 0.106312   LR 0.001000
INFO - Training [38][  100/  391]   Loss 0.041032   Top1 98.570312   Top5 100.000000   BatchTime 0.100911   LR 0.001000
INFO - Training [38][  120/  391]   Loss 0.040471   Top1 98.619792   Top5 100.000000   BatchTime 0.099290   LR 0.001000
INFO - Training [38][  140/  391]   Loss 0.041037   Top1 98.604911   Top5 100.000000   BatchTime 0.099120   LR 0.001000
INFO - Training [38][  160/  391]   Loss 0.041424   Top1 98.583984   Top5 100.000000   BatchTime 0.099450   LR 0.001000
INFO - Training [38][  180/  391]   Loss 0.041110   Top1 98.602431   Top5 100.000000   BatchTime 0.099569   LR 0.001000
INFO - Training [38][  200/  391]   Loss 0.042005   Top1 98.570312   Top5 100.000000   BatchTime 0.099681   LR 0.001000
INFO - Training [38][  220/  391]   Loss 0.041691   Top1 98.572443   Top5 100.000000   BatchTime 0.099767   LR 0.001000
INFO - Training [38][  240/  391]   Loss 0.041254   Top1 98.574219   Top5 100.000000   BatchTime 0.099888   LR 0.001000
INFO - Training [38][  260/  391]   Loss 0.040936   Top1 98.596755   Top5 100.000000   BatchTime 0.099958   LR 0.001000
INFO - Training [38][  280/  391]   Loss 0.040807   Top1 98.599330   Top5 100.000000   BatchTime 0.100039   LR 0.001000
INFO - Training [38][  300/  391]   Loss 0.040590   Top1 98.617188   Top5 100.000000   BatchTime 0.100087   LR 0.001000
INFO - Training [38][  320/  391]   Loss 0.040583   Top1 98.618164   Top5 100.000000   BatchTime 0.100171   LR 0.001000
INFO - Training [38][  340/  391]   Loss 0.040777   Top1 98.612132   Top5 100.000000   BatchTime 0.100145   LR 0.001000
INFO - Training [38][  360/  391]   Loss 0.040534   Top1 98.615451   Top5 100.000000   BatchTime 0.100119   LR 0.001000
INFO - Training [38][  380/  391]   Loss 0.040491   Top1 98.608141   Top5 100.000000   BatchTime 0.100072   LR 0.001000
INFO - ==> Top1: 98.608    Top5: 100.000    Loss: 0.040
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [38][   20/   79]   Loss 0.365080   Top1 90.898438   Top5 99.648438   BatchTime 0.129682
INFO - Validation [38][   40/   79]   Loss 0.388405   Top1 90.761719   Top5 99.511719   BatchTime 0.087274
INFO - Validation [38][   60/   79]   Loss 0.385479   Top1 90.768229   Top5 99.583333   BatchTime 0.073207
INFO - ==> Top1: 90.650    Top5: 99.630    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.670   Top5: 99.630] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  39
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [39][   20/  391]   Loss 0.049433   Top1 98.046875   Top5 100.000000   BatchTime 0.189212   LR 0.001000
INFO - Training [39][   40/  391]   Loss 0.045712   Top1 98.164062   Top5 100.000000   BatchTime 0.135749   LR 0.001000
INFO - Training [39][   60/  391]   Loss 0.042430   Top1 98.411458   Top5 100.000000   BatchTime 0.118041   LR 0.001000
INFO - Training [39][   80/  391]   Loss 0.044040   Top1 98.330078   Top5 100.000000   BatchTime 0.110461   LR 0.001000
INFO - Training [39][  100/  391]   Loss 0.044923   Top1 98.289062   Top5 100.000000   BatchTime 0.105155   LR 0.001000
INFO - Training [39][  120/  391]   Loss 0.044986   Top1 98.339844   Top5 100.000000   BatchTime 0.101227   LR 0.001000
INFO - Training [39][  140/  391]   Loss 0.043940   Top1 98.387277   Top5 100.000000   BatchTime 0.098597   LR 0.001000
INFO - Training [39][  160/  391]   Loss 0.043420   Top1 98.408203   Top5 100.000000   BatchTime 0.097331   LR 0.001000
INFO - Training [39][  180/  391]   Loss 0.043507   Top1 98.407118   Top5 100.000000   BatchTime 0.094766   LR 0.001000
INFO - Training [39][  200/  391]   Loss 0.043300   Top1 98.410156   Top5 100.000000   BatchTime 0.092641   LR 0.001000
INFO - Training [39][  220/  391]   Loss 0.042926   Top1 98.433949   Top5 100.000000   BatchTime 0.090854   LR 0.001000
INFO - Training [39][  240/  391]   Loss 0.042323   Top1 98.450521   Top5 100.000000   BatchTime 0.089227   LR 0.001000
INFO - Training [39][  260/  391]   Loss 0.043229   Top1 98.413462   Top5 100.000000   BatchTime 0.088073   LR 0.001000
INFO - Training [39][  280/  391]   Loss 0.043467   Top1 98.404018   Top5 100.000000   BatchTime 0.087303   LR 0.001000
INFO - Training [39][  300/  391]   Loss 0.043029   Top1 98.434896   Top5 100.000000   BatchTime 0.086485   LR 0.001000
INFO - Training [39][  320/  391]   Loss 0.042847   Top1 98.447266   Top5 99.997559   BatchTime 0.085825   LR 0.001000
INFO - Training [39][  340/  391]   Loss 0.042576   Top1 98.467371   Top5 99.997702   BatchTime 0.084975   LR 0.001000
INFO - Training [39][  360/  391]   Loss 0.042440   Top1 98.470052   Top5 99.997830   BatchTime 0.084162   LR 0.001000
INFO - Training [39][  380/  391]   Loss 0.042012   Top1 98.486842   Top5 99.997944   BatchTime 0.083526   LR 0.001000
INFO - ==> Top1: 98.490    Top5: 99.998    Loss: 0.042
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [39][   20/   79]   Loss 0.382432   Top1 90.859375   Top5 99.648438   BatchTime 0.121423
INFO - Validation [39][   40/   79]   Loss 0.397792   Top1 90.722656   Top5 99.589844   BatchTime 0.077062
INFO - Validation [39][   60/   79]   Loss 0.389678   Top1 90.937500   Top5 99.635417   BatchTime 0.062914
INFO - ==> Top1: 90.800    Top5: 99.660    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  40
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [40][   20/  391]   Loss 0.036025   Top1 98.554688   Top5 100.000000   BatchTime 0.184918   LR 0.001000
INFO - Training [40][   40/  391]   Loss 0.038932   Top1 98.476562   Top5 100.000000   BatchTime 0.143444   LR 0.001000
INFO - Training [40][   60/  391]   Loss 0.038699   Top1 98.632812   Top5 100.000000   BatchTime 0.129509   LR 0.001000
INFO - Training [40][   80/  391]   Loss 0.039458   Top1 98.623047   Top5 99.990234   BatchTime 0.122311   LR 0.001000
INFO - Training [40][  100/  391]   Loss 0.040626   Top1 98.609375   Top5 99.992188   BatchTime 0.117939   LR 0.001000
INFO - Training [40][  120/  391]   Loss 0.039995   Top1 98.665365   Top5 99.993490   BatchTime 0.115161   LR 0.001000
INFO - Training [40][  140/  391]   Loss 0.039557   Top1 98.660714   Top5 99.994420   BatchTime 0.113086   LR 0.001000
INFO - Training [40][  160/  391]   Loss 0.039530   Top1 98.652344   Top5 99.995117   BatchTime 0.111547   LR 0.001000
INFO - Training [40][  180/  391]   Loss 0.039703   Top1 98.632812   Top5 99.995660   BatchTime 0.110441   LR 0.001000
INFO - Training [40][  200/  391]   Loss 0.040604   Top1 98.613281   Top5 99.996094   BatchTime 0.109563   LR 0.001000
INFO - Training [40][  220/  391]   Loss 0.041492   Top1 98.572443   Top5 99.996449   BatchTime 0.108806   LR 0.001000
INFO - Training [40][  240/  391]   Loss 0.041615   Top1 98.574219   Top5 99.996745   BatchTime 0.108089   LR 0.001000
INFO - Training [40][  260/  391]   Loss 0.042133   Top1 98.557692   Top5 99.996995   BatchTime 0.107529   LR 0.001000
INFO - Training [40][  280/  391]   Loss 0.042082   Top1 98.563058   Top5 99.997210   BatchTime 0.107133   LR 0.001000
INFO - Training [40][  300/  391]   Loss 0.042581   Top1 98.536458   Top5 99.997396   BatchTime 0.106680   LR 0.001000
INFO - Training [40][  320/  391]   Loss 0.042493   Top1 98.542480   Top5 99.997559   BatchTime 0.106302   LR 0.001000
INFO - Training [40][  340/  391]   Loss 0.042201   Top1 98.536305   Top5 99.997702   BatchTime 0.105903   LR 0.001000
INFO - Training [40][  360/  391]   Loss 0.042212   Top1 98.537326   Top5 99.997830   BatchTime 0.105488   LR 0.001000
INFO - Training [40][  380/  391]   Loss 0.042222   Top1 98.542352   Top5 99.997944   BatchTime 0.105060   LR 0.001000
INFO - ==> Top1: 98.546    Top5: 99.998    Loss: 0.042
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [40][   20/   79]   Loss 0.380906   Top1 90.781250   Top5 99.609375   BatchTime 0.119144
INFO - Validation [40][   40/   79]   Loss 0.404827   Top1 90.644531   Top5 99.550781   BatchTime 0.072479
INFO - Validation [40][   60/   79]   Loss 0.394957   Top1 90.742188   Top5 99.622396   BatchTime 0.057209
INFO - ==> Top1: 90.590    Top5: 99.660    Loss: 0.388
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  41
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [41][   20/  391]   Loss 0.038421   Top1 98.515625   Top5 100.000000   BatchTime 0.169701   LR 0.001000
INFO - Training [41][   40/  391]   Loss 0.040438   Top1 98.554688   Top5 100.000000   BatchTime 0.137186   LR 0.001000
INFO - Training [41][   60/  391]   Loss 0.039161   Top1 98.606771   Top5 100.000000   BatchTime 0.125014   LR 0.001000
INFO - Training [41][   80/  391]   Loss 0.039741   Top1 98.593750   Top5 100.000000   BatchTime 0.118832   LR 0.001000
INFO - Training [41][  100/  391]   Loss 0.040842   Top1 98.539062   Top5 100.000000   BatchTime 0.114993   LR 0.001000
INFO - Training [41][  120/  391]   Loss 0.039991   Top1 98.535156   Top5 100.000000   BatchTime 0.112704   LR 0.001000
INFO - Training [41][  140/  391]   Loss 0.039781   Top1 98.554688   Top5 100.000000   BatchTime 0.111559   LR 0.001000
INFO - Training [41][  160/  391]   Loss 0.039822   Top1 98.569336   Top5 100.000000   BatchTime 0.110169   LR 0.001000
INFO - Training [41][  180/  391]   Loss 0.039905   Top1 98.598090   Top5 99.995660   BatchTime 0.109083   LR 0.001000
INFO - Training [41][  200/  391]   Loss 0.039914   Top1 98.605469   Top5 99.996094   BatchTime 0.108383   LR 0.001000
INFO - Training [41][  220/  391]   Loss 0.041260   Top1 98.551136   Top5 99.996449   BatchTime 0.107684   LR 0.001000
INFO - Training [41][  240/  391]   Loss 0.040915   Top1 98.561198   Top5 99.996745   BatchTime 0.107056   LR 0.001000
INFO - Training [41][  260/  391]   Loss 0.041159   Top1 98.548678   Top5 99.996995   BatchTime 0.106507   LR 0.001000
INFO - Training [41][  280/  391]   Loss 0.040869   Top1 98.557478   Top5 99.997210   BatchTime 0.106080   LR 0.001000
INFO - Training [41][  300/  391]   Loss 0.041094   Top1 98.549479   Top5 99.997396   BatchTime 0.105645   LR 0.001000
INFO - Training [41][  320/  391]   Loss 0.040955   Top1 98.559570   Top5 99.995117   BatchTime 0.105272   LR 0.001000
INFO - Training [41][  340/  391]   Loss 0.040558   Top1 98.575368   Top5 99.995404   BatchTime 0.104952   LR 0.001000
INFO - Training [41][  360/  391]   Loss 0.040669   Top1 98.578559   Top5 99.995660   BatchTime 0.104660   LR 0.001000
INFO - Training [41][  380/  391]   Loss 0.040332   Top1 98.601974   Top5 99.995888   BatchTime 0.104242   LR 0.001000
INFO - ==> Top1: 98.586    Top5: 99.996    Loss: 0.040
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [41][   20/   79]   Loss 0.376839   Top1 90.625000   Top5 99.648438   BatchTime 0.121141
INFO - Validation [41][   40/   79]   Loss 0.397928   Top1 90.566406   Top5 99.550781   BatchTime 0.073962
INFO - Validation [41][   60/   79]   Loss 0.390328   Top1 90.703125   Top5 99.622396   BatchTime 0.058223
INFO - ==> Top1: 90.590    Top5: 99.630    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  42
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [42][   20/  391]   Loss 0.044006   Top1 98.476562   Top5 100.000000   BatchTime 0.170944   LR 0.001000
INFO - Training [42][   40/  391]   Loss 0.037020   Top1 98.789062   Top5 100.000000   BatchTime 0.138078   LR 0.001000
INFO - Training [42][   60/  391]   Loss 0.040184   Top1 98.645833   Top5 100.000000   BatchTime 0.125708   LR 0.001000
INFO - Training [42][   80/  391]   Loss 0.040741   Top1 98.642578   Top5 100.000000   BatchTime 0.119512   LR 0.001000
INFO - Training [42][  100/  391]   Loss 0.041588   Top1 98.601562   Top5 100.000000   BatchTime 0.115629   LR 0.001000
INFO - Training [42][  120/  391]   Loss 0.040058   Top1 98.626302   Top5 100.000000   BatchTime 0.113378   LR 0.001000
INFO - Training [42][  140/  391]   Loss 0.039383   Top1 98.655134   Top5 100.000000   BatchTime 0.111641   LR 0.001000
INFO - Training [42][  160/  391]   Loss 0.040129   Top1 98.632812   Top5 100.000000   BatchTime 0.110418   LR 0.001000
INFO - Training [42][  180/  391]   Loss 0.039727   Top1 98.641493   Top5 100.000000   BatchTime 0.109327   LR 0.001000
INFO - Training [42][  200/  391]   Loss 0.040411   Top1 98.625000   Top5 100.000000   BatchTime 0.108509   LR 0.001000
INFO - Training [42][  220/  391]   Loss 0.040583   Top1 98.618608   Top5 99.996449   BatchTime 0.107714   LR 0.001000
INFO - Training [42][  240/  391]   Loss 0.040918   Top1 98.616536   Top5 99.996745   BatchTime 0.107109   LR 0.001000
INFO - Training [42][  260/  391]   Loss 0.040346   Top1 98.638822   Top5 99.996995   BatchTime 0.106599   LR 0.001000
INFO - Training [42][  280/  391]   Loss 0.040156   Top1 98.627232   Top5 99.997210   BatchTime 0.106221   LR 0.001000
INFO - Training [42][  300/  391]   Loss 0.040746   Top1 98.609375   Top5 99.997396   BatchTime 0.105870   LR 0.001000
INFO - Training [42][  320/  391]   Loss 0.040523   Top1 98.608398   Top5 99.997559   BatchTime 0.105615   LR 0.001000
INFO - Training [42][  340/  391]   Loss 0.040474   Top1 98.619026   Top5 99.997702   BatchTime 0.105355   LR 0.001000
INFO - Training [42][  360/  391]   Loss 0.040699   Top1 98.604601   Top5 99.997830   BatchTime 0.104980   LR 0.001000
INFO - Training [42][  380/  391]   Loss 0.040737   Top1 98.583470   Top5 99.997944   BatchTime 0.104604   LR 0.001000
INFO - ==> Top1: 98.580    Top5: 99.998    Loss: 0.041
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [42][   20/   79]   Loss 0.377084   Top1 90.351562   Top5 99.570312   BatchTime 0.121824
INFO - Validation [42][   40/   79]   Loss 0.406141   Top1 90.449219   Top5 99.472656   BatchTime 0.074233
INFO - Validation [42][   60/   79]   Loss 0.397138   Top1 90.651042   Top5 99.531250   BatchTime 0.058315
INFO - ==> Top1: 90.650    Top5: 99.590    Loss: 0.388
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  43
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [43][   20/  391]   Loss 0.044505   Top1 98.242188   Top5 100.000000   BatchTime 0.176106   LR 0.001000
INFO - Training [43][   40/  391]   Loss 0.042109   Top1 98.574219   Top5 100.000000   BatchTime 0.136964   LR 0.001000
INFO - Training [43][   60/  391]   Loss 0.043305   Top1 98.502604   Top5 100.000000   BatchTime 0.124731   LR 0.001000
INFO - Training [43][   80/  391]   Loss 0.041961   Top1 98.574219   Top5 100.000000   BatchTime 0.118898   LR 0.001000
INFO - Training [43][  100/  391]   Loss 0.041020   Top1 98.617188   Top5 100.000000   BatchTime 0.115364   LR 0.001000
INFO - Training [43][  120/  391]   Loss 0.040210   Top1 98.665365   Top5 100.000000   BatchTime 0.113072   LR 0.001000
INFO - Training [43][  140/  391]   Loss 0.039165   Top1 98.666295   Top5 100.000000   BatchTime 0.111235   LR 0.001000
INFO - Training [43][  160/  391]   Loss 0.038910   Top1 98.681641   Top5 100.000000   BatchTime 0.109944   LR 0.001000
INFO - Training [43][  180/  391]   Loss 0.039021   Top1 98.654514   Top5 100.000000   BatchTime 0.108838   LR 0.001000
INFO - Training [43][  200/  391]   Loss 0.038096   Top1 98.691406   Top5 100.000000   BatchTime 0.108706   LR 0.001000
INFO - Training [43][  220/  391]   Loss 0.038350   Top1 98.689631   Top5 100.000000   BatchTime 0.107938   LR 0.001000
INFO - Training [43][  240/  391]   Loss 0.038476   Top1 98.681641   Top5 100.000000   BatchTime 0.107400   LR 0.001000
INFO - Training [43][  260/  391]   Loss 0.038803   Top1 98.662861   Top5 100.000000   BatchTime 0.106824   LR 0.001000
INFO - Training [43][  280/  391]   Loss 0.039016   Top1 98.666295   Top5 100.000000   BatchTime 0.106332   LR 0.001000
INFO - Training [43][  300/  391]   Loss 0.038891   Top1 98.661458   Top5 100.000000   BatchTime 0.105887   LR 0.001000
INFO - Training [43][  320/  391]   Loss 0.039533   Top1 98.625488   Top5 100.000000   BatchTime 0.105526   LR 0.001000
INFO - Training [43][  340/  391]   Loss 0.039267   Top1 98.644301   Top5 100.000000   BatchTime 0.105148   LR 0.001000
INFO - Training [43][  360/  391]   Loss 0.039409   Top1 98.643663   Top5 100.000000   BatchTime 0.104782   LR 0.001000
INFO - Training [43][  380/  391]   Loss 0.039639   Top1 98.632812   Top5 100.000000   BatchTime 0.104405   LR 0.001000
INFO - ==> Top1: 98.620    Top5: 100.000    Loss: 0.040
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [43][   20/   79]   Loss 0.379302   Top1 90.820312   Top5 99.648438   BatchTime 0.125660
INFO - Validation [43][   40/   79]   Loss 0.402055   Top1 90.703125   Top5 99.531250   BatchTime 0.076027
INFO - Validation [43][   60/   79]   Loss 0.397076   Top1 90.611979   Top5 99.583333   BatchTime 0.059665
INFO - ==> Top1: 90.500    Top5: 99.620    Loss: 0.390
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  44
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [44][   20/  391]   Loss 0.040376   Top1 98.437500   Top5 100.000000   BatchTime 0.171298   LR 0.001000
INFO - Training [44][   40/  391]   Loss 0.038252   Top1 98.554688   Top5 100.000000   BatchTime 0.136486   LR 0.001000
INFO - Training [44][   60/  391]   Loss 0.041860   Top1 98.424479   Top5 100.000000   BatchTime 0.124760   LR 0.001000
INFO - Training [44][   80/  391]   Loss 0.040268   Top1 98.603516   Top5 100.000000   BatchTime 0.118688   LR 0.001000
INFO - Training [44][  100/  391]   Loss 0.040038   Top1 98.617188   Top5 100.000000   BatchTime 0.115298   LR 0.001000
INFO - Training [44][  120/  391]   Loss 0.041325   Top1 98.528646   Top5 100.000000   BatchTime 0.112759   LR 0.001000
INFO - Training [44][  140/  391]   Loss 0.039936   Top1 98.565848   Top5 100.000000   BatchTime 0.111067   LR 0.001000
INFO - Training [44][  160/  391]   Loss 0.039403   Top1 98.583984   Top5 100.000000   BatchTime 0.109857   LR 0.001000
INFO - Training [44][  180/  391]   Loss 0.039527   Top1 98.580729   Top5 100.000000   BatchTime 0.108908   LR 0.001000
INFO - Training [44][  200/  391]   Loss 0.039291   Top1 98.582031   Top5 100.000000   BatchTime 0.108122   LR 0.001000
INFO - Training [44][  220/  391]   Loss 0.039046   Top1 98.590199   Top5 100.000000   BatchTime 0.107525   LR 0.001000
INFO - Training [44][  240/  391]   Loss 0.038661   Top1 98.590495   Top5 100.000000   BatchTime 0.106928   LR 0.001000
INFO - Training [44][  260/  391]   Loss 0.038826   Top1 98.608774   Top5 100.000000   BatchTime 0.106466   LR 0.001000
INFO - Training [44][  280/  391]   Loss 0.039516   Top1 98.585379   Top5 100.000000   BatchTime 0.106023   LR 0.001000
INFO - Training [44][  300/  391]   Loss 0.039024   Top1 98.601562   Top5 100.000000   BatchTime 0.105631   LR 0.001000
INFO - Training [44][  320/  391]   Loss 0.039385   Top1 98.583984   Top5 100.000000   BatchTime 0.105293   LR 0.001000
INFO - Training [44][  340/  391]   Loss 0.039546   Top1 98.575368   Top5 100.000000   BatchTime 0.104969   LR 0.001000
INFO - Training [44][  360/  391]   Loss 0.039517   Top1 98.572049   Top5 100.000000   BatchTime 0.104642   LR 0.001000
INFO - Training [44][  380/  391]   Loss 0.039354   Top1 98.585526   Top5 100.000000   BatchTime 0.104280   LR 0.001000
INFO - ==> Top1: 98.580    Top5: 100.000    Loss: 0.040
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [44][   20/   79]   Loss 0.368021   Top1 91.054688   Top5 99.648438   BatchTime 0.124455
INFO - Validation [44][   40/   79]   Loss 0.403432   Top1 90.859375   Top5 99.570312   BatchTime 0.075252
INFO - Validation [44][   60/   79]   Loss 0.395633   Top1 90.690104   Top5 99.609375   BatchTime 0.058836
INFO - ==> Top1: 90.650    Top5: 99.660    Loss: 0.388
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  45
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [45][   20/  391]   Loss 0.041430   Top1 98.554688   Top5 100.000000   BatchTime 0.170284   LR 0.001000
INFO - Training [45][   40/  391]   Loss 0.040797   Top1 98.574219   Top5 100.000000   BatchTime 0.133540   LR 0.001000
INFO - Training [45][   60/  391]   Loss 0.039740   Top1 98.645833   Top5 100.000000   BatchTime 0.122946   LR 0.001000
INFO - Training [45][   80/  391]   Loss 0.038511   Top1 98.691406   Top5 100.000000   BatchTime 0.117659   LR 0.001000
INFO - Training [45][  100/  391]   Loss 0.038640   Top1 98.718750   Top5 100.000000   BatchTime 0.114371   LR 0.001000
INFO - Training [45][  120/  391]   Loss 0.037535   Top1 98.717448   Top5 100.000000   BatchTime 0.112265   LR 0.001000
INFO - Training [45][  140/  391]   Loss 0.038245   Top1 98.677455   Top5 100.000000   BatchTime 0.110661   LR 0.001000
INFO - Training [45][  160/  391]   Loss 0.038222   Top1 98.671875   Top5 100.000000   BatchTime 0.109439   LR 0.001000
INFO - Training [45][  180/  391]   Loss 0.038900   Top1 98.667535   Top5 100.000000   BatchTime 0.108545   LR 0.001000
INFO - Training [45][  200/  391]   Loss 0.038896   Top1 98.656250   Top5 100.000000   BatchTime 0.107760   LR 0.001000
INFO - Training [45][  220/  391]   Loss 0.039091   Top1 98.643466   Top5 100.000000   BatchTime 0.107105   LR 0.001000
INFO - Training [45][  240/  391]   Loss 0.039072   Top1 98.658854   Top5 100.000000   BatchTime 0.106487   LR 0.001000
INFO - Training [45][  260/  391]   Loss 0.038823   Top1 98.650841   Top5 100.000000   BatchTime 0.106007   LR 0.001000
INFO - Training [45][  280/  391]   Loss 0.039237   Top1 98.630022   Top5 100.000000   BatchTime 0.105911   LR 0.001000
INFO - Training [45][  300/  391]   Loss 0.039554   Top1 98.622396   Top5 100.000000   BatchTime 0.105537   LR 0.001000
INFO - Training [45][  320/  391]   Loss 0.039516   Top1 98.632812   Top5 100.000000   BatchTime 0.105218   LR 0.001000
INFO - Training [45][  340/  391]   Loss 0.039178   Top1 98.648897   Top5 100.000000   BatchTime 0.104872   LR 0.001000
INFO - Training [45][  360/  391]   Loss 0.039564   Top1 98.626302   Top5 100.000000   BatchTime 0.104488   LR 0.001000
INFO - Training [45][  380/  391]   Loss 0.039298   Top1 98.645148   Top5 100.000000   BatchTime 0.104161   LR 0.001000
INFO - ==> Top1: 98.656    Top5: 100.000    Loss: 0.039
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [45][   20/   79]   Loss 0.369958   Top1 91.328125   Top5 99.648438   BatchTime 0.123470
INFO - Validation [45][   40/   79]   Loss 0.398906   Top1 90.703125   Top5 99.550781   BatchTime 0.074684
INFO - Validation [45][   60/   79]   Loss 0.389624   Top1 90.729167   Top5 99.557292   BatchTime 0.058408
INFO - ==> Top1: 90.680    Top5: 99.600    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  46
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [46][   20/  391]   Loss 0.036137   Top1 98.710938   Top5 100.000000   BatchTime 0.174012   LR 0.001000
INFO - Training [46][   40/  391]   Loss 0.035355   Top1 98.769531   Top5 100.000000   BatchTime 0.134864   LR 0.001000
INFO - Training [46][   60/  391]   Loss 0.036541   Top1 98.736979   Top5 100.000000   BatchTime 0.124281   LR 0.001000
INFO - Training [46][   80/  391]   Loss 0.035414   Top1 98.789062   Top5 99.990234   BatchTime 0.118914   LR 0.001000
INFO - Training [46][  100/  391]   Loss 0.035469   Top1 98.773438   Top5 99.992188   BatchTime 0.115167   LR 0.001000
INFO - Training [46][  120/  391]   Loss 0.034821   Top1 98.776042   Top5 99.993490   BatchTime 0.112721   LR 0.001000
INFO - Training [46][  140/  391]   Loss 0.035557   Top1 98.772321   Top5 99.988839   BatchTime 0.111227   LR 0.001000
INFO - Training [46][  160/  391]   Loss 0.034656   Top1 98.813477   Top5 99.990234   BatchTime 0.109895   LR 0.001000
INFO - Training [46][  180/  391]   Loss 0.035049   Top1 98.793403   Top5 99.991319   BatchTime 0.108924   LR 0.001000
INFO - Training [46][  200/  391]   Loss 0.034829   Top1 98.820312   Top5 99.992188   BatchTime 0.108105   LR 0.001000
INFO - Training [46][  220/  391]   Loss 0.035817   Top1 98.781960   Top5 99.992898   BatchTime 0.107446   LR 0.001000
INFO - Training [46][  240/  391]   Loss 0.036043   Top1 98.763021   Top5 99.993490   BatchTime 0.106670   LR 0.001000
INFO - Training [46][  260/  391]   Loss 0.036212   Top1 98.774038   Top5 99.993990   BatchTime 0.106252   LR 0.001000
INFO - Training [46][  280/  391]   Loss 0.035827   Top1 98.783482   Top5 99.994420   BatchTime 0.105831   LR 0.001000
INFO - Training [46][  300/  391]   Loss 0.036448   Top1 98.739583   Top5 99.994792   BatchTime 0.105479   LR 0.001000
INFO - Training [46][  320/  391]   Loss 0.036648   Top1 98.737793   Top5 99.995117   BatchTime 0.105174   LR 0.001000
INFO - Training [46][  340/  391]   Loss 0.036531   Top1 98.743107   Top5 99.995404   BatchTime 0.104826   LR 0.001000
INFO - Training [46][  360/  391]   Loss 0.036649   Top1 98.743490   Top5 99.995660   BatchTime 0.104526   LR 0.001000
INFO - Training [46][  380/  391]   Loss 0.037138   Top1 98.731497   Top5 99.995888   BatchTime 0.104237   LR 0.001000
INFO - ==> Top1: 98.718    Top5: 99.996    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [46][   20/   79]   Loss 0.370644   Top1 91.054688   Top5 99.687500   BatchTime 0.127056
INFO - Validation [46][   40/   79]   Loss 0.402289   Top1 90.742188   Top5 99.550781   BatchTime 0.076643
INFO - Validation [46][   60/   79]   Loss 0.393816   Top1 90.859375   Top5 99.583333   BatchTime 0.059767
INFO - ==> Top1: 90.690    Top5: 99.630    Loss: 0.388
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  47
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [47][   20/  391]   Loss 0.031119   Top1 99.101562   Top5 100.000000   BatchTime 0.176560   LR 0.001000
INFO - Training [47][   40/  391]   Loss 0.031193   Top1 99.003906   Top5 100.000000   BatchTime 0.130875   LR 0.001000
INFO - Training [47][   60/  391]   Loss 0.033129   Top1 98.906250   Top5 100.000000   BatchTime 0.120465   LR 0.001000
INFO - Training [47][   80/  391]   Loss 0.034333   Top1 98.828125   Top5 100.000000   BatchTime 0.115746   LR 0.001000
INFO - Training [47][  100/  391]   Loss 0.034852   Top1 98.804688   Top5 100.000000   BatchTime 0.111997   LR 0.001000
INFO - Training [47][  120/  391]   Loss 0.035142   Top1 98.802083   Top5 100.000000   BatchTime 0.110151   LR 0.001000
INFO - Training [47][  140/  391]   Loss 0.036525   Top1 98.744420   Top5 100.000000   BatchTime 0.108969   LR 0.001000
INFO - Training [47][  160/  391]   Loss 0.036905   Top1 98.745117   Top5 100.000000   BatchTime 0.108153   LR 0.001000
INFO - Training [47][  180/  391]   Loss 0.036650   Top1 98.763021   Top5 100.000000   BatchTime 0.107223   LR 0.001000
INFO - Training [47][  200/  391]   Loss 0.036755   Top1 98.769531   Top5 100.000000   BatchTime 0.106552   LR 0.001000
INFO - Training [47][  220/  391]   Loss 0.037524   Top1 98.746449   Top5 100.000000   BatchTime 0.105979   LR 0.001000
INFO - Training [47][  240/  391]   Loss 0.037152   Top1 98.763021   Top5 100.000000   BatchTime 0.105490   LR 0.001000
INFO - Training [47][  260/  391]   Loss 0.036986   Top1 98.771034   Top5 100.000000   BatchTime 0.105073   LR 0.001000
INFO - Training [47][  280/  391]   Loss 0.036806   Top1 98.772321   Top5 100.000000   BatchTime 0.104695   LR 0.001000
INFO - Training [47][  300/  391]   Loss 0.036844   Top1 98.776042   Top5 100.000000   BatchTime 0.104443   LR 0.001000
INFO - Training [47][  320/  391]   Loss 0.036924   Top1 98.762207   Top5 100.000000   BatchTime 0.104205   LR 0.001000
INFO - Training [47][  340/  391]   Loss 0.037098   Top1 98.747702   Top5 100.000000   BatchTime 0.103899   LR 0.001000
INFO - Training [47][  360/  391]   Loss 0.037039   Top1 98.741319   Top5 100.000000   BatchTime 0.103950   LR 0.001000
INFO - Training [47][  380/  391]   Loss 0.036972   Top1 98.743832   Top5 100.000000   BatchTime 0.103639   LR 0.001000
INFO - ==> Top1: 98.746    Top5: 100.000    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [47][   20/   79]   Loss 0.380799   Top1 90.585938   Top5 99.687500   BatchTime 0.131956
INFO - Validation [47][   40/   79]   Loss 0.406806   Top1 90.507812   Top5 99.492188   BatchTime 0.084073
INFO - Validation [47][   60/   79]   Loss 0.399690   Top1 90.690104   Top5 99.557292   BatchTime 0.065004
INFO - ==> Top1: 90.630    Top5: 99.590    Loss: 0.394
INFO - Scoreboard best 1 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.700   Top5: 99.580] Sparsity : 0.774
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  48
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [48][   20/  391]   Loss 0.038959   Top1 98.789062   Top5 100.000000   BatchTime 0.178511   LR 0.001000
INFO - Training [48][   40/  391]   Loss 0.036546   Top1 98.886719   Top5 100.000000   BatchTime 0.125995   LR 0.001000
INFO - Training [48][   60/  391]   Loss 0.036564   Top1 98.802083   Top5 100.000000   BatchTime 0.119015   LR 0.001000
INFO - Training [48][   80/  391]   Loss 0.037400   Top1 98.769531   Top5 100.000000   BatchTime 0.114262   LR 0.001000
INFO - Training [48][  100/  391]   Loss 0.037009   Top1 98.828125   Top5 100.000000   BatchTime 0.111614   LR 0.001000
INFO - Training [48][  120/  391]   Loss 0.037795   Top1 98.776042   Top5 100.000000   BatchTime 0.109889   LR 0.001000
INFO - Training [48][  140/  391]   Loss 0.037293   Top1 98.777902   Top5 100.000000   BatchTime 0.108626   LR 0.001000
INFO - Training [48][  160/  391]   Loss 0.038159   Top1 98.720703   Top5 100.000000   BatchTime 0.107727   LR 0.001000
INFO - Training [48][  180/  391]   Loss 0.037880   Top1 98.706597   Top5 100.000000   BatchTime 0.107069   LR 0.001000
INFO - Training [48][  200/  391]   Loss 0.037970   Top1 98.703125   Top5 100.000000   BatchTime 0.106440   LR 0.001000
INFO - Training [48][  220/  391]   Loss 0.037622   Top1 98.725142   Top5 100.000000   BatchTime 0.105994   LR 0.001000
INFO - Training [48][  240/  391]   Loss 0.037174   Top1 98.743490   Top5 100.000000   BatchTime 0.105488   LR 0.001000
INFO - Training [48][  260/  391]   Loss 0.036726   Top1 98.750000   Top5 100.000000   BatchTime 0.105136   LR 0.001000
INFO - Training [48][  280/  391]   Loss 0.036720   Top1 98.750000   Top5 100.000000   BatchTime 0.104839   LR 0.001000
INFO - Training [48][  300/  391]   Loss 0.036615   Top1 98.750000   Top5 100.000000   BatchTime 0.104602   LR 0.001000
INFO - Training [48][  320/  391]   Loss 0.036192   Top1 98.767090   Top5 100.000000   BatchTime 0.104336   LR 0.001000
INFO - Training [48][  340/  391]   Loss 0.036376   Top1 98.736213   Top5 100.000000   BatchTime 0.104092   LR 0.001000
INFO - Training [48][  360/  391]   Loss 0.036470   Top1 98.719618   Top5 100.000000   BatchTime 0.103769   LR 0.001000
INFO - Training [48][  380/  391]   Loss 0.036761   Top1 98.725329   Top5 100.000000   BatchTime 0.103490   LR 0.001000
INFO - ==> Top1: 98.724    Top5: 100.000    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [48][   20/   79]   Loss 0.371464   Top1 91.289062   Top5 99.687500   BatchTime 0.131505
INFO - Validation [48][   40/   79]   Loss 0.405253   Top1 91.035156   Top5 99.511719   BatchTime 0.086594
INFO - Validation [48][   60/   79]   Loss 0.400334   Top1 90.976562   Top5 99.544271   BatchTime 0.066593
INFO - ==> Top1: 90.860    Top5: 99.600    Loss: 0.395
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  49
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [49][   20/  391]   Loss 0.045892   Top1 98.476562   Top5 100.000000   BatchTime 0.178663   LR 0.001000
INFO - Training [49][   40/  391]   Loss 0.044533   Top1 98.457031   Top5 100.000000   BatchTime 0.127585   LR 0.001000
INFO - Training [49][   60/  391]   Loss 0.041049   Top1 98.619792   Top5 100.000000   BatchTime 0.119136   LR 0.001000
INFO - Training [49][   80/  391]   Loss 0.040114   Top1 98.681641   Top5 100.000000   BatchTime 0.114304   LR 0.001000
INFO - Training [49][  100/  391]   Loss 0.041929   Top1 98.570312   Top5 100.000000   BatchTime 0.111738   LR 0.001000
INFO - Training [49][  120/  391]   Loss 0.041800   Top1 98.613281   Top5 100.000000   BatchTime 0.109757   LR 0.001000
INFO - Training [49][  140/  391]   Loss 0.039814   Top1 98.683036   Top5 100.000000   BatchTime 0.108381   LR 0.001000
INFO - Training [49][  160/  391]   Loss 0.039340   Top1 98.681641   Top5 100.000000   BatchTime 0.107021   LR 0.001000
INFO - Training [49][  180/  391]   Loss 0.039227   Top1 98.684896   Top5 100.000000   BatchTime 0.106147   LR 0.001000
INFO - Training [49][  200/  391]   Loss 0.038805   Top1 98.695312   Top5 100.000000   BatchTime 0.105557   LR 0.001000
INFO - Training [49][  220/  391]   Loss 0.039214   Top1 98.678977   Top5 100.000000   BatchTime 0.105104   LR 0.001000
INFO - Training [49][  240/  391]   Loss 0.038505   Top1 98.691406   Top5 100.000000   BatchTime 0.104744   LR 0.001000
INFO - Training [49][  260/  391]   Loss 0.038091   Top1 98.695913   Top5 100.000000   BatchTime 0.104463   LR 0.001000
INFO - Training [49][  280/  391]   Loss 0.037783   Top1 98.719308   Top5 100.000000   BatchTime 0.104165   LR 0.001000
INFO - Training [49][  300/  391]   Loss 0.037332   Top1 98.729167   Top5 100.000000   BatchTime 0.103969   LR 0.001000
INFO - Training [49][  320/  391]   Loss 0.037459   Top1 98.718262   Top5 100.000000   BatchTime 0.103792   LR 0.001000
INFO - Training [49][  340/  391]   Loss 0.037833   Top1 98.710938   Top5 100.000000   BatchTime 0.103572   LR 0.001000
INFO - Training [49][  360/  391]   Loss 0.037506   Top1 98.728299   Top5 100.000000   BatchTime 0.103339   LR 0.001000
INFO - Training [49][  380/  391]   Loss 0.037667   Top1 98.719161   Top5 100.000000   BatchTime 0.103079   LR 0.001000
INFO - ==> Top1: 98.712    Top5: 100.000    Loss: 0.038
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [49][   20/   79]   Loss 0.378366   Top1 90.429688   Top5 99.804688   BatchTime 0.130753
INFO - Validation [49][   40/   79]   Loss 0.410115   Top1 90.214844   Top5 99.609375   BatchTime 0.086772
INFO - Validation [49][   60/   79]   Loss 0.406498   Top1 90.494792   Top5 99.622396   BatchTime 0.067809
INFO - ==> Top1: 90.520    Top5: 99.670    Loss: 0.399
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  50
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [50][   20/  391]   Loss 0.026830   Top1 99.062500   Top5 100.000000   BatchTime 0.173233   LR 0.001000
INFO - Training [50][   40/  391]   Loss 0.031120   Top1 98.750000   Top5 100.000000   BatchTime 0.126731   LR 0.001000
INFO - Training [50][   60/  391]   Loss 0.031570   Top1 98.802083   Top5 100.000000   BatchTime 0.115071   LR 0.001000
INFO - Training [50][   80/  391]   Loss 0.034888   Top1 98.701172   Top5 100.000000   BatchTime 0.111368   LR 0.001000
INFO - Training [50][  100/  391]   Loss 0.034026   Top1 98.742188   Top5 100.000000   BatchTime 0.109163   LR 0.001000
INFO - Training [50][  120/  391]   Loss 0.035109   Top1 98.710938   Top5 100.000000   BatchTime 0.107649   LR 0.001000
INFO - Training [50][  140/  391]   Loss 0.035320   Top1 98.710938   Top5 100.000000   BatchTime 0.106637   LR 0.001000
INFO - Training [50][  160/  391]   Loss 0.035544   Top1 98.706055   Top5 100.000000   BatchTime 0.105966   LR 0.001000
INFO - Training [50][  180/  391]   Loss 0.035421   Top1 98.719618   Top5 100.000000   BatchTime 0.105282   LR 0.001000
INFO - Training [50][  200/  391]   Loss 0.035569   Top1 98.710938   Top5 100.000000   BatchTime 0.104854   LR 0.001000
INFO - Training [50][  220/  391]   Loss 0.036093   Top1 98.700284   Top5 100.000000   BatchTime 0.104511   LR 0.001000
INFO - Training [50][  240/  391]   Loss 0.035990   Top1 98.691406   Top5 100.000000   BatchTime 0.104290   LR 0.001000
INFO - Training [50][  260/  391]   Loss 0.036473   Top1 98.677885   Top5 100.000000   BatchTime 0.103952   LR 0.001000
INFO - Training [50][  280/  391]   Loss 0.037242   Top1 98.652344   Top5 100.000000   BatchTime 0.103768   LR 0.001000
INFO - Training [50][  300/  391]   Loss 0.037123   Top1 98.640625   Top5 100.000000   BatchTime 0.103534   LR 0.001000
INFO - Training [50][  320/  391]   Loss 0.036987   Top1 98.652344   Top5 100.000000   BatchTime 0.103381   LR 0.001000
INFO - Training [50][  340/  391]   Loss 0.037256   Top1 98.644301   Top5 100.000000   BatchTime 0.103132   LR 0.001000
INFO - Training [50][  360/  391]   Loss 0.037145   Top1 98.637153   Top5 100.000000   BatchTime 0.102925   LR 0.001000
INFO - Training [50][  380/  391]   Loss 0.036969   Top1 98.645148   Top5 100.000000   BatchTime 0.102727   LR 0.001000
INFO - ==> Top1: 98.642    Top5: 100.000    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [50][   20/   79]   Loss 0.380777   Top1 90.703125   Top5 99.648438   BatchTime 0.129310
INFO - Validation [50][   40/   79]   Loss 0.409226   Top1 90.527344   Top5 99.550781   BatchTime 0.086946
INFO - Validation [50][   60/   79]   Loss 0.405134   Top1 90.598958   Top5 99.583333   BatchTime 0.071225
INFO - ==> Top1: 90.440    Top5: 99.630    Loss: 0.399
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  51
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [51][   20/  391]   Loss 0.030507   Top1 99.062500   Top5 100.000000   BatchTime 0.174353   LR 0.001000
INFO - Training [51][   40/  391]   Loss 0.033420   Top1 98.828125   Top5 100.000000   BatchTime 0.130829   LR 0.001000
INFO - Training [51][   60/  391]   Loss 0.032280   Top1 98.880208   Top5 100.000000   BatchTime 0.116690   LR 0.001000
INFO - Training [51][   80/  391]   Loss 0.031555   Top1 98.906250   Top5 100.000000   BatchTime 0.112666   LR 0.001000
INFO - Training [51][  100/  391]   Loss 0.032714   Top1 98.843750   Top5 100.000000   BatchTime 0.110273   LR 0.001000
INFO - Training [51][  120/  391]   Loss 0.033414   Top1 98.873698   Top5 100.000000   BatchTime 0.108677   LR 0.001000
INFO - Training [51][  140/  391]   Loss 0.033070   Top1 98.889509   Top5 100.000000   BatchTime 0.107712   LR 0.001000
INFO - Training [51][  160/  391]   Loss 0.033305   Top1 98.886719   Top5 100.000000   BatchTime 0.106887   LR 0.001000
INFO - Training [51][  180/  391]   Loss 0.034058   Top1 98.871528   Top5 99.995660   BatchTime 0.106267   LR 0.001000
INFO - Training [51][  200/  391]   Loss 0.034039   Top1 98.839844   Top5 99.996094   BatchTime 0.105772   LR 0.001000
INFO - Training [51][  220/  391]   Loss 0.034672   Top1 98.831676   Top5 99.996449   BatchTime 0.105253   LR 0.001000
INFO - Training [51][  240/  391]   Loss 0.034287   Top1 98.844401   Top5 99.996745   BatchTime 0.104571   LR 0.001000
INFO - Training [51][  260/  391]   Loss 0.033984   Top1 98.855168   Top5 99.996995   BatchTime 0.104207   LR 0.001000
INFO - Training [51][  280/  391]   Loss 0.034661   Top1 98.814174   Top5 99.997210   BatchTime 0.103997   LR 0.001000
INFO - Training [51][  300/  391]   Loss 0.034560   Top1 98.817708   Top5 99.997396   BatchTime 0.103825   LR 0.001000
INFO - Training [51][  320/  391]   Loss 0.034401   Top1 98.820801   Top5 99.997559   BatchTime 0.103680   LR 0.001000
INFO - Training [51][  340/  391]   Loss 0.034128   Top1 98.837316   Top5 99.997702   BatchTime 0.103439   LR 0.001000
INFO - Training [51][  360/  391]   Loss 0.034612   Top1 98.819444   Top5 99.997830   BatchTime 0.103280   LR 0.001000
INFO - Training [51][  380/  391]   Loss 0.034879   Top1 98.803454   Top5 99.997944   BatchTime 0.103057   LR 0.001000
INFO - ==> Top1: 98.800    Top5: 99.998    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [51][   20/   79]   Loss 0.380329   Top1 90.585938   Top5 99.648438   BatchTime 0.131126
INFO - Validation [51][   40/   79]   Loss 0.405335   Top1 90.468750   Top5 99.511719   BatchTime 0.088263
INFO - Validation [51][   60/   79]   Loss 0.402649   Top1 90.455729   Top5 99.557292   BatchTime 0.073577
INFO - ==> Top1: 90.520    Top5: 99.610    Loss: 0.398
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  52
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [52][   20/  391]   Loss 0.033003   Top1 98.906250   Top5 100.000000   BatchTime 0.178005   LR 0.001000
INFO - Training [52][   40/  391]   Loss 0.035552   Top1 98.789062   Top5 100.000000   BatchTime 0.135570   LR 0.001000
INFO - Training [52][   60/  391]   Loss 0.038496   Top1 98.697917   Top5 100.000000   BatchTime 0.119306   LR 0.001000
INFO - Training [52][   80/  391]   Loss 0.036742   Top1 98.759766   Top5 100.000000   BatchTime 0.114933   LR 0.001000
INFO - Training [52][  100/  391]   Loss 0.039060   Top1 98.671875   Top5 100.000000   BatchTime 0.112138   LR 0.001000
INFO - Training [52][  120/  391]   Loss 0.038616   Top1 98.710938   Top5 100.000000   BatchTime 0.110204   LR 0.001000
INFO - Training [52][  140/  391]   Loss 0.038319   Top1 98.699777   Top5 100.000000   BatchTime 0.108849   LR 0.001000
INFO - Training [52][  160/  391]   Loss 0.038574   Top1 98.691406   Top5 100.000000   BatchTime 0.107875   LR 0.001000
INFO - Training [52][  180/  391]   Loss 0.038198   Top1 98.680556   Top5 100.000000   BatchTime 0.107017   LR 0.001000
INFO - Training [52][  200/  391]   Loss 0.038003   Top1 98.687500   Top5 100.000000   BatchTime 0.106357   LR 0.001000
INFO - Training [52][  220/  391]   Loss 0.038463   Top1 98.668324   Top5 100.000000   BatchTime 0.105797   LR 0.001000
INFO - Training [52][  240/  391]   Loss 0.038727   Top1 98.658854   Top5 100.000000   BatchTime 0.105267   LR 0.001000
INFO - Training [52][  260/  391]   Loss 0.038779   Top1 98.650841   Top5 100.000000   BatchTime 0.104792   LR 0.001000
INFO - Training [52][  280/  391]   Loss 0.037822   Top1 98.694196   Top5 100.000000   BatchTime 0.104427   LR 0.001000
INFO - Training [52][  300/  391]   Loss 0.037272   Top1 98.726562   Top5 100.000000   BatchTime 0.104073   LR 0.001000
INFO - Training [52][  320/  391]   Loss 0.037539   Top1 98.715820   Top5 100.000000   BatchTime 0.103888   LR 0.001000
INFO - Training [52][  340/  391]   Loss 0.037398   Top1 98.715533   Top5 100.000000   BatchTime 0.103684   LR 0.001000
INFO - Training [52][  360/  391]   Loss 0.037221   Top1 98.721788   Top5 100.000000   BatchTime 0.103422   LR 0.001000
INFO - Training [52][  380/  391]   Loss 0.037161   Top1 98.721217   Top5 100.000000   BatchTime 0.103147   LR 0.001000
INFO - ==> Top1: 98.722    Top5: 100.000    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [52][   20/   79]   Loss 0.384543   Top1 91.015625   Top5 99.687500   BatchTime 0.130284
INFO - Validation [52][   40/   79]   Loss 0.414900   Top1 90.644531   Top5 99.609375   BatchTime 0.087447
INFO - Validation [52][   60/   79]   Loss 0.408909   Top1 90.742188   Top5 99.635417   BatchTime 0.073823
INFO - ==> Top1: 90.710    Top5: 99.670    Loss: 0.400
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  53
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [53][   20/  391]   Loss 0.033320   Top1 98.828125   Top5 100.000000   BatchTime 0.175308   LR 0.001000
INFO - Training [53][   40/  391]   Loss 0.031848   Top1 98.828125   Top5 100.000000   BatchTime 0.131288   LR 0.001000
INFO - Training [53][   60/  391]   Loss 0.030541   Top1 98.867188   Top5 100.000000   BatchTime 0.114944   LR 0.001000
INFO - Training [53][   80/  391]   Loss 0.028977   Top1 99.003906   Top5 100.000000   BatchTime 0.111274   LR 0.001000
INFO - Training [53][  100/  391]   Loss 0.030268   Top1 98.945312   Top5 100.000000   BatchTime 0.109027   LR 0.001000
INFO - Training [53][  120/  391]   Loss 0.031017   Top1 98.912760   Top5 100.000000   BatchTime 0.107549   LR 0.001000
INFO - Training [53][  140/  391]   Loss 0.032452   Top1 98.850446   Top5 100.000000   BatchTime 0.106570   LR 0.001000
INFO - Training [53][  160/  391]   Loss 0.033573   Top1 98.823242   Top5 100.000000   BatchTime 0.105810   LR 0.001000
INFO - Training [53][  180/  391]   Loss 0.033706   Top1 98.819444   Top5 100.000000   BatchTime 0.105129   LR 0.001000
INFO - Training [53][  200/  391]   Loss 0.033348   Top1 98.816406   Top5 100.000000   BatchTime 0.104608   LR 0.001000
INFO - Training [53][  220/  391]   Loss 0.033599   Top1 98.803267   Top5 100.000000   BatchTime 0.104232   LR 0.001000
INFO - Training [53][  240/  391]   Loss 0.033651   Top1 98.798828   Top5 100.000000   BatchTime 0.103893   LR 0.001000
INFO - Training [53][  260/  391]   Loss 0.033697   Top1 98.810096   Top5 100.000000   BatchTime 0.103514   LR 0.001000
INFO - Training [53][  280/  391]   Loss 0.033893   Top1 98.808594   Top5 100.000000   BatchTime 0.103060   LR 0.001000
INFO - Training [53][  300/  391]   Loss 0.034497   Top1 98.794271   Top5 99.997396   BatchTime 0.102806   LR 0.001000
INFO - Training [53][  320/  391]   Loss 0.034202   Top1 98.813477   Top5 99.997559   BatchTime 0.102731   LR 0.001000
INFO - Training [53][  340/  391]   Loss 0.034996   Top1 98.795956   Top5 99.997702   BatchTime 0.102536   LR 0.001000
INFO - Training [53][  360/  391]   Loss 0.034990   Top1 98.786892   Top5 99.997830   BatchTime 0.102284   LR 0.001000
INFO - Training [53][  380/  391]   Loss 0.035326   Top1 98.770559   Top5 99.997944   BatchTime 0.102090   LR 0.001000
INFO - ==> Top1: 98.762    Top5: 99.998    Loss: 0.036
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [53][   20/   79]   Loss 0.382791   Top1 90.625000   Top5 99.609375   BatchTime 0.130129
INFO - Validation [53][   40/   79]   Loss 0.413846   Top1 90.585938   Top5 99.531250   BatchTime 0.087597
INFO - Validation [53][   60/   79]   Loss 0.408595   Top1 90.716146   Top5 99.531250   BatchTime 0.073546
INFO - ==> Top1: 90.660    Top5: 99.570    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  54
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [54][   20/  391]   Loss 0.037901   Top1 98.867188   Top5 100.000000   BatchTime 0.175393   LR 0.001000
INFO - Training [54][   40/  391]   Loss 0.039504   Top1 98.769531   Top5 100.000000   BatchTime 0.132147   LR 0.001000
INFO - Training [54][   60/  391]   Loss 0.038597   Top1 98.736979   Top5 100.000000   BatchTime 0.112830   LR 0.001000
INFO - Training [54][   80/  391]   Loss 0.038103   Top1 98.720703   Top5 100.000000   BatchTime 0.110918   LR 0.001000
INFO - Training [54][  100/  391]   Loss 0.039657   Top1 98.601562   Top5 100.000000   BatchTime 0.108905   LR 0.001000
INFO - Training [54][  120/  391]   Loss 0.040005   Top1 98.613281   Top5 100.000000   BatchTime 0.107534   LR 0.001000
INFO - Training [54][  140/  391]   Loss 0.039468   Top1 98.643973   Top5 100.000000   BatchTime 0.106390   LR 0.001000
INFO - Training [54][  160/  391]   Loss 0.038042   Top1 98.671875   Top5 100.000000   BatchTime 0.105471   LR 0.001000
INFO - Training [54][  180/  391]   Loss 0.038278   Top1 98.680556   Top5 100.000000   BatchTime 0.104992   LR 0.001000
INFO - Training [54][  200/  391]   Loss 0.038769   Top1 98.683594   Top5 100.000000   BatchTime 0.104532   LR 0.001000
INFO - Training [54][  220/  391]   Loss 0.038198   Top1 98.689631   Top5 100.000000   BatchTime 0.104177   LR 0.001000
INFO - Training [54][  240/  391]   Loss 0.037613   Top1 98.717448   Top5 100.000000   BatchTime 0.103927   LR 0.001000
INFO - Training [54][  260/  391]   Loss 0.037038   Top1 98.750000   Top5 100.000000   BatchTime 0.103651   LR 0.001000
INFO - Training [54][  280/  391]   Loss 0.037161   Top1 98.755580   Top5 100.000000   BatchTime 0.103469   LR 0.001000
INFO - Training [54][  300/  391]   Loss 0.037049   Top1 98.755208   Top5 99.997396   BatchTime 0.103207   LR 0.001000
INFO - Training [54][  320/  391]   Loss 0.036702   Top1 98.769531   Top5 99.997559   BatchTime 0.103016   LR 0.001000
INFO - Training [54][  340/  391]   Loss 0.036509   Top1 98.763787   Top5 99.997702   BatchTime 0.102783   LR 0.001000
INFO - Training [54][  360/  391]   Loss 0.036196   Top1 98.786892   Top5 99.997830   BatchTime 0.102545   LR 0.001000
INFO - Training [54][  380/  391]   Loss 0.035933   Top1 98.793174   Top5 99.997944   BatchTime 0.102331   LR 0.001000
INFO - ==> Top1: 98.790    Top5: 99.998    Loss: 0.036
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [54][   20/   79]   Loss 0.388992   Top1 90.546875   Top5 99.687500   BatchTime 0.132281
INFO - Validation [54][   40/   79]   Loss 0.421902   Top1 90.292969   Top5 99.589844   BatchTime 0.088343
INFO - Validation [54][   60/   79]   Loss 0.416311   Top1 90.429688   Top5 99.622396   BatchTime 0.074255
INFO - ==> Top1: 90.380    Top5: 99.650    Loss: 0.408
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  55
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [55][   20/  391]   Loss 0.037357   Top1 98.867188   Top5 100.000000   BatchTime 0.178673   LR 0.001000
INFO - Training [55][   40/  391]   Loss 0.036283   Top1 98.750000   Top5 100.000000   BatchTime 0.131106   LR 0.001000
INFO - Training [55][   60/  391]   Loss 0.035924   Top1 98.763021   Top5 100.000000   BatchTime 0.113185   LR 0.001000
INFO - Training [55][   80/  391]   Loss 0.034434   Top1 98.828125   Top5 100.000000   BatchTime 0.109505   LR 0.001000
INFO - Training [55][  100/  391]   Loss 0.034283   Top1 98.835938   Top5 100.000000   BatchTime 0.107476   LR 0.001000
INFO - Training [55][  120/  391]   Loss 0.033100   Top1 98.906250   Top5 100.000000   BatchTime 0.106339   LR 0.001000
INFO - Training [55][  140/  391]   Loss 0.034338   Top1 98.828125   Top5 100.000000   BatchTime 0.105441   LR 0.001000
INFO - Training [55][  160/  391]   Loss 0.034147   Top1 98.852539   Top5 100.000000   BatchTime 0.104842   LR 0.001000
INFO - Training [55][  180/  391]   Loss 0.033890   Top1 98.858507   Top5 100.000000   BatchTime 0.104322   LR 0.001000
INFO - Training [55][  200/  391]   Loss 0.033951   Top1 98.875000   Top5 99.996094   BatchTime 0.103852   LR 0.001000
INFO - Training [55][  220/  391]   Loss 0.034055   Top1 98.863636   Top5 99.996449   BatchTime 0.103449   LR 0.001000
INFO - Training [55][  240/  391]   Loss 0.034541   Top1 98.834635   Top5 99.996745   BatchTime 0.103148   LR 0.001000
INFO - Training [55][  260/  391]   Loss 0.034625   Top1 98.816106   Top5 99.996995   BatchTime 0.102832   LR 0.001000
INFO - Training [55][  280/  391]   Loss 0.034577   Top1 98.808594   Top5 99.997210   BatchTime 0.102591   LR 0.001000
INFO - Training [55][  300/  391]   Loss 0.034782   Top1 98.799479   Top5 99.997396   BatchTime 0.102389   LR 0.001000
INFO - Training [55][  320/  391]   Loss 0.034450   Top1 98.820801   Top5 99.997559   BatchTime 0.102253   LR 0.001000
INFO - Training [55][  340/  391]   Loss 0.034439   Top1 98.818934   Top5 99.997702   BatchTime 0.102070   LR 0.001000
INFO - Training [55][  360/  391]   Loss 0.034438   Top1 98.819444   Top5 99.997830   BatchTime 0.101813   LR 0.001000
INFO - Training [55][  380/  391]   Loss 0.034368   Top1 98.817845   Top5 99.997944   BatchTime 0.101643   LR 0.001000
INFO - ==> Top1: 98.818    Top5: 99.996    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [55][   20/   79]   Loss 0.370238   Top1 91.289062   Top5 99.687500   BatchTime 0.131301
INFO - Validation [55][   40/   79]   Loss 0.405816   Top1 90.878906   Top5 99.531250   BatchTime 0.088008
INFO - Validation [55][   60/   79]   Loss 0.405004   Top1 90.885417   Top5 99.557292   BatchTime 0.073714
INFO - ==> Top1: 90.780    Top5: 99.620    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  56
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [56][   20/  391]   Loss 0.041271   Top1 98.671875   Top5 100.000000   BatchTime 0.180683   LR 0.001000
INFO - Training [56][   40/  391]   Loss 0.040726   Top1 98.652344   Top5 100.000000   BatchTime 0.130823   LR 0.001000
INFO - Training [56][   60/  391]   Loss 0.036782   Top1 98.776042   Top5 100.000000   BatchTime 0.114163   LR 0.001000
INFO - Training [56][   80/  391]   Loss 0.037879   Top1 98.740234   Top5 100.000000   BatchTime 0.109515   LR 0.001000
INFO - Training [56][  100/  391]   Loss 0.038075   Top1 98.687500   Top5 100.000000   BatchTime 0.107634   LR 0.001000
INFO - Training [56][  120/  391]   Loss 0.036646   Top1 98.756510   Top5 100.000000   BatchTime 0.106686   LR 0.001000
INFO - Training [56][  140/  391]   Loss 0.036204   Top1 98.755580   Top5 100.000000   BatchTime 0.106760   LR 0.001000
INFO - Training [56][  160/  391]   Loss 0.036398   Top1 98.759766   Top5 100.000000   BatchTime 0.105910   LR 0.001000
INFO - Training [56][  180/  391]   Loss 0.036065   Top1 98.784722   Top5 100.000000   BatchTime 0.105232   LR 0.001000
INFO - Training [56][  200/  391]   Loss 0.035180   Top1 98.835938   Top5 100.000000   BatchTime 0.104744   LR 0.001000
INFO - Training [56][  220/  391]   Loss 0.034487   Top1 98.856534   Top5 100.000000   BatchTime 0.104378   LR 0.001000
INFO - Training [56][  240/  391]   Loss 0.034479   Top1 98.854167   Top5 100.000000   BatchTime 0.104081   LR 0.001000
INFO - Training [56][  260/  391]   Loss 0.034902   Top1 98.846154   Top5 100.000000   BatchTime 0.103881   LR 0.001000
INFO - Training [56][  280/  391]   Loss 0.034976   Top1 98.833705   Top5 100.000000   BatchTime 0.103583   LR 0.001000
INFO - Training [56][  300/  391]   Loss 0.034988   Top1 98.835938   Top5 100.000000   BatchTime 0.103374   LR 0.001000
INFO - Training [56][  320/  391]   Loss 0.035175   Top1 98.815918   Top5 100.000000   BatchTime 0.103216   LR 0.001000
INFO - Training [56][  340/  391]   Loss 0.035283   Top1 98.812040   Top5 100.000000   BatchTime 0.102958   LR 0.001000
INFO - Training [56][  360/  391]   Loss 0.035083   Top1 98.825955   Top5 100.000000   BatchTime 0.102721   LR 0.001000
INFO - Training [56][  380/  391]   Loss 0.034650   Top1 98.838405   Top5 100.000000   BatchTime 0.102523   LR 0.001000
INFO - ==> Top1: 98.820    Top5: 100.000    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [56][   20/   79]   Loss 0.379882   Top1 91.054688   Top5 99.804688   BatchTime 0.130827
INFO - Validation [56][   40/   79]   Loss 0.416234   Top1 90.527344   Top5 99.687500   BatchTime 0.087376
INFO - Validation [56][   60/   79]   Loss 0.410358   Top1 90.729167   Top5 99.674479   BatchTime 0.073243
INFO - ==> Top1: 90.660    Top5: 99.700    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  57
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [57][   20/  391]   Loss 0.036380   Top1 98.828125   Top5 100.000000   BatchTime 0.178710   LR 0.001000
INFO - Training [57][   40/  391]   Loss 0.036594   Top1 98.847656   Top5 100.000000   BatchTime 0.129644   LR 0.001000
INFO - Training [57][   60/  391]   Loss 0.033141   Top1 98.880208   Top5 100.000000   BatchTime 0.114969   LR 0.001000
INFO - Training [57][   80/  391]   Loss 0.033211   Top1 98.847656   Top5 100.000000   BatchTime 0.109145   LR 0.001000
INFO - Training [57][  100/  391]   Loss 0.032919   Top1 98.882812   Top5 100.000000   BatchTime 0.107547   LR 0.001000
INFO - Training [57][  120/  391]   Loss 0.033457   Top1 98.873698   Top5 100.000000   BatchTime 0.106513   LR 0.001000
INFO - Training [57][  140/  391]   Loss 0.033525   Top1 98.856027   Top5 100.000000   BatchTime 0.105599   LR 0.001000
INFO - Training [57][  160/  391]   Loss 0.033472   Top1 98.876953   Top5 100.000000   BatchTime 0.104979   LR 0.001000
INFO - Training [57][  180/  391]   Loss 0.033476   Top1 98.862847   Top5 100.000000   BatchTime 0.104557   LR 0.001000
INFO - Training [57][  200/  391]   Loss 0.034407   Top1 98.843750   Top5 100.000000   BatchTime 0.104080   LR 0.001000
INFO - Training [57][  220/  391]   Loss 0.034574   Top1 98.828125   Top5 100.000000   BatchTime 0.103713   LR 0.001000
INFO - Training [57][  240/  391]   Loss 0.035205   Top1 98.792318   Top5 100.000000   BatchTime 0.103400   LR 0.001000
INFO - Training [57][  260/  391]   Loss 0.034373   Top1 98.813101   Top5 100.000000   BatchTime 0.103122   LR 0.001000
INFO - Training [57][  280/  391]   Loss 0.034242   Top1 98.811384   Top5 100.000000   BatchTime 0.103037   LR 0.001000
INFO - Training [57][  300/  391]   Loss 0.034729   Top1 98.796875   Top5 100.000000   BatchTime 0.102841   LR 0.001000
INFO - Training [57][  320/  391]   Loss 0.034717   Top1 98.791504   Top5 100.000000   BatchTime 0.102702   LR 0.001000
INFO - Training [57][  340/  391]   Loss 0.034701   Top1 98.793658   Top5 100.000000   BatchTime 0.102510   LR 0.001000
INFO - Training [57][  360/  391]   Loss 0.034404   Top1 98.799913   Top5 100.000000   BatchTime 0.102344   LR 0.001000
INFO - Training [57][  380/  391]   Loss 0.034558   Top1 98.803454   Top5 100.000000   BatchTime 0.102155   LR 0.001000
INFO - ==> Top1: 98.792    Top5: 100.000    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [57][   20/   79]   Loss 0.372288   Top1 91.132812   Top5 99.726562   BatchTime 0.129132
INFO - Validation [57][   40/   79]   Loss 0.410137   Top1 90.507812   Top5 99.531250   BatchTime 0.087400
INFO - Validation [57][   60/   79]   Loss 0.406437   Top1 90.585938   Top5 99.557292   BatchTime 0.073234
INFO - ==> Top1: 90.680    Top5: 99.610    Loss: 0.399
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  58
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [58][   20/  391]   Loss 0.034705   Top1 99.023438   Top5 100.000000   BatchTime 0.181161   LR 0.001000
INFO - Training [58][   40/  391]   Loss 0.034564   Top1 98.925781   Top5 100.000000   BatchTime 0.132571   LR 0.001000
INFO - Training [58][   60/  391]   Loss 0.034012   Top1 98.893229   Top5 100.000000   BatchTime 0.117152   LR 0.001000
INFO - Training [58][   80/  391]   Loss 0.035411   Top1 98.857422   Top5 100.000000   BatchTime 0.109538   LR 0.001000
INFO - Training [58][  100/  391]   Loss 0.034411   Top1 98.867188   Top5 99.992188   BatchTime 0.107773   LR 0.001000
INFO - Training [58][  120/  391]   Loss 0.034809   Top1 98.880208   Top5 99.993490   BatchTime 0.106670   LR 0.001000
INFO - Training [58][  140/  391]   Loss 0.034142   Top1 98.911830   Top5 99.994420   BatchTime 0.106023   LR 0.001000
INFO - Training [58][  160/  391]   Loss 0.034630   Top1 98.891602   Top5 99.995117   BatchTime 0.105285   LR 0.001000
INFO - Training [58][  180/  391]   Loss 0.034310   Top1 98.880208   Top5 99.995660   BatchTime 0.104803   LR 0.001000
INFO - Training [58][  200/  391]   Loss 0.034039   Top1 98.886719   Top5 99.996094   BatchTime 0.104388   LR 0.001000
INFO - Training [58][  220/  391]   Loss 0.033858   Top1 98.884943   Top5 99.996449   BatchTime 0.104592   LR 0.001000
INFO - Training [58][  240/  391]   Loss 0.033748   Top1 98.893229   Top5 99.996745   BatchTime 0.104143   LR 0.001000
INFO - Training [58][  260/  391]   Loss 0.034074   Top1 98.876202   Top5 99.996995   BatchTime 0.103944   LR 0.001000
INFO - Training [58][  280/  391]   Loss 0.034028   Top1 98.875558   Top5 99.997210   BatchTime 0.103768   LR 0.001000
INFO - Training [58][  300/  391]   Loss 0.033901   Top1 98.880208   Top5 99.997396   BatchTime 0.103568   LR 0.001000
INFO - Training [58][  320/  391]   Loss 0.034224   Top1 98.859863   Top5 99.997559   BatchTime 0.103381   LR 0.001000
INFO - Training [58][  340/  391]   Loss 0.034252   Top1 98.862592   Top5 99.997702   BatchTime 0.103163   LR 0.001000
INFO - Training [58][  360/  391]   Loss 0.033810   Top1 98.869358   Top5 99.997830   BatchTime 0.102895   LR 0.001000
INFO - Training [58][  380/  391]   Loss 0.034251   Top1 98.854852   Top5 99.997944   BatchTime 0.102697   LR 0.001000
INFO - ==> Top1: 98.842    Top5: 99.998    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [58][   20/   79]   Loss 0.385688   Top1 90.742188   Top5 99.687500   BatchTime 0.131115
INFO - Validation [58][   40/   79]   Loss 0.414363   Top1 90.410156   Top5 99.628906   BatchTime 0.087097
INFO - Validation [58][   60/   79]   Loss 0.409921   Top1 90.585938   Top5 99.648438   BatchTime 0.072754
INFO - ==> Top1: 90.510    Top5: 99.670    Loss: 0.404
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  59
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [59][   20/  391]   Loss 0.033907   Top1 98.828125   Top5 100.000000   BatchTime 0.182924   LR 0.001000
INFO - Training [59][   40/  391]   Loss 0.035574   Top1 98.808594   Top5 100.000000   BatchTime 0.132663   LR 0.001000
INFO - Training [59][   60/  391]   Loss 0.033615   Top1 98.867188   Top5 100.000000   BatchTime 0.117352   LR 0.001000
INFO - Training [59][   80/  391]   Loss 0.032155   Top1 98.867188   Top5 100.000000   BatchTime 0.109084   LR 0.001000
INFO - Training [59][  100/  391]   Loss 0.033597   Top1 98.835938   Top5 100.000000   BatchTime 0.107390   LR 0.001000
INFO - Training [59][  120/  391]   Loss 0.033498   Top1 98.847656   Top5 100.000000   BatchTime 0.106265   LR 0.001000
INFO - Training [59][  140/  391]   Loss 0.032726   Top1 98.872768   Top5 100.000000   BatchTime 0.105507   LR 0.001000
INFO - Training [59][  160/  391]   Loss 0.032781   Top1 98.852539   Top5 100.000000   BatchTime 0.104936   LR 0.001000
INFO - Training [59][  180/  391]   Loss 0.033379   Top1 98.828125   Top5 100.000000   BatchTime 0.104410   LR 0.001000
INFO - Training [59][  200/  391]   Loss 0.034122   Top1 98.792969   Top5 100.000000   BatchTime 0.103990   LR 0.001000
INFO - Training [59][  220/  391]   Loss 0.034780   Top1 98.760653   Top5 100.000000   BatchTime 0.103664   LR 0.001000
INFO - Training [59][  240/  391]   Loss 0.034717   Top1 98.753255   Top5 100.000000   BatchTime 0.103373   LR 0.001000
INFO - Training [59][  260/  391]   Loss 0.035404   Top1 98.731971   Top5 99.996995   BatchTime 0.103114   LR 0.001000
INFO - Training [59][  280/  391]   Loss 0.035353   Top1 98.719308   Top5 99.997210   BatchTime 0.102967   LR 0.001000
INFO - Training [59][  300/  391]   Loss 0.035384   Top1 98.710938   Top5 99.997396   BatchTime 0.102800   LR 0.001000
INFO - Training [59][  320/  391]   Loss 0.035788   Top1 98.688965   Top5 99.997559   BatchTime 0.102636   LR 0.001000
INFO - Training [59][  340/  391]   Loss 0.036153   Top1 98.681066   Top5 99.997702   BatchTime 0.102518   LR 0.001000
INFO - Training [59][  360/  391]   Loss 0.035929   Top1 98.693576   Top5 99.997830   BatchTime 0.102312   LR 0.001000
INFO - Training [59][  380/  391]   Loss 0.035696   Top1 98.704770   Top5 99.997944   BatchTime 0.102170   LR 0.001000
INFO - ==> Top1: 98.708    Top5: 99.998    Loss: 0.036
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [59][   20/   79]   Loss 0.366358   Top1 90.781250   Top5 99.765625   BatchTime 0.131638
INFO - Validation [59][   40/   79]   Loss 0.411448   Top1 90.507812   Top5 99.609375   BatchTime 0.088107
INFO - Validation [59][   60/   79]   Loss 0.405350   Top1 90.703125   Top5 99.661458   BatchTime 0.072906
INFO - ==> Top1: 90.680    Top5: 99.670    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  60
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [60][   20/  391]   Loss 0.040000   Top1 98.632812   Top5 100.000000   BatchTime 0.183018   LR 0.000100
INFO - Training [60][   40/  391]   Loss 0.036623   Top1 98.632812   Top5 100.000000   BatchTime 0.133436   LR 0.000100
INFO - Training [60][   60/  391]   Loss 0.036314   Top1 98.619792   Top5 100.000000   BatchTime 0.117897   LR 0.000100
INFO - Training [60][   80/  391]   Loss 0.035621   Top1 98.662109   Top5 100.000000   BatchTime 0.106907   LR 0.000100
INFO - Training [60][  100/  391]   Loss 0.034882   Top1 98.695312   Top5 100.000000   BatchTime 0.106069   LR 0.000100
INFO - Training [60][  120/  391]   Loss 0.035070   Top1 98.691406   Top5 100.000000   BatchTime 0.105165   LR 0.000100
INFO - Training [60][  140/  391]   Loss 0.034459   Top1 98.694196   Top5 100.000000   BatchTime 0.104616   LR 0.000100
INFO - Training [60][  160/  391]   Loss 0.034647   Top1 98.725586   Top5 100.000000   BatchTime 0.104129   LR 0.000100
INFO - Training [60][  180/  391]   Loss 0.034803   Top1 98.706597   Top5 100.000000   BatchTime 0.103786   LR 0.000100
INFO - Training [60][  200/  391]   Loss 0.034427   Top1 98.742188   Top5 100.000000   BatchTime 0.103495   LR 0.000100
INFO - Training [60][  220/  391]   Loss 0.033997   Top1 98.753551   Top5 100.000000   BatchTime 0.103290   LR 0.000100
INFO - Training [60][  240/  391]   Loss 0.033822   Top1 98.763021   Top5 100.000000   BatchTime 0.103054   LR 0.000100
INFO - Training [60][  260/  391]   Loss 0.034242   Top1 98.753005   Top5 100.000000   BatchTime 0.102906   LR 0.000100
INFO - Training [60][  280/  391]   Loss 0.034011   Top1 98.775112   Top5 100.000000   BatchTime 0.103110   LR 0.000100
INFO - Training [60][  300/  391]   Loss 0.033866   Top1 98.794271   Top5 100.000000   BatchTime 0.102990   LR 0.000100
INFO - Training [60][  320/  391]   Loss 0.033586   Top1 98.806152   Top5 100.000000   BatchTime 0.102874   LR 0.000100
INFO - Training [60][  340/  391]   Loss 0.033829   Top1 98.793658   Top5 100.000000   BatchTime 0.102729   LR 0.000100
INFO - Training [60][  360/  391]   Loss 0.033962   Top1 98.789062   Top5 100.000000   BatchTime 0.102521   LR 0.000100
INFO - Training [60][  380/  391]   Loss 0.033850   Top1 98.782895   Top5 100.000000   BatchTime 0.102332   LR 0.000100
INFO - ==> Top1: 98.786    Top5: 100.000    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [60][   20/   79]   Loss 0.381602   Top1 90.820312   Top5 99.570312   BatchTime 0.133491
INFO - Validation [60][   40/   79]   Loss 0.416434   Top1 90.468750   Top5 99.511719   BatchTime 0.089190
INFO - Validation [60][   60/   79]   Loss 0.410073   Top1 90.638021   Top5 99.583333   BatchTime 0.074528
INFO - ==> Top1: 90.630    Top5: 99.590    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  61
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [61][   20/  391]   Loss 0.035466   Top1 98.867188   Top5 100.000000   BatchTime 0.177969   LR 0.000100
INFO - Training [61][   40/  391]   Loss 0.034653   Top1 98.828125   Top5 100.000000   BatchTime 0.131149   LR 0.000100
INFO - Training [61][   60/  391]   Loss 0.032621   Top1 98.828125   Top5 100.000000   BatchTime 0.116387   LR 0.000100
INFO - Training [61][   80/  391]   Loss 0.033978   Top1 98.828125   Top5 100.000000   BatchTime 0.105828   LR 0.000100
INFO - Training [61][  100/  391]   Loss 0.032383   Top1 98.906250   Top5 100.000000   BatchTime 0.105689   LR 0.000100
INFO - Training [61][  120/  391]   Loss 0.032636   Top1 98.906250   Top5 100.000000   BatchTime 0.104723   LR 0.000100
INFO - Training [61][  140/  391]   Loss 0.033140   Top1 98.895089   Top5 100.000000   BatchTime 0.104323   LR 0.000100
INFO - Training [61][  160/  391]   Loss 0.032509   Top1 98.906250   Top5 100.000000   BatchTime 0.103774   LR 0.000100
INFO - Training [61][  180/  391]   Loss 0.032937   Top1 98.858507   Top5 100.000000   BatchTime 0.103478   LR 0.000100
INFO - Training [61][  200/  391]   Loss 0.033459   Top1 98.847656   Top5 100.000000   BatchTime 0.103174   LR 0.000100
INFO - Training [61][  220/  391]   Loss 0.033490   Top1 98.856534   Top5 100.000000   BatchTime 0.102944   LR 0.000100
INFO - Training [61][  240/  391]   Loss 0.033564   Top1 98.850911   Top5 100.000000   BatchTime 0.102718   LR 0.000100
INFO - Training [61][  260/  391]   Loss 0.032924   Top1 98.876202   Top5 100.000000   BatchTime 0.102608   LR 0.000100
INFO - Training [61][  280/  391]   Loss 0.033416   Top1 98.864397   Top5 100.000000   BatchTime 0.102447   LR 0.000100
INFO - Training [61][  300/  391]   Loss 0.033635   Top1 98.846354   Top5 100.000000   BatchTime 0.102306   LR 0.000100
INFO - Training [61][  320/  391]   Loss 0.034175   Top1 98.806152   Top5 100.000000   BatchTime 0.102185   LR 0.000100
INFO - Training [61][  340/  391]   Loss 0.034406   Top1 98.805147   Top5 100.000000   BatchTime 0.101962   LR 0.000100
INFO - Training [61][  360/  391]   Loss 0.034393   Top1 98.812934   Top5 100.000000   BatchTime 0.101818   LR 0.000100
INFO - Training [61][  380/  391]   Loss 0.034296   Top1 98.826069   Top5 100.000000   BatchTime 0.101642   LR 0.000100
INFO - ==> Top1: 98.810    Top5: 100.000    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [61][   20/   79]   Loss 0.376005   Top1 91.093750   Top5 99.609375   BatchTime 0.131715
INFO - Validation [61][   40/   79]   Loss 0.412066   Top1 90.605469   Top5 99.628906   BatchTime 0.089117
INFO - Validation [61][   60/   79]   Loss 0.405690   Top1 90.742188   Top5 99.661458   BatchTime 0.074497
INFO - ==> Top1: 90.630    Top5: 99.670    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  62
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [62][   20/  391]   Loss 0.034338   Top1 98.945312   Top5 99.960938   BatchTime 0.177819   LR 0.000100
INFO - Training [62][   40/  391]   Loss 0.033839   Top1 98.886719   Top5 99.980469   BatchTime 0.129823   LR 0.000100
INFO - Training [62][   60/  391]   Loss 0.032306   Top1 98.958333   Top5 99.986979   BatchTime 0.116031   LR 0.000100
INFO - Training [62][   80/  391]   Loss 0.033832   Top1 98.964844   Top5 99.990234   BatchTime 0.106036   LR 0.000100
INFO - Training [62][  100/  391]   Loss 0.033592   Top1 98.929688   Top5 99.992188   BatchTime 0.104514   LR 0.000100
INFO - Training [62][  120/  391]   Loss 0.033404   Top1 98.919271   Top5 99.993490   BatchTime 0.103882   LR 0.000100
INFO - Training [62][  140/  391]   Loss 0.033719   Top1 98.883929   Top5 99.994420   BatchTime 0.103486   LR 0.000100
INFO - Training [62][  160/  391]   Loss 0.033378   Top1 98.852539   Top5 99.995117   BatchTime 0.103264   LR 0.000100
INFO - Training [62][  180/  391]   Loss 0.033524   Top1 98.849826   Top5 99.995660   BatchTime 0.103031   LR 0.000100
INFO - Training [62][  200/  391]   Loss 0.033836   Top1 98.828125   Top5 99.996094   BatchTime 0.102818   LR 0.000100
INFO - Training [62][  220/  391]   Loss 0.033965   Top1 98.821023   Top5 99.996449   BatchTime 0.102622   LR 0.000100
INFO - Training [62][  240/  391]   Loss 0.034076   Top1 98.824870   Top5 99.993490   BatchTime 0.102485   LR 0.000100
INFO - Training [62][  260/  391]   Loss 0.034050   Top1 98.834135   Top5 99.993990   BatchTime 0.102397   LR 0.000100
INFO - Training [62][  280/  391]   Loss 0.033644   Top1 98.853237   Top5 99.994420   BatchTime 0.102326   LR 0.000100
INFO - Training [62][  300/  391]   Loss 0.033533   Top1 98.843750   Top5 99.994792   BatchTime 0.102157   LR 0.000100
INFO - Training [62][  320/  391]   Loss 0.033533   Top1 98.845215   Top5 99.995117   BatchTime 0.102368   LR 0.000100
INFO - Training [62][  340/  391]   Loss 0.033455   Top1 98.857996   Top5 99.995404   BatchTime 0.102245   LR 0.000100
INFO - Training [62][  360/  391]   Loss 0.033613   Top1 98.856337   Top5 99.995660   BatchTime 0.102097   LR 0.000100
INFO - Training [62][  380/  391]   Loss 0.033212   Top1 98.871299   Top5 99.995888   BatchTime 0.102001   LR 0.000100
INFO - ==> Top1: 98.866    Top5: 99.996    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [62][   20/   79]   Loss 0.384498   Top1 90.976562   Top5 99.570312   BatchTime 0.131009
INFO - Validation [62][   40/   79]   Loss 0.417953   Top1 90.722656   Top5 99.511719   BatchTime 0.087582
INFO - Validation [62][   60/   79]   Loss 0.409802   Top1 90.768229   Top5 99.557292   BatchTime 0.073438
INFO - ==> Top1: 90.700    Top5: 99.580    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  63
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [63][   20/  391]   Loss 0.031978   Top1 98.945312   Top5 100.000000   BatchTime 0.178205   LR 0.000100
INFO - Training [63][   40/  391]   Loss 0.033841   Top1 98.886719   Top5 100.000000   BatchTime 0.131123   LR 0.000100
INFO - Training [63][   60/  391]   Loss 0.035555   Top1 98.789062   Top5 100.000000   BatchTime 0.114867   LR 0.000100
INFO - Training [63][   80/  391]   Loss 0.036142   Top1 98.750000   Top5 100.000000   BatchTime 0.106586   LR 0.000100
INFO - Training [63][  100/  391]   Loss 0.035797   Top1 98.773438   Top5 100.000000   BatchTime 0.105012   LR 0.000100
INFO - Training [63][  120/  391]   Loss 0.035387   Top1 98.769531   Top5 100.000000   BatchTime 0.104181   LR 0.000100
INFO - Training [63][  140/  391]   Loss 0.036184   Top1 98.727679   Top5 100.000000   BatchTime 0.103851   LR 0.000100
INFO - Training [63][  160/  391]   Loss 0.035874   Top1 98.754883   Top5 100.000000   BatchTime 0.103416   LR 0.000100
INFO - Training [63][  180/  391]   Loss 0.035416   Top1 98.758681   Top5 99.995660   BatchTime 0.103183   LR 0.000100
INFO - Training [63][  200/  391]   Loss 0.035355   Top1 98.773438   Top5 99.996094   BatchTime 0.102932   LR 0.000100
INFO - Training [63][  220/  391]   Loss 0.034962   Top1 98.774858   Top5 99.996449   BatchTime 0.102731   LR 0.000100
INFO - Training [63][  240/  391]   Loss 0.034620   Top1 98.789062   Top5 99.996745   BatchTime 0.102587   LR 0.000100
INFO - Training [63][  260/  391]   Loss 0.034738   Top1 98.780048   Top5 99.996995   BatchTime 0.102416   LR 0.000100
INFO - Training [63][  280/  391]   Loss 0.034750   Top1 98.786272   Top5 99.997210   BatchTime 0.102244   LR 0.000100
INFO - Training [63][  300/  391]   Loss 0.034399   Top1 98.799479   Top5 99.997396   BatchTime 0.102273   LR 0.000100
INFO - Training [63][  320/  391]   Loss 0.034311   Top1 98.820801   Top5 99.997559   BatchTime 0.102157   LR 0.000100
INFO - Training [63][  340/  391]   Loss 0.034264   Top1 98.823529   Top5 99.997702   BatchTime 0.102063   LR 0.000100
INFO - Training [63][  360/  391]   Loss 0.034451   Top1 98.810764   Top5 99.997830   BatchTime 0.101876   LR 0.000100
INFO - Training [63][  380/  391]   Loss 0.034724   Top1 98.801398   Top5 99.997944   BatchTime 0.101718   LR 0.000100
INFO - ==> Top1: 98.794    Top5: 99.998    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [63][   20/   79]   Loss 0.377691   Top1 90.781250   Top5 99.726562   BatchTime 0.132670
INFO - Validation [63][   40/   79]   Loss 0.411730   Top1 90.507812   Top5 99.609375   BatchTime 0.088557
INFO - Validation [63][   60/   79]   Loss 0.405705   Top1 90.611979   Top5 99.635417   BatchTime 0.073997
INFO - ==> Top1: 90.590    Top5: 99.660    Loss: 0.399
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  64
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [64][   20/  391]   Loss 0.031002   Top1 98.828125   Top5 100.000000   BatchTime 0.177758   LR 0.000100
INFO - Training [64][   40/  391]   Loss 0.032633   Top1 98.925781   Top5 100.000000   BatchTime 0.130110   LR 0.000100
INFO - Training [64][   60/  391]   Loss 0.033487   Top1 98.893229   Top5 100.000000   BatchTime 0.113847   LR 0.000100
INFO - Training [64][   80/  391]   Loss 0.033847   Top1 98.828125   Top5 100.000000   BatchTime 0.106629   LR 0.000100
INFO - Training [64][  100/  391]   Loss 0.034082   Top1 98.828125   Top5 100.000000   BatchTime 0.104169   LR 0.000100
INFO - Training [64][  120/  391]   Loss 0.032925   Top1 98.893229   Top5 100.000000   BatchTime 0.103732   LR 0.000100
INFO - Training [64][  140/  391]   Loss 0.033178   Top1 98.895089   Top5 100.000000   BatchTime 0.103268   LR 0.000100
INFO - Training [64][  160/  391]   Loss 0.032942   Top1 98.901367   Top5 100.000000   BatchTime 0.102960   LR 0.000100
INFO - Training [64][  180/  391]   Loss 0.032991   Top1 98.893229   Top5 100.000000   BatchTime 0.102686   LR 0.000100
INFO - Training [64][  200/  391]   Loss 0.033290   Top1 98.875000   Top5 100.000000   BatchTime 0.102572   LR 0.000100
INFO - Training [64][  220/  391]   Loss 0.033944   Top1 98.856534   Top5 100.000000   BatchTime 0.102496   LR 0.000100
INFO - Training [64][  240/  391]   Loss 0.033937   Top1 98.850911   Top5 100.000000   BatchTime 0.102437   LR 0.000100
INFO - Training [64][  260/  391]   Loss 0.033990   Top1 98.837139   Top5 100.000000   BatchTime 0.102268   LR 0.000100
INFO - Training [64][  280/  391]   Loss 0.034039   Top1 98.822545   Top5 100.000000   BatchTime 0.102170   LR 0.000100
INFO - Training [64][  300/  391]   Loss 0.034203   Top1 98.812500   Top5 99.997396   BatchTime 0.102123   LR 0.000100
INFO - Training [64][  320/  391]   Loss 0.033921   Top1 98.828125   Top5 99.995117   BatchTime 0.102045   LR 0.000100
INFO - Training [64][  340/  391]   Loss 0.034183   Top1 98.814338   Top5 99.995404   BatchTime 0.101892   LR 0.000100
INFO - Training [64][  360/  391]   Loss 0.034192   Top1 98.812934   Top5 99.995660   BatchTime 0.101734   LR 0.000100
INFO - Training [64][  380/  391]   Loss 0.034344   Top1 98.801398   Top5 99.995888   BatchTime 0.101565   LR 0.000100
INFO - ==> Top1: 98.806    Top5: 99.996    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [64][   20/   79]   Loss 0.383427   Top1 90.664062   Top5 99.687500   BatchTime 0.131653
INFO - Validation [64][   40/   79]   Loss 0.415544   Top1 90.566406   Top5 99.628906   BatchTime 0.088737
INFO - Validation [64][   60/   79]   Loss 0.409964   Top1 90.638021   Top5 99.635417   BatchTime 0.074142
INFO - ==> Top1: 90.640    Top5: 99.650    Loss: 0.404
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  65
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [65][   20/  391]   Loss 0.032268   Top1 98.710938   Top5 100.000000   BatchTime 0.175810   LR 0.000100
INFO - Training [65][   40/  391]   Loss 0.031516   Top1 98.886719   Top5 100.000000   BatchTime 0.131661   LR 0.000100
INFO - Training [65][   60/  391]   Loss 0.030768   Top1 98.958333   Top5 100.000000   BatchTime 0.115484   LR 0.000100
INFO - Training [65][   80/  391]   Loss 0.033031   Top1 98.867188   Top5 100.000000   BatchTime 0.108334   LR 0.000100
INFO - Training [65][  100/  391]   Loss 0.034012   Top1 98.804688   Top5 100.000000   BatchTime 0.104968   LR 0.000100
INFO - Training [65][  120/  391]   Loss 0.034893   Top1 98.795573   Top5 100.000000   BatchTime 0.104243   LR 0.000100
INFO - Training [65][  140/  391]   Loss 0.034748   Top1 98.805804   Top5 100.000000   BatchTime 0.103765   LR 0.000100
INFO - Training [65][  160/  391]   Loss 0.033630   Top1 98.847656   Top5 100.000000   BatchTime 0.103418   LR 0.000100
INFO - Training [65][  180/  391]   Loss 0.033633   Top1 98.841146   Top5 100.000000   BatchTime 0.103132   LR 0.000100
INFO - Training [65][  200/  391]   Loss 0.033677   Top1 98.839844   Top5 100.000000   BatchTime 0.103063   LR 0.000100
INFO - Training [65][  220/  391]   Loss 0.033323   Top1 98.849432   Top5 100.000000   BatchTime 0.102893   LR 0.000100
INFO - Training [65][  240/  391]   Loss 0.033278   Top1 98.841146   Top5 100.000000   BatchTime 0.102799   LR 0.000100
INFO - Training [65][  260/  391]   Loss 0.033289   Top1 98.825120   Top5 100.000000   BatchTime 0.102673   LR 0.000100
INFO - Training [65][  280/  391]   Loss 0.033833   Top1 98.808594   Top5 100.000000   BatchTime 0.102569   LR 0.000100
INFO - Training [65][  300/  391]   Loss 0.033542   Top1 98.828125   Top5 100.000000   BatchTime 0.102471   LR 0.000100
INFO - Training [65][  320/  391]   Loss 0.033632   Top1 98.825684   Top5 100.000000   BatchTime 0.102401   LR 0.000100
INFO - Training [65][  340/  391]   Loss 0.033568   Top1 98.823529   Top5 100.000000   BatchTime 0.102229   LR 0.000100
INFO - Training [65][  360/  391]   Loss 0.033798   Top1 98.812934   Top5 100.000000   BatchTime 0.102063   LR 0.000100
INFO - Training [65][  380/  391]   Loss 0.033829   Top1 98.805510   Top5 100.000000   BatchTime 0.101906   LR 0.000100
INFO - ==> Top1: 98.820    Top5: 100.000    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [65][   20/   79]   Loss 0.375758   Top1 90.664062   Top5 99.687500   BatchTime 0.132425
INFO - Validation [65][   40/   79]   Loss 0.419207   Top1 90.156250   Top5 99.589844   BatchTime 0.089213
INFO - Validation [65][   60/   79]   Loss 0.413201   Top1 90.429688   Top5 99.622396   BatchTime 0.074826
INFO - ==> Top1: 90.490    Top5: 99.650    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  66
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [66][   20/  391]   Loss 0.035459   Top1 98.789062   Top5 100.000000   BatchTime 0.171013   LR 0.000100
INFO - Training [66][   40/  391]   Loss 0.033172   Top1 98.828125   Top5 100.000000   BatchTime 0.128870   LR 0.000100
INFO - Training [66][   60/  391]   Loss 0.032530   Top1 98.893229   Top5 100.000000   BatchTime 0.113043   LR 0.000100
INFO - Training [66][   80/  391]   Loss 0.033208   Top1 98.916016   Top5 100.000000   BatchTime 0.105728   LR 0.000100
INFO - Training [66][  100/  391]   Loss 0.034081   Top1 98.882812   Top5 100.000000   BatchTime 0.102259   LR 0.000100
INFO - Training [66][  120/  391]   Loss 0.033597   Top1 98.847656   Top5 100.000000   BatchTime 0.102231   LR 0.000100
INFO - Training [66][  140/  391]   Loss 0.036164   Top1 98.716518   Top5 99.994420   BatchTime 0.102200   LR 0.000100
INFO - Training [66][  160/  391]   Loss 0.035858   Top1 98.710938   Top5 99.995117   BatchTime 0.102103   LR 0.000100
INFO - Training [66][  180/  391]   Loss 0.034830   Top1 98.758681   Top5 99.995660   BatchTime 0.101947   LR 0.000100
INFO - Training [66][  200/  391]   Loss 0.034306   Top1 98.773438   Top5 99.996094   BatchTime 0.101844   LR 0.000100
INFO - Training [66][  220/  391]   Loss 0.035039   Top1 98.735795   Top5 99.996449   BatchTime 0.101845   LR 0.000100
INFO - Training [66][  240/  391]   Loss 0.034563   Top1 98.750000   Top5 99.996745   BatchTime 0.101875   LR 0.000100
INFO - Training [66][  260/  391]   Loss 0.034716   Top1 98.750000   Top5 99.996995   BatchTime 0.101856   LR 0.000100
INFO - Training [66][  280/  391]   Loss 0.034529   Top1 98.777902   Top5 99.997210   BatchTime 0.101725   LR 0.000100
INFO - Training [66][  300/  391]   Loss 0.034788   Top1 98.776042   Top5 99.997396   BatchTime 0.101653   LR 0.000100
INFO - Training [66][  320/  391]   Loss 0.034511   Top1 98.789062   Top5 99.997559   BatchTime 0.101541   LR 0.000100
INFO - Training [66][  340/  391]   Loss 0.034057   Top1 98.807445   Top5 99.997702   BatchTime 0.101427   LR 0.000100
INFO - Training [66][  360/  391]   Loss 0.034085   Top1 98.797743   Top5 99.997830   BatchTime 0.101277   LR 0.000100
INFO - Training [66][  380/  391]   Loss 0.033859   Top1 98.803454   Top5 99.997944   BatchTime 0.101171   LR 0.000100
INFO - ==> Top1: 98.806    Top5: 99.998    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [66][   20/   79]   Loss 0.375125   Top1 90.585938   Top5 99.726562   BatchTime 0.132470
INFO - Validation [66][   40/   79]   Loss 0.416724   Top1 90.390625   Top5 99.609375   BatchTime 0.089277
INFO - Validation [66][   60/   79]   Loss 0.408695   Top1 90.598958   Top5 99.609375   BatchTime 0.074428
INFO - ==> Top1: 90.570    Top5: 99.640    Loss: 0.402
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  67
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [67][   20/  391]   Loss 0.027425   Top1 99.101562   Top5 100.000000   BatchTime 0.173955   LR 0.000100
INFO - Training [67][   40/  391]   Loss 0.026408   Top1 99.160156   Top5 100.000000   BatchTime 0.130006   LR 0.000100
INFO - Training [67][   60/  391]   Loss 0.027905   Top1 99.049479   Top5 100.000000   BatchTime 0.114319   LR 0.000100
INFO - Training [67][   80/  391]   Loss 0.028207   Top1 99.033203   Top5 100.000000   BatchTime 0.106704   LR 0.000100
INFO - Training [67][  100/  391]   Loss 0.029278   Top1 99.031250   Top5 100.000000   BatchTime 0.103533   LR 0.000100
INFO - Training [67][  120/  391]   Loss 0.030130   Top1 98.984375   Top5 100.000000   BatchTime 0.103149   LR 0.000100
INFO - Training [67][  140/  391]   Loss 0.030906   Top1 98.962054   Top5 100.000000   BatchTime 0.103025   LR 0.000100
INFO - Training [67][  160/  391]   Loss 0.030843   Top1 98.955078   Top5 100.000000   BatchTime 0.102643   LR 0.000100
INFO - Training [67][  180/  391]   Loss 0.031135   Top1 98.940972   Top5 100.000000   BatchTime 0.102457   LR 0.000100
INFO - Training [67][  200/  391]   Loss 0.031741   Top1 98.910156   Top5 100.000000   BatchTime 0.102353   LR 0.000100
INFO - Training [67][  220/  391]   Loss 0.031846   Top1 98.899148   Top5 100.000000   BatchTime 0.102142   LR 0.000100
INFO - Training [67][  240/  391]   Loss 0.032856   Top1 98.867188   Top5 99.996745   BatchTime 0.101966   LR 0.000100
INFO - Training [67][  260/  391]   Loss 0.032601   Top1 98.885216   Top5 99.996995   BatchTime 0.101775   LR 0.000100
INFO - Training [67][  280/  391]   Loss 0.032848   Top1 98.853237   Top5 99.997210   BatchTime 0.101614   LR 0.000100
INFO - Training [67][  300/  391]   Loss 0.032800   Top1 98.843750   Top5 99.997396   BatchTime 0.101544   LR 0.000100
INFO - Training [67][  320/  391]   Loss 0.032458   Top1 98.845215   Top5 99.997559   BatchTime 0.101458   LR 0.000100
INFO - Training [67][  340/  391]   Loss 0.032625   Top1 98.839614   Top5 99.997702   BatchTime 0.101325   LR 0.000100
INFO - Training [67][  360/  391]   Loss 0.032471   Top1 98.856337   Top5 99.997830   BatchTime 0.101155   LR 0.000100
INFO - Training [67][  380/  391]   Loss 0.032585   Top1 98.858964   Top5 99.997944   BatchTime 0.101046   LR 0.000100
INFO - ==> Top1: 98.854    Top5: 99.998    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [67][   20/   79]   Loss 0.379463   Top1 90.625000   Top5 99.648438   BatchTime 0.133545
INFO - Validation [67][   40/   79]   Loss 0.415161   Top1 90.390625   Top5 99.589844   BatchTime 0.088970
INFO - Validation [67][   60/   79]   Loss 0.408835   Top1 90.611979   Top5 99.583333   BatchTime 0.074176
INFO - ==> Top1: 90.640    Top5: 99.590    Loss: 0.402
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  68
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [68][   20/  391]   Loss 0.030010   Top1 98.867188   Top5 100.000000   BatchTime 0.172052   LR 0.000100
INFO - Training [68][   40/  391]   Loss 0.028106   Top1 98.964844   Top5 100.000000   BatchTime 0.128872   LR 0.000100
INFO - Training [68][   60/  391]   Loss 0.029747   Top1 98.971354   Top5 100.000000   BatchTime 0.113097   LR 0.000100
INFO - Training [68][   80/  391]   Loss 0.031430   Top1 98.896484   Top5 100.000000   BatchTime 0.106588   LR 0.000100
INFO - Training [68][  100/  391]   Loss 0.030971   Top1 98.914062   Top5 100.000000   BatchTime 0.103263   LR 0.000100
INFO - Training [68][  120/  391]   Loss 0.031423   Top1 98.919271   Top5 100.000000   BatchTime 0.102742   LR 0.000100
INFO - Training [68][  140/  391]   Loss 0.030481   Top1 98.950893   Top5 100.000000   BatchTime 0.102391   LR 0.000100
INFO - Training [68][  160/  391]   Loss 0.031614   Top1 98.891602   Top5 100.000000   BatchTime 0.102137   LR 0.000100
INFO - Training [68][  180/  391]   Loss 0.031452   Top1 98.914931   Top5 100.000000   BatchTime 0.101914   LR 0.000100
INFO - Training [68][  200/  391]   Loss 0.031138   Top1 98.945312   Top5 100.000000   BatchTime 0.101757   LR 0.000100
INFO - Training [68][  220/  391]   Loss 0.031695   Top1 98.934659   Top5 100.000000   BatchTime 0.101616   LR 0.000100
INFO - Training [68][  240/  391]   Loss 0.031752   Top1 98.945312   Top5 100.000000   BatchTime 0.101515   LR 0.000100
INFO - Training [68][  260/  391]   Loss 0.032609   Top1 98.912260   Top5 100.000000   BatchTime 0.101349   LR 0.000100
INFO - Training [68][  280/  391]   Loss 0.032493   Top1 98.914621   Top5 100.000000   BatchTime 0.101486   LR 0.000100
INFO - Training [68][  300/  391]   Loss 0.032809   Top1 98.903646   Top5 100.000000   BatchTime 0.101473   LR 0.000100
INFO - Training [68][  320/  391]   Loss 0.033332   Top1 98.876953   Top5 100.000000   BatchTime 0.101451   LR 0.000100
INFO - Training [68][  340/  391]   Loss 0.033789   Top1 98.871783   Top5 100.000000   BatchTime 0.101445   LR 0.000100
INFO - Training [68][  360/  391]   Loss 0.033671   Top1 98.884549   Top5 100.000000   BatchTime 0.101379   LR 0.000100
INFO - Training [68][  380/  391]   Loss 0.034153   Top1 98.858964   Top5 100.000000   BatchTime 0.101257   LR 0.000100
INFO - ==> Top1: 98.860    Top5: 100.000    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [68][   20/   79]   Loss 0.377089   Top1 90.937500   Top5 99.648438   BatchTime 0.132603
INFO - Validation [68][   40/   79]   Loss 0.413058   Top1 90.605469   Top5 99.589844   BatchTime 0.088760
INFO - Validation [68][   60/   79]   Loss 0.406434   Top1 90.768229   Top5 99.609375   BatchTime 0.074360
INFO - ==> Top1: 90.750    Top5: 99.630    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  69
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [69][   20/  391]   Loss 0.034739   Top1 98.867188   Top5 100.000000   BatchTime 0.171035   LR 0.000100
INFO - Training [69][   40/  391]   Loss 0.035731   Top1 98.730469   Top5 100.000000   BatchTime 0.127338   LR 0.000100
INFO - Training [69][   60/  391]   Loss 0.034573   Top1 98.723958   Top5 100.000000   BatchTime 0.112475   LR 0.000100
INFO - Training [69][   80/  391]   Loss 0.035006   Top1 98.730469   Top5 100.000000   BatchTime 0.106824   LR 0.000100
INFO - Training [69][  100/  391]   Loss 0.032450   Top1 98.867188   Top5 100.000000   BatchTime 0.102087   LR 0.000100
INFO - Training [69][  120/  391]   Loss 0.033011   Top1 98.834635   Top5 100.000000   BatchTime 0.101999   LR 0.000100
INFO - Training [69][  140/  391]   Loss 0.032593   Top1 98.844866   Top5 100.000000   BatchTime 0.101670   LR 0.000100
INFO - Training [69][  160/  391]   Loss 0.032776   Top1 98.847656   Top5 100.000000   BatchTime 0.101587   LR 0.000100
INFO - Training [69][  180/  391]   Loss 0.032324   Top1 98.871528   Top5 100.000000   BatchTime 0.101360   LR 0.000100
INFO - Training [69][  200/  391]   Loss 0.031933   Top1 98.894531   Top5 100.000000   BatchTime 0.101293   LR 0.000100
INFO - Training [69][  220/  391]   Loss 0.031906   Top1 98.895597   Top5 100.000000   BatchTime 0.101296   LR 0.000100
INFO - Training [69][  240/  391]   Loss 0.032445   Top1 98.880208   Top5 100.000000   BatchTime 0.101340   LR 0.000100
INFO - Training [69][  260/  391]   Loss 0.032592   Top1 98.876202   Top5 100.000000   BatchTime 0.101262   LR 0.000100
INFO - Training [69][  280/  391]   Loss 0.033114   Top1 98.861607   Top5 100.000000   BatchTime 0.101245   LR 0.000100
INFO - Training [69][  300/  391]   Loss 0.033411   Top1 98.846354   Top5 100.000000   BatchTime 0.101185   LR 0.000100
INFO - Training [69][  320/  391]   Loss 0.032987   Top1 98.862305   Top5 100.000000   BatchTime 0.101123   LR 0.000100
INFO - Training [69][  340/  391]   Loss 0.032967   Top1 98.871783   Top5 100.000000   BatchTime 0.101022   LR 0.000100
INFO - Training [69][  360/  391]   Loss 0.033166   Top1 98.862847   Top5 99.997830   BatchTime 0.100869   LR 0.000100
INFO - Training [69][  380/  391]   Loss 0.033340   Top1 98.844572   Top5 99.997944   BatchTime 0.100803   LR 0.000100
INFO - ==> Top1: 98.854    Top5: 99.998    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [69][   20/   79]   Loss 0.374450   Top1 90.507812   Top5 99.726562   BatchTime 0.131289
INFO - Validation [69][   40/   79]   Loss 0.414608   Top1 90.234375   Top5 99.589844   BatchTime 0.088032
INFO - Validation [69][   60/   79]   Loss 0.406304   Top1 90.677083   Top5 99.609375   BatchTime 0.073542
INFO - ==> Top1: 90.550    Top5: 99.640    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  70
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [70][   20/  391]   Loss 0.029557   Top1 98.867188   Top5 100.000000   BatchTime 0.174145   LR 0.000010
INFO - Training [70][   40/  391]   Loss 0.030051   Top1 98.847656   Top5 100.000000   BatchTime 0.129209   LR 0.000010
INFO - Training [70][   60/  391]   Loss 0.031328   Top1 98.867188   Top5 100.000000   BatchTime 0.113657   LR 0.000010
INFO - Training [70][   80/  391]   Loss 0.031550   Top1 98.867188   Top5 100.000000   BatchTime 0.106873   LR 0.000010
INFO - Training [70][  100/  391]   Loss 0.030083   Top1 98.929688   Top5 100.000000   BatchTime 0.102707   LR 0.000010
INFO - Training [70][  120/  391]   Loss 0.030277   Top1 98.951823   Top5 100.000000   BatchTime 0.102438   LR 0.000010
INFO - Training [70][  140/  391]   Loss 0.032359   Top1 98.844866   Top5 100.000000   BatchTime 0.102421   LR 0.000010
INFO - Training [70][  160/  391]   Loss 0.032701   Top1 98.842773   Top5 100.000000   BatchTime 0.102185   LR 0.000010
INFO - Training [70][  180/  391]   Loss 0.032950   Top1 98.836806   Top5 99.995660   BatchTime 0.101930   LR 0.000010
INFO - Training [70][  200/  391]   Loss 0.033055   Top1 98.835938   Top5 99.996094   BatchTime 0.101813   LR 0.000010
INFO - Training [70][  220/  391]   Loss 0.032470   Top1 98.856534   Top5 99.996449   BatchTime 0.101701   LR 0.000010
INFO - Training [70][  240/  391]   Loss 0.032419   Top1 98.863932   Top5 99.996745   BatchTime 0.101704   LR 0.000010
INFO - Training [70][  260/  391]   Loss 0.032940   Top1 98.861178   Top5 99.996995   BatchTime 0.101636   LR 0.000010
INFO - Training [70][  280/  391]   Loss 0.032851   Top1 98.864397   Top5 99.997210   BatchTime 0.101508   LR 0.000010
INFO - Training [70][  300/  391]   Loss 0.032994   Top1 98.864583   Top5 99.997396   BatchTime 0.101363   LR 0.000010
INFO - Training [70][  320/  391]   Loss 0.033372   Top1 98.845215   Top5 99.997559   BatchTime 0.101093   LR 0.000010
INFO - Training [70][  340/  391]   Loss 0.033083   Top1 98.860294   Top5 99.997702   BatchTime 0.100925   LR 0.000010
INFO - Training [70][  360/  391]   Loss 0.032929   Top1 98.875868   Top5 99.997830   BatchTime 0.100826   LR 0.000010
INFO - Training [70][  380/  391]   Loss 0.033238   Top1 98.856908   Top5 99.997944   BatchTime 0.100726   LR 0.000010
INFO - ==> Top1: 98.850    Top5: 99.998    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [70][   20/   79]   Loss 0.376091   Top1 90.781250   Top5 99.726562   BatchTime 0.130652
INFO - Validation [70][   40/   79]   Loss 0.412319   Top1 90.605469   Top5 99.550781   BatchTime 0.087618
INFO - Validation [70][   60/   79]   Loss 0.407773   Top1 90.651042   Top5 99.583333   BatchTime 0.073160
INFO - ==> Top1: 90.550    Top5: 99.620    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  71
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [71][   20/  391]   Loss 0.027081   Top1 99.062500   Top5 100.000000   BatchTime 0.174395   LR 0.000010
INFO - Training [71][   40/  391]   Loss 0.032040   Top1 98.925781   Top5 100.000000   BatchTime 0.128860   LR 0.000010
INFO - Training [71][   60/  391]   Loss 0.031412   Top1 98.971354   Top5 100.000000   BatchTime 0.113014   LR 0.000010
INFO - Training [71][   80/  391]   Loss 0.032766   Top1 98.906250   Top5 100.000000   BatchTime 0.106438   LR 0.000010
INFO - Training [71][  100/  391]   Loss 0.032551   Top1 98.937500   Top5 100.000000   BatchTime 0.100081   LR 0.000010
INFO - Training [71][  120/  391]   Loss 0.032328   Top1 98.945312   Top5 100.000000   BatchTime 0.101766   LR 0.000010
INFO - Training [71][  140/  391]   Loss 0.033829   Top1 98.900670   Top5 99.994420   BatchTime 0.101511   LR 0.000010
INFO - Training [71][  160/  391]   Loss 0.033363   Top1 98.925781   Top5 99.995117   BatchTime 0.101375   LR 0.000010
INFO - Training [71][  180/  391]   Loss 0.032743   Top1 98.953993   Top5 99.995660   BatchTime 0.101309   LR 0.000010
INFO - Training [71][  200/  391]   Loss 0.032797   Top1 98.937500   Top5 99.996094   BatchTime 0.101231   LR 0.000010
INFO - Training [71][  220/  391]   Loss 0.033191   Top1 98.934659   Top5 99.996449   BatchTime 0.101164   LR 0.000010
INFO - Training [71][  240/  391]   Loss 0.032847   Top1 98.932292   Top5 99.996745   BatchTime 0.101132   LR 0.000010
INFO - Training [71][  260/  391]   Loss 0.033430   Top1 98.906250   Top5 99.996995   BatchTime 0.101057   LR 0.000010
INFO - Training [71][  280/  391]   Loss 0.033351   Top1 98.909040   Top5 99.997210   BatchTime 0.101030   LR 0.000010
INFO - Training [71][  300/  391]   Loss 0.033205   Top1 98.914062   Top5 99.997396   BatchTime 0.100946   LR 0.000010
INFO - Training [71][  320/  391]   Loss 0.033001   Top1 98.923340   Top5 99.997559   BatchTime 0.100906   LR 0.000010
INFO - Training [71][  340/  391]   Loss 0.032944   Top1 98.910846   Top5 99.997702   BatchTime 0.100823   LR 0.000010
INFO - Training [71][  360/  391]   Loss 0.032719   Top1 98.914931   Top5 99.997830   BatchTime 0.100772   LR 0.000010
INFO - Training [71][  380/  391]   Loss 0.032815   Top1 98.918586   Top5 99.997944   BatchTime 0.100756   LR 0.000010
INFO - ==> Top1: 98.920    Top5: 99.998    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [71][   20/   79]   Loss 0.383838   Top1 90.742188   Top5 99.648438   BatchTime 0.130829
INFO - Validation [71][   40/   79]   Loss 0.415249   Top1 90.527344   Top5 99.550781   BatchTime 0.087350
INFO - Validation [71][   60/   79]   Loss 0.409411   Top1 90.598958   Top5 99.570312   BatchTime 0.073255
INFO - ==> Top1: 90.620    Top5: 99.620    Loss: 0.404
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  72
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [72][   20/  391]   Loss 0.033721   Top1 98.671875   Top5 100.000000   BatchTime 0.177290   LR 0.000010
INFO - Training [72][   40/  391]   Loss 0.033869   Top1 98.847656   Top5 100.000000   BatchTime 0.129264   LR 0.000010
INFO - Training [72][   60/  391]   Loss 0.034826   Top1 98.815104   Top5 100.000000   BatchTime 0.112951   LR 0.000010
INFO - Training [72][   80/  391]   Loss 0.033894   Top1 98.896484   Top5 100.000000   BatchTime 0.106125   LR 0.000010
INFO - Training [72][  100/  391]   Loss 0.034217   Top1 98.859375   Top5 100.000000   BatchTime 0.099944   LR 0.000010
INFO - Training [72][  120/  391]   Loss 0.034281   Top1 98.847656   Top5 100.000000   BatchTime 0.099796   LR 0.000010
INFO - Training [72][  140/  391]   Loss 0.033777   Top1 98.856027   Top5 99.994420   BatchTime 0.099854   LR 0.000010
INFO - Training [72][  160/  391]   Loss 0.033744   Top1 98.842773   Top5 99.995117   BatchTime 0.099942   LR 0.000010
INFO - Training [72][  180/  391]   Loss 0.034500   Top1 98.832465   Top5 99.995660   BatchTime 0.100041   LR 0.000010
INFO - Training [72][  200/  391]   Loss 0.034685   Top1 98.828125   Top5 99.996094   BatchTime 0.100003   LR 0.000010
INFO - Training [72][  220/  391]   Loss 0.034770   Top1 98.821023   Top5 99.996449   BatchTime 0.100003   LR 0.000010
INFO - Training [72][  240/  391]   Loss 0.034854   Top1 98.805339   Top5 99.996745   BatchTime 0.100026   LR 0.000010
INFO - Training [72][  260/  391]   Loss 0.034468   Top1 98.816106   Top5 99.996995   BatchTime 0.100018   LR 0.000010
INFO - Training [72][  280/  391]   Loss 0.034548   Top1 98.794643   Top5 99.997210   BatchTime 0.100006   LR 0.000010
INFO - Training [72][  300/  391]   Loss 0.034575   Top1 98.796875   Top5 99.997396   BatchTime 0.100039   LR 0.000010
INFO - Training [72][  320/  391]   Loss 0.034687   Top1 98.808594   Top5 99.997559   BatchTime 0.100043   LR 0.000010
INFO - Training [72][  340/  391]   Loss 0.034252   Top1 98.821232   Top5 99.997702   BatchTime 0.099966   LR 0.000010
INFO - Training [72][  360/  391]   Loss 0.034539   Top1 98.815104   Top5 99.997830   BatchTime 0.099949   LR 0.000010
INFO - Training [72][  380/  391]   Loss 0.034577   Top1 98.817845   Top5 99.997944   BatchTime 0.099875   LR 0.000010
INFO - ==> Top1: 98.806    Top5: 99.998    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [72][   20/   79]   Loss 0.374310   Top1 90.859375   Top5 99.687500   BatchTime 0.132944
INFO - Validation [72][   40/   79]   Loss 0.410199   Top1 90.410156   Top5 99.570312   BatchTime 0.088755
INFO - Validation [72][   60/   79]   Loss 0.408654   Top1 90.429688   Top5 99.622396   BatchTime 0.074318
INFO - ==> Top1: 90.480    Top5: 99.640    Loss: 0.402
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  73
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [73][   20/  391]   Loss 0.028450   Top1 99.023438   Top5 100.000000   BatchTime 0.181650   LR 0.000010
INFO - Training [73][   40/  391]   Loss 0.032098   Top1 98.867188   Top5 100.000000   BatchTime 0.131831   LR 0.000010
INFO - Training [73][   60/  391]   Loss 0.034679   Top1 98.776042   Top5 100.000000   BatchTime 0.116100   LR 0.000010
INFO - Training [73][   80/  391]   Loss 0.033404   Top1 98.828125   Top5 100.000000   BatchTime 0.110109   LR 0.000010
INFO - Training [73][  100/  391]   Loss 0.031729   Top1 98.835938   Top5 100.000000   BatchTime 0.102969   LR 0.000010
INFO - Training [73][  120/  391]   Loss 0.032561   Top1 98.841146   Top5 100.000000   BatchTime 0.103204   LR 0.000010
INFO - Training [73][  140/  391]   Loss 0.033349   Top1 98.800223   Top5 100.000000   BatchTime 0.103483   LR 0.000010
INFO - Training [73][  160/  391]   Loss 0.033373   Top1 98.803711   Top5 100.000000   BatchTime 0.103101   LR 0.000010
INFO - Training [73][  180/  391]   Loss 0.032623   Top1 98.832465   Top5 100.000000   BatchTime 0.102947   LR 0.000010
INFO - Training [73][  200/  391]   Loss 0.032682   Top1 98.847656   Top5 100.000000   BatchTime 0.102714   LR 0.000010
INFO - Training [73][  220/  391]   Loss 0.033172   Top1 98.842330   Top5 100.000000   BatchTime 0.102526   LR 0.000010
INFO - Training [73][  240/  391]   Loss 0.033542   Top1 98.811849   Top5 100.000000   BatchTime 0.102324   LR 0.000010
INFO - Training [73][  260/  391]   Loss 0.033618   Top1 98.813101   Top5 100.000000   BatchTime 0.102154   LR 0.000010
INFO - Training [73][  280/  391]   Loss 0.033319   Top1 98.830915   Top5 100.000000   BatchTime 0.102035   LR 0.000010
INFO - Training [73][  300/  391]   Loss 0.032959   Top1 98.841146   Top5 100.000000   BatchTime 0.101909   LR 0.000010
INFO - Training [73][  320/  391]   Loss 0.032472   Top1 98.859863   Top5 100.000000   BatchTime 0.101836   LR 0.000010
INFO - Training [73][  340/  391]   Loss 0.032410   Top1 98.860294   Top5 100.000000   BatchTime 0.101689   LR 0.000010
INFO - Training [73][  360/  391]   Loss 0.032308   Top1 98.865017   Top5 100.000000   BatchTime 0.101548   LR 0.000010
INFO - Training [73][  380/  391]   Loss 0.032827   Top1 98.867188   Top5 100.000000   BatchTime 0.101456   LR 0.000010
INFO - ==> Top1: 98.866    Top5: 100.000    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [73][   20/   79]   Loss 0.380599   Top1 90.859375   Top5 99.687500   BatchTime 0.130418
INFO - Validation [73][   40/   79]   Loss 0.416436   Top1 90.566406   Top5 99.550781   BatchTime 0.088241
INFO - Validation [73][   60/   79]   Loss 0.409819   Top1 90.651042   Top5 99.570312   BatchTime 0.073665
INFO - ==> Top1: 90.620    Top5: 99.610    Loss: 0.406
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  74
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [74][   20/  391]   Loss 0.035014   Top1 98.671875   Top5 100.000000   BatchTime 0.175145   LR 0.000010
INFO - Training [74][   40/  391]   Loss 0.027611   Top1 99.023438   Top5 100.000000   BatchTime 0.129525   LR 0.000010
INFO - Training [74][   60/  391]   Loss 0.030315   Top1 98.945312   Top5 100.000000   BatchTime 0.113841   LR 0.000010
INFO - Training [74][   80/  391]   Loss 0.030338   Top1 98.984375   Top5 100.000000   BatchTime 0.107081   LR 0.000010
INFO - Training [74][  100/  391]   Loss 0.030882   Top1 98.976562   Top5 100.000000   BatchTime 0.100894   LR 0.000010
INFO - Training [74][  120/  391]   Loss 0.031408   Top1 98.958333   Top5 100.000000   BatchTime 0.101604   LR 0.000010
INFO - Training [74][  140/  391]   Loss 0.031330   Top1 98.973214   Top5 100.000000   BatchTime 0.101539   LR 0.000010
INFO - Training [74][  160/  391]   Loss 0.031237   Top1 98.989258   Top5 100.000000   BatchTime 0.101539   LR 0.000010
INFO - Training [74][  180/  391]   Loss 0.032171   Top1 98.927951   Top5 100.000000   BatchTime 0.101552   LR 0.000010
INFO - Training [74][  200/  391]   Loss 0.032166   Top1 98.906250   Top5 100.000000   BatchTime 0.101440   LR 0.000010
INFO - Training [74][  220/  391]   Loss 0.031922   Top1 98.902699   Top5 100.000000   BatchTime 0.101382   LR 0.000010
INFO - Training [74][  240/  391]   Loss 0.032310   Top1 98.893229   Top5 100.000000   BatchTime 0.101317   LR 0.000010
INFO - Training [74][  260/  391]   Loss 0.032285   Top1 98.903245   Top5 100.000000   BatchTime 0.101283   LR 0.000010
INFO - Training [74][  280/  391]   Loss 0.032318   Top1 98.906250   Top5 100.000000   BatchTime 0.101226   LR 0.000010
INFO - Training [74][  300/  391]   Loss 0.032424   Top1 98.901042   Top5 100.000000   BatchTime 0.101133   LR 0.000010
INFO - Training [74][  320/  391]   Loss 0.031965   Top1 98.913574   Top5 100.000000   BatchTime 0.101106   LR 0.000010
INFO - Training [74][  340/  391]   Loss 0.032359   Top1 98.885570   Top5 100.000000   BatchTime 0.100981   LR 0.000010
INFO - Training [74][  360/  391]   Loss 0.032449   Top1 98.886719   Top5 100.000000   BatchTime 0.100857   LR 0.000010
INFO - Training [74][  380/  391]   Loss 0.032873   Top1 98.873355   Top5 100.000000   BatchTime 0.100760   LR 0.000010
INFO - ==> Top1: 98.868    Top5: 100.000    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [74][   20/   79]   Loss 0.376177   Top1 90.859375   Top5 99.570312   BatchTime 0.130136
INFO - Validation [74][   40/   79]   Loss 0.410547   Top1 90.683594   Top5 99.531250   BatchTime 0.085987
INFO - Validation [74][   60/   79]   Loss 0.408302   Top1 90.729167   Top5 99.557292   BatchTime 0.072475
INFO - ==> Top1: 90.700    Top5: 99.600    Loss: 0.400
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  75
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [75][   20/  391]   Loss 0.031973   Top1 98.750000   Top5 100.000000   BatchTime 0.178283   LR 0.000010
INFO - Training [75][   40/  391]   Loss 0.035207   Top1 98.769531   Top5 100.000000   BatchTime 0.130860   LR 0.000010
INFO - Training [75][   60/  391]   Loss 0.033534   Top1 98.841146   Top5 99.986979   BatchTime 0.115215   LR 0.000010
INFO - Training [75][   80/  391]   Loss 0.032844   Top1 98.876953   Top5 99.990234   BatchTime 0.108597   LR 0.000010
INFO - Training [75][  100/  391]   Loss 0.033351   Top1 98.859375   Top5 99.992188   BatchTime 0.102521   LR 0.000010
INFO - Training [75][  120/  391]   Loss 0.032396   Top1 98.886719   Top5 99.993490   BatchTime 0.102688   LR 0.000010
INFO - Training [75][  140/  391]   Loss 0.032721   Top1 98.872768   Top5 99.994420   BatchTime 0.102254   LR 0.000010
INFO - Training [75][  160/  391]   Loss 0.033736   Top1 98.818359   Top5 99.995117   BatchTime 0.102013   LR 0.000010
INFO - Training [75][  180/  391]   Loss 0.032774   Top1 98.845486   Top5 99.995660   BatchTime 0.101973   LR 0.000010
INFO - Training [75][  200/  391]   Loss 0.033752   Top1 98.816406   Top5 99.996094   BatchTime 0.102300   LR 0.000010
INFO - Training [75][  220/  391]   Loss 0.033507   Top1 98.817472   Top5 99.996449   BatchTime 0.102133   LR 0.000010
INFO - Training [75][  240/  391]   Loss 0.033367   Top1 98.837891   Top5 99.996745   BatchTime 0.101969   LR 0.000010
INFO - Training [75][  260/  391]   Loss 0.033209   Top1 98.849159   Top5 99.996995   BatchTime 0.101841   LR 0.000010
INFO - Training [75][  280/  391]   Loss 0.033368   Top1 98.839286   Top5 99.994420   BatchTime 0.101666   LR 0.000010
INFO - Training [75][  300/  391]   Loss 0.033163   Top1 98.848958   Top5 99.994792   BatchTime 0.101541   LR 0.000010
INFO - Training [75][  320/  391]   Loss 0.033406   Top1 98.850098   Top5 99.995117   BatchTime 0.101522   LR 0.000010
INFO - Training [75][  340/  391]   Loss 0.033211   Top1 98.869485   Top5 99.995404   BatchTime 0.101463   LR 0.000010
INFO - Training [75][  360/  391]   Loss 0.033256   Top1 98.869358   Top5 99.993490   BatchTime 0.101336   LR 0.000010
INFO - Training [75][  380/  391]   Loss 0.033429   Top1 98.854852   Top5 99.993832   BatchTime 0.101240   LR 0.000010
INFO - ==> Top1: 98.846    Top5: 99.994    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [75][   20/   79]   Loss 0.379206   Top1 91.054688   Top5 99.648438   BatchTime 0.131288
INFO - Validation [75][   40/   79]   Loss 0.419069   Top1 90.644531   Top5 99.570312   BatchTime 0.087926
INFO - Validation [75][   60/   79]   Loss 0.411858   Top1 90.794271   Top5 99.570312   BatchTime 0.073669
INFO - ==> Top1: 90.750    Top5: 99.610    Loss: 0.406
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  76
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [76][   20/  391]   Loss 0.038490   Top1 98.593750   Top5 100.000000   BatchTime 0.177161   LR 0.000010
INFO - Training [76][   40/  391]   Loss 0.040493   Top1 98.574219   Top5 100.000000   BatchTime 0.130146   LR 0.000010
INFO - Training [76][   60/  391]   Loss 0.037612   Top1 98.606771   Top5 100.000000   BatchTime 0.113737   LR 0.000010
INFO - Training [76][   80/  391]   Loss 0.035439   Top1 98.710938   Top5 100.000000   BatchTime 0.106608   LR 0.000010
INFO - Training [76][  100/  391]   Loss 0.033811   Top1 98.804688   Top5 100.000000   BatchTime 0.100622   LR 0.000010
INFO - Training [76][  120/  391]   Loss 0.032928   Top1 98.886719   Top5 100.000000   BatchTime 0.101518   LR 0.000010
INFO - Training [76][  140/  391]   Loss 0.032852   Top1 98.883929   Top5 100.000000   BatchTime 0.101615   LR 0.000010
INFO - Training [76][  160/  391]   Loss 0.032935   Top1 98.857422   Top5 100.000000   BatchTime 0.101423   LR 0.000010
INFO - Training [76][  180/  391]   Loss 0.032677   Top1 98.849826   Top5 100.000000   BatchTime 0.101288   LR 0.000010
INFO - Training [76][  200/  391]   Loss 0.032980   Top1 98.847656   Top5 100.000000   BatchTime 0.101182   LR 0.000010
INFO - Training [76][  220/  391]   Loss 0.033021   Top1 98.835227   Top5 100.000000   BatchTime 0.101011   LR 0.000010
INFO - Training [76][  240/  391]   Loss 0.033201   Top1 98.828125   Top5 100.000000   BatchTime 0.100905   LR 0.000010
INFO - Training [76][  260/  391]   Loss 0.033961   Top1 98.810096   Top5 100.000000   BatchTime 0.100795   LR 0.000010
INFO - Training [76][  280/  391]   Loss 0.033250   Top1 98.833705   Top5 100.000000   BatchTime 0.100840   LR 0.000010
INFO - Training [76][  300/  391]   Loss 0.033103   Top1 98.833333   Top5 100.000000   BatchTime 0.100859   LR 0.000010
INFO - Training [76][  320/  391]   Loss 0.033515   Top1 98.825684   Top5 100.000000   BatchTime 0.100781   LR 0.000010
INFO - Training [76][  340/  391]   Loss 0.033314   Top1 98.839614   Top5 100.000000   BatchTime 0.100699   LR 0.000010
INFO - Training [76][  360/  391]   Loss 0.033297   Top1 98.841146   Top5 100.000000   BatchTime 0.100645   LR 0.000010
INFO - Training [76][  380/  391]   Loss 0.033537   Top1 98.840461   Top5 100.000000   BatchTime 0.100560   LR 0.000010
INFO - ==> Top1: 98.838    Top5: 100.000    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [76][   20/   79]   Loss 0.373400   Top1 90.546875   Top5 99.648438   BatchTime 0.131368
INFO - Validation [76][   40/   79]   Loss 0.413201   Top1 90.449219   Top5 99.531250   BatchTime 0.088266
INFO - Validation [76][   60/   79]   Loss 0.408669   Top1 90.625000   Top5 99.544271   BatchTime 0.073801
INFO - ==> Top1: 90.600    Top5: 99.580    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  77
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [77][   20/  391]   Loss 0.031766   Top1 98.945312   Top5 100.000000   BatchTime 0.181606   LR 0.000010
INFO - Training [77][   40/  391]   Loss 0.032034   Top1 98.984375   Top5 100.000000   BatchTime 0.132589   LR 0.000010
INFO - Training [77][   60/  391]   Loss 0.030615   Top1 99.036458   Top5 100.000000   BatchTime 0.115971   LR 0.000010
INFO - Training [77][   80/  391]   Loss 0.033507   Top1 98.925781   Top5 100.000000   BatchTime 0.109027   LR 0.000010
INFO - Training [77][  100/  391]   Loss 0.033440   Top1 98.898438   Top5 100.000000   BatchTime 0.102178   LR 0.000010
INFO - Training [77][  120/  391]   Loss 0.032809   Top1 98.906250   Top5 100.000000   BatchTime 0.102406   LR 0.000010
INFO - Training [77][  140/  391]   Loss 0.032728   Top1 98.895089   Top5 100.000000   BatchTime 0.102099   LR 0.000010
INFO - Training [77][  160/  391]   Loss 0.033458   Top1 98.872070   Top5 100.000000   BatchTime 0.102040   LR 0.000010
INFO - Training [77][  180/  391]   Loss 0.033271   Top1 98.871528   Top5 100.000000   BatchTime 0.101962   LR 0.000010
INFO - Training [77][  200/  391]   Loss 0.033273   Top1 98.863281   Top5 100.000000   BatchTime 0.101867   LR 0.000010
INFO - Training [77][  220/  391]   Loss 0.033221   Top1 98.881392   Top5 100.000000   BatchTime 0.101695   LR 0.000010
INFO - Training [77][  240/  391]   Loss 0.032869   Top1 98.893229   Top5 100.000000   BatchTime 0.101895   LR 0.000010
INFO - Training [77][  260/  391]   Loss 0.032879   Top1 98.891226   Top5 100.000000   BatchTime 0.101832   LR 0.000010
INFO - Training [77][  280/  391]   Loss 0.032677   Top1 98.914621   Top5 100.000000   BatchTime 0.101644   LR 0.000010
INFO - Training [77][  300/  391]   Loss 0.033008   Top1 98.890625   Top5 100.000000   BatchTime 0.101576   LR 0.000010
INFO - Training [77][  320/  391]   Loss 0.032902   Top1 98.891602   Top5 100.000000   BatchTime 0.101479   LR 0.000010
INFO - Training [77][  340/  391]   Loss 0.032859   Top1 98.887868   Top5 100.000000   BatchTime 0.101359   LR 0.000010
INFO - Training [77][  360/  391]   Loss 0.032523   Top1 98.901910   Top5 100.000000   BatchTime 0.101186   LR 0.000010
INFO - Training [77][  380/  391]   Loss 0.032322   Top1 98.908306   Top5 100.000000   BatchTime 0.101100   LR 0.000010
INFO - ==> Top1: 98.916    Top5: 100.000    Loss: 0.032
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [77][   20/   79]   Loss 0.378906   Top1 90.742188   Top5 99.648438   BatchTime 0.131233
INFO - Validation [77][   40/   79]   Loss 0.418963   Top1 90.234375   Top5 99.531250   BatchTime 0.088589
INFO - Validation [77][   60/   79]   Loss 0.412559   Top1 90.533854   Top5 99.557292   BatchTime 0.074171
INFO - ==> Top1: 90.550    Top5: 99.580    Loss: 0.406
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  78
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [78][   20/  391]   Loss 0.029615   Top1 98.906250   Top5 100.000000   BatchTime 0.178947   LR 0.000010
INFO - Training [78][   40/  391]   Loss 0.031486   Top1 98.945312   Top5 100.000000   BatchTime 0.131668   LR 0.000010
INFO - Training [78][   60/  391]   Loss 0.031666   Top1 98.997396   Top5 100.000000   BatchTime 0.114731   LR 0.000010
INFO - Training [78][   80/  391]   Loss 0.032207   Top1 98.945312   Top5 100.000000   BatchTime 0.107322   LR 0.000010
INFO - Training [78][  100/  391]   Loss 0.032533   Top1 98.921875   Top5 100.000000   BatchTime 0.100527   LR 0.000010
INFO - Training [78][  120/  391]   Loss 0.032774   Top1 98.873698   Top5 100.000000   BatchTime 0.101040   LR 0.000010
INFO - Training [78][  140/  391]   Loss 0.034107   Top1 98.828125   Top5 100.000000   BatchTime 0.101004   LR 0.000010
INFO - Training [78][  160/  391]   Loss 0.034317   Top1 98.837891   Top5 100.000000   BatchTime 0.100855   LR 0.000010
INFO - Training [78][  180/  391]   Loss 0.033733   Top1 98.858507   Top5 100.000000   BatchTime 0.100828   LR 0.000010
INFO - Training [78][  200/  391]   Loss 0.032949   Top1 98.882812   Top5 100.000000   BatchTime 0.100880   LR 0.000010
INFO - Training [78][  220/  391]   Loss 0.033324   Top1 98.881392   Top5 100.000000   BatchTime 0.100886   LR 0.000010
INFO - Training [78][  240/  391]   Loss 0.033049   Top1 98.896484   Top5 100.000000   BatchTime 0.100818   LR 0.000010
INFO - Training [78][  260/  391]   Loss 0.033461   Top1 98.858173   Top5 100.000000   BatchTime 0.100779   LR 0.000010
INFO - Training [78][  280/  391]   Loss 0.033497   Top1 98.867188   Top5 100.000000   BatchTime 0.100745   LR 0.000010
INFO - Training [78][  300/  391]   Loss 0.033855   Top1 98.854167   Top5 99.997396   BatchTime 0.100675   LR 0.000010
INFO - Training [78][  320/  391]   Loss 0.034097   Top1 98.842773   Top5 99.997559   BatchTime 0.100673   LR 0.000010
INFO - Training [78][  340/  391]   Loss 0.034240   Top1 98.839614   Top5 99.997702   BatchTime 0.100556   LR 0.000010
INFO - Training [78][  360/  391]   Loss 0.034294   Top1 98.847656   Top5 99.997830   BatchTime 0.100439   LR 0.000010
INFO - Training [78][  380/  391]   Loss 0.033966   Top1 98.852796   Top5 99.997944   BatchTime 0.100385   LR 0.000010
INFO - ==> Top1: 98.848    Top5: 99.998    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [78][   20/   79]   Loss 0.371136   Top1 90.898438   Top5 99.726562   BatchTime 0.131023
INFO - Validation [78][   40/   79]   Loss 0.412189   Top1 90.664062   Top5 99.589844   BatchTime 0.087360
INFO - Validation [78][   60/   79]   Loss 0.409895   Top1 90.690104   Top5 99.596354   BatchTime 0.073244
INFO - ==> Top1: 90.730    Top5: 99.630    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  79
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [79][   20/  391]   Loss 0.031729   Top1 99.023438   Top5 100.000000   BatchTime 0.177452   LR 0.000010
INFO - Training [79][   40/  391]   Loss 0.036332   Top1 98.632812   Top5 100.000000   BatchTime 0.129674   LR 0.000010
INFO - Training [79][   60/  391]   Loss 0.035278   Top1 98.645833   Top5 100.000000   BatchTime 0.113599   LR 0.000010
INFO - Training [79][   80/  391]   Loss 0.033729   Top1 98.740234   Top5 100.000000   BatchTime 0.107179   LR 0.000010
INFO - Training [79][  100/  391]   Loss 0.034444   Top1 98.695312   Top5 100.000000   BatchTime 0.101083   LR 0.000010
INFO - Training [79][  120/  391]   Loss 0.035073   Top1 98.678385   Top5 100.000000   BatchTime 0.100658   LR 0.000010
INFO - Training [79][  140/  391]   Loss 0.034360   Top1 98.699777   Top5 100.000000   BatchTime 0.100731   LR 0.000010
INFO - Training [79][  160/  391]   Loss 0.034476   Top1 98.681641   Top5 100.000000   BatchTime 0.100608   LR 0.000010
INFO - Training [79][  180/  391]   Loss 0.034340   Top1 98.693576   Top5 100.000000   BatchTime 0.100590   LR 0.000010
INFO - Training [79][  200/  391]   Loss 0.033744   Top1 98.722656   Top5 100.000000   BatchTime 0.100524   LR 0.000010
INFO - Training [79][  220/  391]   Loss 0.034194   Top1 98.721591   Top5 100.000000   BatchTime 0.100565   LR 0.000010
INFO - Training [79][  240/  391]   Loss 0.034209   Top1 98.727214   Top5 100.000000   BatchTime 0.100549   LR 0.000010
INFO - Training [79][  260/  391]   Loss 0.034217   Top1 98.731971   Top5 100.000000   BatchTime 0.100528   LR 0.000010
INFO - Training [79][  280/  391]   Loss 0.034337   Top1 98.730469   Top5 99.997210   BatchTime 0.100520   LR 0.000010
INFO - Training [79][  300/  391]   Loss 0.034170   Top1 98.747396   Top5 99.997396   BatchTime 0.100531   LR 0.000010
INFO - Training [79][  320/  391]   Loss 0.033702   Top1 98.767090   Top5 99.997559   BatchTime 0.100831   LR 0.000010
INFO - Training [79][  340/  391]   Loss 0.033746   Top1 98.766085   Top5 99.997702   BatchTime 0.100723   LR 0.000010
INFO - Training [79][  360/  391]   Loss 0.033872   Top1 98.754340   Top5 99.997830   BatchTime 0.100604   LR 0.000010
INFO - Training [79][  380/  391]   Loss 0.034161   Top1 98.750000   Top5 99.997944   BatchTime 0.100553   LR 0.000010
INFO - ==> Top1: 98.762    Top5: 99.998    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [79][   20/   79]   Loss 0.376343   Top1 90.703125   Top5 99.726562   BatchTime 0.130216
INFO - Validation [79][   40/   79]   Loss 0.415548   Top1 90.585938   Top5 99.609375   BatchTime 0.087585
INFO - Validation [79][   60/   79]   Loss 0.410331   Top1 90.664062   Top5 99.622396   BatchTime 0.073784
INFO - ==> Top1: 90.650    Top5: 99.660    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.860   Top5: 99.600] Sparsity : 0.779
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.830   Top5: 99.660] Sparsity : 0.775
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.800   Top5: 99.660] Sparsity : 0.776
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch80_20221103-203854/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 0.376343   Top1 90.703125   Top5 99.726562   BatchTime 0.129225
INFO - Validation [   40/   79]   Loss 0.415548   Top1 90.585938   Top5 99.609375   BatchTime 0.081052
INFO - Validation [   60/   79]   Loss 0.410331   Top1 90.664062   Top5 99.622396   BatchTime 0.063004
INFO - ==> Top1: 90.650    Top5: 99.660    Loss: 0.403
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/pruned_model/MobileNetv2_cifar10_a8w8_5_epoch80_checkpoint.pth.tar
INFO - Program completed successfully ... exiting ...
INFO - If you have any questions or suggestions, please visit: github.com/zhutmost/lsq-net