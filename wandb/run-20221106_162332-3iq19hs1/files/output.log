INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_imagenet_a8w8_30_epoch80_20221106-162333/MobileNetv2_imagenet_a8w8_30_epoch80_20221106-162333.log
2022-11-06 16:23:33.390742: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-06 16:23:33.513194: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-06 16:23:33.897409: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-06 16:23:33.897452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-06 16:23:33.897459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_imagenet_a8w8_30_epoch80_20221106-162333/tb_runs
********************pre-trained*****************
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:128: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.05
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `CosineLr`
    Update per batch: True
             Group 0: 0.05
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
INFO - Validation [   20/  391]   Loss 27.609890   Top1 0.000000   Top5 0.117188   BatchTime 0.608048
INFO - Validation [   40/  391]   Loss 28.591004   Top1 0.000000   Top5 0.058594   BatchTime 0.392599
INFO - Validation [   60/  391]   Loss 27.567062   Top1 0.000000   Top5 0.403646   BatchTime 0.320468
INFO - Validation [   80/  391]   Loss 24.924523   Top1 0.000000   Top5 0.419922   BatchTime 0.284572
INFO - Validation [  100/  391]   Loss 23.307600   Top1 0.000000   Top5 0.335938   BatchTime 0.263152
INFO - Validation [  120/  391]   Loss 23.457049   Top1 0.312500   Top5 0.657552   BatchTime 0.248813
INFO - Validation [  140/  391]   Loss 24.179314   Top1 0.267857   Top5 0.563616   BatchTime 0.238537
INFO - Validation [  160/  391]   Loss 24.201449   Top1 0.234375   Top5 0.493164   BatchTime 0.230848
INFO - Validation [  180/  391]   Loss 24.038137   Top1 0.208333   Top5 0.438368   BatchTime 0.224771
INFO - Validation [  200/  391]   Loss 24.115841   Top1 0.187500   Top5 0.394531   BatchTime 0.219876
INFO - Validation [  220/  391]   Loss 24.233748   Top1 0.184659   Top5 0.529119   BatchTime 0.216135
INFO - Validation [  240/  391]   Loss 24.254343   Top1 0.169271   Top5 0.488281   BatchTime 0.212843
INFO - Validation [  260/  391]   Loss 24.264850   Top1 0.156250   Top5 0.450721   BatchTime 0.210156
INFO - Validation [  280/  391]   Loss 24.328493   Top1 0.145089   Top5 0.502232   BatchTime 0.207757
INFO - Validation [  300/  391]   Loss 24.268197   Top1 0.135417   Top5 0.611979   BatchTime 0.205646
INFO - Validation [  320/  391]   Loss 24.314753   Top1 0.126953   Top5 0.573730   BatchTime 0.203787
INFO - Validation [  340/  391]   Loss 24.194326   Top1 0.119485   Top5 0.562960   BatchTime 0.201816
INFO - Validation [  360/  391]   Loss 24.101134   Top1 0.112847   Top5 0.531684   BatchTime 0.199872
INFO - Validation [  380/  391]   Loss 24.285498   Top1 0.106908   Top5 0.503701   BatchTime 0.198105
INFO - ==> Top1: 0.104    Top5: 0.490    Loss: 24.380
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.104   Top5: 0.490] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.0592, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0884, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0872, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0421, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0468, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0863, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0293, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0313, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1239, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0248, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0497, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1032, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0292, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0191, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1201, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0228, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0159, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1325, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0232, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0530, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0878, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0234, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1026, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0150, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0124, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1083, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0118, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0133, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.1126, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0210, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0344, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0678, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0116, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0132, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0802, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0093, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0144, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0917, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0092, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0393, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0513, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0154, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0139, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0651, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0120, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0158, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0836, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0056, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0318, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0435, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 7.281072   Top1 0.976562   Top5 4.101562   BatchTime 0.754619   LR 0.050000
INFO - Training [0][   40/10010]   Loss 7.131259   Top1 0.703125   Top5 3.066406   BatchTime 0.599892   LR 0.049998
INFO - Training [0][   60/10010]   Loss 6.992555   Top1 0.651042   Top5 2.864583   BatchTime 0.548613   LR 0.049995
INFO - Training [0][   80/10010]   Loss 6.866402   Top1 0.722656   Top5 3.007812   BatchTime 0.522697   LR 0.049991
INFO - Training [0][  100/10010]   Loss 6.754434   Top1 0.820312   Top5 3.328125   BatchTime 0.507230   LR 0.049987
INFO - Training [0][  120/10010]   Loss 6.676119   Top1 0.930990   Top5 3.567708   BatchTime 0.496879   LR 0.049981
INFO - Training [0][  140/10010]   Loss 6.604322   Top1 0.998884   Top5 3.850446   BatchTime 0.489432   LR 0.049974
INFO - Training [0][  160/10010]   Loss 6.541050   Top1 1.147461   Top5 4.218750   BatchTime 0.484074   LR 0.049966
INFO - Training [0][  180/10010]   Loss 6.491510   Top1 1.245660   Top5 4.505208   BatchTime 0.479819   LR 0.049956
INFO - Training [0][  200/10010]   Loss 6.447882   Top1 1.328125   Top5 4.769531   BatchTime 0.476387   LR 0.049946
INFO - Training [0][  220/10010]   Loss 6.404742   Top1 1.402699   Top5 5.092330   BatchTime 0.473596   LR 0.049935
INFO - Training [0][  240/10010]   Loss 6.364722   Top1 1.513672   Top5 5.426432   BatchTime 0.471264   LR 0.049922
INFO - Training [0][  260/10010]   Loss 6.334982   Top1 1.610577   Top5 5.667067   BatchTime 0.469288   LR 0.049909
INFO - Training [0][  280/10010]   Loss 6.304673   Top1 1.626674   Top5 5.828683   BatchTime 0.467623   LR 0.049894
