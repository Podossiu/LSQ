
Files already downloaded and verified
INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506.log
2022-11-03 23:45:06.442245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-03 23:45:06.595512: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-03 23:45:07.008706: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-03 23:45:07.008775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-03 23:45:07.008785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/tb_runs
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
Files already downloaded and verified
hello
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `cifar10` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): QuanConv2d(
          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): IdentityQuan()
          (quan_a_fn): IdentityQuan()
        )
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv): Sequential(
      (0): QuanConv2d(
        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
        (quan_w_fn): SLsqQuan()
        (quan_a_fn): LsqQuan()
      )
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): QuanLinear(
      in_features=1280, out_features=10, bias=True
      (quan_w_fn): IdentityQuan()
      (quan_a_fn): IdentityQuan()
    )
  )
)
INFO - Validation [   20/   79]   Loss 2.545371   Top1 10.429688   Top5 49.101562   BatchTime 0.216034
INFO - Validation [   40/   79]   Loss 2.549466   Top1 10.175781   Top5 49.941406   BatchTime 0.139246
INFO - Validation [   60/   79]   Loss 2.541519   Top1 10.117188   Top5 50.377604   BatchTime 0.114325
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.546
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - >>>>>>>> Epoch   0
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [0][   20/  391]   Loss 1.534628   Top1 69.648438   Top5 96.796875   BatchTime 0.215106   LR 0.010000
INFO - Training [0][   40/  391]   Loss 1.245050   Top1 71.875000   Top5 97.382812   BatchTime 0.146382   LR 0.010000
INFO - Training [0][   60/  391]   Loss 1.077059   Top1 73.177083   Top5 97.799479   BatchTime 0.138859   LR 0.010000
INFO - Training [0][   80/  391]   Loss 0.970853   Top1 74.599609   Top5 98.066406   BatchTime 0.134948   LR 0.010000
INFO - Training [0][  100/  391]   Loss 0.889976   Top1 75.859375   Top5 98.218750   BatchTime 0.132672   LR 0.010000
INFO - Training [0][  120/  391]   Loss 0.828575   Top1 76.829427   Top5 98.378906   BatchTime 0.131183   LR 0.010000
INFO - Training [0][  140/  391]   Loss 0.780480   Top1 77.700893   Top5 98.510045   BatchTime 0.130094   LR 0.010000
INFO - Training [0][  160/  391]   Loss 0.742105   Top1 78.417969   Top5 98.618164   BatchTime 0.129269   LR 0.010000
INFO - Training [0][  180/  391]   Loss 0.705706   Top1 79.275174   Top5 98.715278   BatchTime 0.128676   LR 0.010000
INFO - Training [0][  200/  391]   Loss 0.680804   Top1 79.796875   Top5 98.792969   BatchTime 0.128121   LR 0.010000
INFO - Training [0][  220/  391]   Loss 0.659019   Top1 80.245028   Top5 98.849432   BatchTime 0.127647   LR 0.010000
INFO - Training [0][  240/  391]   Loss 0.640287   Top1 80.654297   Top5 98.880208   BatchTime 0.127187   LR 0.010000
INFO - Training [0][  260/  391]   Loss 0.622131   Top1 81.063702   Top5 98.948317   BatchTime 0.123802   LR 0.010000
INFO - Training [0][  280/  391]   Loss 0.608576   Top1 81.344866   Top5 98.984375   BatchTime 0.121272   LR 0.010000
INFO - Training [0][  300/  391]   Loss 0.595525   Top1 81.625000   Top5 99.015625   BatchTime 0.119158   LR 0.010000
INFO - Training [0][  320/  391]   Loss 0.582302   Top1 81.914062   Top5 99.064941   BatchTime 0.117619   LR 0.010000
INFO - Training [0][  340/  391]   Loss 0.570795   Top1 82.173713   Top5 99.096967   BatchTime 0.117998   LR 0.010000
INFO - Training [0][  360/  391]   Loss 0.558820   Top1 82.480469   Top5 99.131944   BatchTime 0.118262   LR 0.010000
INFO - Training [0][  380/  391]   Loss 0.550177   Top1 82.707648   Top5 99.152961   BatchTime 0.118715   LR 0.010000
INFO - ==> Top1: 82.842    Top5: 99.168    Loss: 0.545
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [0][   20/   79]   Loss 0.440576   Top1 84.960938   Top5 99.414062   BatchTime 0.152973
INFO - Validation [0][   40/   79]   Loss 0.456445   Top1 84.707031   Top5 99.179688   BatchTime 0.108399
INFO - Validation [0][   60/   79]   Loss 0.461444   Top1 84.830729   Top5 99.114583   BatchTime 0.093939
INFO - ==> Top1: 84.730    Top5: 99.240    Loss: 0.459
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 84.730   Top5: 99.240] Sparsity : 0.592
INFO - Scoreboard best 2 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   1
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [1][   20/  391]   Loss 0.322500   Top1 88.945312   Top5 99.726562   BatchTime 0.208877   LR 0.010000
INFO - Training [1][   40/  391]   Loss 0.306148   Top1 89.550781   Top5 99.687500   BatchTime 0.145845   LR 0.010000
INFO - Training [1][   60/  391]   Loss 0.307201   Top1 89.544271   Top5 99.726562   BatchTime 0.128269   LR 0.010000
INFO - Training [1][   80/  391]   Loss 0.305085   Top1 89.560547   Top5 99.736328   BatchTime 0.118078   LR 0.010000
INFO - Training [1][  100/  391]   Loss 0.302467   Top1 89.578125   Top5 99.757812   BatchTime 0.114351   LR 0.010000
INFO - Training [1][  120/  391]   Loss 0.300586   Top1 89.583333   Top5 99.746094   BatchTime 0.115867   LR 0.010000
INFO - Training [1][  140/  391]   Loss 0.296282   Top1 89.609375   Top5 99.771205   BatchTime 0.117086   LR 0.010000
INFO - Training [1][  160/  391]   Loss 0.295315   Top1 89.643555   Top5 99.775391   BatchTime 0.117901   LR 0.010000
INFO - Training [1][  180/  391]   Loss 0.293445   Top1 89.674479   Top5 99.778646   BatchTime 0.118526   LR 0.010000
INFO - Training [1][  200/  391]   Loss 0.292325   Top1 89.714844   Top5 99.765625   BatchTime 0.119105   LR 0.010000
INFO - Training [1][  220/  391]   Loss 0.293523   Top1 89.651989   Top5 99.747869   BatchTime 0.119571   LR 0.010000
INFO - Training [1][  240/  391]   Loss 0.293060   Top1 89.684245   Top5 99.752604   BatchTime 0.119831   LR 0.010000
INFO - Training [1][  260/  391]   Loss 0.291976   Top1 89.702524   Top5 99.765625   BatchTime 0.120063   LR 0.010000
INFO - Training [1][  280/  391]   Loss 0.289720   Top1 89.801897   Top5 99.765625   BatchTime 0.120269   LR 0.010000
INFO - Training [1][  300/  391]   Loss 0.287135   Top1 89.867188   Top5 99.776042   BatchTime 0.119189   LR 0.010000
INFO - Training [1][  320/  391]   Loss 0.286449   Top1 89.887695   Top5 99.772949   BatchTime 0.116932   LR 0.010000
INFO - Training [1][  340/  391]   Loss 0.285240   Top1 89.917279   Top5 99.784007   BatchTime 0.115250   LR 0.010000
INFO - Training [1][  360/  391]   Loss 0.283727   Top1 89.976128   Top5 99.785156   BatchTime 0.113040   LR 0.010000
INFO - Training [1][  380/  391]   Loss 0.282704   Top1 90.006168   Top5 99.786184   BatchTime 0.113083   LR 0.010000
INFO - ==> Top1: 90.062    Top5: 99.790    Loss: 0.281
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [1][   20/   79]   Loss 0.431682   Top1 86.210938   Top5 99.296875   BatchTime 0.142527
INFO - Validation [1][   40/   79]   Loss 0.428502   Top1 86.367188   Top5 99.257812   BatchTime 0.103626
INFO - Validation [1][   60/   79]   Loss 0.427993   Top1 86.562500   Top5 99.335938   BatchTime 0.090179
INFO - ==> Top1: 86.760    Top5: 99.410    Loss: 0.423
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 86.760   Top5: 99.410] Sparsity : 0.626
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 84.730   Top5: 99.240] Sparsity : 0.592
INFO - Scoreboard best 3 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   2
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [2][   20/  391]   Loss 0.231235   Top1 92.187500   Top5 99.882812   BatchTime 0.211299   LR 0.010000
INFO - Training [2][   40/  391]   Loss 0.229777   Top1 92.089844   Top5 99.902344   BatchTime 0.166932   LR 0.010000
INFO - Training [2][   60/  391]   Loss 0.227388   Top1 92.044271   Top5 99.921875   BatchTime 0.152489   LR 0.010000
INFO - Training [2][   80/  391]   Loss 0.225135   Top1 92.236328   Top5 99.863281   BatchTime 0.145179   LR 0.010000
INFO - Training [2][  100/  391]   Loss 0.223661   Top1 92.195312   Top5 99.867188   BatchTime 0.133315   LR 0.010000
INFO - Training [2][  120/  391]   Loss 0.221642   Top1 92.233073   Top5 99.863281   BatchTime 0.126479   LR 0.010000
INFO - Training [2][  140/  391]   Loss 0.221247   Top1 92.232143   Top5 99.860491   BatchTime 0.121355   LR 0.010000
INFO - Training [2][  160/  391]   Loss 0.221614   Top1 92.211914   Top5 99.863281   BatchTime 0.118280   LR 0.010000
INFO - Training [2][  180/  391]   Loss 0.222901   Top1 92.178819   Top5 99.861111   BatchTime 0.118945   LR 0.010000
INFO - Training [2][  200/  391]   Loss 0.221079   Top1 92.296875   Top5 99.859375   BatchTime 0.119356   LR 0.010000
INFO - Training [2][  220/  391]   Loss 0.221523   Top1 92.301136   Top5 99.865057   BatchTime 0.119739   LR 0.010000
INFO - Training [2][  240/  391]   Loss 0.220850   Top1 92.363281   Top5 99.873047   BatchTime 0.120069   LR 0.010000
INFO - Training [2][  260/  391]   Loss 0.220603   Top1 92.388822   Top5 99.870793   BatchTime 0.120300   LR 0.010000
INFO - Training [2][  280/  391]   Loss 0.220948   Top1 92.413504   Top5 99.868862   BatchTime 0.120500   LR 0.010000
INFO - Training [2][  300/  391]   Loss 0.221647   Top1 92.364583   Top5 99.877604   BatchTime 0.120672   LR 0.010000
INFO - Training [2][  320/  391]   Loss 0.221789   Top1 92.375488   Top5 99.880371   BatchTime 0.120820   LR 0.010000
INFO - Training [2][  340/  391]   Loss 0.222440   Top1 92.366728   Top5 99.887408   BatchTime 0.120944   LR 0.010000
INFO - Training [2][  360/  391]   Loss 0.222097   Top1 92.339410   Top5 99.889323   BatchTime 0.120326   LR 0.010000
INFO - Training [2][  380/  391]   Loss 0.222516   Top1 92.325247   Top5 99.888980   BatchTime 0.118121   LR 0.010000
INFO - ==> Top1: 92.320    Top5: 99.890    Loss: 0.222
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [2][   20/   79]   Loss 0.402749   Top1 87.187500   Top5 99.296875   BatchTime 0.121814
INFO - Validation [2][   40/   79]   Loss 0.412671   Top1 87.441406   Top5 99.238281   BatchTime 0.090499
INFO - Validation [2][   60/   79]   Loss 0.409766   Top1 87.382812   Top5 99.375000   BatchTime 0.081416
INFO - ==> Top1: 87.480    Top5: 99.440    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 87.480   Top5: 99.440] Sparsity : 0.701
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 86.760   Top5: 99.410] Sparsity : 0.626
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 84.730   Top5: 99.240] Sparsity : 0.592
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   3
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [3][   20/  391]   Loss 0.216602   Top1 92.500000   Top5 99.921875   BatchTime 0.212061   LR 0.010000
INFO - Training [3][   40/  391]   Loss 0.198819   Top1 93.164062   Top5 99.863281   BatchTime 0.167736   LR 0.010000
INFO - Training [3][   60/  391]   Loss 0.199177   Top1 93.138021   Top5 99.882812   BatchTime 0.153012   LR 0.010000
INFO - Training [3][   80/  391]   Loss 0.198775   Top1 93.232422   Top5 99.863281   BatchTime 0.145512   LR 0.010000
INFO - Training [3][  100/  391]   Loss 0.202010   Top1 93.156250   Top5 99.867188   BatchTime 0.140935   LR 0.010000
INFO - Training [3][  120/  391]   Loss 0.205148   Top1 93.007812   Top5 99.882812   BatchTime 0.137816   LR 0.010000
INFO - Training [3][  140/  391]   Loss 0.203609   Top1 93.069196   Top5 99.860491   BatchTime 0.135613   LR 0.010000
INFO - Training [3][  160/  391]   Loss 0.205071   Top1 92.998047   Top5 99.863281   BatchTime 0.128299   LR 0.010000
INFO - Training [3][  180/  391]   Loss 0.205589   Top1 92.960069   Top5 99.874132   BatchTime 0.124149   LR 0.010000
INFO - Training [3][  200/  391]   Loss 0.206197   Top1 92.929688   Top5 99.878906   BatchTime 0.120294   LR 0.010000
INFO - Training [3][  220/  391]   Loss 0.207026   Top1 92.897727   Top5 99.886364   BatchTime 0.118532   LR 0.010000
INFO - Training [3][  240/  391]   Loss 0.206942   Top1 92.913411   Top5 99.889323   BatchTime 0.118966   LR 0.010000
INFO - Training [3][  260/  391]   Loss 0.208274   Top1 92.848558   Top5 99.885817   BatchTime 0.119250   LR 0.010000
INFO - Training [3][  280/  391]   Loss 0.209040   Top1 92.826451   Top5 99.891183   BatchTime 0.119578   LR 0.010000
INFO - Training [3][  300/  391]   Loss 0.208004   Top1 92.833333   Top5 99.890625   BatchTime 0.119825   LR 0.010000
INFO - Training [3][  320/  391]   Loss 0.206905   Top1 92.910156   Top5 99.890137   BatchTime 0.120027   LR 0.010000
INFO - Training [3][  340/  391]   Loss 0.207555   Top1 92.879136   Top5 99.889706   BatchTime 0.120231   LR 0.010000
INFO - Training [3][  360/  391]   Loss 0.207730   Top1 92.862413   Top5 99.889323   BatchTime 0.120326   LR 0.010000
INFO - Training [3][  380/  391]   Loss 0.207184   Top1 92.868010   Top5 99.893092   BatchTime 0.120422   LR 0.010000
INFO - ==> Top1: 92.854    Top5: 99.892    Loss: 0.208
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [3][   20/   79]   Loss 0.400713   Top1 87.265625   Top5 99.531250   BatchTime 0.120052
INFO - Validation [3][   40/   79]   Loss 0.393085   Top1 87.519531   Top5 99.433594   BatchTime 0.073390
INFO - Validation [3][   60/   79]   Loss 0.393269   Top1 87.486979   Top5 99.479167   BatchTime 0.058766
INFO - ==> Top1: 87.530    Top5: 99.540    Loss: 0.389
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 87.530   Top5: 99.540] Sparsity : 0.737
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 87.480   Top5: 99.440] Sparsity : 0.701
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 86.760   Top5: 99.410] Sparsity : 0.626
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   4
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [4][   20/  391]   Loss 0.207378   Top1 93.007812   Top5 99.843750   BatchTime 0.198032   LR 0.010000
INFO - Training [4][   40/  391]   Loss 0.212734   Top1 92.656250   Top5 99.863281   BatchTime 0.160677   LR 0.010000
INFO - Training [4][   60/  391]   Loss 0.206082   Top1 92.786458   Top5 99.869792   BatchTime 0.146477   LR 0.010000
INFO - Training [4][   80/  391]   Loss 0.202804   Top1 92.929688   Top5 99.882812   BatchTime 0.140595   LR 0.010000
INFO - Training [4][  100/  391]   Loss 0.202835   Top1 92.835938   Top5 99.882812   BatchTime 0.137305   LR 0.010000
INFO - Training [4][  120/  391]   Loss 0.199260   Top1 93.027344   Top5 99.889323   BatchTime 0.134938   LR 0.010000
INFO - Training [4][  140/  391]   Loss 0.201356   Top1 93.030134   Top5 99.899554   BatchTime 0.133234   LR 0.010000
INFO - Training [4][  160/  391]   Loss 0.199001   Top1 93.115234   Top5 99.902344   BatchTime 0.132105   LR 0.010000
INFO - Training [4][  180/  391]   Loss 0.199753   Top1 93.081597   Top5 99.908854   BatchTime 0.131080   LR 0.010000
INFO - Training [4][  200/  391]   Loss 0.199946   Top1 93.089844   Top5 99.906250   BatchTime 0.130303   LR 0.010000
INFO - Training [4][  220/  391]   Loss 0.198265   Top1 93.149858   Top5 99.914773   BatchTime 0.127975   LR 0.010000
INFO - Training [4][  240/  391]   Loss 0.199256   Top1 93.134766   Top5 99.915365   BatchTime 0.124487   LR 0.010000
INFO - Training [4][  260/  391]   Loss 0.198462   Top1 93.164062   Top5 99.909856   BatchTime 0.122210   LR 0.010000
INFO - Training [4][  280/  391]   Loss 0.198207   Top1 93.136161   Top5 99.907924   BatchTime 0.118836   LR 0.010000
INFO - Training [4][  300/  391]   Loss 0.197806   Top1 93.161458   Top5 99.908854   BatchTime 0.119195   LR 0.010000
INFO - Training [4][  320/  391]   Loss 0.196785   Top1 93.188477   Top5 99.912109   BatchTime 0.119485   LR 0.010000
INFO - Training [4][  340/  391]   Loss 0.195964   Top1 93.216912   Top5 99.914982   BatchTime 0.119687   LR 0.010000
INFO - Training [4][  360/  391]   Loss 0.195568   Top1 93.226997   Top5 99.915365   BatchTime 0.119868   LR 0.010000
INFO - Training [4][  380/  391]   Loss 0.194699   Top1 93.233964   Top5 99.917763   BatchTime 0.120012   LR 0.010000
INFO - ==> Top1: 93.232    Top5: 99.918    Loss: 0.195
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [4][   20/   79]   Loss 0.398575   Top1 88.164062   Top5 99.531250   BatchTime 0.150783
INFO - Validation [4][   40/   79]   Loss 0.405491   Top1 88.203125   Top5 99.277344   BatchTime 0.107736
INFO - Validation [4][   60/   79]   Loss 0.389270   Top1 88.489583   Top5 99.309896   BatchTime 0.093454
INFO - ==> Top1: 88.380    Top5: 99.380    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.380   Top5: 99.380] Sparsity : 0.745
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 87.530   Top5: 99.540] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 87.480   Top5: 99.440] Sparsity : 0.701
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   5
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [5][   20/  391]   Loss 0.192250   Top1 93.398438   Top5 99.960938   BatchTime 0.183464   LR 0.010000
INFO - Training [5][   40/  391]   Loss 0.172417   Top1 93.925781   Top5 99.980469   BatchTime 0.136153   LR 0.010000
INFO - Training [5][   60/  391]   Loss 0.170669   Top1 93.880208   Top5 99.947917   BatchTime 0.117951   LR 0.010000
INFO - Training [5][   80/  391]   Loss 0.167895   Top1 94.052734   Top5 99.960938   BatchTime 0.117109   LR 0.010000
INFO - Training [5][  100/  391]   Loss 0.164153   Top1 94.242188   Top5 99.968750   BatchTime 0.118457   LR 0.010000
INFO - Training [5][  120/  391]   Loss 0.163796   Top1 94.290365   Top5 99.960938   BatchTime 0.119284   LR 0.010000
INFO - Training [5][  140/  391]   Loss 0.163491   Top1 94.308036   Top5 99.949777   BatchTime 0.119823   LR 0.010000
INFO - Training [5][  160/  391]   Loss 0.163626   Top1 94.321289   Top5 99.956055   BatchTime 0.120338   LR 0.010000
INFO - Training [5][  180/  391]   Loss 0.163262   Top1 94.288194   Top5 99.956597   BatchTime 0.120651   LR 0.010000
INFO - Training [5][  200/  391]   Loss 0.163911   Top1 94.261719   Top5 99.945312   BatchTime 0.120927   LR 0.010000
INFO - Training [5][  220/  391]   Loss 0.165864   Top1 94.204545   Top5 99.943182   BatchTime 0.121103   LR 0.010000
INFO - Training [5][  240/  391]   Loss 0.164603   Top1 94.244792   Top5 99.941406   BatchTime 0.121261   LR 0.010000
INFO - Training [5][  260/  391]   Loss 0.164058   Top1 94.239784   Top5 99.945913   BatchTime 0.121427   LR 0.010000
INFO - Training [5][  280/  391]   Loss 0.163457   Top1 94.266183   Top5 99.938616   BatchTime 0.119025   LR 0.010000
INFO - Training [5][  300/  391]   Loss 0.162981   Top1 94.304688   Top5 99.937500   BatchTime 0.117079   LR 0.010000
INFO - Training [5][  320/  391]   Loss 0.162246   Top1 94.360352   Top5 99.936523   BatchTime 0.115481   LR 0.010000
INFO - Training [5][  340/  391]   Loss 0.162501   Top1 94.365809   Top5 99.933364   BatchTime 0.113902   LR 0.010000
INFO - Training [5][  360/  391]   Loss 0.162129   Top1 94.359809   Top5 99.934896   BatchTime 0.114415   LR 0.010000
INFO - Training [5][  380/  391]   Loss 0.162922   Top1 94.309211   Top5 99.934211   BatchTime 0.114891   LR 0.010000
INFO - ==> Top1: 94.292    Top5: 99.934    Loss: 0.163
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [5][   20/   79]   Loss 0.398485   Top1 87.968750   Top5 99.414062   BatchTime 0.149396
INFO - Validation [5][   40/   79]   Loss 0.395669   Top1 88.085938   Top5 99.335938   BatchTime 0.106112
INFO - Validation [5][   60/   79]   Loss 0.384621   Top1 88.463542   Top5 99.401042   BatchTime 0.091312
INFO - ==> Top1: 88.460    Top5: 99.430    Loss: 0.382
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.460   Top5: 99.430] Sparsity : 0.752
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.380   Top5: 99.380] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 87.530   Top5: 99.540] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   6
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [6][   20/  391]   Loss 0.145005   Top1 94.609375   Top5 99.960938   BatchTime 0.209065   LR 0.010000
INFO - Training [6][   40/  391]   Loss 0.148244   Top1 94.589844   Top5 99.921875   BatchTime 0.166278   LR 0.010000
INFO - Training [6][   60/  391]   Loss 0.146092   Top1 94.570312   Top5 99.947917   BatchTime 0.145609   LR 0.010000
INFO - Training [6][   80/  391]   Loss 0.143882   Top1 94.765625   Top5 99.951172   BatchTime 0.129824   LR 0.010000
INFO - Training [6][  100/  391]   Loss 0.146224   Top1 94.687500   Top5 99.953125   BatchTime 0.123045   LR 0.010000
INFO - Training [6][  120/  391]   Loss 0.147028   Top1 94.739583   Top5 99.954427   BatchTime 0.114990   LR 0.010000
INFO - Training [6][  140/  391]   Loss 0.145553   Top1 94.877232   Top5 99.955357   BatchTime 0.115418   LR 0.010000
INFO - Training [6][  160/  391]   Loss 0.142733   Top1 95.000000   Top5 99.960938   BatchTime 0.116383   LR 0.010000
INFO - Training [6][  180/  391]   Loss 0.144562   Top1 94.965278   Top5 99.965278   BatchTime 0.116935   LR 0.010000
INFO - Training [6][  200/  391]   Loss 0.145356   Top1 94.992188   Top5 99.957031   BatchTime 0.117369   LR 0.010000
INFO - Training [6][  220/  391]   Loss 0.145087   Top1 94.985795   Top5 99.960938   BatchTime 0.118133   LR 0.010000
INFO - Training [6][  240/  391]   Loss 0.146020   Top1 94.954427   Top5 99.957682   BatchTime 0.118595   LR 0.010000
INFO - Training [6][  260/  391]   Loss 0.147660   Top1 94.891827   Top5 99.960938   BatchTime 0.119036   LR 0.010000
INFO - Training [6][  280/  391]   Loss 0.147440   Top1 94.868862   Top5 99.958147   BatchTime 0.119364   LR 0.010000
INFO - Training [6][  300/  391]   Loss 0.148747   Top1 94.809896   Top5 99.955729   BatchTime 0.119578   LR 0.010000
INFO - Training [6][  320/  391]   Loss 0.148581   Top1 94.819336   Top5 99.953613   BatchTime 0.119824   LR 0.010000
INFO - Training [6][  340/  391]   Loss 0.149828   Top1 94.756434   Top5 99.949449   BatchTime 0.117893   LR 0.010000
INFO - Training [6][  360/  391]   Loss 0.150873   Top1 94.741753   Top5 99.945747   BatchTime 0.116019   LR 0.010000
INFO - Training [6][  380/  391]   Loss 0.152114   Top1 94.681332   Top5 99.946546   BatchTime 0.114591   LR 0.010000
INFO - ==> Top1: 94.674    Top5: 99.946    Loss: 0.152
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [6][   20/   79]   Loss 0.433901   Top1 87.812500   Top5 99.453125   BatchTime 0.160222
INFO - Validation [6][   40/   79]   Loss 0.424835   Top1 87.949219   Top5 99.433594   BatchTime 0.112807
INFO - Validation [6][   60/   79]   Loss 0.410861   Top1 88.307292   Top5 99.453125   BatchTime 0.096931
INFO - ==> Top1: 88.370    Top5: 99.460    Loss: 0.406
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.460   Top5: 99.430] Sparsity : 0.752
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.380   Top5: 99.380] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 88.370   Top5: 99.460] Sparsity : 0.765
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   7
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [7][   20/  391]   Loss 0.170966   Top1 94.257812   Top5 99.921875   BatchTime 0.209562   LR 0.010000
INFO - Training [7][   40/  391]   Loss 0.173068   Top1 94.218750   Top5 99.902344   BatchTime 0.166636   LR 0.010000
INFO - Training [7][   60/  391]   Loss 0.176019   Top1 93.958333   Top5 99.895833   BatchTime 0.152166   LR 0.010000
INFO - Training [7][   80/  391]   Loss 0.176598   Top1 93.857422   Top5 99.902344   BatchTime 0.146045   LR 0.010000
INFO - Training [7][  100/  391]   Loss 0.174092   Top1 93.945312   Top5 99.906250   BatchTime 0.141376   LR 0.010000
INFO - Training [7][  120/  391]   Loss 0.175186   Top1 93.886719   Top5 99.908854   BatchTime 0.136230   LR 0.010000
INFO - Training [7][  140/  391]   Loss 0.178203   Top1 93.733259   Top5 99.905134   BatchTime 0.128600   LR 0.010000
INFO - Training [7][  160/  391]   Loss 0.177129   Top1 93.745117   Top5 99.902344   BatchTime 0.123858   LR 0.010000
INFO - Training [7][  180/  391]   Loss 0.177282   Top1 93.723958   Top5 99.904514   BatchTime 0.118801   LR 0.010000
INFO - Training [7][  200/  391]   Loss 0.179267   Top1 93.636719   Top5 99.902344   BatchTime 0.118562   LR 0.010000
INFO - Training [7][  220/  391]   Loss 0.179331   Top1 93.664773   Top5 99.904119   BatchTime 0.119117   LR 0.010000
INFO - Training [7][  240/  391]   Loss 0.181388   Top1 93.623047   Top5 99.899089   BatchTime 0.119534   LR 0.010000
INFO - Training [7][  260/  391]   Loss 0.184123   Top1 93.515625   Top5 99.894832   BatchTime 0.119885   LR 0.010000
INFO - Training [7][  280/  391]   Loss 0.186364   Top1 93.437500   Top5 99.896763   BatchTime 0.120243   LR 0.010000
INFO - Training [7][  300/  391]   Loss 0.188351   Top1 93.380208   Top5 99.882812   BatchTime 0.120468   LR 0.010000
INFO - Training [7][  320/  391]   Loss 0.189411   Top1 93.332520   Top5 99.882812   BatchTime 0.120709   LR 0.010000
INFO - Training [7][  340/  391]   Loss 0.191272   Top1 93.267463   Top5 99.882812   BatchTime 0.120852   LR 0.010000
INFO - Training [7][  360/  391]   Loss 0.191890   Top1 93.235677   Top5 99.882812   BatchTime 0.120939   LR 0.010000
INFO - Training [7][  380/  391]   Loss 0.193799   Top1 93.184622   Top5 99.880757   BatchTime 0.121020   LR 0.010000
INFO - ==> Top1: 93.144    Top5: 99.878    Loss: 0.195
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [7][   20/   79]   Loss 0.439943   Top1 86.289062   Top5 99.453125   BatchTime 0.133646
INFO - Validation [7][   40/   79]   Loss 0.430616   Top1 86.835938   Top5 99.316406   BatchTime 0.084962
INFO - Validation [7][   60/   79]   Loss 0.417681   Top1 87.278646   Top5 99.335938   BatchTime 0.065357
INFO - ==> Top1: 87.190    Top5: 99.380    Loss: 0.419
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.460   Top5: 99.430] Sparsity : 0.752
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.380   Top5: 99.380] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 88.370   Top5: 99.460] Sparsity : 0.765
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   8
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [8][   20/  391]   Loss 0.188710   Top1 93.750000   Top5 99.843750   BatchTime 0.219350   LR 0.010000
INFO - Training [8][   40/  391]   Loss 0.200076   Top1 93.007812   Top5 99.882812   BatchTime 0.171623   LR 0.010000
INFO - Training [8][   60/  391]   Loss 0.197028   Top1 92.981771   Top5 99.882812   BatchTime 0.155591   LR 0.010000
INFO - Training [8][   80/  391]   Loss 0.193321   Top1 93.154297   Top5 99.902344   BatchTime 0.147630   LR 0.010000
INFO - Training [8][  100/  391]   Loss 0.196267   Top1 93.085938   Top5 99.890625   BatchTime 0.142845   LR 0.010000
INFO - Training [8][  120/  391]   Loss 0.198803   Top1 93.027344   Top5 99.876302   BatchTime 0.139726   LR 0.010000
INFO - Training [8][  140/  391]   Loss 0.202447   Top1 92.885045   Top5 99.888393   BatchTime 0.137384   LR 0.010000
INFO - Training [8][  160/  391]   Loss 0.203353   Top1 92.807617   Top5 99.902344   BatchTime 0.135619   LR 0.010000
INFO - Training [8][  180/  391]   Loss 0.200810   Top1 92.929688   Top5 99.908854   BatchTime 0.134156   LR 0.010000
INFO - Training [8][  200/  391]   Loss 0.201609   Top1 92.898438   Top5 99.910156   BatchTime 0.128770   LR 0.010000
INFO - Training [8][  220/  391]   Loss 0.202833   Top1 92.879972   Top5 99.914773   BatchTime 0.125452   LR 0.010000
INFO - Training [8][  240/  391]   Loss 0.202640   Top1 92.848307   Top5 99.908854   BatchTime 0.122206   LR 0.010000
INFO - Training [8][  260/  391]   Loss 0.202380   Top1 92.860577   Top5 99.903846   BatchTime 0.119944   LR 0.010000
INFO - Training [8][  280/  391]   Loss 0.203702   Top1 92.845982   Top5 99.902344   BatchTime 0.120239   LR 0.010000
INFO - Training [8][  300/  391]   Loss 0.203358   Top1 92.846354   Top5 99.898438   BatchTime 0.120508   LR 0.010000
INFO - Training [8][  320/  391]   Loss 0.204332   Top1 92.824707   Top5 99.890137   BatchTime 0.120730   LR 0.010000
INFO - Training [8][  340/  391]   Loss 0.203902   Top1 92.849265   Top5 99.889706   BatchTime 0.120525   LR 0.010000
INFO - Training [8][  360/  391]   Loss 0.205578   Top1 92.782118   Top5 99.887153   BatchTime 0.120748   LR 0.010000
INFO - Training [8][  380/  391]   Loss 0.205419   Top1 92.798109   Top5 99.886924   BatchTime 0.120897   LR 0.010000
INFO - ==> Top1: 92.798    Top5: 99.886    Loss: 0.205
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [8][   20/   79]   Loss 0.375708   Top1 88.437500   Top5 99.453125   BatchTime 0.150290
INFO - Validation [8][   40/   79]   Loss 0.383558   Top1 88.691406   Top5 99.335938   BatchTime 0.108472
INFO - Validation [8][   60/   79]   Loss 0.374224   Top1 88.750000   Top5 99.388021   BatchTime 0.094112
INFO - ==> Top1: 88.450    Top5: 99.450    Loss: 0.378
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.460   Top5: 99.430] Sparsity : 0.752
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 88.450   Top5: 99.450] Sparsity : 0.816
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.380   Top5: 99.380] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   9
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [9][   20/  391]   Loss 0.181375   Top1 93.593750   Top5 99.882812   BatchTime 0.190167   LR 0.010000
INFO - Training [9][   40/  391]   Loss 0.185908   Top1 93.515625   Top5 99.921875   BatchTime 0.139144   LR 0.010000
INFO - Training [9][   60/  391]   Loss 0.182104   Top1 93.763021   Top5 99.934896   BatchTime 0.133418   LR 0.010000
INFO - Training [9][   80/  391]   Loss 0.184257   Top1 93.593750   Top5 99.931641   BatchTime 0.130904   LR 0.010000
INFO - Training [9][  100/  391]   Loss 0.184020   Top1 93.515625   Top5 99.914062   BatchTime 0.129444   LR 0.010000
INFO - Training [9][  120/  391]   Loss 0.186279   Top1 93.515625   Top5 99.908854   BatchTime 0.128332   LR 0.010000
INFO - Training [9][  140/  391]   Loss 0.182948   Top1 93.694196   Top5 99.916295   BatchTime 0.128503   LR 0.010000
INFO - Training [9][  160/  391]   Loss 0.180574   Top1 93.789062   Top5 99.912109   BatchTime 0.127692   LR 0.010000
INFO - Training [9][  180/  391]   Loss 0.180397   Top1 93.815104   Top5 99.908854   BatchTime 0.127118   LR 0.010000
INFO - Training [9][  200/  391]   Loss 0.178138   Top1 93.867188   Top5 99.910156   BatchTime 0.126671   LR 0.010000
INFO - Training [9][  220/  391]   Loss 0.177333   Top1 93.895597   Top5 99.918324   BatchTime 0.126334   LR 0.010000
INFO - Training [9][  240/  391]   Loss 0.176840   Top1 93.886719   Top5 99.921875   BatchTime 0.125612   LR 0.010000
INFO - Training [9][  260/  391]   Loss 0.177280   Top1 93.828125   Top5 99.921875   BatchTime 0.122369   LR 0.010000
INFO - Training [9][  280/  391]   Loss 0.177549   Top1 93.836496   Top5 99.919085   BatchTime 0.120096   LR 0.010000
INFO - Training [9][  300/  391]   Loss 0.178063   Top1 93.828125   Top5 99.924479   BatchTime 0.117601   LR 0.010000
INFO - Training [9][  320/  391]   Loss 0.178648   Top1 93.796387   Top5 99.926758   BatchTime 0.117231   LR 0.010000
INFO - Training [9][  340/  391]   Loss 0.178964   Top1 93.805147   Top5 99.926471   BatchTime 0.117610   LR 0.010000
INFO - Training [9][  360/  391]   Loss 0.178537   Top1 93.832465   Top5 99.924045   BatchTime 0.117875   LR 0.010000
INFO - Training [9][  380/  391]   Loss 0.179342   Top1 93.813734   Top5 99.923931   BatchTime 0.118162   LR 0.010000
INFO - ==> Top1: 93.806    Top5: 99.924    Loss: 0.180
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [9][   20/   79]   Loss 0.385537   Top1 88.203125   Top5 99.414062   BatchTime 0.152087
INFO - Validation [9][   40/   79]   Loss 0.390348   Top1 88.457031   Top5 99.375000   BatchTime 0.107564
INFO - Validation [9][   60/   79]   Loss 0.390051   Top1 88.385417   Top5 99.453125   BatchTime 0.093796
INFO - ==> Top1: 88.370    Top5: 99.450    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.460   Top5: 99.430] Sparsity : 0.752
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 88.450   Top5: 99.450] Sparsity : 0.816
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.380   Top5: 99.380] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  10
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [10][   20/  391]   Loss 0.162319   Top1 94.843750   Top5 99.843750   BatchTime 0.213801   LR 0.010000
INFO - Training [10][   40/  391]   Loss 0.162878   Top1 94.628906   Top5 99.863281   BatchTime 0.144862   LR 0.010000
INFO - Training [10][   60/  391]   Loss 0.163764   Top1 94.648438   Top5 99.882812   BatchTime 0.127522   LR 0.010000
INFO - Training [10][   80/  391]   Loss 0.159306   Top1 94.775391   Top5 99.912109   BatchTime 0.117769   LR 0.010000
INFO - Training [10][  100/  391]   Loss 0.158486   Top1 94.679688   Top5 99.929688   BatchTime 0.115711   LR 0.010000
INFO - Training [10][  120/  391]   Loss 0.158104   Top1 94.641927   Top5 99.934896   BatchTime 0.117100   LR 0.010000
INFO - Training [10][  140/  391]   Loss 0.159315   Top1 94.542411   Top5 99.938616   BatchTime 0.117977   LR 0.010000
INFO - Training [10][  160/  391]   Loss 0.159250   Top1 94.482422   Top5 99.941406   BatchTime 0.118751   LR 0.010000
INFO - Training [10][  180/  391]   Loss 0.158391   Top1 94.483507   Top5 99.934896   BatchTime 0.119319   LR 0.010000
INFO - Training [10][  200/  391]   Loss 0.158456   Top1 94.554688   Top5 99.929688   BatchTime 0.119714   LR 0.010000
INFO - Training [10][  220/  391]   Loss 0.158345   Top1 94.506392   Top5 99.936080   BatchTime 0.120054   LR 0.010000
INFO - Training [10][  240/  391]   Loss 0.159679   Top1 94.440104   Top5 99.938151   BatchTime 0.120310   LR 0.010000
INFO - Training [10][  260/  391]   Loss 0.161411   Top1 94.371995   Top5 99.939904   BatchTime 0.120598   LR 0.010000
INFO - Training [10][  280/  391]   Loss 0.162116   Top1 94.386161   Top5 99.935826   BatchTime 0.120805   LR 0.010000
INFO - Training [10][  300/  391]   Loss 0.163012   Top1 94.385417   Top5 99.932292   BatchTime 0.119301   LR 0.010000
INFO - Training [10][  320/  391]   Loss 0.163183   Top1 94.379883   Top5 99.931641   BatchTime 0.117160   LR 0.010000
INFO - Training [10][  340/  391]   Loss 0.162594   Top1 94.391085   Top5 99.933364   BatchTime 0.115860   LR 0.010000
INFO - Training [10][  360/  391]   Loss 0.162931   Top1 94.375000   Top5 99.932726   BatchTime 0.113305   LR 0.010000
INFO - Training [10][  380/  391]   Loss 0.162989   Top1 94.381168   Top5 99.930099   BatchTime 0.113589   LR 0.010000
INFO - ==> Top1: 94.376    Top5: 99.930    Loss: 0.163
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [10][   20/   79]   Loss 0.382245   Top1 89.140625   Top5 99.492188   BatchTime 0.149919
INFO - Validation [10][   40/   79]   Loss 0.389767   Top1 89.101562   Top5 99.433594   BatchTime 0.106625
INFO - Validation [10][   60/   79]   Loss 0.378473   Top1 89.088542   Top5 99.492188   BatchTime 0.089651
INFO - ==> Top1: 88.930    Top5: 99.490    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [10][Top1: 88.930   Top5: 99.490] Sparsity : 0.820
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.460   Top5: 99.430] Sparsity : 0.752
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 88.450   Top5: 99.450] Sparsity : 0.816
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  11
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [11][   20/  391]   Loss 0.139510   Top1 95.195312   Top5 100.000000   BatchTime 0.212158   LR 0.010000
INFO - Training [11][   40/  391]   Loss 0.145219   Top1 94.726562   Top5 100.000000   BatchTime 0.167908   LR 0.010000
INFO - Training [11][   60/  391]   Loss 0.146571   Top1 94.557292   Top5 100.000000   BatchTime 0.152818   LR 0.010000
INFO - Training [11][   80/  391]   Loss 0.143324   Top1 94.824219   Top5 99.970703   BatchTime 0.145909   LR 0.010000
INFO - Training [11][  100/  391]   Loss 0.142587   Top1 94.898438   Top5 99.976562   BatchTime 0.131967   LR 0.010000
INFO - Training [11][  120/  391]   Loss 0.144440   Top1 94.811198   Top5 99.980469   BatchTime 0.125397   LR 0.010000
INFO - Training [11][  140/  391]   Loss 0.145384   Top1 94.815848   Top5 99.983259   BatchTime 0.119781   LR 0.010000
INFO - Training [11][  160/  391]   Loss 0.150394   Top1 94.711914   Top5 99.975586   BatchTime 0.116465   LR 0.010000
INFO - Training [11][  180/  391]   Loss 0.150047   Top1 94.691840   Top5 99.973958   BatchTime 0.117822   LR 0.010000
INFO - Training [11][  200/  391]   Loss 0.150100   Top1 94.695312   Top5 99.968750   BatchTime 0.118309   LR 0.010000
INFO - Training [11][  220/  391]   Loss 0.150091   Top1 94.680398   Top5 99.960938   BatchTime 0.118875   LR 0.010000
INFO - Training [11][  240/  391]   Loss 0.150514   Top1 94.658203   Top5 99.957682   BatchTime 0.119295   LR 0.010000
INFO - Training [11][  260/  391]   Loss 0.150577   Top1 94.651442   Top5 99.951923   BatchTime 0.119699   LR 0.010000
INFO - Training [11][  280/  391]   Loss 0.150738   Top1 94.665179   Top5 99.944196   BatchTime 0.119963   LR 0.010000
INFO - Training [11][  300/  391]   Loss 0.150758   Top1 94.658854   Top5 99.942708   BatchTime 0.120172   LR 0.010000
INFO - Training [11][  320/  391]   Loss 0.150125   Top1 94.677734   Top5 99.941406   BatchTime 0.120359   LR 0.010000
INFO - Training [11][  340/  391]   Loss 0.151182   Top1 94.646140   Top5 99.944853   BatchTime 0.120505   LR 0.010000
INFO - Training [11][  360/  391]   Loss 0.150671   Top1 94.661458   Top5 99.943576   BatchTime 0.119517   LR 0.010000
INFO - Training [11][  380/  391]   Loss 0.152321   Top1 94.636102   Top5 99.938322   BatchTime 0.117408   LR 0.010000
INFO - ==> Top1: 94.634    Top5: 99.938    Loss: 0.152
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [11][   20/   79]   Loss 0.398050   Top1 88.906250   Top5 99.218750   BatchTime 0.120401
INFO - Validation [11][   40/   79]   Loss 0.390700   Top1 88.925781   Top5 99.277344   BatchTime 0.094233
INFO - Validation [11][   60/   79]   Loss 0.377462   Top1 89.244792   Top5 99.348958   BatchTime 0.084418
INFO - ==> Top1: 89.140    Top5: 99.420    Loss: 0.376
INFO - Scoreboard best 1 ==> Epoch [11][Top1: 89.140   Top5: 99.420] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 88.930   Top5: 99.490] Sparsity : 0.820
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 88.460   Top5: 99.430] Sparsity : 0.752
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  12
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [12][   20/  391]   Loss 0.133285   Top1 95.273438   Top5 99.843750   BatchTime 0.210188   LR 0.010000
INFO - Training [12][   40/  391]   Loss 0.127644   Top1 95.605469   Top5 99.882812   BatchTime 0.167260   LR 0.010000
INFO - Training [12][   60/  391]   Loss 0.126362   Top1 95.625000   Top5 99.895833   BatchTime 0.153158   LR 0.010000
INFO - Training [12][   80/  391]   Loss 0.126272   Top1 95.546875   Top5 99.921875   BatchTime 0.145475   LR 0.010000
INFO - Training [12][  100/  391]   Loss 0.131466   Top1 95.390625   Top5 99.937500   BatchTime 0.141082   LR 0.010000
INFO - Training [12][  120/  391]   Loss 0.132110   Top1 95.410156   Top5 99.941406   BatchTime 0.138176   LR 0.010000
INFO - Training [12][  140/  391]   Loss 0.133508   Top1 95.318080   Top5 99.949777   BatchTime 0.135754   LR 0.010000
INFO - Training [12][  160/  391]   Loss 0.132828   Top1 95.302734   Top5 99.951172   BatchTime 0.128422   LR 0.010000
INFO - Training [12][  180/  391]   Loss 0.133186   Top1 95.264757   Top5 99.952257   BatchTime 0.124542   LR 0.010000
INFO - Training [12][  200/  391]   Loss 0.134618   Top1 95.191406   Top5 99.953125   BatchTime 0.120704   LR 0.010000
INFO - Training [12][  220/  391]   Loss 0.136528   Top1 95.142045   Top5 99.950284   BatchTime 0.118735   LR 0.010000
INFO - Training [12][  240/  391]   Loss 0.137107   Top1 95.120443   Top5 99.954427   BatchTime 0.119205   LR 0.010000
INFO - Training [12][  260/  391]   Loss 0.137669   Top1 95.051082   Top5 99.954928   BatchTime 0.119613   LR 0.010000
INFO - Training [12][  280/  391]   Loss 0.139857   Top1 95.008371   Top5 99.952567   BatchTime 0.119957   LR 0.010000
INFO - Training [12][  300/  391]   Loss 0.140728   Top1 94.986979   Top5 99.950521   BatchTime 0.120201   LR 0.010000
INFO - Training [12][  320/  391]   Loss 0.140623   Top1 95.009766   Top5 99.953613   BatchTime 0.120436   LR 0.010000
INFO - Training [12][  340/  391]   Loss 0.140202   Top1 95.057445   Top5 99.951746   BatchTime 0.120626   LR 0.010000
INFO - Training [12][  360/  391]   Loss 0.139222   Top1 95.084635   Top5 99.954427   BatchTime 0.120730   LR 0.010000
INFO - Training [12][  380/  391]   Loss 0.139627   Top1 95.059622   Top5 99.954770   BatchTime 0.120847   LR 0.010000
INFO - ==> Top1: 95.052    Top5: 99.952    Loss: 0.140
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [12][   20/   79]   Loss 0.366584   Top1 89.023438   Top5 99.492188   BatchTime 0.120884
INFO - Validation [12][   40/   79]   Loss 0.367442   Top1 89.511719   Top5 99.492188   BatchTime 0.074366
INFO - Validation [12][   60/   79]   Loss 0.367168   Top1 89.492188   Top5 99.518229   BatchTime 0.060664
INFO - ==> Top1: 89.390    Top5: 99.570    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Scoreboard best 2 ==> Epoch [11][Top1: 89.140   Top5: 99.420] Sparsity : 0.822
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 88.930   Top5: 99.490] Sparsity : 0.820
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  13
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [13][   20/  391]   Loss 0.126824   Top1 95.625000   Top5 99.882812   BatchTime 0.213972   LR 0.010000
INFO - Training [13][   40/  391]   Loss 0.127611   Top1 95.390625   Top5 99.941406   BatchTime 0.168420   LR 0.010000
INFO - Training [13][   60/  391]   Loss 0.123691   Top1 95.703125   Top5 99.960938   BatchTime 0.153916   LR 0.010000
INFO - Training [13][   80/  391]   Loss 0.129164   Top1 95.556641   Top5 99.941406   BatchTime 0.146415   LR 0.010000
INFO - Training [13][  100/  391]   Loss 0.129432   Top1 95.523438   Top5 99.945312   BatchTime 0.141907   LR 0.010000
INFO - Training [13][  120/  391]   Loss 0.132024   Top1 95.423177   Top5 99.947917   BatchTime 0.138055   LR 0.010000
INFO - Training [13][  140/  391]   Loss 0.129886   Top1 95.530134   Top5 99.938616   BatchTime 0.136040   LR 0.010000
INFO - Training [13][  160/  391]   Loss 0.131217   Top1 95.429688   Top5 99.941406   BatchTime 0.134324   LR 0.010000
INFO - Training [13][  180/  391]   Loss 0.131782   Top1 95.373264   Top5 99.939236   BatchTime 0.133086   LR 0.010000
INFO - Training [13][  200/  391]   Loss 0.132956   Top1 95.343750   Top5 99.933594   BatchTime 0.132052   LR 0.010000
INFO - Training [13][  220/  391]   Loss 0.133960   Top1 95.301847   Top5 99.936080   BatchTime 0.128537   LR 0.010000
INFO - Training [13][  240/  391]   Loss 0.135035   Top1 95.253906   Top5 99.938151   BatchTime 0.125253   LR 0.010000
INFO - Training [13][  260/  391]   Loss 0.134551   Top1 95.255409   Top5 99.942909   BatchTime 0.122770   LR 0.010000
INFO - Training [13][  280/  391]   Loss 0.134136   Top1 95.273438   Top5 99.944196   BatchTime 0.120724   LR 0.010000
INFO - Training [13][  300/  391]   Loss 0.134889   Top1 95.255208   Top5 99.942708   BatchTime 0.121094   LR 0.010000
INFO - Training [13][  320/  391]   Loss 0.135586   Top1 95.222168   Top5 99.946289   BatchTime 0.121243   LR 0.010000
INFO - Training [13][  340/  391]   Loss 0.135327   Top1 95.218290   Top5 99.947151   BatchTime 0.121389   LR 0.010000
INFO - Training [13][  360/  391]   Loss 0.135544   Top1 95.214844   Top5 99.950087   BatchTime 0.121482   LR 0.010000
INFO - Training [13][  380/  391]   Loss 0.135791   Top1 95.213816   Top5 99.950658   BatchTime 0.121539   LR 0.010000
INFO - ==> Top1: 95.218    Top5: 99.952    Loss: 0.136
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [13][   20/   79]   Loss 0.386747   Top1 88.906250   Top5 99.335938   BatchTime 0.151519
INFO - Validation [13][   40/   79]   Loss 0.381723   Top1 89.375000   Top5 99.316406   BatchTime 0.108177
INFO - Validation [13][   60/   79]   Loss 0.372703   Top1 89.492188   Top5 99.375000   BatchTime 0.093061
INFO - ==> Top1: 89.520    Top5: 99.470    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 89.140   Top5: 99.420] Sparsity : 0.822
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  14
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [14][   20/  391]   Loss 0.119382   Top1 96.054688   Top5 99.960938   BatchTime 0.185632   LR 0.010000
INFO - Training [14][   40/  391]   Loss 0.111736   Top1 96.328125   Top5 99.960938   BatchTime 0.138384   LR 0.010000
INFO - Training [14][   60/  391]   Loss 0.117637   Top1 96.054688   Top5 99.960938   BatchTime 0.118627   LR 0.010000
INFO - Training [14][   80/  391]   Loss 0.118252   Top1 96.035156   Top5 99.951172   BatchTime 0.111469   LR 0.010000
INFO - Training [14][  100/  391]   Loss 0.119766   Top1 95.960938   Top5 99.960938   BatchTime 0.105537   LR 0.010000
INFO - Training [14][  120/  391]   Loss 0.121842   Top1 95.826823   Top5 99.954427   BatchTime 0.100105   LR 0.010000
INFO - Training [14][  140/  391]   Loss 0.123555   Top1 95.736607   Top5 99.955357   BatchTime 0.096319   LR 0.010000
INFO - Training [14][  160/  391]   Loss 0.124752   Top1 95.673828   Top5 99.956055   BatchTime 0.093353   LR 0.010000
INFO - Training [14][  180/  391]   Loss 0.124285   Top1 95.642361   Top5 99.960938   BatchTime 0.091097   LR 0.010000
INFO - Training [14][  200/  391]   Loss 0.125097   Top1 95.628906   Top5 99.957031   BatchTime 0.089672   LR 0.010000
INFO - Training [14][  220/  391]   Loss 0.126202   Top1 95.593040   Top5 99.953835   BatchTime 0.088640   LR 0.010000
INFO - Training [14][  240/  391]   Loss 0.126010   Top1 95.569661   Top5 99.957682   BatchTime 0.087789   LR 0.010000
INFO - Training [14][  260/  391]   Loss 0.126760   Top1 95.534856   Top5 99.960938   BatchTime 0.086886   LR 0.010000
INFO - Training [14][  280/  391]   Loss 0.127161   Top1 95.532924   Top5 99.963728   BatchTime 0.086029   LR 0.010000
INFO - Training [14][  300/  391]   Loss 0.128061   Top1 95.476562   Top5 99.960938   BatchTime 0.085168   LR 0.010000
INFO - Training [14][  320/  391]   Loss 0.128798   Top1 95.468750   Top5 99.963379   BatchTime 0.085174   LR 0.010000
INFO - Training [14][  340/  391]   Loss 0.130586   Top1 95.427390   Top5 99.958640   BatchTime 0.084609   LR 0.010000
INFO - Training [14][  360/  391]   Loss 0.131250   Top1 95.414497   Top5 99.958767   BatchTime 0.084736   LR 0.010000
INFO - Training [14][  380/  391]   Loss 0.131412   Top1 95.427632   Top5 99.956826   BatchTime 0.084587   LR 0.010000
INFO - ==> Top1: 95.408    Top5: 99.956    Loss: 0.132
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [14][   20/   79]   Loss 0.384390   Top1 89.101562   Top5 99.570312   BatchTime 0.152793
INFO - Validation [14][   40/   79]   Loss 0.381373   Top1 89.472656   Top5 99.511719   BatchTime 0.107917
INFO - Validation [14][   60/   79]   Loss 0.374715   Top1 89.531250   Top5 99.531250   BatchTime 0.093588
INFO - ==> Top1: 89.190    Top5: 99.580    Loss: 0.377
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 89.190   Top5: 99.580] Sparsity : 0.826
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  15
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [15][   20/  391]   Loss 0.113209   Top1 95.937500   Top5 99.921875   BatchTime 0.213660   LR 0.010000
INFO - Training [15][   40/  391]   Loss 0.113287   Top1 95.937500   Top5 99.960938   BatchTime 0.169291   LR 0.010000
INFO - Training [15][   60/  391]   Loss 0.113957   Top1 95.937500   Top5 99.947917   BatchTime 0.154137   LR 0.010000
INFO - Training [15][   80/  391]   Loss 0.120635   Top1 95.742188   Top5 99.951172   BatchTime 0.146376   LR 0.010000
INFO - Training [15][  100/  391]   Loss 0.119926   Top1 95.820312   Top5 99.960938   BatchTime 0.141599   LR 0.010000
INFO - Training [15][  120/  391]   Loss 0.117220   Top1 95.957031   Top5 99.967448   BatchTime 0.134428   LR 0.010000
INFO - Training [15][  140/  391]   Loss 0.119190   Top1 95.842634   Top5 99.972098   BatchTime 0.126864   LR 0.010000
INFO - Training [15][  160/  391]   Loss 0.120361   Top1 95.776367   Top5 99.965820   BatchTime 0.122241   LR 0.010000
INFO - Training [15][  180/  391]   Loss 0.121854   Top1 95.707465   Top5 99.960938   BatchTime 0.118220   LR 0.010000
INFO - Training [15][  200/  391]   Loss 0.121394   Top1 95.765625   Top5 99.964844   BatchTime 0.117739   LR 0.010000
INFO - Training [15][  220/  391]   Loss 0.121998   Top1 95.752841   Top5 99.960938   BatchTime 0.118349   LR 0.010000
INFO - Training [15][  240/  391]   Loss 0.122321   Top1 95.735677   Top5 99.960938   BatchTime 0.118643   LR 0.010000
INFO - Training [15][  260/  391]   Loss 0.121515   Top1 95.790264   Top5 99.954928   BatchTime 0.119054   LR 0.010000
INFO - Training [15][  280/  391]   Loss 0.123699   Top1 95.725446   Top5 99.955357   BatchTime 0.119673   LR 0.010000
INFO - Training [15][  300/  391]   Loss 0.124224   Top1 95.697917   Top5 99.955729   BatchTime 0.119970   LR 0.010000
INFO - Training [15][  320/  391]   Loss 0.124476   Top1 95.700684   Top5 99.956055   BatchTime 0.120155   LR 0.010000
INFO - Training [15][  340/  391]   Loss 0.123799   Top1 95.693934   Top5 99.956342   BatchTime 0.120328   LR 0.010000
INFO - Training [15][  360/  391]   Loss 0.123412   Top1 95.692274   Top5 99.956597   BatchTime 0.120448   LR 0.010000
INFO - Training [15][  380/  391]   Loss 0.123341   Top1 95.701069   Top5 99.956826   BatchTime 0.120533   LR 0.010000
INFO - ==> Top1: 95.704    Top5: 99.956    Loss: 0.123
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [15][   20/   79]   Loss 0.397338   Top1 88.789062   Top5 99.570312   BatchTime 0.133634
INFO - Validation [15][   40/   79]   Loss 0.390607   Top1 89.570312   Top5 99.550781   BatchTime 0.085338
INFO - Validation [15][   60/   79]   Loss 0.379169   Top1 89.570312   Top5 99.596354   BatchTime 0.065734
INFO - ==> Top1: 89.480    Top5: 99.620    Loss: 0.376
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  16
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [16][   20/  391]   Loss 0.109322   Top1 96.015625   Top5 100.000000   BatchTime 0.220487   LR 0.010000
INFO - Training [16][   40/  391]   Loss 0.106709   Top1 96.210938   Top5 100.000000   BatchTime 0.172588   LR 0.010000
INFO - Training [16][   60/  391]   Loss 0.109854   Top1 96.028646   Top5 99.947917   BatchTime 0.156558   LR 0.010000
INFO - Training [16][   80/  391]   Loss 0.108950   Top1 96.132812   Top5 99.941406   BatchTime 0.148305   LR 0.010000
INFO - Training [16][  100/  391]   Loss 0.111526   Top1 96.023438   Top5 99.921875   BatchTime 0.143348   LR 0.010000
INFO - Training [16][  120/  391]   Loss 0.113295   Top1 95.957031   Top5 99.928385   BatchTime 0.140076   LR 0.010000
INFO - Training [16][  140/  391]   Loss 0.113563   Top1 95.982143   Top5 99.938616   BatchTime 0.137565   LR 0.010000
INFO - Training [16][  160/  391]   Loss 0.113761   Top1 95.991211   Top5 99.946289   BatchTime 0.135790   LR 0.010000
INFO - Training [16][  180/  391]   Loss 0.115832   Top1 95.937500   Top5 99.947917   BatchTime 0.134343   LR 0.010000
INFO - Training [16][  200/  391]   Loss 0.116271   Top1 95.902344   Top5 99.953125   BatchTime 0.130140   LR 0.010000
INFO - Training [16][  220/  391]   Loss 0.116105   Top1 95.898438   Top5 99.957386   BatchTime 0.126240   LR 0.010000
INFO - Training [16][  240/  391]   Loss 0.117669   Top1 95.820312   Top5 99.957682   BatchTime 0.123228   LR 0.010000
INFO - Training [16][  260/  391]   Loss 0.118877   Top1 95.784255   Top5 99.960938   BatchTime 0.119439   LR 0.010000
INFO - Training [16][  280/  391]   Loss 0.119765   Top1 95.775670   Top5 99.952567   BatchTime 0.120070   LR 0.010000
INFO - Training [16][  300/  391]   Loss 0.120541   Top1 95.750000   Top5 99.947917   BatchTime 0.120279   LR 0.010000
INFO - Training [16][  320/  391]   Loss 0.120184   Top1 95.771484   Top5 99.948730   BatchTime 0.120482   LR 0.010000
INFO - Training [16][  340/  391]   Loss 0.119792   Top1 95.762868   Top5 99.951746   BatchTime 0.120629   LR 0.010000
INFO - Training [16][  360/  391]   Loss 0.119497   Top1 95.785590   Top5 99.952257   BatchTime 0.120759   LR 0.010000
INFO - Training [16][  380/  391]   Loss 0.118115   Top1 95.840872   Top5 99.954770   BatchTime 0.120911   LR 0.010000
INFO - ==> Top1: 95.810    Top5: 99.956    Loss: 0.118
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [16][   20/   79]   Loss 0.387658   Top1 88.593750   Top5 99.648438   BatchTime 0.148959
INFO - Validation [16][   40/   79]   Loss 0.389820   Top1 88.964844   Top5 99.570312   BatchTime 0.106935
INFO - Validation [16][   60/   79]   Loss 0.383962   Top1 89.192708   Top5 99.583333   BatchTime 0.093024
INFO - ==> Top1: 89.240    Top5: 99.580    Loss: 0.382
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  17
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [17][   20/  391]   Loss 0.092974   Top1 96.718750   Top5 100.000000   BatchTime 0.182182   LR 0.010000
INFO - Training [17][   40/  391]   Loss 0.105327   Top1 96.250000   Top5 99.980469   BatchTime 0.132328   LR 0.010000
INFO - Training [17][   60/  391]   Loss 0.104891   Top1 96.263021   Top5 99.973958   BatchTime 0.124373   LR 0.010000
INFO - Training [17][   80/  391]   Loss 0.107600   Top1 96.230469   Top5 99.980469   BatchTime 0.124439   LR 0.010000
INFO - Training [17][  100/  391]   Loss 0.107740   Top1 96.226562   Top5 99.976562   BatchTime 0.124276   LR 0.010000
INFO - Training [17][  120/  391]   Loss 0.112471   Top1 95.996094   Top5 99.973958   BatchTime 0.124219   LR 0.010000
INFO - Training [17][  140/  391]   Loss 0.112492   Top1 95.965402   Top5 99.977679   BatchTime 0.124196   LR 0.010000
INFO - Training [17][  160/  391]   Loss 0.113720   Top1 95.917969   Top5 99.980469   BatchTime 0.124136   LR 0.010000
INFO - Training [17][  180/  391]   Loss 0.114064   Top1 95.924479   Top5 99.978299   BatchTime 0.124053   LR 0.010000
INFO - Training [17][  200/  391]   Loss 0.113560   Top1 95.953125   Top5 99.980469   BatchTime 0.123991   LR 0.010000
INFO - Training [17][  220/  391]   Loss 0.112742   Top1 95.983665   Top5 99.978693   BatchTime 0.123927   LR 0.010000
INFO - Training [17][  240/  391]   Loss 0.113950   Top1 95.917969   Top5 99.977214   BatchTime 0.123868   LR 0.010000
INFO - Training [17][  260/  391]   Loss 0.114016   Top1 95.961538   Top5 99.978966   BatchTime 0.121915   LR 0.010000
INFO - Training [17][  280/  391]   Loss 0.114138   Top1 95.973772   Top5 99.980469   BatchTime 0.119212   LR 0.010000
INFO - Training [17][  300/  391]   Loss 0.114659   Top1 95.945312   Top5 99.981771   BatchTime 0.117536   LR 0.010000
INFO - Training [17][  320/  391]   Loss 0.114832   Top1 95.932617   Top5 99.978027   BatchTime 0.115384   LR 0.010000
INFO - Training [17][  340/  391]   Loss 0.114442   Top1 95.962776   Top5 99.979320   BatchTime 0.115459   LR 0.010000
INFO - Training [17][  360/  391]   Loss 0.114551   Top1 95.946181   Top5 99.978299   BatchTime 0.115951   LR 0.010000
INFO - Training [17][  380/  391]   Loss 0.114886   Top1 95.929276   Top5 99.979441   BatchTime 0.116363   LR 0.010000
INFO - ==> Top1: 95.900    Top5: 99.980    Loss: 0.115
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [17][   20/   79]   Loss 0.401023   Top1 89.375000   Top5 99.375000   BatchTime 0.149525
INFO - Validation [17][   40/   79]   Loss 0.404246   Top1 89.316406   Top5 99.335938   BatchTime 0.106622
INFO - Validation [17][   60/   79]   Loss 0.392828   Top1 89.440104   Top5 99.440104   BatchTime 0.092100
INFO - ==> Top1: 89.230    Top5: 99.500    Loss: 0.392
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  18
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [18][   20/  391]   Loss 0.104246   Top1 96.367188   Top5 99.960938   BatchTime 0.208665   LR 0.010000
INFO - Training [18][   40/  391]   Loss 0.111961   Top1 96.132812   Top5 99.960938   BatchTime 0.165913   LR 0.010000
INFO - Training [18][   60/  391]   Loss 0.105837   Top1 96.354167   Top5 99.973958   BatchTime 0.136185   LR 0.010000
INFO - Training [18][   80/  391]   Loss 0.105424   Top1 96.318359   Top5 99.970703   BatchTime 0.124561   LR 0.010000
INFO - Training [18][  100/  391]   Loss 0.104123   Top1 96.273438   Top5 99.976562   BatchTime 0.117674   LR 0.010000
INFO - Training [18][  120/  391]   Loss 0.107389   Top1 96.171875   Top5 99.980469   BatchTime 0.112523   LR 0.010000
INFO - Training [18][  140/  391]   Loss 0.107707   Top1 96.127232   Top5 99.977679   BatchTime 0.113870   LR 0.010000
INFO - Training [18][  160/  391]   Loss 0.108217   Top1 96.103516   Top5 99.980469   BatchTime 0.115177   LR 0.010000
INFO - Training [18][  180/  391]   Loss 0.107408   Top1 96.197917   Top5 99.978299   BatchTime 0.116131   LR 0.010000
INFO - Training [18][  200/  391]   Loss 0.108494   Top1 96.207031   Top5 99.968750   BatchTime 0.116844   LR 0.010000
INFO - Training [18][  220/  391]   Loss 0.112414   Top1 96.044034   Top5 99.971591   BatchTime 0.117650   LR 0.010000
INFO - Training [18][  240/  391]   Loss 0.114388   Top1 95.979818   Top5 99.964193   BatchTime 0.118199   LR 0.010000
INFO - Training [18][  260/  391]   Loss 0.114875   Top1 95.973558   Top5 99.966947   BatchTime 0.118633   LR 0.010000
INFO - Training [18][  280/  391]   Loss 0.115747   Top1 95.929129   Top5 99.969308   BatchTime 0.118969   LR 0.010000
INFO - Training [18][  300/  391]   Loss 0.116517   Top1 95.906250   Top5 99.971354   BatchTime 0.119260   LR 0.010000
INFO - Training [18][  320/  391]   Loss 0.118501   Top1 95.830078   Top5 99.965820   BatchTime 0.119190   LR 0.010000
INFO - Training [18][  340/  391]   Loss 0.120964   Top1 95.762868   Top5 99.965533   BatchTime 0.116533   LR 0.010000
INFO - Training [18][  360/  391]   Loss 0.122919   Top1 95.707465   Top5 99.967448   BatchTime 0.115147   LR 0.010000
INFO - Training [18][  380/  391]   Loss 0.124512   Top1 95.653783   Top5 99.967105   BatchTime 0.113643   LR 0.010000
INFO - ==> Top1: 95.620    Top5: 99.968    Loss: 0.125
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [18][   20/   79]   Loss 0.404832   Top1 88.515625   Top5 99.648438   BatchTime 0.154273
INFO - Validation [18][   40/   79]   Loss 0.399811   Top1 88.984375   Top5 99.531250   BatchTime 0.109435
INFO - Validation [18][   60/   79]   Loss 0.393942   Top1 89.036458   Top5 99.557292   BatchTime 0.093838
INFO - ==> Top1: 88.930    Top5: 99.540    Loss: 0.394
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  19
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [19][   20/  391]   Loss 0.165069   Top1 93.671875   Top5 99.882812   BatchTime 0.211021   LR 0.010000
INFO - Training [19][   40/  391]   Loss 0.148242   Top1 94.394531   Top5 99.921875   BatchTime 0.167922   LR 0.010000
INFO - Training [19][   60/  391]   Loss 0.149576   Top1 94.479167   Top5 99.934896   BatchTime 0.153317   LR 0.010000
INFO - Training [19][   80/  391]   Loss 0.143852   Top1 94.697266   Top5 99.941406   BatchTime 0.145786   LR 0.010000
INFO - Training [19][  100/  391]   Loss 0.146196   Top1 94.656250   Top5 99.937500   BatchTime 0.141421   LR 0.010000
INFO - Training [19][  120/  391]   Loss 0.148243   Top1 94.596354   Top5 99.934896   BatchTime 0.133698   LR 0.010000
INFO - Training [19][  140/  391]   Loss 0.147105   Top1 94.587054   Top5 99.938616   BatchTime 0.126825   LR 0.010000
INFO - Training [19][  160/  391]   Loss 0.147798   Top1 94.575195   Top5 99.941406   BatchTime 0.122505   LR 0.010000
INFO - Training [19][  180/  391]   Loss 0.145676   Top1 94.644097   Top5 99.939236   BatchTime 0.117479   LR 0.010000
INFO - Training [19][  200/  391]   Loss 0.144805   Top1 94.679688   Top5 99.945312   BatchTime 0.118548   LR 0.010000
INFO - Training [19][  220/  391]   Loss 0.144615   Top1 94.708807   Top5 99.943182   BatchTime 0.119055   LR 0.010000
INFO - Training [19][  240/  391]   Loss 0.146058   Top1 94.700521   Top5 99.944661   BatchTime 0.119477   LR 0.010000
INFO - Training [19][  260/  391]   Loss 0.146592   Top1 94.696514   Top5 99.948918   BatchTime 0.119850   LR 0.010000
INFO - Training [19][  280/  391]   Loss 0.145940   Top1 94.734933   Top5 99.946987   BatchTime 0.120128   LR 0.010000
INFO - Training [19][  300/  391]   Loss 0.146677   Top1 94.695312   Top5 99.950521   BatchTime 0.120324   LR 0.010000
INFO - Training [19][  320/  391]   Loss 0.147497   Top1 94.689941   Top5 99.948730   BatchTime 0.120551   LR 0.010000
INFO - Training [19][  340/  391]   Loss 0.148872   Top1 94.639246   Top5 99.942555   BatchTime 0.120659   LR 0.010000
INFO - Training [19][  360/  391]   Loss 0.149532   Top1 94.596354   Top5 99.943576   BatchTime 0.120813   LR 0.010000
INFO - Training [19][  380/  391]   Loss 0.150086   Top1 94.582648   Top5 99.946546   BatchTime 0.120925   LR 0.010000
INFO - ==> Top1: 94.572    Top5: 99.946    Loss: 0.151
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [19][   20/   79]   Loss 0.401324   Top1 88.476562   Top5 99.414062   BatchTime 0.141090
INFO - Validation [19][   40/   79]   Loss 0.398623   Top1 88.613281   Top5 99.414062   BatchTime 0.084056
INFO - Validation [19][   60/   79]   Loss 0.387419   Top1 89.140625   Top5 99.466146   BatchTime 0.064745
INFO - ==> Top1: 89.040    Top5: 99.440    Loss: 0.385
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  20
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [20][   20/  391]   Loss 0.140771   Top1 94.765625   Top5 99.960938   BatchTime 0.222777   LR 0.010000
INFO - Training [20][   40/  391]   Loss 0.132907   Top1 95.449219   Top5 99.960938   BatchTime 0.172116   LR 0.010000
INFO - Training [20][   60/  391]   Loss 0.130737   Top1 95.546875   Top5 99.947917   BatchTime 0.156361   LR 0.010000
INFO - Training [20][   80/  391]   Loss 0.133505   Top1 95.400391   Top5 99.951172   BatchTime 0.148568   LR 0.010000
INFO - Training [20][  100/  391]   Loss 0.138195   Top1 95.265625   Top5 99.953125   BatchTime 0.143668   LR 0.010000
INFO - Training [20][  120/  391]   Loss 0.140093   Top1 95.130208   Top5 99.954427   BatchTime 0.140353   LR 0.010000
INFO - Training [20][  140/  391]   Loss 0.143018   Top1 94.905134   Top5 99.955357   BatchTime 0.137946   LR 0.010000
INFO - Training [20][  160/  391]   Loss 0.141816   Top1 94.980469   Top5 99.951172   BatchTime 0.136114   LR 0.010000
INFO - Training [20][  180/  391]   Loss 0.141930   Top1 95.000000   Top5 99.956597   BatchTime 0.134804   LR 0.010000
INFO - Training [20][  200/  391]   Loss 0.143207   Top1 94.960938   Top5 99.960938   BatchTime 0.128867   LR 0.010000
INFO - Training [20][  220/  391]   Loss 0.146468   Top1 94.861506   Top5 99.960938   BatchTime 0.125464   LR 0.010000
INFO - Training [20][  240/  391]   Loss 0.148152   Top1 94.794922   Top5 99.960938   BatchTime 0.122420   LR 0.010000
INFO - Training [20][  260/  391]   Loss 0.149878   Top1 94.723558   Top5 99.960938   BatchTime 0.120330   LR 0.010000
INFO - Training [20][  280/  391]   Loss 0.150866   Top1 94.695871   Top5 99.963728   BatchTime 0.120494   LR 0.010000
INFO - Training [20][  300/  391]   Loss 0.151281   Top1 94.690104   Top5 99.963542   BatchTime 0.120719   LR 0.010000
INFO - Training [20][  320/  391]   Loss 0.152161   Top1 94.624023   Top5 99.963379   BatchTime 0.120911   LR 0.010000
INFO - Training [20][  340/  391]   Loss 0.152988   Top1 94.574908   Top5 99.965533   BatchTime 0.121063   LR 0.010000
INFO - Training [20][  360/  391]   Loss 0.153573   Top1 94.559462   Top5 99.963108   BatchTime 0.121201   LR 0.010000
INFO - Training [20][  380/  391]   Loss 0.153483   Top1 94.568257   Top5 99.962993   BatchTime 0.121339   LR 0.010000
INFO - ==> Top1: 94.574    Top5: 99.964    Loss: 0.154
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [20][   20/   79]   Loss 0.397398   Top1 88.437500   Top5 99.453125   BatchTime 0.150056
INFO - Validation [20][   40/   79]   Loss 0.387339   Top1 88.828125   Top5 99.472656   BatchTime 0.107842
INFO - Validation [20][   60/   79]   Loss 0.381901   Top1 88.841146   Top5 99.544271   BatchTime 0.093772
INFO - ==> Top1: 89.000    Top5: 99.560    Loss: 0.379
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  21
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [21][   20/  391]   Loss 0.149821   Top1 94.960938   Top5 99.921875   BatchTime 0.186005   LR 0.010000
INFO - Training [21][   40/  391]   Loss 0.142863   Top1 95.097656   Top5 99.941406   BatchTime 0.132231   LR 0.010000
INFO - Training [21][   60/  391]   Loss 0.146936   Top1 94.921875   Top5 99.960938   BatchTime 0.129528   LR 0.010000
INFO - Training [21][   80/  391]   Loss 0.149747   Top1 94.726562   Top5 99.970703   BatchTime 0.128316   LR 0.010000
INFO - Training [21][  100/  391]   Loss 0.146989   Top1 94.781250   Top5 99.968750   BatchTime 0.127275   LR 0.010000
INFO - Training [21][  120/  391]   Loss 0.152168   Top1 94.641927   Top5 99.967448   BatchTime 0.126773   LR 0.010000
INFO - Training [21][  140/  391]   Loss 0.150781   Top1 94.715402   Top5 99.960938   BatchTime 0.126609   LR 0.010000
INFO - Training [21][  160/  391]   Loss 0.151775   Top1 94.672852   Top5 99.956055   BatchTime 0.126356   LR 0.010000
INFO - Training [21][  180/  391]   Loss 0.150086   Top1 94.700521   Top5 99.952257   BatchTime 0.126006   LR 0.010000
INFO - Training [21][  200/  391]   Loss 0.150162   Top1 94.726562   Top5 99.937500   BatchTime 0.125760   LR 0.010000
INFO - Training [21][  220/  391]   Loss 0.152329   Top1 94.680398   Top5 99.928977   BatchTime 0.125570   LR 0.010000
INFO - Training [21][  240/  391]   Loss 0.150406   Top1 94.755859   Top5 99.934896   BatchTime 0.125340   LR 0.010000
INFO - Training [21][  260/  391]   Loss 0.150519   Top1 94.726562   Top5 99.936899   BatchTime 0.122197   LR 0.010000
INFO - Training [21][  280/  391]   Loss 0.150428   Top1 94.743304   Top5 99.941406   BatchTime 0.119955   LR 0.010000
INFO - Training [21][  300/  391]   Loss 0.151214   Top1 94.708333   Top5 99.940104   BatchTime 0.117765   LR 0.010000
INFO - Training [21][  320/  391]   Loss 0.152111   Top1 94.645996   Top5 99.941406   BatchTime 0.116022   LR 0.010000
INFO - Training [21][  340/  391]   Loss 0.152756   Top1 94.641544   Top5 99.942555   BatchTime 0.116468   LR 0.010000
INFO - Training [21][  360/  391]   Loss 0.153981   Top1 94.589844   Top5 99.943576   BatchTime 0.116887   LR 0.010000
INFO - Training [21][  380/  391]   Loss 0.154673   Top1 94.574424   Top5 99.938322   BatchTime 0.117203   LR 0.010000
INFO - ==> Top1: 94.584    Top5: 99.936    Loss: 0.155
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [21][   20/   79]   Loss 0.404422   Top1 88.554688   Top5 99.375000   BatchTime 0.150103
INFO - Validation [21][   40/   79]   Loss 0.389073   Top1 88.378906   Top5 99.511719   BatchTime 0.107772
INFO - Validation [21][   60/   79]   Loss 0.380423   Top1 88.632812   Top5 99.557292   BatchTime 0.092753
INFO - ==> Top1: 88.630    Top5: 99.540    Loss: 0.376
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  22
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [22][   20/  391]   Loss 0.126687   Top1 95.546875   Top5 99.960938   BatchTime 0.212334   LR 0.010000
INFO - Training [22][   40/  391]   Loss 0.141619   Top1 94.941406   Top5 99.980469   BatchTime 0.158708   LR 0.010000
INFO - Training [22][   60/  391]   Loss 0.143450   Top1 94.895833   Top5 99.973958   BatchTime 0.133851   LR 0.010000
INFO - Training [22][   80/  391]   Loss 0.147424   Top1 94.755859   Top5 99.941406   BatchTime 0.123625   LR 0.010000
INFO - Training [22][  100/  391]   Loss 0.148206   Top1 94.765625   Top5 99.937500   BatchTime 0.114042   LR 0.010000
INFO - Training [22][  120/  391]   Loss 0.150270   Top1 94.700521   Top5 99.934896   BatchTime 0.115225   LR 0.010000
INFO - Training [22][  140/  391]   Loss 0.149726   Top1 94.720982   Top5 99.933036   BatchTime 0.116569   LR 0.010000
INFO - Training [22][  160/  391]   Loss 0.149564   Top1 94.707031   Top5 99.931641   BatchTime 0.117506   LR 0.010000
INFO - Training [22][  180/  391]   Loss 0.149987   Top1 94.670139   Top5 99.930556   BatchTime 0.118297   LR 0.010000
INFO - Training [22][  200/  391]   Loss 0.150538   Top1 94.640625   Top5 99.937500   BatchTime 0.118850   LR 0.010000
INFO - Training [22][  220/  391]   Loss 0.151155   Top1 94.588068   Top5 99.936080   BatchTime 0.119352   LR 0.010000
INFO - Training [22][  240/  391]   Loss 0.149911   Top1 94.632161   Top5 99.941406   BatchTime 0.119240   LR 0.010000
INFO - Training [22][  260/  391]   Loss 0.150775   Top1 94.582332   Top5 99.942909   BatchTime 0.119565   LR 0.010000
INFO - Training [22][  280/  391]   Loss 0.149732   Top1 94.623326   Top5 99.944196   BatchTime 0.119800   LR 0.010000
INFO - Training [22][  300/  391]   Loss 0.150335   Top1 94.611979   Top5 99.942708   BatchTime 0.120047   LR 0.010000
INFO - Training [22][  320/  391]   Loss 0.150317   Top1 94.626465   Top5 99.946289   BatchTime 0.118122   LR 0.010000
INFO - Training [22][  340/  391]   Loss 0.150502   Top1 94.618566   Top5 99.949449   BatchTime 0.116237   LR 0.010000
INFO - Training [22][  360/  391]   Loss 0.149309   Top1 94.646267   Top5 99.947917   BatchTime 0.114852   LR 0.010000
INFO - Training [22][  380/  391]   Loss 0.151490   Top1 94.553865   Top5 99.944490   BatchTime 0.112527   LR 0.010000
INFO - ==> Top1: 94.546    Top5: 99.946    Loss: 0.151
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [22][   20/   79]   Loss 0.396790   Top1 88.085938   Top5 99.335938   BatchTime 0.150386
INFO - Validation [22][   40/   79]   Loss 0.382050   Top1 88.515625   Top5 99.375000   BatchTime 0.106443
INFO - Validation [22][   60/   79]   Loss 0.379367   Top1 88.763021   Top5 99.479167   BatchTime 0.092799
INFO - ==> Top1: 88.610    Top5: 99.530    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  23
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [23][   20/  391]   Loss 0.162741   Top1 93.867188   Top5 99.921875   BatchTime 0.214092   LR 0.010000
INFO - Training [23][   40/  391]   Loss 0.145708   Top1 94.726562   Top5 99.902344   BatchTime 0.168830   LR 0.010000
INFO - Training [23][   60/  391]   Loss 0.147290   Top1 94.726562   Top5 99.921875   BatchTime 0.153647   LR 0.010000
INFO - Training [23][   80/  391]   Loss 0.144378   Top1 94.824219   Top5 99.931641   BatchTime 0.146153   LR 0.010000
INFO - Training [23][  100/  391]   Loss 0.143028   Top1 95.000000   Top5 99.937500   BatchTime 0.138722   LR 0.010000
INFO - Training [23][  120/  391]   Loss 0.139879   Top1 95.091146   Top5 99.934896   BatchTime 0.129176   LR 0.010000
INFO - Training [23][  140/  391]   Loss 0.139454   Top1 95.106027   Top5 99.933036   BatchTime 0.123375   LR 0.010000
INFO - Training [23][  160/  391]   Loss 0.141541   Top1 95.039062   Top5 99.926758   BatchTime 0.118726   LR 0.010000
INFO - Training [23][  180/  391]   Loss 0.140833   Top1 95.056424   Top5 99.930556   BatchTime 0.118291   LR 0.010000
INFO - Training [23][  200/  391]   Loss 0.141910   Top1 95.035156   Top5 99.925781   BatchTime 0.118860   LR 0.010000
INFO - Training [23][  220/  391]   Loss 0.142190   Top1 95.042614   Top5 99.928977   BatchTime 0.119387   LR 0.010000
INFO - Training [23][  240/  391]   Loss 0.142657   Top1 94.990234   Top5 99.928385   BatchTime 0.119802   LR 0.010000
INFO - Training [23][  260/  391]   Loss 0.141845   Top1 94.990986   Top5 99.930889   BatchTime 0.120117   LR 0.010000
INFO - Training [23][  280/  391]   Loss 0.142950   Top1 94.972098   Top5 99.933036   BatchTime 0.120406   LR 0.010000
INFO - Training [23][  300/  391]   Loss 0.143863   Top1 94.924479   Top5 99.934896   BatchTime 0.120667   LR 0.010000
INFO - Training [23][  320/  391]   Loss 0.143226   Top1 94.956055   Top5 99.938965   BatchTime 0.120834   LR 0.010000
INFO - Training [23][  340/  391]   Loss 0.142837   Top1 94.983915   Top5 99.937960   BatchTime 0.120956   LR 0.010000
INFO - Training [23][  360/  391]   Loss 0.142411   Top1 95.017361   Top5 99.941406   BatchTime 0.121038   LR 0.010000
INFO - Training [23][  380/  391]   Loss 0.142203   Top1 95.028783   Top5 99.944490   BatchTime 0.119410   LR 0.010000
INFO - ==> Top1: 95.008    Top5: 99.946    Loss: 0.142
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [23][   20/   79]   Loss 0.402114   Top1 88.984375   Top5 99.453125   BatchTime 0.119409
INFO - Validation [23][   40/   79]   Loss 0.398909   Top1 88.886719   Top5 99.453125   BatchTime 0.072870
INFO - Validation [23][   60/   79]   Loss 0.392820   Top1 89.127604   Top5 99.466146   BatchTime 0.058110
INFO - ==> Top1: 89.290    Top5: 99.460    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.390   Top5: 99.570] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  24
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [24][   20/  391]   Loss 0.124096   Top1 95.429688   Top5 99.960938   BatchTime 0.212501   LR 0.010000
INFO - Training [24][   40/  391]   Loss 0.130432   Top1 95.136719   Top5 99.960938   BatchTime 0.168384   LR 0.010000
INFO - Training [24][   60/  391]   Loss 0.129420   Top1 95.208333   Top5 99.973958   BatchTime 0.153552   LR 0.010000
INFO - Training [24][   80/  391]   Loss 0.128009   Top1 95.283203   Top5 99.970703   BatchTime 0.146266   LR 0.010000
INFO - Training [24][  100/  391]   Loss 0.128973   Top1 95.312500   Top5 99.968750   BatchTime 0.142651   LR 0.010000
INFO - Training [24][  120/  391]   Loss 0.128632   Top1 95.332031   Top5 99.973958   BatchTime 0.139334   LR 0.010000
INFO - Training [24][  140/  391]   Loss 0.126631   Top1 95.418527   Top5 99.966518   BatchTime 0.137075   LR 0.010000
INFO - Training [24][  160/  391]   Loss 0.126427   Top1 95.419922   Top5 99.965820   BatchTime 0.135307   LR 0.010000
INFO - Training [24][  180/  391]   Loss 0.128545   Top1 95.364583   Top5 99.965278   BatchTime 0.130115   LR 0.010000
INFO - Training [24][  200/  391]   Loss 0.132228   Top1 95.218750   Top5 99.960938   BatchTime 0.125812   LR 0.010000
INFO - Training [24][  220/  391]   Loss 0.132481   Top1 95.245028   Top5 99.964489   BatchTime 0.122764   LR 0.010000
INFO - Training [24][  240/  391]   Loss 0.133415   Top1 95.205078   Top5 99.954427   BatchTime 0.118534   LR 0.010000
INFO - Training [24][  260/  391]   Loss 0.132048   Top1 95.273438   Top5 99.957933   BatchTime 0.119324   LR 0.010000
INFO - Training [24][  280/  391]   Loss 0.131576   Top1 95.284598   Top5 99.960938   BatchTime 0.119663   LR 0.010000
INFO - Training [24][  300/  391]   Loss 0.133169   Top1 95.265625   Top5 99.960938   BatchTime 0.119995   LR 0.010000
INFO - Training [24][  320/  391]   Loss 0.133368   Top1 95.236816   Top5 99.960938   BatchTime 0.120278   LR 0.010000
INFO - Training [24][  340/  391]   Loss 0.134372   Top1 95.211397   Top5 99.960938   BatchTime 0.120541   LR 0.010000
INFO - Training [24][  360/  391]   Loss 0.135985   Top1 95.143229   Top5 99.958767   BatchTime 0.120715   LR 0.010000
INFO - Training [24][  380/  391]   Loss 0.136666   Top1 95.133635   Top5 99.954770   BatchTime 0.120881   LR 0.010000
INFO - ==> Top1: 95.140    Top5: 99.954    Loss: 0.136
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [24][   20/   79]   Loss 0.384415   Top1 89.531250   Top5 99.531250   BatchTime 0.147909
INFO - Validation [24][   40/   79]   Loss 0.377650   Top1 89.511719   Top5 99.492188   BatchTime 0.106175
INFO - Validation [24][   60/   79]   Loss 0.372802   Top1 89.583333   Top5 99.557292   BatchTime 0.082081
INFO - ==> Top1: 89.520    Top5: 99.590    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  25
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [25][   20/  391]   Loss 0.129417   Top1 95.429688   Top5 99.960938   BatchTime 0.177432   LR 0.010000
INFO - Training [25][   40/  391]   Loss 0.130910   Top1 95.488281   Top5 99.960938   BatchTime 0.137803   LR 0.010000
INFO - Training [25][   60/  391]   Loss 0.126432   Top1 95.611979   Top5 99.947917   BatchTime 0.133205   LR 0.010000
INFO - Training [25][   80/  391]   Loss 0.129143   Top1 95.595703   Top5 99.951172   BatchTime 0.130877   LR 0.010000
INFO - Training [25][  100/  391]   Loss 0.132391   Top1 95.453125   Top5 99.953125   BatchTime 0.129536   LR 0.010000
INFO - Training [25][  120/  391]   Loss 0.131830   Top1 95.436198   Top5 99.941406   BatchTime 0.128474   LR 0.010000
INFO - Training [25][  140/  391]   Loss 0.130935   Top1 95.429688   Top5 99.949777   BatchTime 0.127993   LR 0.010000
INFO - Training [25][  160/  391]   Loss 0.132719   Top1 95.400391   Top5 99.946289   BatchTime 0.127452   LR 0.010000
INFO - Training [25][  180/  391]   Loss 0.133211   Top1 95.355903   Top5 99.943576   BatchTime 0.126981   LR 0.010000
INFO - Training [25][  200/  391]   Loss 0.133934   Top1 95.335938   Top5 99.941406   BatchTime 0.126600   LR 0.010000
INFO - Training [25][  220/  391]   Loss 0.133990   Top1 95.294744   Top5 99.946733   BatchTime 0.126314   LR 0.010000
INFO - Training [25][  240/  391]   Loss 0.134164   Top1 95.309245   Top5 99.941406   BatchTime 0.124630   LR 0.010000
INFO - Training [25][  260/  391]   Loss 0.132667   Top1 95.366587   Top5 99.939904   BatchTime 0.121808   LR 0.010000
INFO - Training [25][  280/  391]   Loss 0.131148   Top1 95.418527   Top5 99.944196   BatchTime 0.119835   LR 0.010000
INFO - Training [25][  300/  391]   Loss 0.131163   Top1 95.429688   Top5 99.947917   BatchTime 0.117134   LR 0.010000
INFO - Training [25][  320/  391]   Loss 0.131467   Top1 95.415039   Top5 99.948730   BatchTime 0.117197   LR 0.010000
INFO - Training [25][  340/  391]   Loss 0.129821   Top1 95.457261   Top5 99.951746   BatchTime 0.117565   LR 0.010000
INFO - Training [25][  360/  391]   Loss 0.131378   Top1 95.418837   Top5 99.950087   BatchTime 0.117887   LR 0.010000
INFO - Training [25][  380/  391]   Loss 0.130716   Top1 95.433799   Top5 99.950658   BatchTime 0.118179   LR 0.010000
INFO - ==> Top1: 95.424    Top5: 99.952    Loss: 0.131
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [25][   20/   79]   Loss 0.388519   Top1 88.867188   Top5 99.609375   BatchTime 0.150402
INFO - Validation [25][   40/   79]   Loss 0.383929   Top1 88.925781   Top5 99.531250   BatchTime 0.106968
INFO - Validation [25][   60/   79]   Loss 0.381981   Top1 89.114583   Top5 99.570312   BatchTime 0.092972
INFO - ==> Top1: 89.090    Top5: 99.610    Loss: 0.378
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  26
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [26][   20/  391]   Loss 0.119874   Top1 95.703125   Top5 100.000000   BatchTime 0.205923   LR 0.010000
INFO - Training [26][   40/  391]   Loss 0.121421   Top1 95.625000   Top5 99.980469   BatchTime 0.145414   LR 0.010000
INFO - Training [26][   60/  391]   Loss 0.120734   Top1 95.664062   Top5 99.986979   BatchTime 0.126901   LR 0.010000
INFO - Training [26][   80/  391]   Loss 0.123642   Top1 95.537109   Top5 99.990234   BatchTime 0.116763   LR 0.010000
INFO - Training [26][  100/  391]   Loss 0.123782   Top1 95.445312   Top5 99.992188   BatchTime 0.114142   LR 0.010000
INFO - Training [26][  120/  391]   Loss 0.122067   Top1 95.546875   Top5 99.980469   BatchTime 0.115831   LR 0.010000
INFO - Training [26][  140/  391]   Loss 0.123819   Top1 95.452009   Top5 99.983259   BatchTime 0.117012   LR 0.010000
INFO - Training [26][  160/  391]   Loss 0.125514   Top1 95.385742   Top5 99.970703   BatchTime 0.118411   LR 0.010000
INFO - Training [26][  180/  391]   Loss 0.125043   Top1 95.468750   Top5 99.973958   BatchTime 0.119092   LR 0.010000
INFO - Training [26][  200/  391]   Loss 0.122901   Top1 95.527344   Top5 99.972656   BatchTime 0.119602   LR 0.010000
INFO - Training [26][  220/  391]   Loss 0.122840   Top1 95.529119   Top5 99.971591   BatchTime 0.119965   LR 0.010000
INFO - Training [26][  240/  391]   Loss 0.122941   Top1 95.566406   Top5 99.967448   BatchTime 0.120272   LR 0.010000
INFO - Training [26][  260/  391]   Loss 0.122722   Top1 95.585938   Top5 99.969952   BatchTime 0.120475   LR 0.010000
INFO - Training [26][  280/  391]   Loss 0.122566   Top1 95.627790   Top5 99.969308   BatchTime 0.120687   LR 0.010000
INFO - Training [26][  300/  391]   Loss 0.121433   Top1 95.658854   Top5 99.971354   BatchTime 0.119122   LR 0.010000
INFO - Training [26][  320/  391]   Loss 0.122474   Top1 95.637207   Top5 99.970703   BatchTime 0.116892   LR 0.010000
INFO - Training [26][  340/  391]   Loss 0.123477   Top1 95.608915   Top5 99.970129   BatchTime 0.115272   LR 0.010000
INFO - Training [26][  360/  391]   Loss 0.125277   Top1 95.562066   Top5 99.969618   BatchTime 0.112908   LR 0.010000
INFO - Training [26][  380/  391]   Loss 0.127156   Top1 95.505757   Top5 99.969161   BatchTime 0.113029   LR 0.010000
INFO - ==> Top1: 95.466    Top5: 99.968    Loss: 0.129
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [26][   20/   79]   Loss 0.409109   Top1 87.890625   Top5 99.453125   BatchTime 0.150386
INFO - Validation [26][   40/   79]   Loss 0.390464   Top1 88.398438   Top5 99.453125   BatchTime 0.107914
INFO - Validation [26][   60/   79]   Loss 0.385730   Top1 88.593750   Top5 99.518229   BatchTime 0.093944
INFO - ==> Top1: 88.480    Top5: 99.480    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  27
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [27][   20/  391]   Loss 0.158593   Top1 94.531250   Top5 99.960938   BatchTime 0.211385   LR 0.010000
INFO - Training [27][   40/  391]   Loss 0.159989   Top1 94.492188   Top5 99.960938   BatchTime 0.167440   LR 0.010000
INFO - Training [27][   60/  391]   Loss 0.163335   Top1 94.348958   Top5 99.947917   BatchTime 0.152570   LR 0.010000
INFO - Training [27][   80/  391]   Loss 0.161065   Top1 94.433594   Top5 99.941406   BatchTime 0.145718   LR 0.010000
INFO - Training [27][  100/  391]   Loss 0.159971   Top1 94.523438   Top5 99.937500   BatchTime 0.131715   LR 0.010000
INFO - Training [27][  120/  391]   Loss 0.161860   Top1 94.459635   Top5 99.941406   BatchTime 0.125032   LR 0.010000
INFO - Training [27][  140/  391]   Loss 0.162026   Top1 94.402902   Top5 99.949777   BatchTime 0.119752   LR 0.010000
INFO - Training [27][  160/  391]   Loss 0.164350   Top1 94.350586   Top5 99.936523   BatchTime 0.117382   LR 0.010000
INFO - Training [27][  180/  391]   Loss 0.165339   Top1 94.344618   Top5 99.934896   BatchTime 0.118349   LR 0.010000
INFO - Training [27][  200/  391]   Loss 0.167492   Top1 94.226562   Top5 99.937500   BatchTime 0.118943   LR 0.010000
INFO - Training [27][  220/  391]   Loss 0.167199   Top1 94.218750   Top5 99.943182   BatchTime 0.119246   LR 0.010000
INFO - Training [27][  240/  391]   Loss 0.167646   Top1 94.166667   Top5 99.938151   BatchTime 0.119636   LR 0.010000
INFO - Training [27][  260/  391]   Loss 0.165818   Top1 94.209736   Top5 99.939904   BatchTime 0.119893   LR 0.010000
INFO - Training [27][  280/  391]   Loss 0.165697   Top1 94.224330   Top5 99.941406   BatchTime 0.120197   LR 0.010000
INFO - Training [27][  300/  391]   Loss 0.167162   Top1 94.171875   Top5 99.934896   BatchTime 0.120467   LR 0.010000
INFO - Training [27][  320/  391]   Loss 0.168137   Top1 94.118652   Top5 99.936523   BatchTime 0.120655   LR 0.010000
INFO - Training [27][  340/  391]   Loss 0.167189   Top1 94.149816   Top5 99.933364   BatchTime 0.120798   LR 0.010000
INFO - Training [27][  360/  391]   Loss 0.167785   Top1 94.125434   Top5 99.934896   BatchTime 0.119967   LR 0.010000
INFO - Training [27][  380/  391]   Loss 0.167825   Top1 94.115954   Top5 99.938322   BatchTime 0.117848   LR 0.010000
INFO - ==> Top1: 94.098    Top5: 99.936    Loss: 0.168
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [27][   20/   79]   Loss 0.410477   Top1 88.046875   Top5 99.609375   BatchTime 0.120938
INFO - Validation [27][   40/   79]   Loss 0.405655   Top1 88.417969   Top5 99.472656   BatchTime 0.088788
INFO - Validation [27][   60/   79]   Loss 0.405693   Top1 88.333333   Top5 99.544271   BatchTime 0.080850
INFO - ==> Top1: 88.330    Top5: 99.550    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  28
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [28][   20/  391]   Loss 0.173535   Top1 93.476562   Top5 99.843750   BatchTime 0.211081   LR 0.010000
INFO - Training [28][   40/  391]   Loss 0.167198   Top1 93.847656   Top5 99.902344   BatchTime 0.168760   LR 0.010000
INFO - Training [28][   60/  391]   Loss 0.172187   Top1 93.815104   Top5 99.895833   BatchTime 0.153784   LR 0.010000
INFO - Training [28][   80/  391]   Loss 0.167671   Top1 94.121094   Top5 99.921875   BatchTime 0.146451   LR 0.010000
INFO - Training [28][  100/  391]   Loss 0.164641   Top1 94.265625   Top5 99.937500   BatchTime 0.141845   LR 0.010000
INFO - Training [28][  120/  391]   Loss 0.167127   Top1 94.186198   Top5 99.928385   BatchTime 0.138727   LR 0.010000
INFO - Training [28][  140/  391]   Loss 0.161741   Top1 94.369420   Top5 99.933036   BatchTime 0.136513   LR 0.010000
INFO - Training [28][  160/  391]   Loss 0.160082   Top1 94.379883   Top5 99.931641   BatchTime 0.129718   LR 0.010000
INFO - Training [28][  180/  391]   Loss 0.157746   Top1 94.453125   Top5 99.934896   BatchTime 0.125200   LR 0.010000
INFO - Training [28][  200/  391]   Loss 0.158234   Top1 94.433594   Top5 99.937500   BatchTime 0.122087   LR 0.010000
INFO - Training [28][  220/  391]   Loss 0.155790   Top1 94.527699   Top5 99.939631   BatchTime 0.117722   LR 0.010000
INFO - Training [28][  240/  391]   Loss 0.155160   Top1 94.560547   Top5 99.938151   BatchTime 0.119037   LR 0.010000
INFO - Training [28][  260/  391]   Loss 0.154517   Top1 94.573317   Top5 99.936899   BatchTime 0.119493   LR 0.010000
INFO - Training [28][  280/  391]   Loss 0.153491   Top1 94.584263   Top5 99.941406   BatchTime 0.119858   LR 0.010000
INFO - Training [28][  300/  391]   Loss 0.152831   Top1 94.609375   Top5 99.942708   BatchTime 0.120132   LR 0.010000
INFO - Training [28][  320/  391]   Loss 0.153469   Top1 94.580078   Top5 99.943848   BatchTime 0.120390   LR 0.010000
INFO - Training [28][  340/  391]   Loss 0.154660   Top1 94.568015   Top5 99.937960   BatchTime 0.120590   LR 0.010000
INFO - Training [28][  360/  391]   Loss 0.154158   Top1 94.605035   Top5 99.939236   BatchTime 0.120744   LR 0.010000
INFO - Training [28][  380/  391]   Loss 0.155080   Top1 94.580592   Top5 99.940378   BatchTime 0.120868   LR 0.010000
INFO - ==> Top1: 94.568    Top5: 99.940    Loss: 0.155
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [28][   20/   79]   Loss 0.382118   Top1 88.867188   Top5 99.335938   BatchTime 0.132050
INFO - Validation [28][   40/   79]   Loss 0.382694   Top1 89.160156   Top5 99.355469   BatchTime 0.079485
INFO - Validation [28][   60/   79]   Loss 0.386884   Top1 89.075521   Top5 99.388021   BatchTime 0.061700
INFO - ==> Top1: 89.010    Top5: 99.430    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  29
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [29][   20/  391]   Loss 0.147824   Top1 94.492188   Top5 100.000000   BatchTime 0.184944   LR 0.010000
INFO - Training [29][   40/  391]   Loss 0.142039   Top1 94.765625   Top5 99.960938   BatchTime 0.156268   LR 0.010000
INFO - Training [29][   60/  391]   Loss 0.141425   Top1 94.934896   Top5 99.960938   BatchTime 0.145550   LR 0.010000
INFO - Training [29][   80/  391]   Loss 0.143977   Top1 94.863281   Top5 99.970703   BatchTime 0.140410   LR 0.010000
INFO - Training [29][  100/  391]   Loss 0.141935   Top1 94.976562   Top5 99.960938   BatchTime 0.137242   LR 0.010000
INFO - Training [29][  120/  391]   Loss 0.140459   Top1 95.019531   Top5 99.960938   BatchTime 0.135042   LR 0.010000
INFO - Training [29][  140/  391]   Loss 0.141966   Top1 94.933036   Top5 99.960938   BatchTime 0.133599   LR 0.010000
INFO - Training [29][  160/  391]   Loss 0.144188   Top1 94.882812   Top5 99.956055   BatchTime 0.132356   LR 0.010000
INFO - Training [29][  180/  391]   Loss 0.144580   Top1 94.895833   Top5 99.947917   BatchTime 0.131359   LR 0.010000
INFO - Training [29][  200/  391]   Loss 0.146958   Top1 94.835938   Top5 99.941406   BatchTime 0.130485   LR 0.010000
INFO - Training [29][  220/  391]   Loss 0.146408   Top1 94.829545   Top5 99.946733   BatchTime 0.129048   LR 0.010000
INFO - Training [29][  240/  391]   Loss 0.146376   Top1 94.817708   Top5 99.941406   BatchTime 0.124943   LR 0.010000
INFO - Training [29][  260/  391]   Loss 0.145848   Top1 94.843750   Top5 99.936899   BatchTime 0.122098   LR 0.010000
INFO - Training [29][  280/  391]   Loss 0.146410   Top1 94.838170   Top5 99.938616   BatchTime 0.119425   LR 0.010000
INFO - Training [29][  300/  391]   Loss 0.145500   Top1 94.864583   Top5 99.940104   BatchTime 0.118220   LR 0.010000
INFO - Training [29][  320/  391]   Loss 0.146459   Top1 94.829102   Top5 99.938965   BatchTime 0.118648   LR 0.010000
INFO - Training [29][  340/  391]   Loss 0.145929   Top1 94.855239   Top5 99.940257   BatchTime 0.118959   LR 0.010000
INFO - Training [29][  360/  391]   Loss 0.146253   Top1 94.869792   Top5 99.943576   BatchTime 0.118936   LR 0.010000
INFO - Training [29][  380/  391]   Loss 0.146282   Top1 94.868421   Top5 99.940378   BatchTime 0.119153   LR 0.010000
INFO - ==> Top1: 94.840    Top5: 99.938    Loss: 0.147
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [29][   20/   79]   Loss 0.376460   Top1 89.414062   Top5 99.609375   BatchTime 0.150925
INFO - Validation [29][   40/   79]   Loss 0.366603   Top1 89.199219   Top5 99.511719   BatchTime 0.107936
INFO - Validation [29][   60/   79]   Loss 0.363015   Top1 89.388021   Top5 99.544271   BatchTime 0.093400
INFO - ==> Top1: 89.290    Top5: 99.530    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Scoreboard best 2 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.480   Top5: 99.620] Sparsity : 0.827
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  30
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [30][   20/  391]   Loss 0.142907   Top1 94.726562   Top5 100.000000   BatchTime 0.185307   LR 0.001000
INFO - Training [30][   40/  391]   Loss 0.134481   Top1 95.000000   Top5 100.000000   BatchTime 0.136156   LR 0.001000
INFO - Training [30][   60/  391]   Loss 0.129163   Top1 95.377604   Top5 99.973958   BatchTime 0.122399   LR 0.001000
INFO - Training [30][   80/  391]   Loss 0.128788   Top1 95.410156   Top5 99.951172   BatchTime 0.110255   LR 0.001000
INFO - Training [30][  100/  391]   Loss 0.127397   Top1 95.398438   Top5 99.960938   BatchTime 0.114093   LR 0.001000
INFO - Training [30][  120/  391]   Loss 0.127102   Top1 95.468750   Top5 99.960938   BatchTime 0.115859   LR 0.001000
INFO - Training [30][  140/  391]   Loss 0.125078   Top1 95.558036   Top5 99.966518   BatchTime 0.117050   LR 0.001000
INFO - Training [30][  160/  391]   Loss 0.123494   Top1 95.634766   Top5 99.960938   BatchTime 0.117894   LR 0.001000
INFO - Training [30][  180/  391]   Loss 0.123848   Top1 95.642361   Top5 99.956597   BatchTime 0.118574   LR 0.001000
INFO - Training [30][  200/  391]   Loss 0.123912   Top1 95.664062   Top5 99.960938   BatchTime 0.119099   LR 0.001000
INFO - Training [30][  220/  391]   Loss 0.124015   Top1 95.656960   Top5 99.964489   BatchTime 0.119498   LR 0.001000
INFO - Training [30][  240/  391]   Loss 0.123484   Top1 95.647786   Top5 99.967448   BatchTime 0.119774   LR 0.001000
INFO - Training [30][  260/  391]   Loss 0.124530   Top1 95.640024   Top5 99.960938   BatchTime 0.120061   LR 0.001000
INFO - Training [30][  280/  391]   Loss 0.124008   Top1 95.641741   Top5 99.963728   BatchTime 0.120283   LR 0.001000
INFO - Training [30][  300/  391]   Loss 0.123595   Top1 95.674479   Top5 99.958333   BatchTime 0.117669   LR 0.001000
INFO - Training [30][  320/  391]   Loss 0.122716   Top1 95.690918   Top5 99.958496   BatchTime 0.116053   LR 0.001000
INFO - Training [30][  340/  391]   Loss 0.122910   Top1 95.696232   Top5 99.958640   BatchTime 0.114316   LR 0.001000
INFO - Training [30][  360/  391]   Loss 0.122569   Top1 95.700955   Top5 99.956597   BatchTime 0.112818   LR 0.001000
INFO - Training [30][  380/  391]   Loss 0.121969   Top1 95.711349   Top5 99.956826   BatchTime 0.113462   LR 0.001000
INFO - ==> Top1: 95.700    Top5: 99.956    Loss: 0.122
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [30][   20/   79]   Loss 0.365677   Top1 89.375000   Top5 99.648438   BatchTime 0.149307
INFO - Validation [30][   40/   79]   Loss 0.352250   Top1 89.746094   Top5 99.648438   BatchTime 0.106193
INFO - Validation [30][   60/   79]   Loss 0.351192   Top1 89.882812   Top5 99.596354   BatchTime 0.092466
INFO - ==> Top1: 89.770    Top5: 99.590    Loss: 0.352
INFO - Scoreboard best 1 ==> Epoch [30][Top1: 89.770   Top5: 99.590] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Scoreboard best 3 ==> Epoch [13][Top1: 89.520   Top5: 99.470] Sparsity : 0.824
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  31
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [31][   20/  391]   Loss 0.120844   Top1 96.054688   Top5 99.960938   BatchTime 0.214320   LR 0.001000
INFO - Training [31][   40/  391]   Loss 0.115899   Top1 96.132812   Top5 99.980469   BatchTime 0.168641   LR 0.001000
INFO - Training [31][   60/  391]   Loss 0.115738   Top1 96.002604   Top5 99.960938   BatchTime 0.153664   LR 0.001000
INFO - Training [31][   80/  391]   Loss 0.115678   Top1 95.937500   Top5 99.960938   BatchTime 0.141581   LR 0.001000
INFO - Training [31][  100/  391]   Loss 0.115539   Top1 95.882812   Top5 99.960938   BatchTime 0.130196   LR 0.001000
INFO - Training [31][  120/  391]   Loss 0.114383   Top1 95.957031   Top5 99.960938   BatchTime 0.124410   LR 0.001000
INFO - Training [31][  140/  391]   Loss 0.113992   Top1 95.937500   Top5 99.966518   BatchTime 0.117254   LR 0.001000
INFO - Training [31][  160/  391]   Loss 0.114125   Top1 95.888672   Top5 99.965820   BatchTime 0.118038   LR 0.001000
INFO - Training [31][  180/  391]   Loss 0.113530   Top1 95.894097   Top5 99.969618   BatchTime 0.118817   LR 0.001000
INFO - Training [31][  200/  391]   Loss 0.112869   Top1 95.933594   Top5 99.972656   BatchTime 0.119365   LR 0.001000
INFO - Training [31][  220/  391]   Loss 0.112748   Top1 95.937500   Top5 99.971591   BatchTime 0.119735   LR 0.001000
INFO - Training [31][  240/  391]   Loss 0.111909   Top1 95.966797   Top5 99.973958   BatchTime 0.120105   LR 0.001000
INFO - Training [31][  260/  391]   Loss 0.112328   Top1 95.970553   Top5 99.972957   BatchTime 0.120371   LR 0.001000
INFO - Training [31][  280/  391]   Loss 0.111989   Top1 95.970982   Top5 99.974888   BatchTime 0.120662   LR 0.001000
INFO - Training [31][  300/  391]   Loss 0.112338   Top1 95.953125   Top5 99.976562   BatchTime 0.120822   LR 0.001000
INFO - Training [31][  320/  391]   Loss 0.111029   Top1 96.000977   Top5 99.975586   BatchTime 0.120944   LR 0.001000
INFO - Training [31][  340/  391]   Loss 0.111022   Top1 96.004136   Top5 99.977022   BatchTime 0.121061   LR 0.001000
INFO - Training [31][  360/  391]   Loss 0.111163   Top1 96.006944   Top5 99.978299   BatchTime 0.118902   LR 0.001000
INFO - Training [31][  380/  391]   Loss 0.111463   Top1 96.001234   Top5 99.977385   BatchTime 0.117136   LR 0.001000
INFO - ==> Top1: 95.986    Top5: 99.978    Loss: 0.111
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [31][   20/   79]   Loss 0.357294   Top1 90.117188   Top5 99.609375   BatchTime 0.144540
INFO - Validation [31][   40/   79]   Loss 0.349094   Top1 90.097656   Top5 99.589844   BatchTime 0.104725
INFO - Validation [31][   60/   79]   Loss 0.348417   Top1 90.221354   Top5 99.570312   BatchTime 0.090799
INFO - ==> Top1: 90.100    Top5: 99.580    Loss: 0.349
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [30][Top1: 89.770   Top5: 99.590] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [24][Top1: 89.520   Top5: 99.590] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  32
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [32][   20/  391]   Loss 0.105730   Top1 96.445312   Top5 100.000000   BatchTime 0.213540   LR 0.001000
INFO - Training [32][   40/  391]   Loss 0.111028   Top1 96.171875   Top5 99.960938   BatchTime 0.168953   LR 0.001000
INFO - Training [32][   60/  391]   Loss 0.108642   Top1 96.145833   Top5 99.973958   BatchTime 0.153895   LR 0.001000
INFO - Training [32][   80/  391]   Loss 0.110505   Top1 96.083984   Top5 99.970703   BatchTime 0.146449   LR 0.001000
INFO - Training [32][  100/  391]   Loss 0.110840   Top1 96.085938   Top5 99.976562   BatchTime 0.142088   LR 0.001000
INFO - Training [32][  120/  391]   Loss 0.108327   Top1 96.171875   Top5 99.980469   BatchTime 0.138979   LR 0.001000
INFO - Training [32][  140/  391]   Loss 0.107970   Top1 96.261161   Top5 99.977679   BatchTime 0.134913   LR 0.001000
INFO - Training [32][  160/  391]   Loss 0.108086   Top1 96.220703   Top5 99.980469   BatchTime 0.128103   LR 0.001000
INFO - Training [32][  180/  391]   Loss 0.108722   Top1 96.184896   Top5 99.973958   BatchTime 0.123969   LR 0.001000
INFO - Training [32][  200/  391]   Loss 0.109250   Top1 96.164062   Top5 99.972656   BatchTime 0.120152   LR 0.001000
INFO - Training [32][  220/  391]   Loss 0.109624   Top1 96.175426   Top5 99.971591   BatchTime 0.119132   LR 0.001000
INFO - Training [32][  240/  391]   Loss 0.108735   Top1 96.207682   Top5 99.973958   BatchTime 0.119533   LR 0.001000
INFO - Training [32][  260/  391]   Loss 0.108518   Top1 96.222957   Top5 99.975962   BatchTime 0.120016   LR 0.001000
INFO - Training [32][  280/  391]   Loss 0.107439   Top1 96.258371   Top5 99.977679   BatchTime 0.120240   LR 0.001000
INFO - Training [32][  300/  391]   Loss 0.107866   Top1 96.229167   Top5 99.976562   BatchTime 0.120482   LR 0.001000
INFO - Training [32][  320/  391]   Loss 0.108161   Top1 96.193848   Top5 99.978027   BatchTime 0.120668   LR 0.001000
INFO - Training [32][  340/  391]   Loss 0.108698   Top1 96.181066   Top5 99.979320   BatchTime 0.121073   LR 0.001000
INFO - Training [32][  360/  391]   Loss 0.109099   Top1 96.180556   Top5 99.978299   BatchTime 0.121202   LR 0.001000
INFO - Training [32][  380/  391]   Loss 0.109113   Top1 96.173931   Top5 99.979441   BatchTime 0.121268   LR 0.001000
INFO - ==> Top1: 96.156    Top5: 99.980    Loss: 0.109
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [32][   20/   79]   Loss 0.352487   Top1 90.039062   Top5 99.687500   BatchTime 0.122433
INFO - Validation [32][   40/   79]   Loss 0.347264   Top1 90.136719   Top5 99.667969   BatchTime 0.074379
INFO - Validation [32][   60/   79]   Loss 0.344983   Top1 90.156250   Top5 99.674479   BatchTime 0.062387
INFO - ==> Top1: 90.040    Top5: 99.680    Loss: 0.344
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.040   Top5: 99.680] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [30][Top1: 89.770   Top5: 99.590] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  33
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [33][   20/  391]   Loss 0.105884   Top1 96.562500   Top5 99.960938   BatchTime 0.213890   LR 0.001000
INFO - Training [33][   40/  391]   Loss 0.107750   Top1 96.484375   Top5 99.960938   BatchTime 0.169170   LR 0.001000
INFO - Training [33][   60/  391]   Loss 0.105778   Top1 96.536458   Top5 99.973958   BatchTime 0.154449   LR 0.001000
INFO - Training [33][   80/  391]   Loss 0.106193   Top1 96.367188   Top5 99.970703   BatchTime 0.146889   LR 0.001000
INFO - Training [33][  100/  391]   Loss 0.103631   Top1 96.507812   Top5 99.968750   BatchTime 0.142475   LR 0.001000
INFO - Training [33][  120/  391]   Loss 0.104649   Top1 96.406250   Top5 99.973958   BatchTime 0.139392   LR 0.001000
INFO - Training [33][  140/  391]   Loss 0.105290   Top1 96.356027   Top5 99.972098   BatchTime 0.137311   LR 0.001000
INFO - Training [33][  160/  391]   Loss 0.104301   Top1 96.416016   Top5 99.975586   BatchTime 0.135605   LR 0.001000
INFO - Training [33][  180/  391]   Loss 0.105292   Top1 96.371528   Top5 99.969618   BatchTime 0.134240   LR 0.001000
INFO - Training [33][  200/  391]   Loss 0.105283   Top1 96.375000   Top5 99.968750   BatchTime 0.133115   LR 0.001000
INFO - Training [33][  220/  391]   Loss 0.104786   Top1 96.431108   Top5 99.968040   BatchTime 0.129410   LR 0.001000
INFO - Training [33][  240/  391]   Loss 0.103660   Top1 96.471354   Top5 99.970703   BatchTime 0.125852   LR 0.001000
INFO - Training [33][  260/  391]   Loss 0.102929   Top1 96.505409   Top5 99.972957   BatchTime 0.123239   LR 0.001000
INFO - Training [33][  280/  391]   Loss 0.102977   Top1 96.526228   Top5 99.974888   BatchTime 0.119970   LR 0.001000
INFO - Training [33][  300/  391]   Loss 0.103505   Top1 96.505208   Top5 99.971354   BatchTime 0.120639   LR 0.001000
INFO - Training [33][  320/  391]   Loss 0.103490   Top1 96.481934   Top5 99.973145   BatchTime 0.120832   LR 0.001000
INFO - Training [33][  340/  391]   Loss 0.103872   Top1 96.463695   Top5 99.972426   BatchTime 0.120959   LR 0.001000
INFO - Training [33][  360/  391]   Loss 0.103992   Top1 96.464844   Top5 99.971788   BatchTime 0.121135   LR 0.001000
INFO - Training [33][  380/  391]   Loss 0.104667   Top1 96.435033   Top5 99.971217   BatchTime 0.121296   LR 0.001000
INFO - ==> Top1: 96.430    Top5: 99.970    Loss: 0.105
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [33][   20/   79]   Loss 0.357914   Top1 90.156250   Top5 99.570312   BatchTime 0.149395
INFO - Validation [33][   40/   79]   Loss 0.351094   Top1 90.078125   Top5 99.609375   BatchTime 0.107419
INFO - Validation [33][   60/   79]   Loss 0.351714   Top1 90.039062   Top5 99.596354   BatchTime 0.093824
INFO - ==> Top1: 89.980    Top5: 99.590    Loss: 0.351
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.040   Top5: 99.680] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 89.980   Top5: 99.590] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  34
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [34][   20/  391]   Loss 0.097953   Top1 96.718750   Top5 99.960938   BatchTime 0.181070   LR 0.001000
INFO - Training [34][   40/  391]   Loss 0.103889   Top1 96.621094   Top5 99.941406   BatchTime 0.138919   LR 0.001000
INFO - Training [34][   60/  391]   Loss 0.099386   Top1 96.731771   Top5 99.947917   BatchTime 0.118088   LR 0.001000
INFO - Training [34][   80/  391]   Loss 0.097635   Top1 96.748047   Top5 99.960938   BatchTime 0.119002   LR 0.001000
INFO - Training [34][  100/  391]   Loss 0.096486   Top1 96.820312   Top5 99.968750   BatchTime 0.120117   LR 0.001000
INFO - Training [34][  120/  391]   Loss 0.096781   Top1 96.809896   Top5 99.967448   BatchTime 0.120482   LR 0.001000
INFO - Training [34][  140/  391]   Loss 0.096961   Top1 96.813616   Top5 99.966518   BatchTime 0.121051   LR 0.001000
INFO - Training [34][  160/  391]   Loss 0.096703   Top1 96.752930   Top5 99.965820   BatchTime 0.121462   LR 0.001000
INFO - Training [34][  180/  391]   Loss 0.098074   Top1 96.692708   Top5 99.969618   BatchTime 0.121750   LR 0.001000
INFO - Training [34][  200/  391]   Loss 0.098440   Top1 96.652344   Top5 99.972656   BatchTime 0.122006   LR 0.001000
INFO - Training [34][  220/  391]   Loss 0.099028   Top1 96.622869   Top5 99.971591   BatchTime 0.122109   LR 0.001000
INFO - Training [34][  240/  391]   Loss 0.098684   Top1 96.650391   Top5 99.973958   BatchTime 0.122209   LR 0.001000
INFO - Training [34][  260/  391]   Loss 0.099648   Top1 96.577524   Top5 99.972957   BatchTime 0.122305   LR 0.001000
INFO - Training [34][  280/  391]   Loss 0.100832   Top1 96.556920   Top5 99.974888   BatchTime 0.119741   LR 0.001000
INFO - Training [34][  300/  391]   Loss 0.101256   Top1 96.533854   Top5 99.976562   BatchTime 0.117753   LR 0.001000
INFO - Training [34][  320/  391]   Loss 0.100971   Top1 96.530762   Top5 99.975586   BatchTime 0.116122   LR 0.001000
INFO - Training [34][  340/  391]   Loss 0.101338   Top1 96.505055   Top5 99.977022   BatchTime 0.114317   LR 0.001000
INFO - Training [34][  360/  391]   Loss 0.101391   Top1 96.501736   Top5 99.978299   BatchTime 0.114912   LR 0.001000
INFO - Training [34][  380/  391]   Loss 0.101340   Top1 96.513158   Top5 99.977385   BatchTime 0.115374   LR 0.001000
INFO - ==> Top1: 96.506    Top5: 99.978    Loss: 0.101
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [34][   20/   79]   Loss 0.356836   Top1 90.195312   Top5 99.648438   BatchTime 0.150920
INFO - Validation [34][   40/   79]   Loss 0.350365   Top1 89.980469   Top5 99.609375   BatchTime 0.108029
INFO - Validation [34][   60/   79]   Loss 0.351067   Top1 89.947917   Top5 99.570312   BatchTime 0.093764
INFO - ==> Top1: 89.980    Top5: 99.610    Loss: 0.350
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.040   Top5: 99.680] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [34][Top1: 89.980   Top5: 99.610] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  35
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [35][   20/  391]   Loss 0.086948   Top1 96.914062   Top5 100.000000   BatchTime 0.208802   LR 0.001000
INFO - Training [35][   40/  391]   Loss 0.093127   Top1 96.660156   Top5 99.980469   BatchTime 0.165936   LR 0.001000
INFO - Training [35][   60/  391]   Loss 0.095879   Top1 96.692708   Top5 99.986979   BatchTime 0.142996   LR 0.001000
INFO - Training [35][   80/  391]   Loss 0.095794   Top1 96.796875   Top5 99.990234   BatchTime 0.129044   LR 0.001000
INFO - Training [35][  100/  391]   Loss 0.096159   Top1 96.718750   Top5 99.984375   BatchTime 0.122528   LR 0.001000
INFO - Training [35][  120/  391]   Loss 0.098020   Top1 96.562500   Top5 99.980469   BatchTime 0.114602   LR 0.001000
INFO - Training [35][  140/  391]   Loss 0.097039   Top1 96.562500   Top5 99.983259   BatchTime 0.114808   LR 0.001000
INFO - Training [35][  160/  391]   Loss 0.095450   Top1 96.655273   Top5 99.985352   BatchTime 0.116072   LR 0.001000
INFO - Training [35][  180/  391]   Loss 0.095396   Top1 96.671007   Top5 99.986979   BatchTime 0.117044   LR 0.001000
INFO - Training [35][  200/  391]   Loss 0.095564   Top1 96.644531   Top5 99.988281   BatchTime 0.117737   LR 0.001000
INFO - Training [35][  220/  391]   Loss 0.094853   Top1 96.686790   Top5 99.989347   BatchTime 0.118238   LR 0.001000
INFO - Training [35][  240/  391]   Loss 0.094467   Top1 96.722005   Top5 99.990234   BatchTime 0.118699   LR 0.001000
INFO - Training [35][  260/  391]   Loss 0.094301   Top1 96.742788   Top5 99.987981   BatchTime 0.119105   LR 0.001000
INFO - Training [35][  280/  391]   Loss 0.094472   Top1 96.743862   Top5 99.986049   BatchTime 0.119420   LR 0.001000
INFO - Training [35][  300/  391]   Loss 0.095425   Top1 96.692708   Top5 99.981771   BatchTime 0.119687   LR 0.001000
INFO - Training [35][  320/  391]   Loss 0.096077   Top1 96.652832   Top5 99.982910   BatchTime 0.119968   LR 0.001000
INFO - Training [35][  340/  391]   Loss 0.096110   Top1 96.654412   Top5 99.983915   BatchTime 0.118098   LR 0.001000
INFO - Training [35][  360/  391]   Loss 0.097179   Top1 96.612413   Top5 99.984809   BatchTime 0.116315   LR 0.001000
INFO - Training [35][  380/  391]   Loss 0.097376   Top1 96.593339   Top5 99.985609   BatchTime 0.114915   LR 0.001000
INFO - ==> Top1: 96.602    Top5: 99.984    Loss: 0.097
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [35][   20/   79]   Loss 0.359584   Top1 90.156250   Top5 99.570312   BatchTime 0.158154
INFO - Validation [35][   40/   79]   Loss 0.351000   Top1 90.019531   Top5 99.609375   BatchTime 0.110508
INFO - Validation [35][   60/   79]   Loss 0.352535   Top1 90.078125   Top5 99.622396   BatchTime 0.095416
INFO - ==> Top1: 90.050    Top5: 99.630    Loss: 0.352
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.050   Top5: 99.630] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.040   Top5: 99.680] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  36
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [36][   20/  391]   Loss 0.104303   Top1 96.484375   Top5 100.000000   BatchTime 0.212095   LR 0.001000
INFO - Training [36][   40/  391]   Loss 0.104521   Top1 96.289062   Top5 100.000000   BatchTime 0.168071   LR 0.001000
INFO - Training [36][   60/  391]   Loss 0.099271   Top1 96.536458   Top5 100.000000   BatchTime 0.153371   LR 0.001000
INFO - Training [36][   80/  391]   Loss 0.097279   Top1 96.542969   Top5 100.000000   BatchTime 0.145853   LR 0.001000
INFO - Training [36][  100/  391]   Loss 0.096795   Top1 96.539062   Top5 100.000000   BatchTime 0.141302   LR 0.001000
INFO - Training [36][  120/  391]   Loss 0.097385   Top1 96.529948   Top5 100.000000   BatchTime 0.136542   LR 0.001000
INFO - Training [36][  140/  391]   Loss 0.098523   Top1 96.473214   Top5 100.000000   BatchTime 0.128279   LR 0.001000
INFO - Training [36][  160/  391]   Loss 0.100089   Top1 96.425781   Top5 100.000000   BatchTime 0.123588   LR 0.001000
INFO - Training [36][  180/  391]   Loss 0.099772   Top1 96.458333   Top5 100.000000   BatchTime 0.119056   LR 0.001000
INFO - Training [36][  200/  391]   Loss 0.099310   Top1 96.441406   Top5 100.000000   BatchTime 0.117516   LR 0.001000
INFO - Training [36][  220/  391]   Loss 0.099221   Top1 96.424006   Top5 100.000000   BatchTime 0.118194   LR 0.001000
INFO - Training [36][  240/  391]   Loss 0.098747   Top1 96.458333   Top5 100.000000   BatchTime 0.118703   LR 0.001000
INFO - Training [36][  260/  391]   Loss 0.098973   Top1 96.457332   Top5 100.000000   BatchTime 0.119183   LR 0.001000
INFO - Training [36][  280/  391]   Loss 0.098856   Top1 96.487165   Top5 100.000000   BatchTime 0.119421   LR 0.001000
INFO - Training [36][  300/  391]   Loss 0.098496   Top1 96.497396   Top5 99.992188   BatchTime 0.119758   LR 0.001000
INFO - Training [36][  320/  391]   Loss 0.098952   Top1 96.479492   Top5 99.992676   BatchTime 0.120048   LR 0.001000
INFO - Training [36][  340/  391]   Loss 0.098893   Top1 96.486673   Top5 99.993107   BatchTime 0.120288   LR 0.001000
INFO - Training [36][  360/  391]   Loss 0.098769   Top1 96.497396   Top5 99.993490   BatchTime 0.120423   LR 0.001000
INFO - Training [36][  380/  391]   Loss 0.098806   Top1 96.504934   Top5 99.989720   BatchTime 0.120556   LR 0.001000
INFO - ==> Top1: 96.508    Top5: 99.990    Loss: 0.099
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [36][   20/   79]   Loss 0.355311   Top1 90.156250   Top5 99.570312   BatchTime 0.137949
INFO - Validation [36][   40/   79]   Loss 0.350250   Top1 90.175781   Top5 99.628906   BatchTime 0.090095
INFO - Validation [36][   60/   79]   Loss 0.353449   Top1 90.130208   Top5 99.622396   BatchTime 0.070805
INFO - ==> Top1: 89.990    Top5: 99.650    Loss: 0.354
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [35][Top1: 90.050   Top5: 99.630] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.040   Top5: 99.680] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  37
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [37][   20/  391]   Loss 0.098353   Top1 96.484375   Top5 99.960938   BatchTime 0.222709   LR 0.001000
INFO - Training [37][   40/  391]   Loss 0.094900   Top1 96.582031   Top5 99.960938   BatchTime 0.173048   LR 0.001000
INFO - Training [37][   60/  391]   Loss 0.093513   Top1 96.744792   Top5 99.973958   BatchTime 0.156642   LR 0.001000
INFO - Training [37][   80/  391]   Loss 0.094270   Top1 96.738281   Top5 99.980469   BatchTime 0.148543   LR 0.001000
INFO - Training [37][  100/  391]   Loss 0.094978   Top1 96.656250   Top5 99.984375   BatchTime 0.143738   LR 0.001000
INFO - Training [37][  120/  391]   Loss 0.094610   Top1 96.686198   Top5 99.986979   BatchTime 0.140525   LR 0.001000
INFO - Training [37][  140/  391]   Loss 0.094276   Top1 96.657366   Top5 99.988839   BatchTime 0.138181   LR 0.001000
INFO - Training [37][  160/  391]   Loss 0.094242   Top1 96.640625   Top5 99.990234   BatchTime 0.136350   LR 0.001000
INFO - Training [37][  180/  391]   Loss 0.094664   Top1 96.644965   Top5 99.991319   BatchTime 0.134844   LR 0.001000
INFO - Training [37][  200/  391]   Loss 0.094713   Top1 96.605469   Top5 99.992188   BatchTime 0.131588   LR 0.001000
INFO - Training [37][  220/  391]   Loss 0.095574   Top1 96.569602   Top5 99.992898   BatchTime 0.127010   LR 0.001000
INFO - Training [37][  240/  391]   Loss 0.095848   Top1 96.562500   Top5 99.993490   BatchTime 0.123890   LR 0.001000
INFO - Training [37][  260/  391]   Loss 0.095403   Top1 96.598558   Top5 99.993990   BatchTime 0.120802   LR 0.001000
INFO - Training [37][  280/  391]   Loss 0.095903   Top1 96.570871   Top5 99.991629   BatchTime 0.119678   LR 0.001000
INFO - Training [37][  300/  391]   Loss 0.097322   Top1 96.518229   Top5 99.989583   BatchTime 0.120024   LR 0.001000
INFO - Training [37][  320/  391]   Loss 0.097658   Top1 96.511230   Top5 99.987793   BatchTime 0.120266   LR 0.001000
INFO - Training [37][  340/  391]   Loss 0.097644   Top1 96.523438   Top5 99.988511   BatchTime 0.120464   LR 0.001000
INFO - Training [37][  360/  391]   Loss 0.097784   Top1 96.536458   Top5 99.989149   BatchTime 0.120677   LR 0.001000
INFO - Training [37][  380/  391]   Loss 0.097992   Top1 96.521382   Top5 99.987664   BatchTime 0.120861   LR 0.001000
INFO - ==> Top1: 96.490    Top5: 99.988    Loss: 0.099
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [37][   20/   79]   Loss 0.363868   Top1 90.195312   Top5 99.609375   BatchTime 0.151297
INFO - Validation [37][   40/   79]   Loss 0.357888   Top1 90.175781   Top5 99.570312   BatchTime 0.108305
INFO - Validation [37][   60/   79]   Loss 0.355924   Top1 90.247396   Top5 99.583333   BatchTime 0.094040
INFO - ==> Top1: 90.150    Top5: 99.610    Loss: 0.356
INFO - Scoreboard best 1 ==> Epoch [37][Top1: 90.150   Top5: 99.610] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [35][Top1: 90.050   Top5: 99.630] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  38
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [38][   20/  391]   Loss 0.095226   Top1 96.835938   Top5 100.000000   BatchTime 0.184320   LR 0.001000
INFO - Training [38][   40/  391]   Loss 0.091549   Top1 96.875000   Top5 100.000000   BatchTime 0.135904   LR 0.001000
INFO - Training [38][   60/  391]   Loss 0.092115   Top1 96.888021   Top5 100.000000   BatchTime 0.123998   LR 0.001000
INFO - Training [38][   80/  391]   Loss 0.096005   Top1 96.679688   Top5 99.980469   BatchTime 0.124047   LR 0.001000
INFO - Training [38][  100/  391]   Loss 0.093974   Top1 96.773438   Top5 99.984375   BatchTime 0.123922   LR 0.001000
INFO - Training [38][  120/  391]   Loss 0.094725   Top1 96.731771   Top5 99.986979   BatchTime 0.124244   LR 0.001000
INFO - Training [38][  140/  391]   Loss 0.095374   Top1 96.685268   Top5 99.983259   BatchTime 0.124091   LR 0.001000
INFO - Training [38][  160/  391]   Loss 0.094729   Top1 96.689453   Top5 99.985352   BatchTime 0.124117   LR 0.001000
INFO - Training [38][  180/  391]   Loss 0.093509   Top1 96.731771   Top5 99.986979   BatchTime 0.124076   LR 0.001000
INFO - Training [38][  200/  391]   Loss 0.093690   Top1 96.726562   Top5 99.988281   BatchTime 0.124055   LR 0.001000
INFO - Training [38][  220/  391]   Loss 0.094775   Top1 96.654830   Top5 99.985795   BatchTime 0.123931   LR 0.001000
INFO - Training [38][  240/  391]   Loss 0.094922   Top1 96.640625   Top5 99.980469   BatchTime 0.123948   LR 0.001000
INFO - Training [38][  260/  391]   Loss 0.095312   Top1 96.631611   Top5 99.978966   BatchTime 0.122457   LR 0.001000
INFO - Training [38][  280/  391]   Loss 0.096148   Top1 96.590402   Top5 99.980469   BatchTime 0.119614   LR 0.001000
INFO - Training [38][  300/  391]   Loss 0.096569   Top1 96.583333   Top5 99.973958   BatchTime 0.117569   LR 0.001000
INFO - Training [38][  320/  391]   Loss 0.096667   Top1 96.572266   Top5 99.970703   BatchTime 0.115315   LR 0.001000
INFO - Training [38][  340/  391]   Loss 0.096387   Top1 96.601562   Top5 99.972426   BatchTime 0.115277   LR 0.001000
INFO - Training [38][  360/  391]   Loss 0.095309   Top1 96.640625   Top5 99.973958   BatchTime 0.115750   LR 0.001000
INFO - Training [38][  380/  391]   Loss 0.095481   Top1 96.636513   Top5 99.973273   BatchTime 0.116150   LR 0.001000
INFO - ==> Top1: 96.646    Top5: 99.972    Loss: 0.095
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [38][   20/   79]   Loss 0.358762   Top1 90.390625   Top5 99.570312   BatchTime 0.146472
INFO - Validation [38][   40/   79]   Loss 0.347298   Top1 90.332031   Top5 99.628906   BatchTime 0.105980
INFO - Validation [38][   60/   79]   Loss 0.348428   Top1 90.377604   Top5 99.635417   BatchTime 0.092568
INFO - ==> Top1: 90.330    Top5: 99.670    Loss: 0.348
INFO - Scoreboard best 1 ==> Epoch [38][Top1: 90.330   Top5: 99.670] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 90.150   Top5: 99.610] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [31][Top1: 90.100   Top5: 99.580] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  39
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [39][   20/  391]   Loss 0.099678   Top1 96.796875   Top5 99.960938   BatchTime 0.209497   LR 0.001000
INFO - Training [39][   40/  391]   Loss 0.095481   Top1 96.914062   Top5 99.960938   BatchTime 0.167013   LR 0.001000
INFO - Training [39][   60/  391]   Loss 0.090813   Top1 96.940104   Top5 99.947917   BatchTime 0.136132   LR 0.001000
INFO - Training [39][   80/  391]   Loss 0.094067   Top1 96.708984   Top5 99.951172   BatchTime 0.126505   LR 0.001000
INFO - Training [39][  100/  391]   Loss 0.097041   Top1 96.578125   Top5 99.960938   BatchTime 0.118465   LR 0.001000
INFO - Training [39][  120/  391]   Loss 0.097317   Top1 96.621094   Top5 99.967448   BatchTime 0.115196   LR 0.001000
INFO - Training [39][  140/  391]   Loss 0.095487   Top1 96.696429   Top5 99.966518   BatchTime 0.116441   LR 0.001000
INFO - Training [39][  160/  391]   Loss 0.094585   Top1 96.723633   Top5 99.965820   BatchTime 0.117470   LR 0.001000
INFO - Training [39][  180/  391]   Loss 0.095229   Top1 96.666667   Top5 99.969618   BatchTime 0.118218   LR 0.001000
INFO - Training [39][  200/  391]   Loss 0.094657   Top1 96.699219   Top5 99.972656   BatchTime 0.118915   LR 0.001000
INFO - Training [39][  220/  391]   Loss 0.093964   Top1 96.732955   Top5 99.971591   BatchTime 0.119335   LR 0.001000
INFO - Training [39][  240/  391]   Loss 0.093897   Top1 96.767578   Top5 99.970703   BatchTime 0.119787   LR 0.001000
INFO - Training [39][  260/  391]   Loss 0.095373   Top1 96.688702   Top5 99.972957   BatchTime 0.120066   LR 0.001000
INFO - Training [39][  280/  391]   Loss 0.095469   Top1 96.704799   Top5 99.974888   BatchTime 0.120300   LR 0.001000
INFO - Training [39][  300/  391]   Loss 0.095718   Top1 96.679688   Top5 99.976562   BatchTime 0.120544   LR 0.001000
INFO - Training [39][  320/  391]   Loss 0.095345   Top1 96.706543   Top5 99.975586   BatchTime 0.119622   LR 0.001000
INFO - Training [39][  340/  391]   Loss 0.095273   Top1 96.700368   Top5 99.974724   BatchTime 0.117419   LR 0.001000
INFO - Training [39][  360/  391]   Loss 0.095528   Top1 96.688368   Top5 99.976128   BatchTime 0.115705   LR 0.001000
INFO - Training [39][  380/  391]   Loss 0.095179   Top1 96.694079   Top5 99.977385   BatchTime 0.113880   LR 0.001000
INFO - ==> Top1: 96.712    Top5: 99.978    Loss: 0.095
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [39][   20/   79]   Loss 0.364202   Top1 90.585938   Top5 99.570312   BatchTime 0.149705
INFO - Validation [39][   40/   79]   Loss 0.357650   Top1 90.156250   Top5 99.609375   BatchTime 0.107386
INFO - Validation [39][   60/   79]   Loss 0.354569   Top1 90.221354   Top5 99.596354   BatchTime 0.092992
INFO - ==> Top1: 90.130    Top5: 99.640    Loss: 0.354
INFO - Scoreboard best 1 ==> Epoch [38][Top1: 90.330   Top5: 99.670] Sparsity : 0.871
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 90.150   Top5: 99.610] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.130   Top5: 99.640] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  40
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [40][   20/  391]   Loss 0.087228   Top1 97.148438   Top5 100.000000   BatchTime 0.212374   LR 0.001000
INFO - Training [40][   40/  391]   Loss 0.089242   Top1 96.816406   Top5 99.980469   BatchTime 0.167996   LR 0.001000
INFO - Training [40][   60/  391]   Loss 0.090411   Top1 96.809896   Top5 99.986979   BatchTime 0.153210   LR 0.001000
INFO - Training [40][   80/  391]   Loss 0.091041   Top1 96.787109   Top5 99.960938   BatchTime 0.145622   LR 0.001000
INFO - Training [40][  100/  391]   Loss 0.092668   Top1 96.750000   Top5 99.953125   BatchTime 0.141050   LR 0.001000
INFO - Training [40][  120/  391]   Loss 0.091405   Top1 96.829427   Top5 99.960938   BatchTime 0.131297   LR 0.001000
INFO - Training [40][  140/  391]   Loss 0.091034   Top1 96.841518   Top5 99.966518   BatchTime 0.125594   LR 0.001000
INFO - Training [40][  160/  391]   Loss 0.093362   Top1 96.748047   Top5 99.970703   BatchTime 0.121125   LR 0.001000
INFO - Training [40][  180/  391]   Loss 0.093145   Top1 96.757812   Top5 99.973958   BatchTime 0.117791   LR 0.001000
INFO - Training [40][  200/  391]   Loss 0.093415   Top1 96.789062   Top5 99.972656   BatchTime 0.118336   LR 0.001000
INFO - Training [40][  220/  391]   Loss 0.093143   Top1 96.796875   Top5 99.975142   BatchTime 0.118879   LR 0.001000
INFO - Training [40][  240/  391]   Loss 0.092921   Top1 96.800130   Top5 99.973958   BatchTime 0.119308   LR 0.001000
INFO - Training [40][  260/  391]   Loss 0.093186   Top1 96.787861   Top5 99.972957   BatchTime 0.119682   LR 0.001000
INFO - Training [40][  280/  391]   Loss 0.093650   Top1 96.791295   Top5 99.974888   BatchTime 0.120026   LR 0.001000
INFO - Training [40][  300/  391]   Loss 0.094059   Top1 96.770833   Top5 99.971354   BatchTime 0.120321   LR 0.001000
INFO - Training [40][  320/  391]   Loss 0.094593   Top1 96.740723   Top5 99.968262   BatchTime 0.120488   LR 0.001000
INFO - Training [40][  340/  391]   Loss 0.094606   Top1 96.730239   Top5 99.970129   BatchTime 0.120666   LR 0.001000
INFO - Training [40][  360/  391]   Loss 0.094684   Top1 96.703559   Top5 99.971788   BatchTime 0.120797   LR 0.001000
INFO - Training [40][  380/  391]   Loss 0.094659   Top1 96.700247   Top5 99.973273   BatchTime 0.120258   LR 0.001000
INFO - ==> Top1: 96.716    Top5: 99.974    Loss: 0.094
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [40][   20/   79]   Loss 0.361267   Top1 90.742188   Top5 99.609375   BatchTime 0.126740
INFO - Validation [40][   40/   79]   Loss 0.356454   Top1 90.429688   Top5 99.648438   BatchTime 0.076433
INFO - Validation [40][   60/   79]   Loss 0.357403   Top1 90.455729   Top5 99.635417   BatchTime 0.059708
INFO - ==> Top1: 90.370    Top5: 99.670    Loss: 0.355
INFO - Scoreboard best 1 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [38][Top1: 90.330   Top5: 99.670] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 90.150   Top5: 99.610] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  41
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [41][   20/  391]   Loss 0.087869   Top1 96.953125   Top5 99.960938   BatchTime 0.214146   LR 0.001000
INFO - Training [41][   40/  391]   Loss 0.086341   Top1 97.070312   Top5 99.980469   BatchTime 0.169511   LR 0.001000
INFO - Training [41][   60/  391]   Loss 0.090792   Top1 96.783854   Top5 99.986979   BatchTime 0.154333   LR 0.001000
INFO - Training [41][   80/  391]   Loss 0.088909   Top1 96.875000   Top5 99.980469   BatchTime 0.146989   LR 0.001000
INFO - Training [41][  100/  391]   Loss 0.091539   Top1 96.750000   Top5 99.976562   BatchTime 0.142351   LR 0.001000
INFO - Training [41][  120/  391]   Loss 0.090529   Top1 96.796875   Top5 99.980469   BatchTime 0.139071   LR 0.001000
INFO - Training [41][  140/  391]   Loss 0.089503   Top1 96.808036   Top5 99.983259   BatchTime 0.136819   LR 0.001000
INFO - Training [41][  160/  391]   Loss 0.089650   Top1 96.816406   Top5 99.985352   BatchTime 0.135612   LR 0.001000
INFO - Training [41][  180/  391]   Loss 0.089783   Top1 96.835938   Top5 99.982639   BatchTime 0.133560   LR 0.001000
INFO - Training [41][  200/  391]   Loss 0.090231   Top1 96.816406   Top5 99.984375   BatchTime 0.128189   LR 0.001000
INFO - Training [41][  220/  391]   Loss 0.092410   Top1 96.747159   Top5 99.982244   BatchTime 0.124664   LR 0.001000
INFO - Training [41][  240/  391]   Loss 0.092234   Top1 96.731771   Top5 99.980469   BatchTime 0.121700   LR 0.001000
INFO - Training [41][  260/  391]   Loss 0.092876   Top1 96.721755   Top5 99.978966   BatchTime 0.120646   LR 0.001000
INFO - Training [41][  280/  391]   Loss 0.092419   Top1 96.757812   Top5 99.980469   BatchTime 0.120883   LR 0.001000
INFO - Training [41][  300/  391]   Loss 0.092726   Top1 96.744792   Top5 99.976562   BatchTime 0.121046   LR 0.001000
INFO - Training [41][  320/  391]   Loss 0.091989   Top1 96.774902   Top5 99.978027   BatchTime 0.121266   LR 0.001000
INFO - Training [41][  340/  391]   Loss 0.091835   Top1 96.767004   Top5 99.979320   BatchTime 0.121408   LR 0.001000
INFO - Training [41][  360/  391]   Loss 0.092199   Top1 96.759983   Top5 99.976128   BatchTime 0.121523   LR 0.001000
INFO - Training [41][  380/  391]   Loss 0.092788   Top1 96.733141   Top5 99.975329   BatchTime 0.121654   LR 0.001000
INFO - ==> Top1: 96.720    Top5: 99.974    Loss: 0.093
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [41][   20/   79]   Loss 0.350170   Top1 90.429688   Top5 99.648438   BatchTime 0.150917
INFO - Validation [41][   40/   79]   Loss 0.344917   Top1 90.390625   Top5 99.687500   BatchTime 0.106976
INFO - Validation [41][   60/   79]   Loss 0.349979   Top1 90.338542   Top5 99.674479   BatchTime 0.091793
INFO - ==> Top1: 90.250    Top5: 99.700    Loss: 0.351
INFO - Scoreboard best 1 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [38][Top1: 90.330   Top5: 99.670] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [41][Top1: 90.250   Top5: 99.700] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  42
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [42][   20/  391]   Loss 0.097928   Top1 96.171875   Top5 99.960938   BatchTime 0.183553   LR 0.001000
INFO - Training [42][   40/  391]   Loss 0.089363   Top1 96.621094   Top5 99.980469   BatchTime 0.135628   LR 0.001000
INFO - Training [42][   60/  391]   Loss 0.092234   Top1 96.640625   Top5 99.986979   BatchTime 0.131471   LR 0.001000
INFO - Training [42][   80/  391]   Loss 0.091179   Top1 96.738281   Top5 99.990234   BatchTime 0.129664   LR 0.001000
INFO - Training [42][  100/  391]   Loss 0.092044   Top1 96.742188   Top5 99.984375   BatchTime 0.128477   LR 0.001000
INFO - Training [42][  120/  391]   Loss 0.091110   Top1 96.790365   Top5 99.986979   BatchTime 0.127762   LR 0.001000
INFO - Training [42][  140/  391]   Loss 0.090398   Top1 96.835938   Top5 99.983259   BatchTime 0.127167   LR 0.001000
INFO - Training [42][  160/  391]   Loss 0.090225   Top1 96.850586   Top5 99.985352   BatchTime 0.126920   LR 0.001000
INFO - Training [42][  180/  391]   Loss 0.092770   Top1 96.736111   Top5 99.986979   BatchTime 0.126463   LR 0.001000
INFO - Training [42][  200/  391]   Loss 0.092523   Top1 96.773438   Top5 99.988281   BatchTime 0.126155   LR 0.001000
INFO - Training [42][  220/  391]   Loss 0.092625   Top1 96.775568   Top5 99.989347   BatchTime 0.125841   LR 0.001000
INFO - Training [42][  240/  391]   Loss 0.092418   Top1 96.774089   Top5 99.986979   BatchTime 0.125138   LR 0.001000
INFO - Training [42][  260/  391]   Loss 0.090807   Top1 96.829928   Top5 99.987981   BatchTime 0.121744   LR 0.001000
INFO - Training [42][  280/  391]   Loss 0.090604   Top1 96.833147   Top5 99.988839   BatchTime 0.119411   LR 0.001000
INFO - Training [42][  300/  391]   Loss 0.091182   Top1 96.812500   Top5 99.989583   BatchTime 0.117297   LR 0.001000
INFO - Training [42][  320/  391]   Loss 0.091214   Top1 96.809082   Top5 99.987793   BatchTime 0.116407   LR 0.001000
INFO - Training [42][  340/  391]   Loss 0.090828   Top1 96.819853   Top5 99.988511   BatchTime 0.116850   LR 0.001000
INFO - Training [42][  360/  391]   Loss 0.090794   Top1 96.818576   Top5 99.989149   BatchTime 0.117208   LR 0.001000
INFO - Training [42][  380/  391]   Loss 0.090710   Top1 96.829770   Top5 99.987664   BatchTime 0.117570   LR 0.001000
INFO - ==> Top1: 96.822    Top5: 99.988    Loss: 0.091
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [42][   20/   79]   Loss 0.359973   Top1 90.507812   Top5 99.648438   BatchTime 0.152358
INFO - Validation [42][   40/   79]   Loss 0.355339   Top1 90.410156   Top5 99.609375   BatchTime 0.108765
INFO - Validation [42][   60/   79]   Loss 0.356178   Top1 90.299479   Top5 99.609375   BatchTime 0.093891
INFO - ==> Top1: 90.220    Top5: 99.630    Loss: 0.353
INFO - Scoreboard best 1 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [38][Top1: 90.330   Top5: 99.670] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [41][Top1: 90.250   Top5: 99.700] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  43
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [43][   20/  391]   Loss 0.094984   Top1 96.757812   Top5 100.000000   BatchTime 0.211815   LR 0.001000
INFO - Training [43][   40/  391]   Loss 0.095534   Top1 96.601562   Top5 100.000000   BatchTime 0.146862   LR 0.001000
INFO - Training [43][   60/  391]   Loss 0.092837   Top1 96.731771   Top5 100.000000   BatchTime 0.127880   LR 0.001000
INFO - Training [43][   80/  391]   Loss 0.091423   Top1 96.708984   Top5 100.000000   BatchTime 0.118614   LR 0.001000
INFO - Training [43][  100/  391]   Loss 0.092187   Top1 96.656250   Top5 99.976562   BatchTime 0.112411   LR 0.001000
INFO - Training [43][  120/  391]   Loss 0.092183   Top1 96.653646   Top5 99.980469   BatchTime 0.114346   LR 0.001000
INFO - Training [43][  140/  391]   Loss 0.091022   Top1 96.713170   Top5 99.983259   BatchTime 0.115768   LR 0.001000
INFO - Training [43][  160/  391]   Loss 0.091096   Top1 96.728516   Top5 99.980469   BatchTime 0.116815   LR 0.001000
INFO - Training [43][  180/  391]   Loss 0.091473   Top1 96.701389   Top5 99.982639   BatchTime 0.117651   LR 0.001000
INFO - Training [43][  200/  391]   Loss 0.091252   Top1 96.691406   Top5 99.984375   BatchTime 0.118238   LR 0.001000
INFO - Training [43][  220/  391]   Loss 0.091726   Top1 96.683239   Top5 99.985795   BatchTime 0.118689   LR 0.001000
INFO - Training [43][  240/  391]   Loss 0.091023   Top1 96.744792   Top5 99.986979   BatchTime 0.119049   LR 0.001000
INFO - Training [43][  260/  391]   Loss 0.090756   Top1 96.745793   Top5 99.984976   BatchTime 0.119370   LR 0.001000
INFO - Training [43][  280/  391]   Loss 0.090389   Top1 96.782924   Top5 99.983259   BatchTime 0.119656   LR 0.001000
INFO - Training [43][  300/  391]   Loss 0.091014   Top1 96.765625   Top5 99.981771   BatchTime 0.119561   LR 0.001000
INFO - Training [43][  320/  391]   Loss 0.091445   Top1 96.757812   Top5 99.982910   BatchTime 0.116805   LR 0.001000
INFO - Training [43][  340/  391]   Loss 0.091137   Top1 96.755515   Top5 99.981618   BatchTime 0.115272   LR 0.001000
INFO - Training [43][  360/  391]   Loss 0.091376   Top1 96.751302   Top5 99.980469   BatchTime 0.113534   LR 0.001000
INFO - Training [43][  380/  391]   Loss 0.091388   Top1 96.743421   Top5 99.979441   BatchTime 0.112674   LR 0.001000
INFO - ==> Top1: 96.716    Top5: 99.980    Loss: 0.092
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [43][   20/   79]   Loss 0.358812   Top1 90.468750   Top5 99.648438   BatchTime 0.150902
INFO - Validation [43][   40/   79]   Loss 0.353248   Top1 90.253906   Top5 99.648438   BatchTime 0.108153
INFO - Validation [43][   60/   79]   Loss 0.356095   Top1 90.143229   Top5 99.635417   BatchTime 0.094030
INFO - ==> Top1: 90.150    Top5: 99.670    Loss: 0.354
INFO - Scoreboard best 1 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [38][Top1: 90.330   Top5: 99.670] Sparsity : 0.871
INFO - Scoreboard best 3 ==> Epoch [41][Top1: 90.250   Top5: 99.700] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  44
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [44][   20/  391]   Loss 0.094393   Top1 96.640625   Top5 100.000000   BatchTime 0.213982   LR 0.001000
INFO - Training [44][   40/  391]   Loss 0.092387   Top1 96.777344   Top5 100.000000   BatchTime 0.168783   LR 0.001000
INFO - Training [44][   60/  391]   Loss 0.096046   Top1 96.679688   Top5 100.000000   BatchTime 0.153631   LR 0.001000
INFO - Training [44][   80/  391]   Loss 0.095494   Top1 96.748047   Top5 99.990234   BatchTime 0.146279   LR 0.001000
INFO - Training [44][  100/  391]   Loss 0.094269   Top1 96.789062   Top5 99.992188   BatchTime 0.135391   LR 0.001000
INFO - Training [44][  120/  391]   Loss 0.094764   Top1 96.790365   Top5 99.993490   BatchTime 0.127692   LR 0.001000
INFO - Training [44][  140/  391]   Loss 0.093508   Top1 96.808036   Top5 99.988839   BatchTime 0.122857   LR 0.001000
INFO - Training [44][  160/  391]   Loss 0.092762   Top1 96.806641   Top5 99.990234   BatchTime 0.117858   LR 0.001000
INFO - Training [44][  180/  391]   Loss 0.093275   Top1 96.753472   Top5 99.986979   BatchTime 0.118710   LR 0.001000
INFO - Training [44][  200/  391]   Loss 0.094649   Top1 96.703125   Top5 99.980469   BatchTime 0.119314   LR 0.001000
INFO - Training [44][  220/  391]   Loss 0.093891   Top1 96.761364   Top5 99.982244   BatchTime 0.119759   LR 0.001000
INFO - Training [44][  240/  391]   Loss 0.092465   Top1 96.819661   Top5 99.980469   BatchTime 0.120099   LR 0.001000
INFO - Training [44][  260/  391]   Loss 0.092082   Top1 96.802885   Top5 99.981971   BatchTime 0.120438   LR 0.001000
INFO - Training [44][  280/  391]   Loss 0.091962   Top1 96.752232   Top5 99.980469   BatchTime 0.120669   LR 0.001000
INFO - Training [44][  300/  391]   Loss 0.090855   Top1 96.791667   Top5 99.981771   BatchTime 0.120891   LR 0.001000
INFO - Training [44][  320/  391]   Loss 0.090950   Top1 96.796875   Top5 99.982910   BatchTime 0.121048   LR 0.001000
INFO - Training [44][  340/  391]   Loss 0.090839   Top1 96.796875   Top5 99.983915   BatchTime 0.121185   LR 0.001000
INFO - Training [44][  360/  391]   Loss 0.090415   Top1 96.809896   Top5 99.984809   BatchTime 0.121151   LR 0.001000
INFO - Training [44][  380/  391]   Loss 0.090586   Top1 96.811266   Top5 99.983553   BatchTime 0.118646   LR 0.001000
INFO - ==> Top1: 96.814    Top5: 99.984    Loss: 0.090
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [44][   20/   79]   Loss 0.357434   Top1 90.390625   Top5 99.648438   BatchTime 0.119249
INFO - Validation [44][   40/   79]   Loss 0.354087   Top1 90.312500   Top5 99.628906   BatchTime 0.074013
INFO - Validation [44][   60/   79]   Loss 0.357200   Top1 90.390625   Top5 99.635417   BatchTime 0.070250
INFO - ==> Top1: 90.380    Top5: 99.660    Loss: 0.354
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [38][Top1: 90.330   Top5: 99.670] Sparsity : 0.871
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  45
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [45][   20/  391]   Loss 0.080781   Top1 96.992188   Top5 100.000000   BatchTime 0.210523   LR 0.001000
INFO - Training [45][   40/  391]   Loss 0.083859   Top1 97.031250   Top5 99.980469   BatchTime 0.167104   LR 0.001000
INFO - Training [45][   60/  391]   Loss 0.087021   Top1 96.927083   Top5 99.986979   BatchTime 0.152828   LR 0.001000
INFO - Training [45][   80/  391]   Loss 0.087070   Top1 96.972656   Top5 99.990234   BatchTime 0.145779   LR 0.001000
INFO - Training [45][  100/  391]   Loss 0.088919   Top1 96.898438   Top5 99.992188   BatchTime 0.141333   LR 0.001000
INFO - Training [45][  120/  391]   Loss 0.086732   Top1 96.927083   Top5 99.993490   BatchTime 0.138354   LR 0.001000
INFO - Training [45][  140/  391]   Loss 0.086411   Top1 96.953125   Top5 99.994420   BatchTime 0.136240   LR 0.001000
INFO - Training [45][  160/  391]   Loss 0.086503   Top1 96.992188   Top5 99.995117   BatchTime 0.132597   LR 0.001000
INFO - Training [45][  180/  391]   Loss 0.086561   Top1 96.979167   Top5 99.995660   BatchTime 0.127143   LR 0.001000
INFO - Training [45][  200/  391]   Loss 0.087012   Top1 96.957031   Top5 99.988281   BatchTime 0.123208   LR 0.001000
INFO - Training [45][  220/  391]   Loss 0.087129   Top1 96.985085   Top5 99.989347   BatchTime 0.119437   LR 0.001000
INFO - Training [45][  240/  391]   Loss 0.087392   Top1 96.972656   Top5 99.990234   BatchTime 0.118910   LR 0.001000
INFO - Training [45][  260/  391]   Loss 0.086861   Top1 96.992188   Top5 99.990986   BatchTime 0.119314   LR 0.001000
INFO - Training [45][  280/  391]   Loss 0.087192   Top1 96.972656   Top5 99.991629   BatchTime 0.120075   LR 0.001000
INFO - Training [45][  300/  391]   Loss 0.088298   Top1 96.921875   Top5 99.992188   BatchTime 0.120373   LR 0.001000
INFO - Training [45][  320/  391]   Loss 0.088513   Top1 96.931152   Top5 99.992676   BatchTime 0.120599   LR 0.001000
INFO - Training [45][  340/  391]   Loss 0.088308   Top1 96.939338   Top5 99.993107   BatchTime 0.120809   LR 0.001000
INFO - Training [45][  360/  391]   Loss 0.088970   Top1 96.929253   Top5 99.993490   BatchTime 0.120818   LR 0.001000
INFO - Training [45][  380/  391]   Loss 0.089012   Top1 96.946957   Top5 99.993832   BatchTime 0.120948   LR 0.001000
INFO - ==> Top1: 96.906    Top5: 99.994    Loss: 0.090
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [45][   20/   79]   Loss 0.352491   Top1 90.898438   Top5 99.687500   BatchTime 0.146056
INFO - Validation [45][   40/   79]   Loss 0.353033   Top1 90.449219   Top5 99.726562   BatchTime 0.088301
INFO - Validation [45][   60/   79]   Loss 0.354176   Top1 90.416667   Top5 99.700521   BatchTime 0.067591
INFO - ==> Top1: 90.370    Top5: 99.740    Loss: 0.351
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  46
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [46][   20/  391]   Loss 0.075689   Top1 97.148438   Top5 100.000000   BatchTime 0.176587   LR 0.001000
INFO - Training [46][   40/  391]   Loss 0.081955   Top1 97.070312   Top5 100.000000   BatchTime 0.154427   LR 0.001000
INFO - Training [46][   60/  391]   Loss 0.083858   Top1 97.148438   Top5 99.973958   BatchTime 0.144254   LR 0.001000
INFO - Training [46][   80/  391]   Loss 0.085522   Top1 97.011719   Top5 99.980469   BatchTime 0.139219   LR 0.001000
INFO - Training [46][  100/  391]   Loss 0.085137   Top1 97.023438   Top5 99.976562   BatchTime 0.136067   LR 0.001000
INFO - Training [46][  120/  391]   Loss 0.084069   Top1 97.057292   Top5 99.980469   BatchTime 0.134118   LR 0.001000
INFO - Training [46][  140/  391]   Loss 0.084680   Top1 97.047991   Top5 99.983259   BatchTime 0.132610   LR 0.001000
INFO - Training [46][  160/  391]   Loss 0.083990   Top1 97.094727   Top5 99.980469   BatchTime 0.131526   LR 0.001000
INFO - Training [46][  180/  391]   Loss 0.084765   Top1 97.065972   Top5 99.982639   BatchTime 0.130638   LR 0.001000
INFO - Training [46][  200/  391]   Loss 0.084398   Top1 97.085938   Top5 99.980469   BatchTime 0.129877   LR 0.001000
INFO - Training [46][  220/  391]   Loss 0.085512   Top1 97.066761   Top5 99.982244   BatchTime 0.129397   LR 0.001000
INFO - Training [46][  240/  391]   Loss 0.084861   Top1 97.086589   Top5 99.983724   BatchTime 0.124785   LR 0.001000
INFO - Training [46][  260/  391]   Loss 0.085178   Top1 97.067308   Top5 99.981971   BatchTime 0.122359   LR 0.001000
INFO - Training [46][  280/  391]   Loss 0.084299   Top1 97.092634   Top5 99.980469   BatchTime 0.119864   LR 0.001000
INFO - Training [46][  300/  391]   Loss 0.085694   Top1 97.049479   Top5 99.979167   BatchTime 0.118004   LR 0.001000
INFO - Training [46][  320/  391]   Loss 0.086209   Top1 97.006836   Top5 99.980469   BatchTime 0.118425   LR 0.001000
INFO - Training [46][  340/  391]   Loss 0.086099   Top1 97.019761   Top5 99.979320   BatchTime 0.118712   LR 0.001000
INFO - Training [46][  360/  391]   Loss 0.086044   Top1 97.031250   Top5 99.980469   BatchTime 0.118993   LR 0.001000
INFO - Training [46][  380/  391]   Loss 0.086392   Top1 97.006579   Top5 99.979441   BatchTime 0.119246   LR 0.001000
INFO - ==> Top1: 96.998    Top5: 99.978    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [46][   20/   79]   Loss 0.367710   Top1 90.156250   Top5 99.609375   BatchTime 0.151028
INFO - Validation [46][   40/   79]   Loss 0.358290   Top1 90.058594   Top5 99.648438   BatchTime 0.107552
INFO - Validation [46][   60/   79]   Loss 0.359764   Top1 90.208333   Top5 99.648438   BatchTime 0.093402
INFO - ==> Top1: 90.070    Top5: 99.670    Loss: 0.356
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  47
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [47][   20/  391]   Loss 0.071829   Top1 97.617188   Top5 99.921875   BatchTime 0.182824   LR 0.001000
INFO - Training [47][   40/  391]   Loss 0.078950   Top1 97.207031   Top5 99.960938   BatchTime 0.134445   LR 0.001000
INFO - Training [47][   60/  391]   Loss 0.082570   Top1 97.161458   Top5 99.960938   BatchTime 0.120394   LR 0.001000
INFO - Training [47][   80/  391]   Loss 0.085423   Top1 97.109375   Top5 99.970703   BatchTime 0.109822   LR 0.001000
INFO - Training [47][  100/  391]   Loss 0.086726   Top1 97.054688   Top5 99.976562   BatchTime 0.113437   LR 0.001000
INFO - Training [47][  120/  391]   Loss 0.089134   Top1 96.927083   Top5 99.980469   BatchTime 0.115295   LR 0.001000
INFO - Training [47][  140/  391]   Loss 0.089299   Top1 96.919643   Top5 99.983259   BatchTime 0.116659   LR 0.001000
INFO - Training [47][  160/  391]   Loss 0.088979   Top1 96.958008   Top5 99.985352   BatchTime 0.117667   LR 0.001000
INFO - Training [47][  180/  391]   Loss 0.088243   Top1 96.957465   Top5 99.986979   BatchTime 0.118353   LR 0.001000
INFO - Training [47][  200/  391]   Loss 0.088253   Top1 96.960938   Top5 99.984375   BatchTime 0.118865   LR 0.001000
INFO - Training [47][  220/  391]   Loss 0.088653   Top1 96.938920   Top5 99.985795   BatchTime 0.119335   LR 0.001000
INFO - Training [47][  240/  391]   Loss 0.088091   Top1 96.953125   Top5 99.983724   BatchTime 0.119667   LR 0.001000
INFO - Training [47][  260/  391]   Loss 0.087544   Top1 96.959135   Top5 99.984976   BatchTime 0.119972   LR 0.001000
INFO - Training [47][  280/  391]   Loss 0.086763   Top1 96.975446   Top5 99.983259   BatchTime 0.120081   LR 0.001000
INFO - Training [47][  300/  391]   Loss 0.087004   Top1 96.958333   Top5 99.984375   BatchTime 0.117525   LR 0.001000
INFO - Training [47][  320/  391]   Loss 0.086964   Top1 96.950684   Top5 99.985352   BatchTime 0.115823   LR 0.001000
INFO - Training [47][  340/  391]   Loss 0.087677   Top1 96.916360   Top5 99.983915   BatchTime 0.114134   LR 0.001000
INFO - Training [47][  360/  391]   Loss 0.087448   Top1 96.937934   Top5 99.982639   BatchTime 0.113191   LR 0.001000
INFO - Training [47][  380/  391]   Loss 0.088373   Top1 96.914062   Top5 99.981497   BatchTime 0.113738   LR 0.001000
INFO - ==> Top1: 96.916    Top5: 99.982    Loss: 0.089
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [47][   20/   79]   Loss 0.364432   Top1 90.625000   Top5 99.570312   BatchTime 0.151141
INFO - Validation [47][   40/   79]   Loss 0.364392   Top1 90.273438   Top5 99.609375   BatchTime 0.107968
INFO - Validation [47][   60/   79]   Loss 0.364870   Top1 90.117188   Top5 99.622396   BatchTime 0.092737
INFO - ==> Top1: 90.020    Top5: 99.610    Loss: 0.360
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [40][Top1: 90.370   Top5: 99.670] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  48
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [48][   20/  391]   Loss 0.092013   Top1 96.679688   Top5 100.000000   BatchTime 0.211374   LR 0.001000
INFO - Training [48][   40/  391]   Loss 0.092986   Top1 96.738281   Top5 100.000000   BatchTime 0.167092   LR 0.001000
INFO - Training [48][   60/  391]   Loss 0.089189   Top1 96.875000   Top5 100.000000   BatchTime 0.152679   LR 0.001000
INFO - Training [48][   80/  391]   Loss 0.088928   Top1 96.953125   Top5 99.990234   BatchTime 0.139665   LR 0.001000
INFO - Training [48][  100/  391]   Loss 0.087924   Top1 96.960938   Top5 99.992188   BatchTime 0.128197   LR 0.001000
INFO - Training [48][  120/  391]   Loss 0.088714   Top1 96.907552   Top5 99.993490   BatchTime 0.121669   LR 0.001000
INFO - Training [48][  140/  391]   Loss 0.088100   Top1 96.925223   Top5 99.988839   BatchTime 0.116383   LR 0.001000
INFO - Training [48][  160/  391]   Loss 0.088785   Top1 96.933594   Top5 99.985352   BatchTime 0.115372   LR 0.001000
INFO - Training [48][  180/  391]   Loss 0.087728   Top1 96.931424   Top5 99.986979   BatchTime 0.116329   LR 0.001000
INFO - Training [48][  200/  391]   Loss 0.087955   Top1 96.914062   Top5 99.984375   BatchTime 0.117113   LR 0.001000
INFO - Training [48][  220/  391]   Loss 0.087752   Top1 96.928267   Top5 99.985795   BatchTime 0.117819   LR 0.001000
INFO - Training [48][  240/  391]   Loss 0.086085   Top1 96.975911   Top5 99.986979   BatchTime 0.118392   LR 0.001000
INFO - Training [48][  260/  391]   Loss 0.086760   Top1 96.953125   Top5 99.987981   BatchTime 0.118908   LR 0.001000
INFO - Training [48][  280/  391]   Loss 0.087458   Top1 96.933594   Top5 99.988839   BatchTime 0.119356   LR 0.001000
INFO - Training [48][  300/  391]   Loss 0.086930   Top1 96.960938   Top5 99.989583   BatchTime 0.119620   LR 0.001000
INFO - Training [48][  320/  391]   Loss 0.086817   Top1 96.967773   Top5 99.987793   BatchTime 0.119910   LR 0.001000
INFO - Training [48][  340/  391]   Loss 0.086571   Top1 96.996783   Top5 99.988511   BatchTime 0.120087   LR 0.001000
INFO - Training [48][  360/  391]   Loss 0.086810   Top1 96.987847   Top5 99.986979   BatchTime 0.118542   LR 0.001000
INFO - Training [48][  380/  391]   Loss 0.086393   Top1 97.004523   Top5 99.985609   BatchTime 0.116608   LR 0.001000
INFO - ==> Top1: 97.002    Top5: 99.986    Loss: 0.087
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [48][   20/   79]   Loss 0.364130   Top1 90.937500   Top5 99.648438   BatchTime 0.127244
INFO - Validation [48][   40/   79]   Loss 0.359267   Top1 90.644531   Top5 99.628906   BatchTime 0.084554
INFO - Validation [48][   60/   79]   Loss 0.359118   Top1 90.468750   Top5 99.648438   BatchTime 0.070430
INFO - ==> Top1: 90.400    Top5: 99.670    Loss: 0.357
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  49
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [49][   20/  391]   Loss 0.092737   Top1 96.718750   Top5 99.960938   BatchTime 0.163078   LR 0.001000
INFO - Training [49][   40/  391]   Loss 0.087831   Top1 96.933594   Top5 99.960938   BatchTime 0.118657   LR 0.001000
INFO - Training [49][   60/  391]   Loss 0.084779   Top1 96.940104   Top5 99.973958   BatchTime 0.103947   LR 0.001000
INFO - Training [49][   80/  391]   Loss 0.084341   Top1 96.894531   Top5 99.980469   BatchTime 0.097720   LR 0.001000
INFO - Training [49][  100/  391]   Loss 0.084517   Top1 96.882812   Top5 99.976562   BatchTime 0.093264   LR 0.001000
INFO - Training [49][  120/  391]   Loss 0.085466   Top1 96.842448   Top5 99.980469   BatchTime 0.090349   LR 0.001000
INFO - Training [49][  140/  391]   Loss 0.086129   Top1 96.858259   Top5 99.983259   BatchTime 0.088388   LR 0.001000
INFO - Training [49][  160/  391]   Loss 0.086128   Top1 96.860352   Top5 99.980469   BatchTime 0.086605   LR 0.001000
INFO - Training [49][  180/  391]   Loss 0.086069   Top1 96.861979   Top5 99.982639   BatchTime 0.085483   LR 0.001000
INFO - Training [49][  200/  391]   Loss 0.086390   Top1 96.886719   Top5 99.984375   BatchTime 0.085549   LR 0.001000
INFO - Training [49][  220/  391]   Loss 0.086072   Top1 96.871449   Top5 99.985795   BatchTime 0.085078   LR 0.001000
INFO - Training [49][  240/  391]   Loss 0.085509   Top1 96.907552   Top5 99.986979   BatchTime 0.085541   LR 0.001000
INFO - Training [49][  260/  391]   Loss 0.085425   Top1 96.917067   Top5 99.987981   BatchTime 0.085351   LR 0.001000
INFO - Training [49][  280/  391]   Loss 0.085597   Top1 96.891741   Top5 99.983259   BatchTime 0.087042   LR 0.001000
INFO - Training [49][  300/  391]   Loss 0.085842   Top1 96.880208   Top5 99.981771   BatchTime 0.089490   LR 0.001000
INFO - Training [49][  320/  391]   Loss 0.086368   Top1 96.857910   Top5 99.982910   BatchTime 0.091628   LR 0.001000
INFO - Training [49][  340/  391]   Loss 0.086256   Top1 96.872702   Top5 99.983915   BatchTime 0.093482   LR 0.001000
INFO - Training [49][  360/  391]   Loss 0.086838   Top1 96.872830   Top5 99.982639   BatchTime 0.095210   LR 0.001000
INFO - Training [49][  380/  391]   Loss 0.086923   Top1 96.893503   Top5 99.981497   BatchTime 0.096697   LR 0.001000
INFO - ==> Top1: 96.904    Top5: 99.982    Loss: 0.087
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [49][   20/   79]   Loss 0.372265   Top1 89.960938   Top5 99.570312   BatchTime 0.151439
INFO - Validation [49][   40/   79]   Loss 0.360446   Top1 90.097656   Top5 99.609375   BatchTime 0.108451
INFO - Validation [49][   60/   79]   Loss 0.359721   Top1 90.338542   Top5 99.622396   BatchTime 0.094202
INFO - ==> Top1: 90.280    Top5: 99.650    Loss: 0.355
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  50
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [50][   20/  391]   Loss 0.077804   Top1 97.578125   Top5 99.960938   BatchTime 0.194174   LR 0.001000
INFO - Training [50][   40/  391]   Loss 0.077983   Top1 97.265625   Top5 99.980469   BatchTime 0.139995   LR 0.001000
INFO - Training [50][   60/  391]   Loss 0.084337   Top1 96.966146   Top5 99.986979   BatchTime 0.124693   LR 0.001000
INFO - Training [50][   80/  391]   Loss 0.086165   Top1 96.845703   Top5 99.990234   BatchTime 0.124825   LR 0.001000
INFO - Training [50][  100/  391]   Loss 0.084320   Top1 96.953125   Top5 99.992188   BatchTime 0.124546   LR 0.001000
INFO - Training [50][  120/  391]   Loss 0.085620   Top1 96.881510   Top5 99.993490   BatchTime 0.124531   LR 0.001000
INFO - Training [50][  140/  391]   Loss 0.086050   Top1 96.875000   Top5 99.983259   BatchTime 0.124427   LR 0.001000
INFO - Training [50][  160/  391]   Loss 0.086323   Top1 96.914062   Top5 99.985352   BatchTime 0.124313   LR 0.001000
INFO - Training [50][  180/  391]   Loss 0.087582   Top1 96.909722   Top5 99.986979   BatchTime 0.124128   LR 0.001000
INFO - Training [50][  200/  391]   Loss 0.086432   Top1 96.937500   Top5 99.988281   BatchTime 0.124031   LR 0.001000
INFO - Training [50][  220/  391]   Loss 0.086193   Top1 96.938920   Top5 99.985795   BatchTime 0.123961   LR 0.001000
INFO - Training [50][  240/  391]   Loss 0.085976   Top1 96.959635   Top5 99.986979   BatchTime 0.123921   LR 0.001000
INFO - Training [50][  260/  391]   Loss 0.085960   Top1 96.962139   Top5 99.987981   BatchTime 0.123013   LR 0.001000
INFO - Training [50][  280/  391]   Loss 0.085418   Top1 97.006138   Top5 99.988839   BatchTime 0.119920   LR 0.001000
INFO - Training [50][  300/  391]   Loss 0.085901   Top1 96.973958   Top5 99.986979   BatchTime 0.117888   LR 0.001000
INFO - Training [50][  320/  391]   Loss 0.085705   Top1 96.992188   Top5 99.987793   BatchTime 0.115806   LR 0.001000
INFO - Training [50][  340/  391]   Loss 0.085659   Top1 97.008272   Top5 99.988511   BatchTime 0.115082   LR 0.001000
INFO - Training [50][  360/  391]   Loss 0.085233   Top1 97.037760   Top5 99.989149   BatchTime 0.115547   LR 0.001000
INFO - Training [50][  380/  391]   Loss 0.084802   Top1 97.029194   Top5 99.989720   BatchTime 0.115952   LR 0.001000
INFO - ==> Top1: 97.000    Top5: 99.988    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [50][   20/   79]   Loss 0.363571   Top1 90.351562   Top5 99.609375   BatchTime 0.152380
INFO - Validation [50][   40/   79]   Loss 0.360597   Top1 90.273438   Top5 99.609375   BatchTime 0.107521
INFO - Validation [50][   60/   79]   Loss 0.363844   Top1 90.286458   Top5 99.635417   BatchTime 0.093073
INFO - ==> Top1: 90.210    Top5: 99.660    Loss: 0.360
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  51
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [51][   20/  391]   Loss 0.084681   Top1 97.031250   Top5 99.960938   BatchTime 0.208013   LR 0.001000
INFO - Training [51][   40/  391]   Loss 0.090036   Top1 96.718750   Top5 99.980469   BatchTime 0.165584   LR 0.001000
INFO - Training [51][   60/  391]   Loss 0.085100   Top1 96.966146   Top5 99.973958   BatchTime 0.140946   LR 0.001000
INFO - Training [51][   80/  391]   Loss 0.083339   Top1 97.011719   Top5 99.980469   BatchTime 0.127012   LR 0.001000
INFO - Training [51][  100/  391]   Loss 0.084167   Top1 97.031250   Top5 99.984375   BatchTime 0.119886   LR 0.001000
INFO - Training [51][  120/  391]   Loss 0.083865   Top1 97.024740   Top5 99.986979   BatchTime 0.113073   LR 0.001000
INFO - Training [51][  140/  391]   Loss 0.082969   Top1 97.109375   Top5 99.988839   BatchTime 0.115220   LR 0.001000
INFO - Training [51][  160/  391]   Loss 0.082966   Top1 97.119141   Top5 99.990234   BatchTime 0.116239   LR 0.001000
INFO - Training [51][  180/  391]   Loss 0.083099   Top1 97.109375   Top5 99.991319   BatchTime 0.117078   LR 0.001000
INFO - Training [51][  200/  391]   Loss 0.082998   Top1 97.156250   Top5 99.992188   BatchTime 0.117769   LR 0.001000
INFO - Training [51][  220/  391]   Loss 0.083928   Top1 97.127131   Top5 99.985795   BatchTime 0.118298   LR 0.001000
INFO - Training [51][  240/  391]   Loss 0.083784   Top1 97.119141   Top5 99.986979   BatchTime 0.118738   LR 0.001000
INFO - Training [51][  260/  391]   Loss 0.083468   Top1 97.115385   Top5 99.987981   BatchTime 0.119082   LR 0.001000
INFO - Training [51][  280/  391]   Loss 0.085222   Top1 97.034040   Top5 99.988839   BatchTime 0.119338   LR 0.001000
INFO - Training [51][  300/  391]   Loss 0.085363   Top1 97.026042   Top5 99.986979   BatchTime 0.119584   LR 0.001000
INFO - Training [51][  320/  391]   Loss 0.085130   Top1 97.023926   Top5 99.985352   BatchTime 0.119799   LR 0.001000
INFO - Training [51][  340/  391]   Loss 0.085033   Top1 97.015165   Top5 99.986213   BatchTime 0.117127   LR 0.001000
INFO - Training [51][  360/  391]   Loss 0.084938   Top1 97.009549   Top5 99.986979   BatchTime 0.115653   LR 0.001000
INFO - Training [51][  380/  391]   Loss 0.085662   Top1 96.981908   Top5 99.985609   BatchTime 0.114095   LR 0.001000
INFO - ==> Top1: 96.986    Top5: 99.986    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [51][   20/   79]   Loss 0.369330   Top1 90.507812   Top5 99.609375   BatchTime 0.160639
INFO - Validation [51][   40/   79]   Loss 0.365201   Top1 90.332031   Top5 99.589844   BatchTime 0.111642
INFO - Validation [51][   60/   79]   Loss 0.366986   Top1 90.377604   Top5 99.622396   BatchTime 0.095350
INFO - ==> Top1: 90.260    Top5: 99.650    Loss: 0.364
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  52
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [52][   20/  391]   Loss 0.075168   Top1 96.914062   Top5 100.000000   BatchTime 0.212395   LR 0.001000
INFO - Training [52][   40/  391]   Loss 0.083324   Top1 96.835938   Top5 100.000000   BatchTime 0.168337   LR 0.001000
INFO - Training [52][   60/  391]   Loss 0.086936   Top1 96.822917   Top5 100.000000   BatchTime 0.156214   LR 0.001000
INFO - Training [52][   80/  391]   Loss 0.086318   Top1 96.865234   Top5 100.000000   BatchTime 0.147902   LR 0.001000
INFO - Training [52][  100/  391]   Loss 0.089725   Top1 96.703125   Top5 100.000000   BatchTime 0.142862   LR 0.001000
INFO - Training [52][  120/  391]   Loss 0.091038   Top1 96.712240   Top5 100.000000   BatchTime 0.134097   LR 0.001000
INFO - Training [52][  140/  391]   Loss 0.090795   Top1 96.713170   Top5 99.994420   BatchTime 0.127368   LR 0.001000
INFO - Training [52][  160/  391]   Loss 0.090183   Top1 96.704102   Top5 99.995117   BatchTime 0.123017   LR 0.001000
INFO - Training [52][  180/  391]   Loss 0.088241   Top1 96.783854   Top5 99.995660   BatchTime 0.119099   LR 0.001000
INFO - Training [52][  200/  391]   Loss 0.087832   Top1 96.804688   Top5 99.996094   BatchTime 0.119454   LR 0.001000
INFO - Training [52][  220/  391]   Loss 0.087759   Top1 96.793324   Top5 99.996449   BatchTime 0.119795   LR 0.001000
INFO - Training [52][  240/  391]   Loss 0.087423   Top1 96.800130   Top5 99.996745   BatchTime 0.120122   LR 0.001000
INFO - Training [52][  260/  391]   Loss 0.087160   Top1 96.823918   Top5 99.996995   BatchTime 0.120351   LR 0.001000
INFO - Training [52][  280/  391]   Loss 0.086492   Top1 96.869420   Top5 99.997210   BatchTime 0.120174   LR 0.001000
INFO - Training [52][  300/  391]   Loss 0.086619   Top1 96.880208   Top5 99.997396   BatchTime 0.120414   LR 0.001000
INFO - Training [52][  320/  391]   Loss 0.087002   Top1 96.872559   Top5 99.995117   BatchTime 0.120657   LR 0.001000
INFO - Training [52][  340/  391]   Loss 0.086432   Top1 96.904871   Top5 99.993107   BatchTime 0.120750   LR 0.001000
INFO - Training [52][  360/  391]   Loss 0.085771   Top1 96.933594   Top5 99.993490   BatchTime 0.120846   LR 0.001000
INFO - Training [52][  380/  391]   Loss 0.086096   Top1 96.932566   Top5 99.993832   BatchTime 0.120682   LR 0.001000
INFO - ==> Top1: 96.940    Top5: 99.992    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [52][   20/   79]   Loss 0.360964   Top1 90.234375   Top5 99.687500   BatchTime 0.132432
INFO - Validation [52][   40/   79]   Loss 0.360451   Top1 90.117188   Top5 99.667969   BatchTime 0.079570
INFO - Validation [52][   60/   79]   Loss 0.363145   Top1 90.156250   Top5 99.674479   BatchTime 0.061696
INFO - ==> Top1: 90.040    Top5: 99.670    Loss: 0.362
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  53
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [53][   20/  391]   Loss 0.085769   Top1 97.031250   Top5 100.000000   BatchTime 0.222047   LR 0.001000
INFO - Training [53][   40/  391]   Loss 0.077534   Top1 97.285156   Top5 100.000000   BatchTime 0.173000   LR 0.001000
INFO - Training [53][   60/  391]   Loss 0.077530   Top1 97.291667   Top5 99.986979   BatchTime 0.156484   LR 0.001000
INFO - Training [53][   80/  391]   Loss 0.078316   Top1 97.333984   Top5 99.970703   BatchTime 0.148181   LR 0.001000
INFO - Training [53][  100/  391]   Loss 0.080846   Top1 97.218750   Top5 99.976562   BatchTime 0.143503   LR 0.001000
INFO - Training [53][  120/  391]   Loss 0.081150   Top1 97.207031   Top5 99.980469   BatchTime 0.140147   LR 0.001000
INFO - Training [53][  140/  391]   Loss 0.082075   Top1 97.159598   Top5 99.977679   BatchTime 0.137711   LR 0.001000
INFO - Training [53][  160/  391]   Loss 0.083543   Top1 97.148438   Top5 99.975586   BatchTime 0.135936   LR 0.001000
INFO - Training [53][  180/  391]   Loss 0.083023   Top1 97.178819   Top5 99.978299   BatchTime 0.134463   LR 0.001000
INFO - Training [53][  200/  391]   Loss 0.082823   Top1 97.175781   Top5 99.976562   BatchTime 0.128754   LR 0.001000
INFO - Training [53][  220/  391]   Loss 0.083456   Top1 97.155540   Top5 99.975142   BatchTime 0.125345   LR 0.001000
INFO - Training [53][  240/  391]   Loss 0.083015   Top1 97.180990   Top5 99.977214   BatchTime 0.122320   LR 0.001000
INFO - Training [53][  260/  391]   Loss 0.083378   Top1 97.172476   Top5 99.975962   BatchTime 0.119945   LR 0.001000
INFO - Training [53][  280/  391]   Loss 0.082512   Top1 97.201451   Top5 99.974888   BatchTime 0.120210   LR 0.001000
INFO - Training [53][  300/  391]   Loss 0.082857   Top1 97.192708   Top5 99.971354   BatchTime 0.120395   LR 0.001000
INFO - Training [53][  320/  391]   Loss 0.082954   Top1 97.160645   Top5 99.973145   BatchTime 0.120604   LR 0.001000
INFO - Training [53][  340/  391]   Loss 0.082946   Top1 97.159926   Top5 99.974724   BatchTime 0.120744   LR 0.001000
INFO - Training [53][  360/  391]   Loss 0.083059   Top1 97.154948   Top5 99.973958   BatchTime 0.120861   LR 0.001000
INFO - Training [53][  380/  391]   Loss 0.082896   Top1 97.154605   Top5 99.975329   BatchTime 0.120990   LR 0.001000
INFO - ==> Top1: 97.148    Top5: 99.976    Loss: 0.083
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [53][   20/   79]   Loss 0.376273   Top1 90.195312   Top5 99.687500   BatchTime 0.149001
INFO - Validation [53][   40/   79]   Loss 0.372249   Top1 89.980469   Top5 99.648438   BatchTime 0.107055
INFO - Validation [53][   60/   79]   Loss 0.371635   Top1 90.195312   Top5 99.648438   BatchTime 0.093270
INFO - ==> Top1: 90.070    Top5: 99.660    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  54
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [54][   20/  391]   Loss 0.088410   Top1 96.875000   Top5 100.000000   BatchTime 0.187574   LR 0.001000
INFO - Training [54][   40/  391]   Loss 0.087147   Top1 96.953125   Top5 100.000000   BatchTime 0.131110   LR 0.001000
INFO - Training [54][   60/  391]   Loss 0.084724   Top1 97.044271   Top5 99.986979   BatchTime 0.127277   LR 0.001000
INFO - Training [54][   80/  391]   Loss 0.082923   Top1 97.167969   Top5 99.990234   BatchTime 0.127749   LR 0.001000
INFO - Training [54][  100/  391]   Loss 0.082405   Top1 97.125000   Top5 99.992188   BatchTime 0.126699   LR 0.001000
INFO - Training [54][  120/  391]   Loss 0.080660   Top1 97.200521   Top5 99.993490   BatchTime 0.126129   LR 0.001000
INFO - Training [54][  140/  391]   Loss 0.082892   Top1 97.103795   Top5 99.994420   BatchTime 0.125798   LR 0.001000
INFO - Training [54][  160/  391]   Loss 0.081436   Top1 97.128906   Top5 99.990234   BatchTime 0.125501   LR 0.001000
INFO - Training [54][  180/  391]   Loss 0.083342   Top1 97.048611   Top5 99.991319   BatchTime 0.125292   LR 0.001000
INFO - Training [54][  200/  391]   Loss 0.084104   Top1 97.007812   Top5 99.992188   BatchTime 0.125107   LR 0.001000
INFO - Training [54][  220/  391]   Loss 0.084126   Top1 97.013494   Top5 99.989347   BatchTime 0.124968   LR 0.001000
INFO - Training [54][  240/  391]   Loss 0.083609   Top1 97.018229   Top5 99.990234   BatchTime 0.124787   LR 0.001000
INFO - Training [54][  260/  391]   Loss 0.083290   Top1 97.067308   Top5 99.990986   BatchTime 0.121586   LR 0.001000
INFO - Training [54][  280/  391]   Loss 0.083878   Top1 97.042411   Top5 99.991629   BatchTime 0.119390   LR 0.001000
INFO - Training [54][  300/  391]   Loss 0.083726   Top1 97.075521   Top5 99.986979   BatchTime 0.117586   LR 0.001000
INFO - Training [54][  320/  391]   Loss 0.083639   Top1 97.075195   Top5 99.987793   BatchTime 0.116058   LR 0.001000
INFO - Training [54][  340/  391]   Loss 0.083705   Top1 97.049632   Top5 99.988511   BatchTime 0.116451   LR 0.001000
INFO - Training [54][  360/  391]   Loss 0.083892   Top1 97.039931   Top5 99.989149   BatchTime 0.116847   LR 0.001000
INFO - Training [54][  380/  391]   Loss 0.084080   Top1 97.039474   Top5 99.989720   BatchTime 0.117190   LR 0.001000
INFO - ==> Top1: 97.030    Top5: 99.990    Loss: 0.084
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [54][   20/   79]   Loss 0.373213   Top1 90.351562   Top5 99.609375   BatchTime 0.152778
INFO - Validation [54][   40/   79]   Loss 0.371027   Top1 90.039062   Top5 99.648438   BatchTime 0.104969
INFO - Validation [54][   60/   79]   Loss 0.366968   Top1 90.156250   Top5 99.661458   BatchTime 0.090982
INFO - ==> Top1: 90.050    Top5: 99.680    Loss: 0.361
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  55
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [55][   20/  391]   Loss 0.092232   Top1 96.796875   Top5 100.000000   BatchTime 0.208073   LR 0.001000
INFO - Training [55][   40/  391]   Loss 0.087236   Top1 96.933594   Top5 99.980469   BatchTime 0.157977   LR 0.001000
INFO - Training [55][   60/  391]   Loss 0.084454   Top1 96.992188   Top5 99.986979   BatchTime 0.132707   LR 0.001000
INFO - Training [55][   80/  391]   Loss 0.082997   Top1 97.050781   Top5 99.980469   BatchTime 0.122065   LR 0.001000
INFO - Training [55][  100/  391]   Loss 0.082065   Top1 97.140625   Top5 99.976562   BatchTime 0.113810   LR 0.001000
INFO - Training [55][  120/  391]   Loss 0.081923   Top1 97.154948   Top5 99.980469   BatchTime 0.113798   LR 0.001000
INFO - Training [55][  140/  391]   Loss 0.083201   Top1 97.098214   Top5 99.977679   BatchTime 0.115260   LR 0.001000
INFO - Training [55][  160/  391]   Loss 0.080933   Top1 97.158203   Top5 99.980469   BatchTime 0.116373   LR 0.001000
INFO - Training [55][  180/  391]   Loss 0.081003   Top1 97.144097   Top5 99.982639   BatchTime 0.117236   LR 0.001000
INFO - Training [55][  200/  391]   Loss 0.080262   Top1 97.152344   Top5 99.976562   BatchTime 0.117874   LR 0.001000
INFO - Training [55][  220/  391]   Loss 0.080294   Top1 97.151989   Top5 99.971591   BatchTime 0.118489   LR 0.001000
INFO - Training [55][  240/  391]   Loss 0.080860   Top1 97.128906   Top5 99.973958   BatchTime 0.118948   LR 0.001000
INFO - Training [55][  260/  391]   Loss 0.082097   Top1 97.088341   Top5 99.969952   BatchTime 0.119291   LR 0.001000
INFO - Training [55][  280/  391]   Loss 0.081838   Top1 97.140067   Top5 99.972098   BatchTime 0.119546   LR 0.001000
INFO - Training [55][  300/  391]   Loss 0.082575   Top1 97.101562   Top5 99.973958   BatchTime 0.119760   LR 0.001000
INFO - Training [55][  320/  391]   Loss 0.082541   Top1 97.104492   Top5 99.975586   BatchTime 0.117934   LR 0.001000
INFO - Training [55][  340/  391]   Loss 0.082949   Top1 97.090993   Top5 99.977022   BatchTime 0.116186   LR 0.001000
INFO - Training [55][  360/  391]   Loss 0.083654   Top1 97.065972   Top5 99.978299   BatchTime 0.114825   LR 0.001000
INFO - Training [55][  380/  391]   Loss 0.083995   Top1 97.045641   Top5 99.977385   BatchTime 0.112522   LR 0.001000
INFO - ==> Top1: 97.072    Top5: 99.978    Loss: 0.084
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [55][   20/   79]   Loss 0.365585   Top1 90.351562   Top5 99.609375   BatchTime 0.150952
INFO - Validation [55][   40/   79]   Loss 0.365292   Top1 90.214844   Top5 99.609375   BatchTime 0.106806
INFO - Validation [55][   60/   79]   Loss 0.369705   Top1 90.338542   Top5 99.609375   BatchTime 0.092063
INFO - ==> Top1: 90.220    Top5: 99.630    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  56
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [56][   20/  391]   Loss 0.082201   Top1 97.109375   Top5 99.960938   BatchTime 0.213648   LR 0.001000
INFO - Training [56][   40/  391]   Loss 0.079679   Top1 97.285156   Top5 99.980469   BatchTime 0.168607   LR 0.001000
INFO - Training [56][   60/  391]   Loss 0.077507   Top1 97.356771   Top5 99.973958   BatchTime 0.153482   LR 0.001000
INFO - Training [56][   80/  391]   Loss 0.076938   Top1 97.382812   Top5 99.980469   BatchTime 0.145978   LR 0.001000
INFO - Training [56][  100/  391]   Loss 0.078754   Top1 97.265625   Top5 99.984375   BatchTime 0.140405   LR 0.001000
INFO - Training [56][  120/  391]   Loss 0.076754   Top1 97.324219   Top5 99.986979   BatchTime 0.130646   LR 0.001000
INFO - Training [56][  140/  391]   Loss 0.078297   Top1 97.271205   Top5 99.983259   BatchTime 0.124855   LR 0.001000
INFO - Training [56][  160/  391]   Loss 0.080371   Top1 97.177734   Top5 99.985352   BatchTime 0.120218   LR 0.001000
INFO - Training [56][  180/  391]   Loss 0.080995   Top1 97.161458   Top5 99.986979   BatchTime 0.119005   LR 0.001000
INFO - Training [56][  200/  391]   Loss 0.079857   Top1 97.195312   Top5 99.984375   BatchTime 0.119511   LR 0.001000
INFO - Training [56][  220/  391]   Loss 0.079925   Top1 97.187500   Top5 99.985795   BatchTime 0.119870   LR 0.001000
INFO - Training [56][  240/  391]   Loss 0.079883   Top1 97.184245   Top5 99.986979   BatchTime 0.120205   LR 0.001000
INFO - Training [56][  260/  391]   Loss 0.081179   Top1 97.109375   Top5 99.981971   BatchTime 0.120495   LR 0.001000
INFO - Training [56][  280/  391]   Loss 0.081230   Top1 97.117746   Top5 99.983259   BatchTime 0.120743   LR 0.001000
INFO - Training [56][  300/  391]   Loss 0.081054   Top1 97.117188   Top5 99.984375   BatchTime 0.120938   LR 0.001000
INFO - Training [56][  320/  391]   Loss 0.080702   Top1 97.116699   Top5 99.985352   BatchTime 0.121043   LR 0.001000
INFO - Training [56][  340/  391]   Loss 0.080974   Top1 97.102482   Top5 99.983915   BatchTime 0.121159   LR 0.001000
INFO - Training [56][  360/  391]   Loss 0.081112   Top1 97.096354   Top5 99.982639   BatchTime 0.121255   LR 0.001000
INFO - Training [56][  380/  391]   Loss 0.081088   Top1 97.099095   Top5 99.981497   BatchTime 0.119874   LR 0.001000
INFO - ==> Top1: 97.076    Top5: 99.980    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [56][   20/   79]   Loss 0.375820   Top1 90.273438   Top5 99.570312   BatchTime 0.120526
INFO - Validation [56][   40/   79]   Loss 0.368926   Top1 90.292969   Top5 99.609375   BatchTime 0.073467
INFO - Validation [56][   60/   79]   Loss 0.371525   Top1 90.390625   Top5 99.609375   BatchTime 0.059556
INFO - ==> Top1: 90.140    Top5: 99.660    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  57
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [57][   20/  391]   Loss 0.071142   Top1 97.617188   Top5 99.960938   BatchTime 0.212665   LR 0.001000
INFO - Training [57][   40/  391]   Loss 0.077205   Top1 97.343750   Top5 99.941406   BatchTime 0.168303   LR 0.001000
INFO - Training [57][   60/  391]   Loss 0.076912   Top1 97.382812   Top5 99.960938   BatchTime 0.153346   LR 0.001000
INFO - Training [57][   80/  391]   Loss 0.077772   Top1 97.304688   Top5 99.970703   BatchTime 0.146167   LR 0.001000
INFO - Training [57][  100/  391]   Loss 0.077757   Top1 97.351562   Top5 99.976562   BatchTime 0.141683   LR 0.001000
INFO - Training [57][  120/  391]   Loss 0.080678   Top1 97.174479   Top5 99.973958   BatchTime 0.138633   LR 0.001000
INFO - Training [57][  140/  391]   Loss 0.081496   Top1 97.092634   Top5 99.972098   BatchTime 0.136440   LR 0.001000
INFO - Training [57][  160/  391]   Loss 0.081593   Top1 97.055664   Top5 99.975586   BatchTime 0.134632   LR 0.001000
INFO - Training [57][  180/  391]   Loss 0.080932   Top1 97.061632   Top5 99.978299   BatchTime 0.129271   LR 0.001000
INFO - Training [57][  200/  391]   Loss 0.081065   Top1 97.089844   Top5 99.980469   BatchTime 0.124999   LR 0.001000
INFO - Training [57][  220/  391]   Loss 0.081059   Top1 97.070312   Top5 99.978693   BatchTime 0.122210   LR 0.001000
INFO - Training [57][  240/  391]   Loss 0.081745   Top1 97.063802   Top5 99.980469   BatchTime 0.119496   LR 0.001000
INFO - Training [57][  260/  391]   Loss 0.080413   Top1 97.112380   Top5 99.981971   BatchTime 0.119808   LR 0.001000
INFO - Training [57][  280/  391]   Loss 0.080988   Top1 97.095424   Top5 99.983259   BatchTime 0.120127   LR 0.001000
INFO - Training [57][  300/  391]   Loss 0.081285   Top1 97.080729   Top5 99.984375   BatchTime 0.120365   LR 0.001000
INFO - Training [57][  320/  391]   Loss 0.081362   Top1 97.099609   Top5 99.985352   BatchTime 0.120646   LR 0.001000
INFO - Training [57][  340/  391]   Loss 0.081202   Top1 97.116268   Top5 99.986213   BatchTime 0.120812   LR 0.001000
INFO - Training [57][  360/  391]   Loss 0.081672   Top1 97.100694   Top5 99.982639   BatchTime 0.120981   LR 0.001000
INFO - Training [57][  380/  391]   Loss 0.081538   Top1 97.113487   Top5 99.983553   BatchTime 0.121074   LR 0.001000
INFO - ==> Top1: 97.102    Top5: 99.984    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [57][   20/   79]   Loss 0.388978   Top1 90.156250   Top5 99.648438   BatchTime 0.150185
INFO - Validation [57][   40/   79]   Loss 0.375159   Top1 90.058594   Top5 99.609375   BatchTime 0.104848
INFO - Validation [57][   60/   79]   Loss 0.372405   Top1 90.065104   Top5 99.622396   BatchTime 0.078996
INFO - ==> Top1: 90.030    Top5: 99.620    Loss: 0.368
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  58
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [58][   20/  391]   Loss 0.081917   Top1 97.070312   Top5 100.000000   BatchTime 0.179804   LR 0.001000
INFO - Training [58][   40/  391]   Loss 0.081257   Top1 97.207031   Top5 100.000000   BatchTime 0.144916   LR 0.001000
INFO - Training [58][   60/  391]   Loss 0.081474   Top1 97.148438   Top5 100.000000   BatchTime 0.137829   LR 0.001000
INFO - Training [58][   80/  391]   Loss 0.081430   Top1 97.158203   Top5 100.000000   BatchTime 0.134395   LR 0.001000
INFO - Training [58][  100/  391]   Loss 0.078984   Top1 97.171875   Top5 100.000000   BatchTime 0.132438   LR 0.001000
INFO - Training [58][  120/  391]   Loss 0.080362   Top1 97.083333   Top5 100.000000   BatchTime 0.131042   LR 0.001000
INFO - Training [58][  140/  391]   Loss 0.080355   Top1 97.081473   Top5 100.000000   BatchTime 0.130037   LR 0.001000
INFO - Training [58][  160/  391]   Loss 0.081231   Top1 97.001953   Top5 99.995117   BatchTime 0.129247   LR 0.001000
INFO - Training [58][  180/  391]   Loss 0.080429   Top1 97.035590   Top5 99.991319   BatchTime 0.128596   LR 0.001000
INFO - Training [58][  200/  391]   Loss 0.079646   Top1 97.035156   Top5 99.992188   BatchTime 0.128027   LR 0.001000
INFO - Training [58][  220/  391]   Loss 0.079064   Top1 97.077415   Top5 99.992898   BatchTime 0.127974   LR 0.001000
INFO - Training [58][  240/  391]   Loss 0.078636   Top1 97.106120   Top5 99.993490   BatchTime 0.125367   LR 0.001000
INFO - Training [58][  260/  391]   Loss 0.079258   Top1 97.085337   Top5 99.993990   BatchTime 0.122628   LR 0.001000
INFO - Training [58][  280/  391]   Loss 0.080347   Top1 97.059152   Top5 99.994420   BatchTime 0.120745   LR 0.001000
INFO - Training [58][  300/  391]   Loss 0.080842   Top1 97.065104   Top5 99.994792   BatchTime 0.118571   LR 0.001000
INFO - Training [58][  320/  391]   Loss 0.081121   Top1 97.053223   Top5 99.995117   BatchTime 0.118792   LR 0.001000
INFO - Training [58][  340/  391]   Loss 0.080855   Top1 97.061121   Top5 99.995404   BatchTime 0.119073   LR 0.001000
INFO - Training [58][  360/  391]   Loss 0.081292   Top1 97.052951   Top5 99.995660   BatchTime 0.119305   LR 0.001000
INFO - Training [58][  380/  391]   Loss 0.081728   Top1 97.066201   Top5 99.995888   BatchTime 0.119521   LR 0.001000
INFO - ==> Top1: 97.060    Top5: 99.996    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [58][   20/   79]   Loss 0.378233   Top1 90.390625   Top5 99.570312   BatchTime 0.151036
INFO - Validation [58][   40/   79]   Loss 0.369630   Top1 90.351562   Top5 99.609375   BatchTime 0.107520
INFO - Validation [58][   60/   79]   Loss 0.369681   Top1 90.442708   Top5 99.622396   BatchTime 0.093359
INFO - ==> Top1: 90.320    Top5: 99.650    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  59
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [59][   20/  391]   Loss 0.063598   Top1 97.773438   Top5 100.000000   BatchTime 0.196296   LR 0.001000
INFO - Training [59][   40/  391]   Loss 0.068965   Top1 97.558594   Top5 100.000000   BatchTime 0.140421   LR 0.001000
INFO - Training [59][   60/  391]   Loss 0.071808   Top1 97.447917   Top5 100.000000   BatchTime 0.126468   LR 0.001000
INFO - Training [59][   80/  391]   Loss 0.072041   Top1 97.412109   Top5 100.000000   BatchTime 0.113367   LR 0.001000
INFO - Training [59][  100/  391]   Loss 0.075558   Top1 97.242188   Top5 100.000000   BatchTime 0.114430   LR 0.001000
INFO - Training [59][  120/  391]   Loss 0.074017   Top1 97.304688   Top5 99.993490   BatchTime 0.115957   LR 0.001000
INFO - Training [59][  140/  391]   Loss 0.073932   Top1 97.349330   Top5 99.988839   BatchTime 0.117148   LR 0.001000
INFO - Training [59][  160/  391]   Loss 0.074570   Top1 97.329102   Top5 99.990234   BatchTime 0.117981   LR 0.001000
INFO - Training [59][  180/  391]   Loss 0.075723   Top1 97.282986   Top5 99.991319   BatchTime 0.118739   LR 0.001000
INFO - Training [59][  200/  391]   Loss 0.076764   Top1 97.242188   Top5 99.992188   BatchTime 0.119276   LR 0.001000
INFO - Training [59][  220/  391]   Loss 0.077282   Top1 97.215909   Top5 99.989347   BatchTime 0.119688   LR 0.001000
INFO - Training [59][  240/  391]   Loss 0.077539   Top1 97.223307   Top5 99.990234   BatchTime 0.119995   LR 0.001000
INFO - Training [59][  260/  391]   Loss 0.077674   Top1 97.232572   Top5 99.990986   BatchTime 0.120286   LR 0.001000
INFO - Training [59][  280/  391]   Loss 0.078302   Top1 97.218192   Top5 99.991629   BatchTime 0.120491   LR 0.001000
INFO - Training [59][  300/  391]   Loss 0.078030   Top1 97.229167   Top5 99.992188   BatchTime 0.118203   LR 0.001000
INFO - Training [59][  320/  391]   Loss 0.079547   Top1 97.199707   Top5 99.990234   BatchTime 0.116359   LR 0.001000
INFO - Training [59][  340/  391]   Loss 0.080592   Top1 97.162224   Top5 99.990809   BatchTime 0.114743   LR 0.001000
INFO - Training [59][  360/  391]   Loss 0.081147   Top1 97.131076   Top5 99.991319   BatchTime 0.112886   LR 0.001000
INFO - Training [59][  380/  391]   Loss 0.081025   Top1 97.134046   Top5 99.989720   BatchTime 0.113379   LR 0.001000
INFO - ==> Top1: 97.134    Top5: 99.990    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [59][   20/   79]   Loss 0.377560   Top1 89.843750   Top5 99.765625   BatchTime 0.153370
INFO - Validation [59][   40/   79]   Loss 0.371205   Top1 89.824219   Top5 99.628906   BatchTime 0.108136
INFO - Validation [59][   60/   79]   Loss 0.375161   Top1 89.843750   Top5 99.622396   BatchTime 0.093707
INFO - ==> Top1: 89.860    Top5: 99.640    Loss: 0.370
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  60
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [60][   20/  391]   Loss 0.080844   Top1 97.070312   Top5 100.000000   BatchTime 0.213425   LR 0.000100
INFO - Training [60][   40/  391]   Loss 0.080073   Top1 97.226562   Top5 100.000000   BatchTime 0.168155   LR 0.000100
INFO - Training [60][   60/  391]   Loss 0.079237   Top1 97.239583   Top5 100.000000   BatchTime 0.153246   LR 0.000100
INFO - Training [60][   80/  391]   Loss 0.079371   Top1 97.138672   Top5 100.000000   BatchTime 0.142314   LR 0.000100
INFO - Training [60][  100/  391]   Loss 0.082930   Top1 97.000000   Top5 100.000000   BatchTime 0.130586   LR 0.000100
INFO - Training [60][  120/  391]   Loss 0.081813   Top1 97.031250   Top5 100.000000   BatchTime 0.123502   LR 0.000100
INFO - Training [60][  140/  391]   Loss 0.081583   Top1 97.003348   Top5 100.000000   BatchTime 0.117991   LR 0.000100
INFO - Training [60][  160/  391]   Loss 0.080814   Top1 97.031250   Top5 100.000000   BatchTime 0.116925   LR 0.000100
INFO - Training [60][  180/  391]   Loss 0.080198   Top1 97.052951   Top5 100.000000   BatchTime 0.117672   LR 0.000100
INFO - Training [60][  200/  391]   Loss 0.080154   Top1 97.023438   Top5 100.000000   BatchTime 0.118299   LR 0.000100
INFO - Training [60][  220/  391]   Loss 0.080030   Top1 97.049006   Top5 100.000000   BatchTime 0.118880   LR 0.000100
INFO - Training [60][  240/  391]   Loss 0.079437   Top1 97.096354   Top5 100.000000   BatchTime 0.119318   LR 0.000100
INFO - Training [60][  260/  391]   Loss 0.080401   Top1 97.064303   Top5 99.996995   BatchTime 0.119801   LR 0.000100
INFO - Training [60][  280/  391]   Loss 0.080618   Top1 97.059152   Top5 99.997210   BatchTime 0.120419   LR 0.000100
INFO - Training [60][  300/  391]   Loss 0.081036   Top1 97.041667   Top5 99.997396   BatchTime 0.120646   LR 0.000100
INFO - Training [60][  320/  391]   Loss 0.080809   Top1 97.038574   Top5 99.997559   BatchTime 0.120820   LR 0.000100
INFO - Training [60][  340/  391]   Loss 0.080856   Top1 97.056526   Top5 99.997702   BatchTime 0.120973   LR 0.000100
INFO - Training [60][  360/  391]   Loss 0.080953   Top1 97.059462   Top5 99.997830   BatchTime 0.119438   LR 0.000100
INFO - Training [60][  380/  391]   Loss 0.080785   Top1 97.080592   Top5 99.997944   BatchTime 0.117760   LR 0.000100
INFO - ==> Top1: 97.066    Top5: 99.998    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [60][   20/   79]   Loss 0.390363   Top1 90.156250   Top5 99.687500   BatchTime 0.141604
INFO - Validation [60][   40/   79]   Loss 0.375577   Top1 90.136719   Top5 99.667969   BatchTime 0.102520
INFO - Validation [60][   60/   79]   Loss 0.374701   Top1 90.247396   Top5 99.674479   BatchTime 0.089527
INFO - ==> Top1: 90.130    Top5: 99.680    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  61
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [61][   20/  391]   Loss 0.080440   Top1 97.304688   Top5 100.000000   BatchTime 0.211301   LR 0.000100
INFO - Training [61][   40/  391]   Loss 0.081057   Top1 97.265625   Top5 100.000000   BatchTime 0.167758   LR 0.000100
INFO - Training [61][   60/  391]   Loss 0.078499   Top1 97.356771   Top5 100.000000   BatchTime 0.153205   LR 0.000100
INFO - Training [61][   80/  391]   Loss 0.077634   Top1 97.343750   Top5 100.000000   BatchTime 0.145732   LR 0.000100
INFO - Training [61][  100/  391]   Loss 0.076300   Top1 97.343750   Top5 100.000000   BatchTime 0.141143   LR 0.000100
INFO - Training [61][  120/  391]   Loss 0.077661   Top1 97.291667   Top5 99.993490   BatchTime 0.138163   LR 0.000100
INFO - Training [61][  140/  391]   Loss 0.077174   Top1 97.327009   Top5 99.994420   BatchTime 0.134325   LR 0.000100
INFO - Training [61][  160/  391]   Loss 0.077138   Top1 97.358398   Top5 99.990234   BatchTime 0.127515   LR 0.000100
INFO - Training [61][  180/  391]   Loss 0.078964   Top1 97.287326   Top5 99.991319   BatchTime 0.123482   LR 0.000100
INFO - Training [61][  200/  391]   Loss 0.079735   Top1 97.246094   Top5 99.988281   BatchTime 0.119439   LR 0.000100
INFO - Training [61][  220/  391]   Loss 0.079913   Top1 97.244318   Top5 99.989347   BatchTime 0.118174   LR 0.000100
INFO - Training [61][  240/  391]   Loss 0.079049   Top1 97.285156   Top5 99.990234   BatchTime 0.118649   LR 0.000100
INFO - Training [61][  260/  391]   Loss 0.079481   Top1 97.253606   Top5 99.990986   BatchTime 0.119157   LR 0.000100
INFO - Training [61][  280/  391]   Loss 0.080098   Top1 97.215402   Top5 99.991629   BatchTime 0.119479   LR 0.000100
INFO - Training [61][  300/  391]   Loss 0.080288   Top1 97.205729   Top5 99.992188   BatchTime 0.119766   LR 0.000100
INFO - Training [61][  320/  391]   Loss 0.080170   Top1 97.216797   Top5 99.990234   BatchTime 0.119989   LR 0.000100
INFO - Training [61][  340/  391]   Loss 0.080332   Top1 97.217371   Top5 99.986213   BatchTime 0.120219   LR 0.000100
INFO - Training [61][  360/  391]   Loss 0.080568   Top1 97.211372   Top5 99.986979   BatchTime 0.120353   LR 0.000100
INFO - Training [61][  380/  391]   Loss 0.079823   Top1 97.234786   Top5 99.987664   BatchTime 0.120469   LR 0.000100
INFO - ==> Top1: 97.228    Top5: 99.988    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [61][   20/   79]   Loss 0.393679   Top1 90.234375   Top5 99.609375   BatchTime 0.121471
INFO - Validation [61][   40/   79]   Loss 0.378171   Top1 90.117188   Top5 99.648438   BatchTime 0.073857
INFO - Validation [61][   60/   79]   Loss 0.377196   Top1 90.182292   Top5 99.674479   BatchTime 0.061101
INFO - ==> Top1: 90.150    Top5: 99.690    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  62
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [62][   20/  391]   Loss 0.079343   Top1 97.304688   Top5 100.000000   BatchTime 0.210148   LR 0.000100
INFO - Training [62][   40/  391]   Loss 0.078496   Top1 97.363281   Top5 100.000000   BatchTime 0.166962   LR 0.000100
INFO - Training [62][   60/  391]   Loss 0.076010   Top1 97.343750   Top5 100.000000   BatchTime 0.152643   LR 0.000100
INFO - Training [62][   80/  391]   Loss 0.078898   Top1 97.265625   Top5 99.980469   BatchTime 0.145309   LR 0.000100
INFO - Training [62][  100/  391]   Loss 0.079458   Top1 97.257812   Top5 99.984375   BatchTime 0.141086   LR 0.000100
INFO - Training [62][  120/  391]   Loss 0.080336   Top1 97.167969   Top5 99.986979   BatchTime 0.138214   LR 0.000100
INFO - Training [62][  140/  391]   Loss 0.080481   Top1 97.126116   Top5 99.988839   BatchTime 0.136246   LR 0.000100
INFO - Training [62][  160/  391]   Loss 0.079898   Top1 97.158203   Top5 99.990234   BatchTime 0.134648   LR 0.000100
INFO - Training [62][  180/  391]   Loss 0.079444   Top1 97.174479   Top5 99.991319   BatchTime 0.133290   LR 0.000100
INFO - Training [62][  200/  391]   Loss 0.080044   Top1 97.128906   Top5 99.992188   BatchTime 0.132301   LR 0.000100
INFO - Training [62][  220/  391]   Loss 0.080256   Top1 97.084517   Top5 99.989347   BatchTime 0.129121   LR 0.000100
INFO - Training [62][  240/  391]   Loss 0.080538   Top1 97.073568   Top5 99.990234   BatchTime 0.125346   LR 0.000100
INFO - Training [62][  260/  391]   Loss 0.079805   Top1 97.130409   Top5 99.987981   BatchTime 0.122852   LR 0.000100
INFO - Training [62][  280/  391]   Loss 0.079471   Top1 97.173549   Top5 99.988839   BatchTime 0.119653   LR 0.000100
INFO - Training [62][  300/  391]   Loss 0.079290   Top1 97.182292   Top5 99.984375   BatchTime 0.119186   LR 0.000100
INFO - Training [62][  320/  391]   Loss 0.079249   Top1 97.199707   Top5 99.985352   BatchTime 0.119435   LR 0.000100
INFO - Training [62][  340/  391]   Loss 0.079377   Top1 97.203585   Top5 99.981618   BatchTime 0.119669   LR 0.000100
INFO - Training [62][  360/  391]   Loss 0.078865   Top1 97.220052   Top5 99.982639   BatchTime 0.120091   LR 0.000100
INFO - Training [62][  380/  391]   Loss 0.078590   Top1 97.214227   Top5 99.983553   BatchTime 0.120238   LR 0.000100
INFO - ==> Top1: 97.184    Top5: 99.982    Loss: 0.079
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [62][   20/   79]   Loss 0.382397   Top1 89.882812   Top5 99.570312   BatchTime 0.152211
INFO - Validation [62][   40/   79]   Loss 0.372805   Top1 89.882812   Top5 99.589844   BatchTime 0.108149
INFO - Validation [62][   60/   79]   Loss 0.372309   Top1 90.117188   Top5 99.596354   BatchTime 0.093740
INFO - ==> Top1: 90.060    Top5: 99.620    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  63
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [63][   20/  391]   Loss 0.081276   Top1 97.031250   Top5 100.000000   BatchTime 0.177537   LR 0.000100
INFO - Training [63][   40/  391]   Loss 0.079654   Top1 97.246094   Top5 100.000000   BatchTime 0.134060   LR 0.000100
INFO - Training [63][   60/  391]   Loss 0.077839   Top1 97.252604   Top5 100.000000   BatchTime 0.118773   LR 0.000100
INFO - Training [63][   80/  391]   Loss 0.080545   Top1 97.138672   Top5 100.000000   BatchTime 0.116482   LR 0.000100
INFO - Training [63][  100/  391]   Loss 0.078370   Top1 97.250000   Top5 100.000000   BatchTime 0.117902   LR 0.000100
INFO - Training [63][  120/  391]   Loss 0.077561   Top1 97.246094   Top5 100.000000   BatchTime 0.118859   LR 0.000100
INFO - Training [63][  140/  391]   Loss 0.077569   Top1 97.254464   Top5 100.000000   BatchTime 0.119541   LR 0.000100
INFO - Training [63][  160/  391]   Loss 0.077276   Top1 97.265625   Top5 100.000000   BatchTime 0.120011   LR 0.000100
INFO - Training [63][  180/  391]   Loss 0.077854   Top1 97.256944   Top5 99.991319   BatchTime 0.120384   LR 0.000100
INFO - Training [63][  200/  391]   Loss 0.078015   Top1 97.242188   Top5 99.992188   BatchTime 0.120743   LR 0.000100
INFO - Training [63][  220/  391]   Loss 0.078772   Top1 97.201705   Top5 99.992898   BatchTime 0.120946   LR 0.000100
INFO - Training [63][  240/  391]   Loss 0.078895   Top1 97.223307   Top5 99.993490   BatchTime 0.121119   LR 0.000100
INFO - Training [63][  260/  391]   Loss 0.079461   Top1 97.172476   Top5 99.993990   BatchTime 0.121213   LR 0.000100
INFO - Training [63][  280/  391]   Loss 0.079724   Top1 97.145647   Top5 99.994420   BatchTime 0.119384   LR 0.000100
INFO - Training [63][  300/  391]   Loss 0.079278   Top1 97.169271   Top5 99.992188   BatchTime 0.117229   LR 0.000100
INFO - Training [63][  320/  391]   Loss 0.079421   Top1 97.172852   Top5 99.992676   BatchTime 0.115768   LR 0.000100
INFO - Training [63][  340/  391]   Loss 0.080013   Top1 97.164522   Top5 99.993107   BatchTime 0.113201   LR 0.000100
INFO - Training [63][  360/  391]   Loss 0.080217   Top1 97.152778   Top5 99.991319   BatchTime 0.113805   LR 0.000100
INFO - Training [63][  380/  391]   Loss 0.080538   Top1 97.150493   Top5 99.991776   BatchTime 0.114334   LR 0.000100
INFO - ==> Top1: 97.146    Top5: 99.990    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [63][   20/   79]   Loss 0.383930   Top1 89.843750   Top5 99.726562   BatchTime 0.151742
INFO - Validation [63][   40/   79]   Loss 0.375178   Top1 89.707031   Top5 99.707031   BatchTime 0.108470
INFO - Validation [63][   60/   79]   Loss 0.376879   Top1 89.791667   Top5 99.713542   BatchTime 0.093420
INFO - ==> Top1: 89.860    Top5: 99.720    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  64
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [64][   20/  391]   Loss 0.074258   Top1 97.148438   Top5 100.000000   BatchTime 0.211681   LR 0.000100
INFO - Training [64][   40/  391]   Loss 0.072681   Top1 97.480469   Top5 100.000000   BatchTime 0.167808   LR 0.000100
INFO - Training [64][   60/  391]   Loss 0.074604   Top1 97.395833   Top5 100.000000   BatchTime 0.149879   LR 0.000100
INFO - Training [64][   80/  391]   Loss 0.074684   Top1 97.402344   Top5 100.000000   BatchTime 0.132019   LR 0.000100
INFO - Training [64][  100/  391]   Loss 0.074717   Top1 97.367188   Top5 100.000000   BatchTime 0.123525   LR 0.000100
INFO - Training [64][  120/  391]   Loss 0.074408   Top1 97.408854   Top5 99.993490   BatchTime 0.116878   LR 0.000100
INFO - Training [64][  140/  391]   Loss 0.074326   Top1 97.410714   Top5 99.988839   BatchTime 0.115163   LR 0.000100
INFO - Training [64][  160/  391]   Loss 0.076026   Top1 97.343750   Top5 99.990234   BatchTime 0.116257   LR 0.000100
INFO - Training [64][  180/  391]   Loss 0.076680   Top1 97.335069   Top5 99.991319   BatchTime 0.117273   LR 0.000100
INFO - Training [64][  200/  391]   Loss 0.077430   Top1 97.312500   Top5 99.992188   BatchTime 0.117945   LR 0.000100
INFO - Training [64][  220/  391]   Loss 0.077605   Top1 97.315341   Top5 99.992898   BatchTime 0.118540   LR 0.000100
INFO - Training [64][  240/  391]   Loss 0.077701   Top1 97.304688   Top5 99.993490   BatchTime 0.118598   LR 0.000100
INFO - Training [64][  260/  391]   Loss 0.078103   Top1 97.289663   Top5 99.993990   BatchTime 0.119066   LR 0.000100
INFO - Training [64][  280/  391]   Loss 0.078276   Top1 97.273996   Top5 99.991629   BatchTime 0.119400   LR 0.000100
INFO - Training [64][  300/  391]   Loss 0.078030   Top1 97.276042   Top5 99.992188   BatchTime 0.119669   LR 0.000100
INFO - Training [64][  320/  391]   Loss 0.078814   Top1 97.241211   Top5 99.990234   BatchTime 0.119874   LR 0.000100
INFO - Training [64][  340/  391]   Loss 0.078746   Top1 97.235754   Top5 99.990809   BatchTime 0.118876   LR 0.000100
INFO - Training [64][  360/  391]   Loss 0.078778   Top1 97.239583   Top5 99.991319   BatchTime 0.116625   LR 0.000100
INFO - Training [64][  380/  391]   Loss 0.079019   Top1 97.228618   Top5 99.991776   BatchTime 0.115013   LR 0.000100
INFO - ==> Top1: 97.246    Top5: 99.992    Loss: 0.079
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [64][   20/   79]   Loss 0.383450   Top1 90.195312   Top5 99.609375   BatchTime 0.159942
INFO - Validation [64][   40/   79]   Loss 0.373171   Top1 90.156250   Top5 99.667969   BatchTime 0.112500
INFO - Validation [64][   60/   79]   Loss 0.374644   Top1 90.195312   Top5 99.674479   BatchTime 0.096689
INFO - ==> Top1: 90.220    Top5: 99.680    Loss: 0.370
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  65
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [65][   20/  391]   Loss 0.068163   Top1 97.578125   Top5 99.960938   BatchTime 0.220396   LR 0.000100
INFO - Training [65][   40/  391]   Loss 0.071983   Top1 97.441406   Top5 99.960938   BatchTime 0.172474   LR 0.000100
INFO - Training [65][   60/  391]   Loss 0.070201   Top1 97.447917   Top5 99.973958   BatchTime 0.156309   LR 0.000100
INFO - Training [65][   80/  391]   Loss 0.072462   Top1 97.421875   Top5 99.980469   BatchTime 0.148016   LR 0.000100
INFO - Training [65][  100/  391]   Loss 0.074001   Top1 97.320312   Top5 99.984375   BatchTime 0.142918   LR 0.000100
INFO - Training [65][  120/  391]   Loss 0.074493   Top1 97.298177   Top5 99.986979   BatchTime 0.139860   LR 0.000100
INFO - Training [65][  140/  391]   Loss 0.073401   Top1 97.343750   Top5 99.988839   BatchTime 0.130875   LR 0.000100
INFO - Training [65][  160/  391]   Loss 0.073238   Top1 97.348633   Top5 99.985352   BatchTime 0.126274   LR 0.000100
INFO - Training [65][  180/  391]   Loss 0.073964   Top1 97.322049   Top5 99.986979   BatchTime 0.122102   LR 0.000100
INFO - Training [65][  200/  391]   Loss 0.074382   Top1 97.320312   Top5 99.988281   BatchTime 0.119944   LR 0.000100
INFO - Training [65][  220/  391]   Loss 0.074785   Top1 97.311790   Top5 99.985795   BatchTime 0.120296   LR 0.000100
INFO - Training [65][  240/  391]   Loss 0.074199   Top1 97.324219   Top5 99.980469   BatchTime 0.120570   LR 0.000100
INFO - Training [65][  260/  391]   Loss 0.074241   Top1 97.325721   Top5 99.981971   BatchTime 0.120850   LR 0.000100
INFO - Training [65][  280/  391]   Loss 0.074952   Top1 97.310268   Top5 99.983259   BatchTime 0.121018   LR 0.000100
INFO - Training [65][  300/  391]   Loss 0.074741   Top1 97.330729   Top5 99.984375   BatchTime 0.121180   LR 0.000100
INFO - Training [65][  320/  391]   Loss 0.074793   Top1 97.319336   Top5 99.982910   BatchTime 0.121333   LR 0.000100
INFO - Training [65][  340/  391]   Loss 0.075521   Top1 97.281710   Top5 99.983915   BatchTime 0.121440   LR 0.000100
INFO - Training [65][  360/  391]   Loss 0.075718   Top1 97.274306   Top5 99.984809   BatchTime 0.121518   LR 0.000100
INFO - Training [65][  380/  391]   Loss 0.075943   Top1 97.263569   Top5 99.983553   BatchTime 0.121585   LR 0.000100
INFO - ==> Top1: 97.278    Top5: 99.984    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [65][   20/   79]   Loss 0.380183   Top1 90.156250   Top5 99.687500   BatchTime 0.134805
INFO - Validation [65][   40/   79]   Loss 0.370569   Top1 90.156250   Top5 99.687500   BatchTime 0.087510
INFO - Validation [65][   60/   79]   Loss 0.369967   Top1 90.182292   Top5 99.674479   BatchTime 0.068644
INFO - ==> Top1: 90.140    Top5: 99.680    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  66
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [66][   20/  391]   Loss 0.072437   Top1 97.187500   Top5 100.000000   BatchTime 0.218334   LR 0.000100
INFO - Training [66][   40/  391]   Loss 0.072450   Top1 97.382812   Top5 100.000000   BatchTime 0.170949   LR 0.000100
INFO - Training [66][   60/  391]   Loss 0.073090   Top1 97.460938   Top5 100.000000   BatchTime 0.155274   LR 0.000100
INFO - Training [66][   80/  391]   Loss 0.076139   Top1 97.343750   Top5 99.980469   BatchTime 0.147340   LR 0.000100
INFO - Training [66][  100/  391]   Loss 0.075416   Top1 97.328125   Top5 99.984375   BatchTime 0.142444   LR 0.000100
INFO - Training [66][  120/  391]   Loss 0.076249   Top1 97.278646   Top5 99.986979   BatchTime 0.139294   LR 0.000100
INFO - Training [66][  140/  391]   Loss 0.076462   Top1 97.276786   Top5 99.988839   BatchTime 0.137001   LR 0.000100
INFO - Training [66][  160/  391]   Loss 0.076284   Top1 97.309570   Top5 99.990234   BatchTime 0.135267   LR 0.000100
INFO - Training [66][  180/  391]   Loss 0.076169   Top1 97.309028   Top5 99.991319   BatchTime 0.133912   LR 0.000100
INFO - Training [66][  200/  391]   Loss 0.076176   Top1 97.281250   Top5 99.992188   BatchTime 0.130176   LR 0.000100
INFO - Training [66][  220/  391]   Loss 0.077601   Top1 97.247869   Top5 99.992898   BatchTime 0.126262   LR 0.000100
INFO - Training [66][  240/  391]   Loss 0.077245   Top1 97.255859   Top5 99.993490   BatchTime 0.123536   LR 0.000100
INFO - Training [66][  260/  391]   Loss 0.077181   Top1 97.247596   Top5 99.993990   BatchTime 0.119734   LR 0.000100
INFO - Training [66][  280/  391]   Loss 0.076243   Top1 97.271205   Top5 99.994420   BatchTime 0.119919   LR 0.000100
INFO - Training [66][  300/  391]   Loss 0.076483   Top1 97.283854   Top5 99.994792   BatchTime 0.120167   LR 0.000100
INFO - Training [66][  320/  391]   Loss 0.076266   Top1 97.287598   Top5 99.995117   BatchTime 0.120413   LR 0.000100
INFO - Training [66][  340/  391]   Loss 0.076017   Top1 97.300092   Top5 99.995404   BatchTime 0.120588   LR 0.000100
INFO - Training [66][  360/  391]   Loss 0.076738   Top1 97.278646   Top5 99.995660   BatchTime 0.120752   LR 0.000100
INFO - Training [66][  380/  391]   Loss 0.076921   Top1 97.282072   Top5 99.993832   BatchTime 0.120847   LR 0.000100
INFO - ==> Top1: 97.288    Top5: 99.994    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [66][   20/   79]   Loss 0.386146   Top1 90.117188   Top5 99.687500   BatchTime 0.150436
INFO - Validation [66][   40/   79]   Loss 0.373129   Top1 90.078125   Top5 99.687500   BatchTime 0.107597
INFO - Validation [66][   60/   79]   Loss 0.373458   Top1 90.247396   Top5 99.687500   BatchTime 0.093547
INFO - ==> Top1: 90.190    Top5: 99.700    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  67
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [67][   20/  391]   Loss 0.064431   Top1 98.125000   Top5 100.000000   BatchTime 0.183085   LR 0.000100
INFO - Training [67][   40/  391]   Loss 0.073987   Top1 97.421875   Top5 99.960938   BatchTime 0.134371   LR 0.000100
INFO - Training [67][   60/  391]   Loss 0.074035   Top1 97.473958   Top5 99.973958   BatchTime 0.126107   LR 0.000100
INFO - Training [67][   80/  391]   Loss 0.074769   Top1 97.412109   Top5 99.980469   BatchTime 0.125708   LR 0.000100
INFO - Training [67][  100/  391]   Loss 0.075526   Top1 97.390625   Top5 99.976562   BatchTime 0.125265   LR 0.000100
INFO - Training [67][  120/  391]   Loss 0.074786   Top1 97.395833   Top5 99.980469   BatchTime 0.124982   LR 0.000100
INFO - Training [67][  140/  391]   Loss 0.075190   Top1 97.421875   Top5 99.983259   BatchTime 0.124774   LR 0.000100
INFO - Training [67][  160/  391]   Loss 0.075151   Top1 97.441406   Top5 99.980469   BatchTime 0.124613   LR 0.000100
INFO - Training [67][  180/  391]   Loss 0.076082   Top1 97.395833   Top5 99.982639   BatchTime 0.124571   LR 0.000100
INFO - Training [67][  200/  391]   Loss 0.077558   Top1 97.339844   Top5 99.984375   BatchTime 0.124376   LR 0.000100
INFO - Training [67][  220/  391]   Loss 0.076723   Top1 97.397017   Top5 99.985795   BatchTime 0.124316   LR 0.000100
INFO - Training [67][  240/  391]   Loss 0.076571   Top1 97.399089   Top5 99.986979   BatchTime 0.124181   LR 0.000100
INFO - Training [67][  260/  391]   Loss 0.075808   Top1 97.385817   Top5 99.987981   BatchTime 0.122319   LR 0.000100
INFO - Training [67][  280/  391]   Loss 0.075405   Top1 97.393973   Top5 99.986049   BatchTime 0.119557   LR 0.000100
INFO - Training [67][  300/  391]   Loss 0.074885   Top1 97.401042   Top5 99.986979   BatchTime 0.117755   LR 0.000100
INFO - Training [67][  320/  391]   Loss 0.074584   Top1 97.390137   Top5 99.987793   BatchTime 0.115043   LR 0.000100
INFO - Training [67][  340/  391]   Loss 0.075443   Top1 97.341452   Top5 99.988511   BatchTime 0.115403   LR 0.000100
INFO - Training [67][  360/  391]   Loss 0.075576   Top1 97.328559   Top5 99.989149   BatchTime 0.115824   LR 0.000100
INFO - Training [67][  380/  391]   Loss 0.075493   Top1 97.329359   Top5 99.989720   BatchTime 0.116221   LR 0.000100
INFO - ==> Top1: 97.314    Top5: 99.988    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [67][   20/   79]   Loss 0.385074   Top1 90.195312   Top5 99.648438   BatchTime 0.155287
INFO - Validation [67][   40/   79]   Loss 0.373943   Top1 90.117188   Top5 99.628906   BatchTime 0.109216
INFO - Validation [67][   60/   79]   Loss 0.373117   Top1 90.286458   Top5 99.648438   BatchTime 0.093685
INFO - ==> Top1: 90.210    Top5: 99.660    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  68
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [68][   20/  391]   Loss 0.080468   Top1 96.914062   Top5 100.000000   BatchTime 0.211866   LR 0.000100
INFO - Training [68][   40/  391]   Loss 0.071649   Top1 97.304688   Top5 100.000000   BatchTime 0.164774   LR 0.000100
INFO - Training [68][   60/  391]   Loss 0.073549   Top1 97.317708   Top5 100.000000   BatchTime 0.135414   LR 0.000100
INFO - Training [68][   80/  391]   Loss 0.075211   Top1 97.246094   Top5 99.990234   BatchTime 0.124786   LR 0.000100
INFO - Training [68][  100/  391]   Loss 0.077258   Top1 97.281250   Top5 99.984375   BatchTime 0.117027   LR 0.000100
INFO - Training [68][  120/  391]   Loss 0.077501   Top1 97.246094   Top5 99.980469   BatchTime 0.114621   LR 0.000100
INFO - Training [68][  140/  391]   Loss 0.075801   Top1 97.332589   Top5 99.983259   BatchTime 0.115914   LR 0.000100
INFO - Training [68][  160/  391]   Loss 0.076271   Top1 97.329102   Top5 99.970703   BatchTime 0.116900   LR 0.000100
INFO - Training [68][  180/  391]   Loss 0.076693   Top1 97.322049   Top5 99.973958   BatchTime 0.117666   LR 0.000100
INFO - Training [68][  200/  391]   Loss 0.075970   Top1 97.355469   Top5 99.972656   BatchTime 0.118239   LR 0.000100
INFO - Training [68][  220/  391]   Loss 0.076862   Top1 97.322443   Top5 99.975142   BatchTime 0.118714   LR 0.000100
INFO - Training [68][  240/  391]   Loss 0.075898   Top1 97.347005   Top5 99.977214   BatchTime 0.119178   LR 0.000100
INFO - Training [68][  260/  391]   Loss 0.076725   Top1 97.298678   Top5 99.975962   BatchTime 0.119487   LR 0.000100
INFO - Training [68][  280/  391]   Loss 0.077457   Top1 97.293527   Top5 99.974888   BatchTime 0.119752   LR 0.000100
INFO - Training [68][  300/  391]   Loss 0.076785   Top1 97.309896   Top5 99.976562   BatchTime 0.119948   LR 0.000100
INFO - Training [68][  320/  391]   Loss 0.076954   Top1 97.307129   Top5 99.978027   BatchTime 0.118962   LR 0.000100
INFO - Training [68][  340/  391]   Loss 0.077700   Top1 97.286305   Top5 99.974724   BatchTime 0.116854   LR 0.000100
INFO - Training [68][  360/  391]   Loss 0.078108   Top1 97.269965   Top5 99.976128   BatchTime 0.115215   LR 0.000100
INFO - Training [68][  380/  391]   Loss 0.078689   Top1 97.228618   Top5 99.977385   BatchTime 0.113314   LR 0.000100
INFO - ==> Top1: 97.238    Top5: 99.978    Loss: 0.079
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [68][   20/   79]   Loss 0.385831   Top1 90.234375   Top5 99.648438   BatchTime 0.149995
INFO - Validation [68][   40/   79]   Loss 0.368856   Top1 90.214844   Top5 99.687500   BatchTime 0.107869
INFO - Validation [68][   60/   79]   Loss 0.369099   Top1 90.286458   Top5 99.687500   BatchTime 0.093552
INFO - ==> Top1: 90.200    Top5: 99.680    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  69
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [69][   20/  391]   Loss 0.071408   Top1 97.304688   Top5 100.000000   BatchTime 0.210244   LR 0.000100
INFO - Training [69][   40/  391]   Loss 0.073652   Top1 97.343750   Top5 99.980469   BatchTime 0.167148   LR 0.000100
INFO - Training [69][   60/  391]   Loss 0.072069   Top1 97.369792   Top5 99.960938   BatchTime 0.152328   LR 0.000100
INFO - Training [69][   80/  391]   Loss 0.073689   Top1 97.294922   Top5 99.970703   BatchTime 0.145241   LR 0.000100
INFO - Training [69][  100/  391]   Loss 0.070277   Top1 97.437500   Top5 99.976562   BatchTime 0.141170   LR 0.000100
INFO - Training [69][  120/  391]   Loss 0.070901   Top1 97.500000   Top5 99.980469   BatchTime 0.130125   LR 0.000100
INFO - Training [69][  140/  391]   Loss 0.072370   Top1 97.444196   Top5 99.977679   BatchTime 0.125768   LR 0.000100
INFO - Training [69][  160/  391]   Loss 0.072415   Top1 97.416992   Top5 99.980469   BatchTime 0.120895   LR 0.000100
INFO - Training [69][  180/  391]   Loss 0.072147   Top1 97.421875   Top5 99.982639   BatchTime 0.119223   LR 0.000100
INFO - Training [69][  200/  391]   Loss 0.071999   Top1 97.398438   Top5 99.984375   BatchTime 0.119520   LR 0.000100
INFO - Training [69][  220/  391]   Loss 0.072492   Top1 97.386364   Top5 99.982244   BatchTime 0.119880   LR 0.000100
INFO - Training [69][  240/  391]   Loss 0.072504   Top1 97.412109   Top5 99.977214   BatchTime 0.120276   LR 0.000100
INFO - Training [69][  260/  391]   Loss 0.073344   Top1 97.424880   Top5 99.975962   BatchTime 0.120544   LR 0.000100
INFO - Training [69][  280/  391]   Loss 0.073925   Top1 97.413504   Top5 99.977679   BatchTime 0.120869   LR 0.000100
INFO - Training [69][  300/  391]   Loss 0.074156   Top1 97.414062   Top5 99.973958   BatchTime 0.121091   LR 0.000100
INFO - Training [69][  320/  391]   Loss 0.074507   Top1 97.404785   Top5 99.975586   BatchTime 0.121238   LR 0.000100
INFO - Training [69][  340/  391]   Loss 0.074411   Top1 97.414982   Top5 99.977022   BatchTime 0.121339   LR 0.000100
INFO - Training [69][  360/  391]   Loss 0.074444   Top1 97.400174   Top5 99.978299   BatchTime 0.121422   LR 0.000100
INFO - Training [69][  380/  391]   Loss 0.074526   Top1 97.399260   Top5 99.975329   BatchTime 0.120053   LR 0.000100
INFO - ==> Top1: 97.402    Top5: 99.974    Loss: 0.075
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [69][   20/   79]   Loss 0.377806   Top1 90.468750   Top5 99.531250   BatchTime 0.127460
INFO - Validation [69][   40/   79]   Loss 0.372334   Top1 90.234375   Top5 99.589844   BatchTime 0.076903
INFO - Validation [69][   60/   79]   Loss 0.372233   Top1 90.247396   Top5 99.609375   BatchTime 0.060041
INFO - ==> Top1: 90.150    Top5: 99.630    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  70
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [70][   20/  391]   Loss 0.077122   Top1 97.226562   Top5 100.000000   BatchTime 0.211914   LR 0.000010
INFO - Training [70][   40/  391]   Loss 0.077971   Top1 97.304688   Top5 99.980469   BatchTime 0.167452   LR 0.000010
INFO - Training [70][   60/  391]   Loss 0.078991   Top1 97.317708   Top5 99.986979   BatchTime 0.152975   LR 0.000010
INFO - Training [70][   80/  391]   Loss 0.079210   Top1 97.285156   Top5 99.990234   BatchTime 0.145635   LR 0.000010
INFO - Training [70][  100/  391]   Loss 0.077349   Top1 97.367188   Top5 99.992188   BatchTime 0.141284   LR 0.000010
INFO - Training [70][  120/  391]   Loss 0.076466   Top1 97.376302   Top5 99.993490   BatchTime 0.138209   LR 0.000010
INFO - Training [70][  140/  391]   Loss 0.078648   Top1 97.282366   Top5 99.994420   BatchTime 0.136055   LR 0.000010
INFO - Training [70][  160/  391]   Loss 0.077357   Top1 97.290039   Top5 99.995117   BatchTime 0.134463   LR 0.000010
INFO - Training [70][  180/  391]   Loss 0.077060   Top1 97.335069   Top5 99.991319   BatchTime 0.131036   LR 0.000010
INFO - Training [70][  200/  391]   Loss 0.077107   Top1 97.316406   Top5 99.992188   BatchTime 0.126514   LR 0.000010
INFO - Training [70][  220/  391]   Loss 0.076432   Top1 97.361506   Top5 99.992898   BatchTime 0.123566   LR 0.000010
INFO - Training [70][  240/  391]   Loss 0.076289   Top1 97.382812   Top5 99.993490   BatchTime 0.119689   LR 0.000010
INFO - Training [70][  260/  391]   Loss 0.076921   Top1 97.346755   Top5 99.993990   BatchTime 0.119783   LR 0.000010
INFO - Training [70][  280/  391]   Loss 0.076825   Top1 97.338170   Top5 99.994420   BatchTime 0.120043   LR 0.000010
INFO - Training [70][  300/  391]   Loss 0.076528   Top1 97.343750   Top5 99.994792   BatchTime 0.120296   LR 0.000010
INFO - Training [70][  320/  391]   Loss 0.076595   Top1 97.333984   Top5 99.990234   BatchTime 0.120576   LR 0.000010
INFO - Training [70][  340/  391]   Loss 0.076507   Top1 97.325368   Top5 99.988511   BatchTime 0.120704   LR 0.000010
INFO - Training [70][  360/  391]   Loss 0.076422   Top1 97.341580   Top5 99.989149   BatchTime 0.120844   LR 0.000010
INFO - Training [70][  380/  391]   Loss 0.076437   Top1 97.349918   Top5 99.987664   BatchTime 0.121002   LR 0.000010
INFO - ==> Top1: 97.362    Top5: 99.986    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [70][   20/   79]   Loss 0.381192   Top1 90.078125   Top5 99.648438   BatchTime 0.151400
INFO - Validation [70][   40/   79]   Loss 0.368491   Top1 90.019531   Top5 99.687500   BatchTime 0.108369
INFO - Validation [70][   60/   79]   Loss 0.368037   Top1 90.208333   Top5 99.713542   BatchTime 0.084357
INFO - ==> Top1: 90.120    Top5: 99.720    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  71
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [71][   20/  391]   Loss 0.073597   Top1 97.148438   Top5 100.000000   BatchTime 0.181096   LR 0.000010
INFO - Training [71][   40/  391]   Loss 0.077757   Top1 97.148438   Top5 99.960938   BatchTime 0.136830   LR 0.000010
INFO - Training [71][   60/  391]   Loss 0.078720   Top1 97.161458   Top5 99.960938   BatchTime 0.132566   LR 0.000010
INFO - Training [71][   80/  391]   Loss 0.079390   Top1 97.119141   Top5 99.970703   BatchTime 0.130626   LR 0.000010
INFO - Training [71][  100/  391]   Loss 0.077207   Top1 97.210938   Top5 99.976562   BatchTime 0.129259   LR 0.000010
INFO - Training [71][  120/  391]   Loss 0.076744   Top1 97.239583   Top5 99.973958   BatchTime 0.128271   LR 0.000010
INFO - Training [71][  140/  391]   Loss 0.078656   Top1 97.165179   Top5 99.977679   BatchTime 0.127694   LR 0.000010
INFO - Training [71][  160/  391]   Loss 0.078725   Top1 97.182617   Top5 99.980469   BatchTime 0.127130   LR 0.000010
INFO - Training [71][  180/  391]   Loss 0.078611   Top1 97.217882   Top5 99.982639   BatchTime 0.127289   LR 0.000010
INFO - Training [71][  200/  391]   Loss 0.078834   Top1 97.191406   Top5 99.984375   BatchTime 0.126708   LR 0.000010
INFO - Training [71][  220/  391]   Loss 0.078295   Top1 97.208807   Top5 99.985795   BatchTime 0.126323   LR 0.000010
INFO - Training [71][  240/  391]   Loss 0.078644   Top1 97.190755   Top5 99.986979   BatchTime 0.124895   LR 0.000010
INFO - Training [71][  260/  391]   Loss 0.077799   Top1 97.214543   Top5 99.987981   BatchTime 0.121826   LR 0.000010
INFO - Training [71][  280/  391]   Loss 0.077822   Top1 97.232143   Top5 99.988839   BatchTime 0.119495   LR 0.000010
INFO - Training [71][  300/  391]   Loss 0.077859   Top1 97.242188   Top5 99.989583   BatchTime 0.117017   LR 0.000010
INFO - Training [71][  320/  391]   Loss 0.077651   Top1 97.236328   Top5 99.990234   BatchTime 0.116929   LR 0.000010
INFO - Training [71][  340/  391]   Loss 0.076981   Top1 97.263327   Top5 99.990809   BatchTime 0.117213   LR 0.000010
INFO - Training [71][  360/  391]   Loss 0.076963   Top1 97.248264   Top5 99.991319   BatchTime 0.117566   LR 0.000010
INFO - Training [71][  380/  391]   Loss 0.076795   Top1 97.255345   Top5 99.989720   BatchTime 0.117868   LR 0.000010
INFO - ==> Top1: 97.256    Top5: 99.990    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [71][   20/   79]   Loss 0.380297   Top1 90.273438   Top5 99.648438   BatchTime 0.151704
INFO - Validation [71][   40/   79]   Loss 0.370027   Top1 90.273438   Top5 99.648438   BatchTime 0.107606
INFO - Validation [71][   60/   79]   Loss 0.370327   Top1 90.273438   Top5 99.674479   BatchTime 0.093005
INFO - ==> Top1: 90.270    Top5: 99.670    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  72
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [72][   20/  391]   Loss 0.071460   Top1 97.343750   Top5 100.000000   BatchTime 0.211775   LR 0.000010
INFO - Training [72][   40/  391]   Loss 0.081756   Top1 96.933594   Top5 99.980469   BatchTime 0.148288   LR 0.000010
INFO - Training [72][   60/  391]   Loss 0.079786   Top1 97.057292   Top5 99.973958   BatchTime 0.129190   LR 0.000010
INFO - Training [72][   80/  391]   Loss 0.081119   Top1 97.031250   Top5 99.980469   BatchTime 0.119692   LR 0.000010
INFO - Training [72][  100/  391]   Loss 0.078001   Top1 97.156250   Top5 99.976562   BatchTime 0.114692   LR 0.000010
INFO - Training [72][  120/  391]   Loss 0.076651   Top1 97.239583   Top5 99.980469   BatchTime 0.116487   LR 0.000010
INFO - Training [72][  140/  391]   Loss 0.075578   Top1 97.282366   Top5 99.983259   BatchTime 0.117637   LR 0.000010
INFO - Training [72][  160/  391]   Loss 0.075614   Top1 97.333984   Top5 99.980469   BatchTime 0.118388   LR 0.000010
INFO - Training [72][  180/  391]   Loss 0.077463   Top1 97.261285   Top5 99.982639   BatchTime 0.119004   LR 0.000010
INFO - Training [72][  200/  391]   Loss 0.076485   Top1 97.320312   Top5 99.984375   BatchTime 0.119493   LR 0.000010
INFO - Training [72][  220/  391]   Loss 0.076317   Top1 97.315341   Top5 99.985795   BatchTime 0.119877   LR 0.000010
INFO - Training [72][  240/  391]   Loss 0.076055   Top1 97.314453   Top5 99.986979   BatchTime 0.120156   LR 0.000010
INFO - Training [72][  260/  391]   Loss 0.075958   Top1 97.298678   Top5 99.987981   BatchTime 0.120348   LR 0.000010
INFO - Training [72][  280/  391]   Loss 0.076659   Top1 97.265625   Top5 99.986049   BatchTime 0.120529   LR 0.000010
INFO - Training [72][  300/  391]   Loss 0.076842   Top1 97.234375   Top5 99.986979   BatchTime 0.120090   LR 0.000010
INFO - Training [72][  320/  391]   Loss 0.077190   Top1 97.214355   Top5 99.987793   BatchTime 0.117690   LR 0.000010
INFO - Training [72][  340/  391]   Loss 0.076705   Top1 97.247243   Top5 99.988511   BatchTime 0.115921   LR 0.000010
INFO - Training [72][  360/  391]   Loss 0.076896   Top1 97.256944   Top5 99.986979   BatchTime 0.114113   LR 0.000010
INFO - Training [72][  380/  391]   Loss 0.077467   Top1 97.245066   Top5 99.987664   BatchTime 0.113518   LR 0.000010
INFO - ==> Top1: 97.244    Top5: 99.988    Loss: 0.078
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [72][   20/   79]   Loss 0.385052   Top1 90.078125   Top5 99.531250   BatchTime 0.151406
INFO - Validation [72][   40/   79]   Loss 0.368696   Top1 90.117188   Top5 99.648438   BatchTime 0.107135
INFO - Validation [72][   60/   79]   Loss 0.373985   Top1 90.182292   Top5 99.648438   BatchTime 0.092545
INFO - ==> Top1: 90.050    Top5: 99.680    Loss: 0.370
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  73
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [73][   20/  391]   Loss 0.079131   Top1 97.304688   Top5 100.000000   BatchTime 0.210901   LR 0.000010
INFO - Training [73][   40/  391]   Loss 0.078595   Top1 97.304688   Top5 100.000000   BatchTime 0.166811   LR 0.000010
INFO - Training [73][   60/  391]   Loss 0.078308   Top1 97.278646   Top5 99.973958   BatchTime 0.152266   LR 0.000010
INFO - Training [73][   80/  391]   Loss 0.076148   Top1 97.353516   Top5 99.980469   BatchTime 0.144991   LR 0.000010
INFO - Training [73][  100/  391]   Loss 0.072802   Top1 97.468750   Top5 99.984375   BatchTime 0.134043   LR 0.000010
INFO - Training [73][  120/  391]   Loss 0.074605   Top1 97.395833   Top5 99.986979   BatchTime 0.126426   LR 0.000010
INFO - Training [73][  140/  391]   Loss 0.074863   Top1 97.421875   Top5 99.983259   BatchTime 0.121627   LR 0.000010
INFO - Training [73][  160/  391]   Loss 0.075972   Top1 97.382812   Top5 99.985352   BatchTime 0.117121   LR 0.000010
INFO - Training [73][  180/  391]   Loss 0.076532   Top1 97.339410   Top5 99.982639   BatchTime 0.118106   LR 0.000010
INFO - Training [73][  200/  391]   Loss 0.076504   Top1 97.335938   Top5 99.984375   BatchTime 0.118663   LR 0.000010
INFO - Training [73][  220/  391]   Loss 0.076952   Top1 97.308239   Top5 99.985795   BatchTime 0.119122   LR 0.000010
INFO - Training [73][  240/  391]   Loss 0.076758   Top1 97.294922   Top5 99.986979   BatchTime 0.119568   LR 0.000010
INFO - Training [73][  260/  391]   Loss 0.076909   Top1 97.292668   Top5 99.984976   BatchTime 0.120242   LR 0.000010
INFO - Training [73][  280/  391]   Loss 0.076416   Top1 97.313058   Top5 99.986049   BatchTime 0.120510   LR 0.000010
INFO - Training [73][  300/  391]   Loss 0.076578   Top1 97.304688   Top5 99.984375   BatchTime 0.120705   LR 0.000010
INFO - Training [73][  320/  391]   Loss 0.075884   Top1 97.326660   Top5 99.985352   BatchTime 0.120838   LR 0.000010
INFO - Training [73][  340/  391]   Loss 0.075497   Top1 97.323070   Top5 99.986213   BatchTime 0.120928   LR 0.000010
INFO - Training [73][  360/  391]   Loss 0.075234   Top1 97.317708   Top5 99.984809   BatchTime 0.120529   LR 0.000010
INFO - Training [73][  380/  391]   Loss 0.075673   Top1 97.321135   Top5 99.985609   BatchTime 0.118042   LR 0.000010
INFO - ==> Top1: 97.318    Top5: 99.984    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [73][   20/   79]   Loss 0.387687   Top1 90.390625   Top5 99.726562   BatchTime 0.123176
INFO - Validation [73][   40/   79]   Loss 0.375246   Top1 90.253906   Top5 99.726562   BatchTime 0.084860
INFO - Validation [73][   60/   79]   Loss 0.375395   Top1 90.260417   Top5 99.713542   BatchTime 0.078076
INFO - ==> Top1: 90.180    Top5: 99.710    Loss: 0.372
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  74
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [74][   20/  391]   Loss 0.081378   Top1 96.992188   Top5 99.960938   BatchTime 0.207663   LR 0.000010
INFO - Training [74][   40/  391]   Loss 0.072349   Top1 97.304688   Top5 99.980469   BatchTime 0.165912   LR 0.000010
INFO - Training [74][   60/  391]   Loss 0.074679   Top1 97.187500   Top5 99.973958   BatchTime 0.152251   LR 0.000010
INFO - Training [74][   80/  391]   Loss 0.075182   Top1 97.226562   Top5 99.970703   BatchTime 0.145167   LR 0.000010
INFO - Training [74][  100/  391]   Loss 0.075064   Top1 97.226562   Top5 99.976562   BatchTime 0.140678   LR 0.000010
INFO - Training [74][  120/  391]   Loss 0.075736   Top1 97.252604   Top5 99.980469   BatchTime 0.137777   LR 0.000010
INFO - Training [74][  140/  391]   Loss 0.074454   Top1 97.360491   Top5 99.983259   BatchTime 0.135678   LR 0.000010
INFO - Training [74][  160/  391]   Loss 0.075168   Top1 97.314453   Top5 99.985352   BatchTime 0.130304   LR 0.000010
INFO - Training [74][  180/  391]   Loss 0.075151   Top1 97.326389   Top5 99.986979   BatchTime 0.125165   LR 0.000010
INFO - Training [74][  200/  391]   Loss 0.074744   Top1 97.363281   Top5 99.988281   BatchTime 0.122204   LR 0.000010
INFO - Training [74][  220/  391]   Loss 0.075246   Top1 97.329545   Top5 99.985795   BatchTime 0.118069   LR 0.000010
INFO - Training [74][  240/  391]   Loss 0.075732   Top1 97.327474   Top5 99.980469   BatchTime 0.118205   LR 0.000010
INFO - Training [74][  260/  391]   Loss 0.075856   Top1 97.295673   Top5 99.981971   BatchTime 0.118629   LR 0.000010
INFO - Training [74][  280/  391]   Loss 0.075749   Top1 97.304688   Top5 99.980469   BatchTime 0.118973   LR 0.000010
INFO - Training [74][  300/  391]   Loss 0.076009   Top1 97.286458   Top5 99.981771   BatchTime 0.119327   LR 0.000010
INFO - Training [74][  320/  391]   Loss 0.076001   Top1 97.277832   Top5 99.982910   BatchTime 0.119538   LR 0.000010
INFO - Training [74][  340/  391]   Loss 0.076293   Top1 97.288603   Top5 99.981618   BatchTime 0.119768   LR 0.000010
INFO - Training [74][  360/  391]   Loss 0.075897   Top1 97.296007   Top5 99.982639   BatchTime 0.119934   LR 0.000010
INFO - Training [74][  380/  391]   Loss 0.076564   Top1 97.275905   Top5 99.983553   BatchTime 0.120064   LR 0.000010
INFO - ==> Top1: 97.272    Top5: 99.984    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [74][   20/   79]   Loss 0.378604   Top1 90.390625   Top5 99.687500   BatchTime 0.140159
INFO - Validation [74][   40/   79]   Loss 0.367892   Top1 90.351562   Top5 99.648438   BatchTime 0.083462
INFO - Validation [74][   60/   79]   Loss 0.369517   Top1 90.299479   Top5 99.661458   BatchTime 0.064408
INFO - ==> Top1: 90.200    Top5: 99.680    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  75
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [75][   20/  391]   Loss 0.076077   Top1 97.343750   Top5 100.000000   BatchTime 0.168973   LR 0.000010
INFO - Training [75][   40/  391]   Loss 0.082378   Top1 96.894531   Top5 99.980469   BatchTime 0.147387   LR 0.000010
INFO - Training [75][   60/  391]   Loss 0.077805   Top1 97.174479   Top5 99.960938   BatchTime 0.139400   LR 0.000010
INFO - Training [75][   80/  391]   Loss 0.075923   Top1 97.333984   Top5 99.960938   BatchTime 0.135569   LR 0.000010
INFO - Training [75][  100/  391]   Loss 0.077246   Top1 97.289062   Top5 99.968750   BatchTime 0.133251   LR 0.000010
INFO - Training [75][  120/  391]   Loss 0.076837   Top1 97.298177   Top5 99.973958   BatchTime 0.131754   LR 0.000010
INFO - Training [75][  140/  391]   Loss 0.076679   Top1 97.310268   Top5 99.977679   BatchTime 0.130598   LR 0.000010
INFO - Training [75][  160/  391]   Loss 0.077435   Top1 97.265625   Top5 99.975586   BatchTime 0.129694   LR 0.000010
INFO - Training [75][  180/  391]   Loss 0.077072   Top1 97.287326   Top5 99.973958   BatchTime 0.128949   LR 0.000010
INFO - Training [75][  200/  391]   Loss 0.077324   Top1 97.277344   Top5 99.976562   BatchTime 0.128337   LR 0.000010
INFO - Training [75][  220/  391]   Loss 0.076267   Top1 97.318892   Top5 99.978693   BatchTime 0.128212   LR 0.000010
INFO - Training [75][  240/  391]   Loss 0.075565   Top1 97.353516   Top5 99.980469   BatchTime 0.123745   LR 0.000010
INFO - Training [75][  260/  391]   Loss 0.076512   Top1 97.319712   Top5 99.978966   BatchTime 0.121272   LR 0.000010
INFO - Training [75][  280/  391]   Loss 0.076059   Top1 97.349330   Top5 99.980469   BatchTime 0.118833   LR 0.000010
INFO - Training [75][  300/  391]   Loss 0.076343   Top1 97.315104   Top5 99.981771   BatchTime 0.117416   LR 0.000010
INFO - Training [75][  320/  391]   Loss 0.076446   Top1 97.304688   Top5 99.982910   BatchTime 0.118091   LR 0.000010
INFO - Training [75][  340/  391]   Loss 0.076936   Top1 97.277114   Top5 99.981618   BatchTime 0.118468   LR 0.000010
INFO - Training [75][  360/  391]   Loss 0.077035   Top1 97.272135   Top5 99.982639   BatchTime 0.118738   LR 0.000010
INFO - Training [75][  380/  391]   Loss 0.077525   Top1 97.259457   Top5 99.981497   BatchTime 0.118951   LR 0.000010
INFO - ==> Top1: 97.240    Top5: 99.982    Loss: 0.078
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [75][   20/   79]   Loss 0.386301   Top1 90.195312   Top5 99.609375   BatchTime 0.151233
INFO - Validation [75][   40/   79]   Loss 0.369811   Top1 90.117188   Top5 99.609375   BatchTime 0.108141
INFO - Validation [75][   60/   79]   Loss 0.372700   Top1 90.091146   Top5 99.635417   BatchTime 0.093896
INFO - ==> Top1: 90.100    Top5: 99.660    Loss: 0.368
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  76
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [76][   20/  391]   Loss 0.072647   Top1 97.578125   Top5 100.000000   BatchTime 0.176573   LR 0.000010
INFO - Training [76][   40/  391]   Loss 0.073487   Top1 97.402344   Top5 99.980469   BatchTime 0.132132   LR 0.000010
INFO - Training [76][   60/  391]   Loss 0.073585   Top1 97.330729   Top5 99.986979   BatchTime 0.118361   LR 0.000010
INFO - Training [76][   80/  391]   Loss 0.070956   Top1 97.431641   Top5 99.990234   BatchTime 0.109055   LR 0.000010
INFO - Training [76][  100/  391]   Loss 0.070244   Top1 97.484375   Top5 99.992188   BatchTime 0.112399   LR 0.000010
INFO - Training [76][  120/  391]   Loss 0.069188   Top1 97.558594   Top5 99.993490   BatchTime 0.114243   LR 0.000010
INFO - Training [76][  140/  391]   Loss 0.071494   Top1 97.511161   Top5 99.994420   BatchTime 0.115659   LR 0.000010
INFO - Training [76][  160/  391]   Loss 0.072390   Top1 97.441406   Top5 99.995117   BatchTime 0.116665   LR 0.000010
INFO - Training [76][  180/  391]   Loss 0.072351   Top1 97.500000   Top5 99.995660   BatchTime 0.117235   LR 0.000010
INFO - Training [76][  200/  391]   Loss 0.072974   Top1 97.468750   Top5 99.996094   BatchTime 0.117923   LR 0.000010
INFO - Training [76][  220/  391]   Loss 0.073284   Top1 97.468040   Top5 99.992898   BatchTime 0.118418   LR 0.000010
INFO - Training [76][  240/  391]   Loss 0.073195   Top1 97.464193   Top5 99.993490   BatchTime 0.118810   LR 0.000010
INFO - Training [76][  260/  391]   Loss 0.074160   Top1 97.412861   Top5 99.993990   BatchTime 0.119113   LR 0.000010
INFO - Training [76][  280/  391]   Loss 0.073622   Top1 97.419085   Top5 99.994420   BatchTime 0.119385   LR 0.000010
INFO - Training [76][  300/  391]   Loss 0.073857   Top1 97.411458   Top5 99.994792   BatchTime 0.116345   LR 0.000010
INFO - Training [76][  320/  391]   Loss 0.073389   Top1 97.429199   Top5 99.995117   BatchTime 0.114898   LR 0.000010
INFO - Training [76][  340/  391]   Loss 0.073758   Top1 97.405790   Top5 99.995404   BatchTime 0.113194   LR 0.000010
INFO - Training [76][  360/  391]   Loss 0.073620   Top1 97.406684   Top5 99.995660   BatchTime 0.112319   LR 0.000010
INFO - Training [76][  380/  391]   Loss 0.073659   Top1 97.415707   Top5 99.995888   BatchTime 0.112879   LR 0.000010
INFO - ==> Top1: 97.406    Top5: 99.996    Loss: 0.074
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [76][   20/   79]   Loss 0.373714   Top1 90.468750   Top5 99.648438   BatchTime 0.149900
INFO - Validation [76][   40/   79]   Loss 0.365661   Top1 90.234375   Top5 99.648438   BatchTime 0.107504
INFO - Validation [76][   60/   79]   Loss 0.368650   Top1 90.247396   Top5 99.661458   BatchTime 0.093381
INFO - ==> Top1: 90.190    Top5: 99.670    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  77
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [77][   20/  391]   Loss 0.065104   Top1 97.734375   Top5 100.000000   BatchTime 0.212269   LR 0.000010
INFO - Training [77][   40/  391]   Loss 0.070060   Top1 97.519531   Top5 99.980469   BatchTime 0.167842   LR 0.000010
INFO - Training [77][   60/  391]   Loss 0.068515   Top1 97.604167   Top5 99.986979   BatchTime 0.152755   LR 0.000010
INFO - Training [77][   80/  391]   Loss 0.070839   Top1 97.500000   Top5 99.990234   BatchTime 0.139148   LR 0.000010
INFO - Training [77][  100/  391]   Loss 0.073073   Top1 97.398438   Top5 99.984375   BatchTime 0.128254   LR 0.000010
INFO - Training [77][  120/  391]   Loss 0.073810   Top1 97.330729   Top5 99.980469   BatchTime 0.122276   LR 0.000010
INFO - Training [77][  140/  391]   Loss 0.074792   Top1 97.338170   Top5 99.977679   BatchTime 0.115912   LR 0.000010
INFO - Training [77][  160/  391]   Loss 0.074395   Top1 97.343750   Top5 99.975586   BatchTime 0.116667   LR 0.000010
INFO - Training [77][  180/  391]   Loss 0.076063   Top1 97.300347   Top5 99.978299   BatchTime 0.117378   LR 0.000010
INFO - Training [77][  200/  391]   Loss 0.075521   Top1 97.324219   Top5 99.980469   BatchTime 0.117974   LR 0.000010
INFO - Training [77][  220/  391]   Loss 0.075469   Top1 97.354403   Top5 99.982244   BatchTime 0.118483   LR 0.000010
INFO - Training [77][  240/  391]   Loss 0.075996   Top1 97.317708   Top5 99.980469   BatchTime 0.118950   LR 0.000010
INFO - Training [77][  260/  391]   Loss 0.075661   Top1 97.310697   Top5 99.981971   BatchTime 0.119280   LR 0.000010
INFO - Training [77][  280/  391]   Loss 0.075056   Top1 97.321429   Top5 99.983259   BatchTime 0.119573   LR 0.000010
INFO - Training [77][  300/  391]   Loss 0.075267   Top1 97.315104   Top5 99.981771   BatchTime 0.119789   LR 0.000010
INFO - Training [77][  320/  391]   Loss 0.075367   Top1 97.319336   Top5 99.982910   BatchTime 0.119978   LR 0.000010
INFO - Training [77][  340/  391]   Loss 0.075257   Top1 97.329963   Top5 99.983915   BatchTime 0.120092   LR 0.000010
INFO - Training [77][  360/  391]   Loss 0.075355   Top1 97.306858   Top5 99.984809   BatchTime 0.118195   LR 0.000010
INFO - Training [77][  380/  391]   Loss 0.075897   Top1 97.277961   Top5 99.985609   BatchTime 0.116620   LR 0.000010
INFO - ==> Top1: 97.286    Top5: 99.986    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [77][   20/   79]   Loss 0.383117   Top1 90.195312   Top5 99.648438   BatchTime 0.155259
INFO - Validation [77][   40/   79]   Loss 0.371764   Top1 90.039062   Top5 99.687500   BatchTime 0.109366
INFO - Validation [77][   60/   79]   Loss 0.371637   Top1 90.117188   Top5 99.674479   BatchTime 0.093809
INFO - ==> Top1: 90.000    Top5: 99.680    Loss: 0.370
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  78
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [78][   20/  391]   Loss 0.075177   Top1 97.148438   Top5 100.000000   BatchTime 0.214717   LR 0.000010
INFO - Training [78][   40/  391]   Loss 0.075361   Top1 97.226562   Top5 100.000000   BatchTime 0.168923   LR 0.000010
INFO - Training [78][   60/  391]   Loss 0.078814   Top1 97.174479   Top5 99.986979   BatchTime 0.153727   LR 0.000010
INFO - Training [78][   80/  391]   Loss 0.079604   Top1 97.070312   Top5 99.990234   BatchTime 0.146021   LR 0.000010
INFO - Training [78][  100/  391]   Loss 0.077507   Top1 97.179688   Top5 99.992188   BatchTime 0.141414   LR 0.000010
INFO - Training [78][  120/  391]   Loss 0.078047   Top1 97.102865   Top5 99.993490   BatchTime 0.138271   LR 0.000010
INFO - Training [78][  140/  391]   Loss 0.078622   Top1 97.154018   Top5 99.994420   BatchTime 0.131971   LR 0.000010
INFO - Training [78][  160/  391]   Loss 0.079327   Top1 97.133789   Top5 99.995117   BatchTime 0.126497   LR 0.000010
INFO - Training [78][  180/  391]   Loss 0.078481   Top1 97.183160   Top5 99.995660   BatchTime 0.122823   LR 0.000010
INFO - Training [78][  200/  391]   Loss 0.076809   Top1 97.257812   Top5 99.996094   BatchTime 0.118173   LR 0.000010
INFO - Training [78][  220/  391]   Loss 0.077355   Top1 97.233665   Top5 99.996449   BatchTime 0.119463   LR 0.000010
INFO - Training [78][  240/  391]   Loss 0.076182   Top1 97.255859   Top5 99.996745   BatchTime 0.119813   LR 0.000010
INFO - Training [78][  260/  391]   Loss 0.076615   Top1 97.262620   Top5 99.993990   BatchTime 0.120004   LR 0.000010
INFO - Training [78][  280/  391]   Loss 0.076896   Top1 97.220982   Top5 99.991629   BatchTime 0.120235   LR 0.000010
INFO - Training [78][  300/  391]   Loss 0.076946   Top1 97.218750   Top5 99.992188   BatchTime 0.120466   LR 0.000010
INFO - Training [78][  320/  391]   Loss 0.077107   Top1 97.231445   Top5 99.992676   BatchTime 0.120549   LR 0.000010
INFO - Training [78][  340/  391]   Loss 0.077311   Top1 97.242647   Top5 99.993107   BatchTime 0.120664   LR 0.000010
INFO - Training [78][  360/  391]   Loss 0.077826   Top1 97.239583   Top5 99.993490   BatchTime 0.120810   LR 0.000010
INFO - Training [78][  380/  391]   Loss 0.077015   Top1 97.267681   Top5 99.991776   BatchTime 0.120883   LR 0.000010
INFO - ==> Top1: 97.272    Top5: 99.992    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [78][   20/   79]   Loss 0.375754   Top1 90.078125   Top5 99.648438   BatchTime 0.120953
INFO - Validation [78][   40/   79]   Loss 0.368181   Top1 90.117188   Top5 99.667969   BatchTime 0.081273
INFO - Validation [78][   60/   79]   Loss 0.371371   Top1 90.208333   Top5 99.674479   BatchTime 0.068212
INFO - ==> Top1: 90.130    Top5: 99.670    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  79
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [79][   20/  391]   Loss 0.076837   Top1 97.148438   Top5 100.000000   BatchTime 0.220129   LR 0.000010
INFO - Training [79][   40/  391]   Loss 0.079477   Top1 97.187500   Top5 100.000000   BatchTime 0.171928   LR 0.000010
INFO - Training [79][   60/  391]   Loss 0.077438   Top1 97.330729   Top5 100.000000   BatchTime 0.156273   LR 0.000010
INFO - Training [79][   80/  391]   Loss 0.074827   Top1 97.392578   Top5 99.990234   BatchTime 0.147806   LR 0.000010
INFO - Training [79][  100/  391]   Loss 0.074184   Top1 97.367188   Top5 99.992188   BatchTime 0.142885   LR 0.000010
INFO - Training [79][  120/  391]   Loss 0.075694   Top1 97.343750   Top5 99.993490   BatchTime 0.139667   LR 0.000010
INFO - Training [79][  140/  391]   Loss 0.075592   Top1 97.332589   Top5 99.988839   BatchTime 0.137289   LR 0.000010
INFO - Training [79][  160/  391]   Loss 0.075455   Top1 97.343750   Top5 99.985352   BatchTime 0.135378   LR 0.000010
INFO - Training [79][  180/  391]   Loss 0.077022   Top1 97.252604   Top5 99.986979   BatchTime 0.134013   LR 0.000010
INFO - Training [79][  200/  391]   Loss 0.076423   Top1 97.250000   Top5 99.988281   BatchTime 0.132038   LR 0.000010
INFO - Training [79][  220/  391]   Loss 0.077795   Top1 97.205256   Top5 99.985795   BatchTime 0.127321   LR 0.000010
INFO - Training [79][  240/  391]   Loss 0.077626   Top1 97.216797   Top5 99.983724   BatchTime 0.124118   LR 0.000010
INFO - Training [79][  260/  391]   Loss 0.078794   Top1 97.193510   Top5 99.978966   BatchTime 0.121079   LR 0.000010
INFO - Training [79][  280/  391]   Loss 0.078788   Top1 97.190290   Top5 99.977679   BatchTime 0.119779   LR 0.000010
INFO - Training [79][  300/  391]   Loss 0.078066   Top1 97.187500   Top5 99.979167   BatchTime 0.119996   LR 0.000010
INFO - Training [79][  320/  391]   Loss 0.077297   Top1 97.233887   Top5 99.978027   BatchTime 0.120227   LR 0.000010
INFO - Training [79][  340/  391]   Loss 0.077470   Top1 97.219669   Top5 99.979320   BatchTime 0.120406   LR 0.000010
INFO - Training [79][  360/  391]   Loss 0.077690   Top1 97.228733   Top5 99.973958   BatchTime 0.120506   LR 0.000010
INFO - Training [79][  380/  391]   Loss 0.077572   Top1 97.234786   Top5 99.975329   BatchTime 0.120682   LR 0.000010
INFO - ==> Top1: 97.238    Top5: 99.976    Loss: 0.078
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [79][   20/   79]   Loss 0.388103   Top1 90.234375   Top5 99.648438   BatchTime 0.149204
INFO - Validation [79][   40/   79]   Loss 0.373076   Top1 90.058594   Top5 99.628906   BatchTime 0.107091
INFO - Validation [79][   60/   79]   Loss 0.372050   Top1 90.117188   Top5 99.661458   BatchTime 0.093203
INFO - ==> Top1: 90.060    Top5: 99.690    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.400   Top5: 99.670] Sparsity : 0.872
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.380   Top5: 99.660] Sparsity : 0.872
INFO - Scoreboard best 3 ==> Epoch [45][Top1: 90.370   Top5: 99.740] Sparsity : 0.872
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_20_epoch80_20221103-234506/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 0.388103   Top1 90.234375   Top5 99.648438   BatchTime 0.133344
INFO - Validation [   40/   79]   Loss 0.373076   Top1 90.058594   Top5 99.628906   BatchTime 0.088250
INFO - Validation [   60/   79]   Loss 0.372050   Top1 90.117188   Top5 99.661458   BatchTime 0.072869
INFO - ==> Top1: 90.060    Top5: 99.690    Loss: 0.367
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/pruned_model/MobileNetv2_cifar10_a8w8_20_epoch80_checkpoint.pth.tar
INFO - Program completed successfully ... exiting ...
INFO - If you have any questions or suggestions, please visit: github.com/zhutmost/lsq-net