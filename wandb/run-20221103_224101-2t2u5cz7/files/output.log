INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102.log
2022-11-03 22:41:02.247916: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-03 22:41:02.371711: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-03 22:41:02.788952: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-03 22:41:02.789000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-03 22:41:02.789006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/tb_runs
Files already downloaded and verified
Files already downloaded and verified
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
hello
********************pre-trained*****************
INFO - Created `MobileNetv2` model for `cifar10` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): QuanConv2d(
          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): IdentityQuan()
          (quan_a_fn): IdentityQuan()
        )
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv): Sequential(
      (0): QuanConv2d(
        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
        (quan_w_fn): SLsqQuan()
        (quan_a_fn): LsqQuan()
      )
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): QuanLinear(
      in_features=1280, out_features=10, bias=True
      (quan_w_fn): IdentityQuan()
      (quan_a_fn): IdentityQuan()
    )
  )
)
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 2.545371   Top1 10.429688   Top5 49.101562   BatchTime 0.156761
INFO - Validation [   40/   79]   Loss 2.549466   Top1 10.175781   Top5 49.941406   BatchTime 0.091258
INFO - Validation [   60/   79]   Loss 2.541519   Top1 10.117188   Top5 50.377604   BatchTime 0.069569
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.546
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - >>>>>>>> Epoch   0
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [0][   20/  391]   Loss 1.646139   Top1 67.421875   Top5 96.171875   BatchTime 0.195054   LR 0.010000
INFO - Training [0][   40/  391]   Loss 1.375846   Top1 68.964844   Top5 96.640625   BatchTime 0.138221   LR 0.010000
INFO - Training [0][   60/  391]   Loss 1.194387   Top1 70.729167   Top5 97.083333   BatchTime 0.118623   LR 0.010000
INFO - Training [0][   80/  391]   Loss 1.063736   Top1 72.304688   Top5 97.500000   BatchTime 0.111008   LR 0.010000
INFO - Training [0][  100/  391]   Loss 0.967177   Top1 73.843750   Top5 97.796875   BatchTime 0.108766   LR 0.010000
INFO - Training [0][  120/  391]   Loss 0.895267   Top1 75.175781   Top5 98.046875   BatchTime 0.107407   LR 0.010000
INFO - Training [0][  140/  391]   Loss 0.835731   Top1 76.439732   Top5 98.253348   BatchTime 0.106534   LR 0.010000
INFO - Training [0][  160/  391]   Loss 0.793395   Top1 77.172852   Top5 98.413086   BatchTime 0.105648   LR 0.010000
INFO - Training [0][  180/  391]   Loss 0.755950   Top1 77.929688   Top5 98.519965   BatchTime 0.105087   LR 0.010000
INFO - Training [0][  200/  391]   Loss 0.728185   Top1 78.460938   Top5 98.582031   BatchTime 0.104581   LR 0.010000
INFO - Training [0][  220/  391]   Loss 0.702529   Top1 78.991477   Top5 98.647017   BatchTime 0.104076   LR 0.010000
INFO - Training [0][  240/  391]   Loss 0.680059   Top1 79.466146   Top5 98.723958   BatchTime 0.103711   LR 0.010000
INFO - Training [0][  260/  391]   Loss 0.659342   Top1 79.891827   Top5 98.786058   BatchTime 0.103335   LR 0.010000
INFO - Training [0][  280/  391]   Loss 0.642410   Top1 80.228795   Top5 98.844866   BatchTime 0.103141   LR 0.010000
INFO - Training [0][  300/  391]   Loss 0.625705   Top1 80.609375   Top5 98.882812   BatchTime 0.102925   LR 0.010000
INFO - Training [0][  320/  391]   Loss 0.611002   Top1 80.983887   Top5 98.928223   BatchTime 0.102720   LR 0.010000
INFO - Training [0][  340/  391]   Loss 0.596504   Top1 81.353401   Top5 98.982077   BatchTime 0.102470   LR 0.010000
INFO - Training [0][  360/  391]   Loss 0.583108   Top1 81.703559   Top5 99.029948   BatchTime 0.102258   LR 0.010000
INFO - Training [0][  380/  391]   Loss 0.572872   Top1 81.990132   Top5 99.060444   BatchTime 0.102259   LR 0.010000
INFO - ==> Top1: 82.194    Top5: 99.072    Loss: 0.567
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [0][   20/   79]   Loss 0.459141   Top1 84.726562   Top5 99.492188   BatchTime 0.134865
INFO - Validation [0][   40/   79]   Loss 0.462543   Top1 84.550781   Top5 99.296875   BatchTime 0.089300
INFO - Validation [0][   60/   79]   Loss 0.462555   Top1 84.570312   Top5 99.322917   BatchTime 0.073885
INFO - ==> Top1: 84.700    Top5: 99.340    Loss: 0.461
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 84.700   Top5: 99.340] Sparsity : 0.583
INFO - Scoreboard best 2 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   1
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [1][   20/  391]   Loss 0.315590   Top1 89.648438   Top5 99.687500   BatchTime 0.187532   LR 0.010000
INFO - Training [1][   40/  391]   Loss 0.303323   Top1 89.492188   Top5 99.726562   BatchTime 0.134820   LR 0.010000
INFO - Training [1][   60/  391]   Loss 0.305401   Top1 89.296875   Top5 99.765625   BatchTime 0.118494   LR 0.010000
INFO - Training [1][   80/  391]   Loss 0.307119   Top1 89.121094   Top5 99.785156   BatchTime 0.112569   LR 0.010000
INFO - Training [1][  100/  391]   Loss 0.304841   Top1 89.257812   Top5 99.765625   BatchTime 0.110118   LR 0.010000
INFO - Training [1][  120/  391]   Loss 0.306545   Top1 89.140625   Top5 99.778646   BatchTime 0.108596   LR 0.010000
INFO - Training [1][  140/  391]   Loss 0.303724   Top1 89.308036   Top5 99.782366   BatchTime 0.107453   LR 0.010000
INFO - Training [1][  160/  391]   Loss 0.302247   Top1 89.384766   Top5 99.780273   BatchTime 0.106557   LR 0.010000
INFO - Training [1][  180/  391]   Loss 0.300172   Top1 89.509549   Top5 99.778646   BatchTime 0.105874   LR 0.010000
INFO - Training [1][  200/  391]   Loss 0.299101   Top1 89.515625   Top5 99.781250   BatchTime 0.105249   LR 0.010000
INFO - Training [1][  220/  391]   Loss 0.298481   Top1 89.573864   Top5 99.776278   BatchTime 0.104825   LR 0.010000
INFO - Training [1][  240/  391]   Loss 0.296241   Top1 89.638672   Top5 99.781901   BatchTime 0.104481   LR 0.010000
INFO - Training [1][  260/  391]   Loss 0.293476   Top1 89.717548   Top5 99.795673   BatchTime 0.104186   LR 0.010000
INFO - Training [1][  280/  391]   Loss 0.290806   Top1 89.799107   Top5 99.807478   BatchTime 0.103989   LR 0.010000
INFO - Training [1][  300/  391]   Loss 0.288208   Top1 89.872396   Top5 99.817708   BatchTime 0.103737   LR 0.010000
INFO - Training [1][  320/  391]   Loss 0.285974   Top1 89.943848   Top5 99.819336   BatchTime 0.103514   LR 0.010000
INFO - Training [1][  340/  391]   Loss 0.284711   Top1 89.986213   Top5 99.818474   BatchTime 0.103313   LR 0.010000
INFO - Training [1][  360/  391]   Loss 0.283211   Top1 90.028212   Top5 99.817708   BatchTime 0.103112   LR 0.010000
INFO - Training [1][  380/  391]   Loss 0.281186   Top1 90.102796   Top5 99.821135   BatchTime 0.102897   LR 0.010000
INFO - ==> Top1: 90.150    Top5: 99.824    Loss: 0.279
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [1][   20/   79]   Loss 0.415566   Top1 86.718750   Top5 99.375000   BatchTime 0.135358
INFO - Validation [1][   40/   79]   Loss 0.424437   Top1 86.816406   Top5 99.238281   BatchTime 0.089866
INFO - Validation [1][   60/   79]   Loss 0.413669   Top1 86.927083   Top5 99.335938   BatchTime 0.074994
INFO - ==> Top1: 86.970    Top5: 99.380    Loss: 0.411
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 86.970   Top5: 99.380] Sparsity : 0.598
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 84.700   Top5: 99.340] Sparsity : 0.583
INFO - Scoreboard best 3 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   2
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [2][   20/  391]   Loss 0.213478   Top1 92.421875   Top5 99.882812   BatchTime 0.182226   LR 0.010000
INFO - Training [2][   40/  391]   Loss 0.220315   Top1 92.226562   Top5 99.863281   BatchTime 0.134879   LR 0.010000
INFO - Training [2][   60/  391]   Loss 0.220134   Top1 92.395833   Top5 99.830729   BatchTime 0.115614   LR 0.010000
INFO - Training [2][   80/  391]   Loss 0.217245   Top1 92.548828   Top5 99.843750   BatchTime 0.111536   LR 0.010000
INFO - Training [2][  100/  391]   Loss 0.216710   Top1 92.406250   Top5 99.859375   BatchTime 0.109273   LR 0.010000
INFO - Training [2][  120/  391]   Loss 0.215778   Top1 92.434896   Top5 99.863281   BatchTime 0.107980   LR 0.010000
INFO - Training [2][  140/  391]   Loss 0.216296   Top1 92.410714   Top5 99.860491   BatchTime 0.106883   LR 0.010000
INFO - Training [2][  160/  391]   Loss 0.216043   Top1 92.373047   Top5 99.858398   BatchTime 0.105945   LR 0.010000
INFO - Training [2][  180/  391]   Loss 0.215768   Top1 92.417535   Top5 99.869792   BatchTime 0.105334   LR 0.010000
INFO - Training [2][  200/  391]   Loss 0.213565   Top1 92.457031   Top5 99.875000   BatchTime 0.104832   LR 0.010000
INFO - Training [2][  220/  391]   Loss 0.212761   Top1 92.457386   Top5 99.879261   BatchTime 0.104378   LR 0.010000
INFO - Training [2][  240/  391]   Loss 0.211550   Top1 92.513021   Top5 99.889323   BatchTime 0.104077   LR 0.010000
INFO - Training [2][  260/  391]   Loss 0.210714   Top1 92.551082   Top5 99.888822   BatchTime 0.103815   LR 0.010000
INFO - Training [2][  280/  391]   Loss 0.209549   Top1 92.586496   Top5 99.893973   BatchTime 0.103597   LR 0.010000
INFO - Training [2][  300/  391]   Loss 0.209536   Top1 92.606771   Top5 99.893229   BatchTime 0.103398   LR 0.010000
INFO - Training [2][  320/  391]   Loss 0.207676   Top1 92.663574   Top5 99.897461   BatchTime 0.103224   LR 0.010000
INFO - Training [2][  340/  391]   Loss 0.207935   Top1 92.637868   Top5 99.894301   BatchTime 0.103025   LR 0.010000
INFO - Training [2][  360/  391]   Loss 0.207509   Top1 92.673611   Top5 99.893663   BatchTime 0.102817   LR 0.010000
INFO - Training [2][  380/  391]   Loss 0.206658   Top1 92.693257   Top5 99.895148   BatchTime 0.102619   LR 0.010000
INFO - ==> Top1: 92.694    Top5: 99.898    Loss: 0.206
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [2][   20/   79]   Loss 0.397613   Top1 88.046875   Top5 99.453125   BatchTime 0.136748
INFO - Validation [2][   40/   79]   Loss 0.401425   Top1 87.714844   Top5 99.414062   BatchTime 0.091396
INFO - Validation [2][   60/   79]   Loss 0.401567   Top1 87.565104   Top5 99.427083   BatchTime 0.075890
INFO - ==> Top1: 87.550    Top5: 99.500    Loss: 0.399
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 87.550   Top5: 99.500] Sparsity : 0.632
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 86.970   Top5: 99.380] Sparsity : 0.598
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 84.700   Top5: 99.340] Sparsity : 0.583
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   3
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [3][   20/  391]   Loss 0.169069   Top1 94.257812   Top5 99.921875   BatchTime 0.182643   LR 0.010000
INFO - Training [3][   40/  391]   Loss 0.159748   Top1 94.511719   Top5 99.902344   BatchTime 0.134029   LR 0.010000
INFO - Training [3][   60/  391]   Loss 0.158203   Top1 94.479167   Top5 99.921875   BatchTime 0.114553   LR 0.010000
INFO - Training [3][   80/  391]   Loss 0.162676   Top1 94.355469   Top5 99.892578   BatchTime 0.112172   LR 0.010000
INFO - Training [3][  100/  391]   Loss 0.167317   Top1 94.171875   Top5 99.914062   BatchTime 0.109872   LR 0.010000
INFO - Training [3][  120/  391]   Loss 0.168315   Top1 94.199219   Top5 99.928385   BatchTime 0.108338   LR 0.010000
INFO - Training [3][  140/  391]   Loss 0.169777   Top1 94.068080   Top5 99.933036   BatchTime 0.107160   LR 0.010000
INFO - Training [3][  160/  391]   Loss 0.173088   Top1 93.945312   Top5 99.926758   BatchTime 0.106290   LR 0.010000
INFO - Training [3][  180/  391]   Loss 0.175993   Top1 93.776042   Top5 99.934896   BatchTime 0.105614   LR 0.010000
INFO - Training [3][  200/  391]   Loss 0.177330   Top1 93.746094   Top5 99.925781   BatchTime 0.105034   LR 0.010000
INFO - Training [3][  220/  391]   Loss 0.178653   Top1 93.686080   Top5 99.928977   BatchTime 0.104581   LR 0.010000
INFO - Training [3][  240/  391]   Loss 0.178374   Top1 93.688151   Top5 99.931641   BatchTime 0.104146   LR 0.010000
INFO - Training [3][  260/  391]   Loss 0.180309   Top1 93.644832   Top5 99.930889   BatchTime 0.103846   LR 0.010000
INFO - Training [3][  280/  391]   Loss 0.181786   Top1 93.627232   Top5 99.930246   BatchTime 0.103633   LR 0.010000
INFO - Training [3][  300/  391]   Loss 0.181218   Top1 93.606771   Top5 99.932292   BatchTime 0.103350   LR 0.010000
INFO - Training [3][  320/  391]   Loss 0.181891   Top1 93.586426   Top5 99.936523   BatchTime 0.103165   LR 0.010000
INFO - Training [3][  340/  391]   Loss 0.181894   Top1 93.598346   Top5 99.940257   BatchTime 0.102893   LR 0.010000
INFO - Training [3][  360/  391]   Loss 0.182129   Top1 93.576389   Top5 99.939236   BatchTime 0.102654   LR 0.010000
INFO - Training [3][  380/  391]   Loss 0.181464   Top1 93.606086   Top5 99.938322   BatchTime 0.102462   LR 0.010000
INFO - ==> Top1: 93.604    Top5: 99.940    Loss: 0.182
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [3][   20/   79]   Loss 0.425066   Top1 87.031250   Top5 99.375000   BatchTime 0.134702
INFO - Validation [3][   40/   79]   Loss 0.418398   Top1 87.460938   Top5 99.335938   BatchTime 0.089786
INFO - Validation [3][   60/   79]   Loss 0.410996   Top1 87.656250   Top5 99.401042   BatchTime 0.075227
INFO - ==> Top1: 87.780    Top5: 99.470    Loss: 0.410
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 87.780   Top5: 99.470] Sparsity : 0.699
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 87.550   Top5: 99.500] Sparsity : 0.632
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 86.970   Top5: 99.380] Sparsity : 0.598
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   4
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [4][   20/  391]   Loss 0.173109   Top1 93.554688   Top5 100.000000   BatchTime 0.181951   LR 0.010000
INFO - Training [4][   40/  391]   Loss 0.180926   Top1 93.261719   Top5 100.000000   BatchTime 0.132913   LR 0.010000
INFO - Training [4][   60/  391]   Loss 0.178997   Top1 93.463542   Top5 99.986979   BatchTime 0.116349   LR 0.010000
INFO - Training [4][   80/  391]   Loss 0.174921   Top1 93.710938   Top5 99.980469   BatchTime 0.112799   LR 0.010000
INFO - Training [4][  100/  391]   Loss 0.174991   Top1 93.718750   Top5 99.984375   BatchTime 0.110343   LR 0.010000
INFO - Training [4][  120/  391]   Loss 0.174255   Top1 93.828125   Top5 99.980469   BatchTime 0.108662   LR 0.010000
INFO - Training [4][  140/  391]   Loss 0.176024   Top1 93.828125   Top5 99.977679   BatchTime 0.107400   LR 0.010000
INFO - Training [4][  160/  391]   Loss 0.174626   Top1 93.862305   Top5 99.965820   BatchTime 0.106514   LR 0.010000
INFO - Training [4][  180/  391]   Loss 0.176987   Top1 93.793403   Top5 99.960938   BatchTime 0.105781   LR 0.010000
INFO - Training [4][  200/  391]   Loss 0.176601   Top1 93.808594   Top5 99.957031   BatchTime 0.105215   LR 0.010000
INFO - Training [4][  220/  391]   Loss 0.176583   Top1 93.813920   Top5 99.957386   BatchTime 0.104845   LR 0.010000
INFO - Training [4][  240/  391]   Loss 0.175575   Top1 93.831380   Top5 99.954427   BatchTime 0.104474   LR 0.010000
INFO - Training [4][  260/  391]   Loss 0.176262   Top1 93.828125   Top5 99.948918   BatchTime 0.104138   LR 0.010000
INFO - Training [4][  280/  391]   Loss 0.176210   Top1 93.842076   Top5 99.946987   BatchTime 0.103834   LR 0.010000
INFO - Training [4][  300/  391]   Loss 0.175742   Top1 93.851562   Top5 99.950521   BatchTime 0.103614   LR 0.010000
INFO - Training [4][  320/  391]   Loss 0.174664   Top1 93.891602   Top5 99.953613   BatchTime 0.103389   LR 0.010000
INFO - Training [4][  340/  391]   Loss 0.173572   Top1 93.913143   Top5 99.956342   BatchTime 0.103133   LR 0.010000
INFO - Training [4][  360/  391]   Loss 0.173217   Top1 93.951823   Top5 99.952257   BatchTime 0.102897   LR 0.010000
INFO - Training [4][  380/  391]   Loss 0.172971   Top1 93.951480   Top5 99.954770   BatchTime 0.102685   LR 0.010000
INFO - ==> Top1: 93.954    Top5: 99.956    Loss: 0.173
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [4][   20/   79]   Loss 0.391053   Top1 88.828125   Top5 99.375000   BatchTime 0.133795
INFO - Validation [4][   40/   79]   Loss 0.394229   Top1 88.671875   Top5 99.453125   BatchTime 0.089959
INFO - Validation [4][   60/   79]   Loss 0.384645   Top1 88.802083   Top5 99.479167   BatchTime 0.074703
INFO - ==> Top1: 88.720    Top5: 99.540    Loss: 0.380
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.720   Top5: 99.540] Sparsity : 0.723
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 87.780   Top5: 99.470] Sparsity : 0.699
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 87.550   Top5: 99.500] Sparsity : 0.632
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   5
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [5][   20/  391]   Loss 0.149415   Top1 94.218750   Top5 99.960938   BatchTime 0.185554   LR 0.010000
INFO - Training [5][   40/  391]   Loss 0.148360   Top1 94.296875   Top5 99.960938   BatchTime 0.134255   LR 0.010000
INFO - Training [5][   60/  391]   Loss 0.152590   Top1 94.309896   Top5 99.973958   BatchTime 0.119482   LR 0.010000
INFO - Training [5][   80/  391]   Loss 0.148535   Top1 94.638672   Top5 99.960938   BatchTime 0.114611   LR 0.010000
INFO - Training [5][  100/  391]   Loss 0.150387   Top1 94.648438   Top5 99.945312   BatchTime 0.111802   LR 0.010000
INFO - Training [5][  120/  391]   Loss 0.152389   Top1 94.622396   Top5 99.941406   BatchTime 0.109849   LR 0.010000
INFO - Training [5][  140/  391]   Loss 0.152377   Top1 94.626116   Top5 99.949777   BatchTime 0.108503   LR 0.010000
INFO - Training [5][  160/  391]   Loss 0.153960   Top1 94.550781   Top5 99.946289   BatchTime 0.107423   LR 0.010000
INFO - Training [5][  180/  391]   Loss 0.155781   Top1 94.474826   Top5 99.947917   BatchTime 0.106785   LR 0.010000
INFO - Training [5][  200/  391]   Loss 0.157766   Top1 94.386719   Top5 99.941406   BatchTime 0.106112   LR 0.010000
INFO - Training [5][  220/  391]   Loss 0.160289   Top1 94.350142   Top5 99.932528   BatchTime 0.105545   LR 0.010000
INFO - Training [5][  240/  391]   Loss 0.160168   Top1 94.397786   Top5 99.931641   BatchTime 0.105122   LR 0.010000
INFO - Training [5][  260/  391]   Loss 0.159862   Top1 94.384014   Top5 99.936899   BatchTime 0.104755   LR 0.010000
INFO - Training [5][  280/  391]   Loss 0.159619   Top1 94.397321   Top5 99.938616   BatchTime 0.104453   LR 0.010000
INFO - Training [5][  300/  391]   Loss 0.158827   Top1 94.437500   Top5 99.940104   BatchTime 0.104200   LR 0.010000
INFO - Training [5][  320/  391]   Loss 0.159431   Top1 94.431152   Top5 99.941406   BatchTime 0.103933   LR 0.010000
INFO - Training [5][  340/  391]   Loss 0.160585   Top1 94.411765   Top5 99.924173   BatchTime 0.103684   LR 0.010000
INFO - Training [5][  360/  391]   Loss 0.159879   Top1 94.431424   Top5 99.926215   BatchTime 0.103466   LR 0.010000
INFO - Training [5][  380/  391]   Loss 0.160021   Top1 94.449013   Top5 99.925987   BatchTime 0.103264   LR 0.010000
INFO - ==> Top1: 94.448    Top5: 99.926    Loss: 0.160
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [5][   20/   79]   Loss 0.401508   Top1 87.890625   Top5 99.335938   BatchTime 0.135436
INFO - Validation [5][   40/   79]   Loss 0.399188   Top1 88.105469   Top5 99.433594   BatchTime 0.090633
INFO - Validation [5][   60/   79]   Loss 0.388269   Top1 88.554688   Top5 99.479167   BatchTime 0.072827
INFO - ==> Top1: 88.440    Top5: 99.510    Loss: 0.389
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.720   Top5: 99.540] Sparsity : 0.723
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.440   Top5: 99.510] Sparsity : 0.741
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 87.780   Top5: 99.470] Sparsity : 0.699
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   6
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [6][   20/  391]   Loss 0.141190   Top1 95.039062   Top5 99.960938   BatchTime 0.178738   LR 0.010000
INFO - Training [6][   40/  391]   Loss 0.149297   Top1 94.707031   Top5 99.921875   BatchTime 0.130698   LR 0.010000
INFO - Training [6][   60/  391]   Loss 0.148466   Top1 94.791667   Top5 99.947917   BatchTime 0.118953   LR 0.010000
INFO - Training [6][   80/  391]   Loss 0.147685   Top1 94.873047   Top5 99.951172   BatchTime 0.114029   LR 0.010000
INFO - Training [6][  100/  391]   Loss 0.148722   Top1 94.796875   Top5 99.953125   BatchTime 0.111196   LR 0.010000
INFO - Training [6][  120/  391]   Loss 0.147669   Top1 94.791667   Top5 99.960938   BatchTime 0.109328   LR 0.010000
INFO - Training [6][  140/  391]   Loss 0.146958   Top1 94.832589   Top5 99.960938   BatchTime 0.107901   LR 0.010000
INFO - Training [6][  160/  391]   Loss 0.142912   Top1 94.980469   Top5 99.965820   BatchTime 0.106952   LR 0.010000
INFO - Training [6][  180/  391]   Loss 0.143398   Top1 94.978299   Top5 99.969618   BatchTime 0.106232   LR 0.010000
INFO - Training [6][  200/  391]   Loss 0.143651   Top1 94.992188   Top5 99.957031   BatchTime 0.105670   LR 0.010000
INFO - Training [6][  220/  391]   Loss 0.142124   Top1 95.021307   Top5 99.960938   BatchTime 0.105174   LR 0.010000
INFO - Training [6][  240/  391]   Loss 0.142224   Top1 95.026042   Top5 99.954427   BatchTime 0.104786   LR 0.010000
INFO - Training [6][  260/  391]   Loss 0.141520   Top1 95.033053   Top5 99.957933   BatchTime 0.104433   LR 0.010000
INFO - Training [6][  280/  391]   Loss 0.141234   Top1 95.061384   Top5 99.958147   BatchTime 0.104197   LR 0.010000
INFO - Training [6][  300/  391]   Loss 0.141458   Top1 95.062500   Top5 99.953125   BatchTime 0.103947   LR 0.010000
INFO - Training [6][  320/  391]   Loss 0.141715   Top1 95.070801   Top5 99.956055   BatchTime 0.103665   LR 0.010000
INFO - Training [6][  340/  391]   Loss 0.142948   Top1 95.027574   Top5 99.956342   BatchTime 0.103461   LR 0.010000
INFO - Training [6][  360/  391]   Loss 0.142709   Top1 95.045573   Top5 99.958767   BatchTime 0.103218   LR 0.010000
INFO - Training [6][  380/  391]   Loss 0.143144   Top1 95.018503   Top5 99.958882   BatchTime 0.102979   LR 0.010000
INFO - ==> Top1: 95.016    Top5: 99.956    Loss: 0.143
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [6][   20/   79]   Loss 0.422120   Top1 88.085938   Top5 99.335938   BatchTime 0.135046
INFO - Validation [6][   40/   79]   Loss 0.412326   Top1 88.339844   Top5 99.335938   BatchTime 0.090430
INFO - Validation [6][   60/   79]   Loss 0.400099   Top1 88.632812   Top5 99.453125   BatchTime 0.071805
INFO - ==> Top1: 88.780    Top5: 99.520    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 88.780   Top5: 99.520] Sparsity : 0.745
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.720   Top5: 99.540] Sparsity : 0.723
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 88.440   Top5: 99.510] Sparsity : 0.741
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   7
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [7][   20/  391]   Loss 0.115545   Top1 96.171875   Top5 99.960938   BatchTime 0.182933   LR 0.010000
INFO - Training [7][   40/  391]   Loss 0.123767   Top1 95.703125   Top5 99.980469   BatchTime 0.128363   LR 0.010000
INFO - Training [7][   60/  391]   Loss 0.127144   Top1 95.677083   Top5 99.973958   BatchTime 0.118998   LR 0.010000
INFO - Training [7][   80/  391]   Loss 0.126813   Top1 95.664062   Top5 99.970703   BatchTime 0.116130   LR 0.010000
INFO - Training [7][  100/  391]   Loss 0.125345   Top1 95.726562   Top5 99.968750   BatchTime 0.112978   LR 0.010000
INFO - Training [7][  120/  391]   Loss 0.127929   Top1 95.605469   Top5 99.973958   BatchTime 0.110467   LR 0.010000
INFO - Training [7][  140/  391]   Loss 0.128117   Top1 95.636161   Top5 99.972098   BatchTime 0.108998   LR 0.010000
INFO - Training [7][  160/  391]   Loss 0.126261   Top1 95.673828   Top5 99.970703   BatchTime 0.108013   LR 0.010000
INFO - Training [7][  180/  391]   Loss 0.126174   Top1 95.668403   Top5 99.969618   BatchTime 0.107178   LR 0.010000
INFO - Training [7][  200/  391]   Loss 0.127438   Top1 95.566406   Top5 99.968750   BatchTime 0.106558   LR 0.010000
INFO - Training [7][  220/  391]   Loss 0.128020   Top1 95.522017   Top5 99.968040   BatchTime 0.105995   LR 0.010000
INFO - Training [7][  240/  391]   Loss 0.128704   Top1 95.475260   Top5 99.970703   BatchTime 0.105544   LR 0.010000
INFO - Training [7][  260/  391]   Loss 0.128228   Top1 95.477764   Top5 99.969952   BatchTime 0.105129   LR 0.010000
INFO - Training [7][  280/  391]   Loss 0.129293   Top1 95.438058   Top5 99.972098   BatchTime 0.104778   LR 0.010000
INFO - Training [7][  300/  391]   Loss 0.129915   Top1 95.403646   Top5 99.971354   BatchTime 0.104412   LR 0.010000
INFO - Training [7][  320/  391]   Loss 0.129963   Top1 95.373535   Top5 99.968262   BatchTime 0.104198   LR 0.010000
INFO - Training [7][  340/  391]   Loss 0.129628   Top1 95.374540   Top5 99.963235   BatchTime 0.103920   LR 0.010000
INFO - Training [7][  360/  391]   Loss 0.128270   Top1 95.425347   Top5 99.965278   BatchTime 0.103643   LR 0.010000
INFO - Training [7][  380/  391]   Loss 0.127909   Top1 95.450247   Top5 99.965049   BatchTime 0.103338   LR 0.010000
INFO - ==> Top1: 95.456    Top5: 99.964    Loss: 0.128
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [7][   20/   79]   Loss 0.403608   Top1 88.789062   Top5 99.453125   BatchTime 0.136642
INFO - Validation [7][   40/   79]   Loss 0.397907   Top1 88.750000   Top5 99.414062   BatchTime 0.088314
INFO - Validation [7][   60/   79]   Loss 0.383094   Top1 88.984375   Top5 99.492188   BatchTime 0.067668
INFO - ==> Top1: 89.030    Top5: 99.530    Loss: 0.380
INFO - Scoreboard best 1 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 88.780   Top5: 99.520] Sparsity : 0.745
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.720   Top5: 99.540] Sparsity : 0.723
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   8
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [8][   20/  391]   Loss 0.104216   Top1 96.406250   Top5 99.960938   BatchTime 0.184761   LR 0.010000
INFO - Training [8][   40/  391]   Loss 0.106857   Top1 96.308594   Top5 99.980469   BatchTime 0.133592   LR 0.010000
INFO - Training [8][   60/  391]   Loss 0.109216   Top1 96.158854   Top5 99.986979   BatchTime 0.122946   LR 0.010000
INFO - Training [8][   80/  391]   Loss 0.108096   Top1 96.201172   Top5 99.980469   BatchTime 0.117200   LR 0.010000
INFO - Training [8][  100/  391]   Loss 0.109604   Top1 96.125000   Top5 99.984375   BatchTime 0.113919   LR 0.010000
INFO - Training [8][  120/  391]   Loss 0.111796   Top1 96.015625   Top5 99.986979   BatchTime 0.111479   LR 0.010000
INFO - Training [8][  140/  391]   Loss 0.111400   Top1 96.043527   Top5 99.983259   BatchTime 0.109934   LR 0.010000
INFO - Training [8][  160/  391]   Loss 0.112738   Top1 95.986328   Top5 99.975586   BatchTime 0.108810   LR 0.010000
INFO - Training [8][  180/  391]   Loss 0.111224   Top1 96.041667   Top5 99.978299   BatchTime 0.107977   LR 0.010000
INFO - Training [8][  200/  391]   Loss 0.110397   Top1 96.058594   Top5 99.972656   BatchTime 0.107158   LR 0.010000
INFO - Training [8][  220/  391]   Loss 0.111431   Top1 96.036932   Top5 99.975142   BatchTime 0.106500   LR 0.010000
INFO - Training [8][  240/  391]   Loss 0.111389   Top1 96.005859   Top5 99.977214   BatchTime 0.105934   LR 0.010000
INFO - Training [8][  260/  391]   Loss 0.111937   Top1 95.988582   Top5 99.978966   BatchTime 0.105452   LR 0.010000
INFO - Training [8][  280/  391]   Loss 0.112279   Top1 95.987723   Top5 99.980469   BatchTime 0.105102   LR 0.010000
INFO - Training [8][  300/  391]   Loss 0.112445   Top1 96.007812   Top5 99.981771   BatchTime 0.104808   LR 0.010000
INFO - Training [8][  320/  391]   Loss 0.113855   Top1 95.937500   Top5 99.980469   BatchTime 0.104496   LR 0.010000
INFO - Training [8][  340/  391]   Loss 0.113758   Top1 95.923713   Top5 99.981618   BatchTime 0.104187   LR 0.010000
INFO - Training [8][  360/  391]   Loss 0.115237   Top1 95.863715   Top5 99.980469   BatchTime 0.103881   LR 0.010000
INFO - Training [8][  380/  391]   Loss 0.115399   Top1 95.877878   Top5 99.979441   BatchTime 0.103592   LR 0.010000
INFO - ==> Top1: 95.888    Top5: 99.980    Loss: 0.115
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [8][   20/   79]   Loss 0.385858   Top1 88.867188   Top5 99.492188   BatchTime 0.135473
INFO - Validation [8][   40/   79]   Loss 0.383200   Top1 89.101562   Top5 99.453125   BatchTime 0.084962
INFO - Validation [8][   60/   79]   Loss 0.370806   Top1 89.453125   Top5 99.505208   BatchTime 0.065351
INFO - ==> Top1: 89.430    Top5: 99.550    Loss: 0.374
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 88.780   Top5: 99.520] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch   9
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [9][   20/  391]   Loss 0.101222   Top1 96.171875   Top5 100.000000   BatchTime 0.182143   LR 0.010000
INFO - Training [9][   40/  391]   Loss 0.110611   Top1 96.113281   Top5 100.000000   BatchTime 0.131621   LR 0.010000
INFO - Training [9][   60/  391]   Loss 0.111137   Top1 96.041667   Top5 100.000000   BatchTime 0.121357   LR 0.010000
INFO - Training [9][   80/  391]   Loss 0.113441   Top1 96.054688   Top5 100.000000   BatchTime 0.115983   LR 0.010000
INFO - Training [9][  100/  391]   Loss 0.114384   Top1 96.062500   Top5 99.976562   BatchTime 0.112973   LR 0.010000
INFO - Training [9][  120/  391]   Loss 0.115311   Top1 96.028646   Top5 99.973958   BatchTime 0.111549   LR 0.010000
INFO - Training [9][  140/  391]   Loss 0.114037   Top1 96.021205   Top5 99.977679   BatchTime 0.110063   LR 0.010000
INFO - Training [9][  160/  391]   Loss 0.112553   Top1 96.093750   Top5 99.980469   BatchTime 0.108882   LR 0.010000
INFO - Training [9][  180/  391]   Loss 0.113600   Top1 96.037326   Top5 99.978299   BatchTime 0.108230   LR 0.010000
INFO - Training [9][  200/  391]   Loss 0.112007   Top1 96.093750   Top5 99.980469   BatchTime 0.107468   LR 0.010000
INFO - Training [9][  220/  391]   Loss 0.112366   Top1 96.107955   Top5 99.978693   BatchTime 0.106873   LR 0.010000
INFO - Training [9][  240/  391]   Loss 0.112669   Top1 96.093750   Top5 99.980469   BatchTime 0.106345   LR 0.010000
INFO - Training [9][  260/  391]   Loss 0.114387   Top1 96.009615   Top5 99.981971   BatchTime 0.105888   LR 0.010000
INFO - Training [9][  280/  391]   Loss 0.115851   Top1 95.962612   Top5 99.980469   BatchTime 0.105602   LR 0.010000
INFO - Training [9][  300/  391]   Loss 0.117773   Top1 95.903646   Top5 99.981771   BatchTime 0.105298   LR 0.010000
INFO - Training [9][  320/  391]   Loss 0.119883   Top1 95.808105   Top5 99.980469   BatchTime 0.104993   LR 0.010000
INFO - Training [9][  340/  391]   Loss 0.121425   Top1 95.735294   Top5 99.979320   BatchTime 0.104694   LR 0.010000
INFO - Training [9][  360/  391]   Loss 0.122452   Top1 95.698785   Top5 99.973958   BatchTime 0.104436   LR 0.010000
INFO - Training [9][  380/  391]   Loss 0.124149   Top1 95.649671   Top5 99.973273   BatchTime 0.104094   LR 0.010000
INFO - ==> Top1: 95.606    Top5: 99.974    Loss: 0.125
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [9][   20/   79]   Loss 0.393650   Top1 88.750000   Top5 99.492188   BatchTime 0.134909
INFO - Validation [9][   40/   79]   Loss 0.401070   Top1 88.261719   Top5 99.414062   BatchTime 0.082680
INFO - Validation [9][   60/   79]   Loss 0.393770   Top1 88.515625   Top5 99.466146   BatchTime 0.063759
INFO - ==> Top1: 88.300    Top5: 99.500    Loss: 0.392
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 88.780   Top5: 99.520] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  10
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [10][   20/  391]   Loss 0.141013   Top1 95.312500   Top5 99.843750   BatchTime 0.180315   LR 0.010000
INFO - Training [10][   40/  391]   Loss 0.140508   Top1 95.253906   Top5 99.902344   BatchTime 0.132842   LR 0.010000
INFO - Training [10][   60/  391]   Loss 0.136108   Top1 95.442708   Top5 99.908854   BatchTime 0.122075   LR 0.010000
INFO - Training [10][   80/  391]   Loss 0.139598   Top1 95.253906   Top5 99.931641   BatchTime 0.116703   LR 0.010000
INFO - Training [10][  100/  391]   Loss 0.143830   Top1 95.000000   Top5 99.929688   BatchTime 0.113439   LR 0.010000
INFO - Training [10][  120/  391]   Loss 0.148792   Top1 94.811198   Top5 99.941406   BatchTime 0.111231   LR 0.010000
INFO - Training [10][  140/  391]   Loss 0.150913   Top1 94.715402   Top5 99.944196   BatchTime 0.109673   LR 0.010000
INFO - Training [10][  160/  391]   Loss 0.153690   Top1 94.604492   Top5 99.936523   BatchTime 0.108433   LR 0.010000
INFO - Training [10][  180/  391]   Loss 0.153837   Top1 94.631076   Top5 99.930556   BatchTime 0.107509   LR 0.010000
INFO - Training [10][  200/  391]   Loss 0.154602   Top1 94.652344   Top5 99.917969   BatchTime 0.106810   LR 0.010000
INFO - Training [10][  220/  391]   Loss 0.157185   Top1 94.570312   Top5 99.921875   BatchTime 0.106289   LR 0.010000
INFO - Training [10][  240/  391]   Loss 0.159101   Top1 94.508464   Top5 99.925130   BatchTime 0.105808   LR 0.010000
INFO - Training [10][  260/  391]   Loss 0.161701   Top1 94.420072   Top5 99.924880   BatchTime 0.105390   LR 0.010000
INFO - Training [10][  280/  391]   Loss 0.163858   Top1 94.324777   Top5 99.921875   BatchTime 0.105018   LR 0.010000
INFO - Training [10][  300/  391]   Loss 0.164587   Top1 94.286458   Top5 99.927083   BatchTime 0.104735   LR 0.010000
INFO - Training [10][  320/  391]   Loss 0.165429   Top1 94.257812   Top5 99.924316   BatchTime 0.104491   LR 0.010000
INFO - Training [10][  340/  391]   Loss 0.166378   Top1 94.250919   Top5 99.919577   BatchTime 0.104174   LR 0.010000
INFO - Training [10][  360/  391]   Loss 0.166922   Top1 94.203559   Top5 99.924045   BatchTime 0.103895   LR 0.010000
INFO - Training [10][  380/  391]   Loss 0.168703   Top1 94.159128   Top5 99.921875   BatchTime 0.103565   LR 0.010000
INFO - ==> Top1: 94.142    Top5: 99.924    Loss: 0.169
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [10][   20/   79]   Loss 0.413665   Top1 87.929688   Top5 99.296875   BatchTime 0.134445
INFO - Validation [10][   40/   79]   Loss 0.404842   Top1 88.046875   Top5 99.375000   BatchTime 0.083098
INFO - Validation [10][   60/   79]   Loss 0.397429   Top1 88.125000   Top5 99.440104   BatchTime 0.064079
INFO - ==> Top1: 88.080    Top5: 99.470    Loss: 0.397
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 88.780   Top5: 99.520] Sparsity : 0.745
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  11
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [11][   20/  391]   Loss 0.167696   Top1 93.750000   Top5 100.000000   BatchTime 0.180388   LR 0.010000
INFO - Training [11][   40/  391]   Loss 0.167991   Top1 94.062500   Top5 100.000000   BatchTime 0.134133   LR 0.010000
INFO - Training [11][   60/  391]   Loss 0.161673   Top1 94.244792   Top5 99.986979   BatchTime 0.123352   LR 0.010000
INFO - Training [11][   80/  391]   Loss 0.161595   Top1 94.296875   Top5 99.960938   BatchTime 0.117981   LR 0.010000
INFO - Training [11][  100/  391]   Loss 0.162567   Top1 94.289062   Top5 99.945312   BatchTime 0.114585   LR 0.010000
INFO - Training [11][  120/  391]   Loss 0.165647   Top1 94.160156   Top5 99.954427   BatchTime 0.112089   LR 0.010000
INFO - Training [11][  140/  391]   Loss 0.164878   Top1 94.162946   Top5 99.955357   BatchTime 0.110418   LR 0.010000
INFO - Training [11][  160/  391]   Loss 0.166724   Top1 94.111328   Top5 99.951172   BatchTime 0.109145   LR 0.010000
INFO - Training [11][  180/  391]   Loss 0.166731   Top1 94.144965   Top5 99.956597   BatchTime 0.108657   LR 0.010000
INFO - Training [11][  200/  391]   Loss 0.167188   Top1 94.152344   Top5 99.953125   BatchTime 0.107476   LR 0.010000
INFO - Training [11][  220/  391]   Loss 0.166158   Top1 94.183239   Top5 99.943182   BatchTime 0.106793   LR 0.010000
INFO - Training [11][  240/  391]   Loss 0.165935   Top1 94.186198   Top5 99.944661   BatchTime 0.106345   LR 0.010000
INFO - Training [11][  260/  391]   Loss 0.165918   Top1 94.200721   Top5 99.945913   BatchTime 0.105928   LR 0.010000
INFO - Training [11][  280/  391]   Loss 0.165266   Top1 94.241071   Top5 99.941406   BatchTime 0.105531   LR 0.010000
INFO - Training [11][  300/  391]   Loss 0.164630   Top1 94.252604   Top5 99.937500   BatchTime 0.105172   LR 0.010000
INFO - Training [11][  320/  391]   Loss 0.164898   Top1 94.255371   Top5 99.938965   BatchTime 0.104923   LR 0.010000
INFO - Training [11][  340/  391]   Loss 0.166203   Top1 94.216452   Top5 99.937960   BatchTime 0.104581   LR 0.010000
INFO - Training [11][  360/  391]   Loss 0.167188   Top1 94.190538   Top5 99.930556   BatchTime 0.104246   LR 0.010000
INFO - Training [11][  380/  391]   Loss 0.167932   Top1 94.189967   Top5 99.921875   BatchTime 0.103897   LR 0.010000
INFO - ==> Top1: 94.208    Top5: 99.922    Loss: 0.168
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [11][   20/   79]   Loss 0.366913   Top1 88.671875   Top5 99.414062   BatchTime 0.133264
INFO - Validation [11][   40/   79]   Loss 0.373199   Top1 89.179688   Top5 99.414062   BatchTime 0.081616
INFO - Validation [11][   60/   79]   Loss 0.375073   Top1 89.153646   Top5 99.453125   BatchTime 0.063062
INFO - ==> Top1: 88.850    Top5: 99.470    Loss: 0.379
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 88.850   Top5: 99.470] Sparsity : 0.814
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  12
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [12][   20/  391]   Loss 0.156066   Top1 94.609375   Top5 99.843750   BatchTime 0.179059   LR 0.010000
INFO - Training [12][   40/  391]   Loss 0.148606   Top1 94.882812   Top5 99.902344   BatchTime 0.133658   LR 0.010000
INFO - Training [12][   60/  391]   Loss 0.148360   Top1 94.869792   Top5 99.921875   BatchTime 0.122739   LR 0.010000
INFO - Training [12][   80/  391]   Loss 0.147177   Top1 94.863281   Top5 99.941406   BatchTime 0.117087   LR 0.010000
INFO - Training [12][  100/  391]   Loss 0.146488   Top1 94.882812   Top5 99.953125   BatchTime 0.113820   LR 0.010000
INFO - Training [12][  120/  391]   Loss 0.149892   Top1 94.785156   Top5 99.947917   BatchTime 0.111599   LR 0.010000
INFO - Training [12][  140/  391]   Loss 0.149809   Top1 94.765625   Top5 99.955357   BatchTime 0.110061   LR 0.010000
INFO - Training [12][  160/  391]   Loss 0.150193   Top1 94.750977   Top5 99.951172   BatchTime 0.108944   LR 0.010000
INFO - Training [12][  180/  391]   Loss 0.151077   Top1 94.709201   Top5 99.956597   BatchTime 0.108009   LR 0.010000
INFO - Training [12][  200/  391]   Loss 0.152347   Top1 94.625000   Top5 99.960938   BatchTime 0.107219   LR 0.010000
INFO - Training [12][  220/  391]   Loss 0.152070   Top1 94.595170   Top5 99.957386   BatchTime 0.106581   LR 0.010000
INFO - Training [12][  240/  391]   Loss 0.152913   Top1 94.544271   Top5 99.957682   BatchTime 0.106108   LR 0.010000
INFO - Training [12][  260/  391]   Loss 0.153656   Top1 94.495192   Top5 99.954928   BatchTime 0.105637   LR 0.010000
INFO - Training [12][  280/  391]   Loss 0.154415   Top1 94.483817   Top5 99.946987   BatchTime 0.105215   LR 0.010000
INFO - Training [12][  300/  391]   Loss 0.154339   Top1 94.479167   Top5 99.950521   BatchTime 0.104887   LR 0.010000
INFO - Training [12][  320/  391]   Loss 0.154077   Top1 94.497070   Top5 99.948730   BatchTime 0.104577   LR 0.010000
INFO - Training [12][  340/  391]   Loss 0.153081   Top1 94.547335   Top5 99.947151   BatchTime 0.104247   LR 0.010000
INFO - Training [12][  360/  391]   Loss 0.152264   Top1 94.578993   Top5 99.950087   BatchTime 0.103928   LR 0.010000
INFO - Training [12][  380/  391]   Loss 0.152890   Top1 94.557977   Top5 99.946546   BatchTime 0.103594   LR 0.010000
INFO - ==> Top1: 94.556    Top5: 99.946    Loss: 0.153
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [12][   20/   79]   Loss 0.364233   Top1 88.554688   Top5 99.570312   BatchTime 0.133092
INFO - Validation [12][   40/   79]   Loss 0.386879   Top1 88.554688   Top5 99.453125   BatchTime 0.079529
INFO - Validation [12][   60/   79]   Loss 0.383983   Top1 88.750000   Top5 99.492188   BatchTime 0.061650
INFO - ==> Top1: 88.630    Top5: 99.540    Loss: 0.382
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 88.850   Top5: 99.470] Sparsity : 0.814
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  13
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [13][   20/  391]   Loss 0.129696   Top1 95.468750   Top5 99.960938   BatchTime 0.178871   LR 0.010000
INFO - Training [13][   40/  391]   Loss 0.125477   Top1 95.644531   Top5 99.960938   BatchTime 0.135716   LR 0.010000
INFO - Training [13][   60/  391]   Loss 0.124797   Top1 95.729167   Top5 99.960938   BatchTime 0.124038   LR 0.010000
INFO - Training [13][   80/  391]   Loss 0.133289   Top1 95.302734   Top5 99.941406   BatchTime 0.118378   LR 0.010000
INFO - Training [13][  100/  391]   Loss 0.134372   Top1 95.257812   Top5 99.945312   BatchTime 0.115085   LR 0.010000
INFO - Training [13][  120/  391]   Loss 0.136596   Top1 95.195312   Top5 99.941406   BatchTime 0.112451   LR 0.010000
INFO - Training [13][  140/  391]   Loss 0.136768   Top1 95.245536   Top5 99.944196   BatchTime 0.110552   LR 0.010000
INFO - Training [13][  160/  391]   Loss 0.138502   Top1 95.175781   Top5 99.946289   BatchTime 0.109329   LR 0.010000
INFO - Training [13][  180/  391]   Loss 0.138508   Top1 95.190972   Top5 99.947917   BatchTime 0.108228   LR 0.010000
INFO - Training [13][  200/  391]   Loss 0.138784   Top1 95.144531   Top5 99.949219   BatchTime 0.107392   LR 0.010000
INFO - Training [13][  220/  391]   Loss 0.137755   Top1 95.191761   Top5 99.950284   BatchTime 0.106751   LR 0.010000
INFO - Training [13][  240/  391]   Loss 0.138721   Top1 95.143229   Top5 99.954427   BatchTime 0.106293   LR 0.010000
INFO - Training [13][  260/  391]   Loss 0.138789   Top1 95.123197   Top5 99.951923   BatchTime 0.106207   LR 0.010000
INFO - Training [13][  280/  391]   Loss 0.138287   Top1 95.156250   Top5 99.944196   BatchTime 0.105749   LR 0.010000
INFO - Training [13][  300/  391]   Loss 0.138902   Top1 95.148438   Top5 99.942708   BatchTime 0.105414   LR 0.010000
INFO - Training [13][  320/  391]   Loss 0.139533   Top1 95.134277   Top5 99.943848   BatchTime 0.105119   LR 0.010000
INFO - Training [13][  340/  391]   Loss 0.139466   Top1 95.153952   Top5 99.944853   BatchTime 0.104750   LR 0.010000
INFO - Training [13][  360/  391]   Loss 0.139207   Top1 95.158420   Top5 99.941406   BatchTime 0.104431   LR 0.010000
INFO - Training [13][  380/  391]   Loss 0.139024   Top1 95.168586   Top5 99.942434   BatchTime 0.104070   LR 0.010000
INFO - ==> Top1: 95.140    Top5: 99.940    Loss: 0.140
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [13][   20/   79]   Loss 0.413832   Top1 87.812500   Top5 99.375000   BatchTime 0.132187
INFO - Validation [13][   40/   79]   Loss 0.418274   Top1 88.183594   Top5 99.433594   BatchTime 0.079152
INFO - Validation [13][   60/   79]   Loss 0.411323   Top1 88.502604   Top5 99.505208   BatchTime 0.061437
INFO - ==> Top1: 88.310    Top5: 99.490    Loss: 0.412
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Scoreboard best 3 ==> Epoch [11][Top1: 88.850   Top5: 99.470] Sparsity : 0.814
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  14
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [14][   20/  391]   Loss 0.114947   Top1 95.859375   Top5 99.960938   BatchTime 0.175150   LR 0.010000
INFO - Training [14][   40/  391]   Loss 0.115704   Top1 95.976562   Top5 99.980469   BatchTime 0.134052   LR 0.010000
INFO - Training [14][   60/  391]   Loss 0.120911   Top1 95.794271   Top5 99.973958   BatchTime 0.122957   LR 0.010000
INFO - Training [14][   80/  391]   Loss 0.122246   Top1 95.693359   Top5 99.980469   BatchTime 0.117217   LR 0.010000
INFO - Training [14][  100/  391]   Loss 0.124357   Top1 95.617188   Top5 99.984375   BatchTime 0.113964   LR 0.010000
INFO - Training [14][  120/  391]   Loss 0.127031   Top1 95.572917   Top5 99.980469   BatchTime 0.111707   LR 0.010000
INFO - Training [14][  140/  391]   Loss 0.126939   Top1 95.524554   Top5 99.977679   BatchTime 0.110014   LR 0.010000
INFO - Training [14][  160/  391]   Loss 0.128742   Top1 95.454102   Top5 99.975586   BatchTime 0.108827   LR 0.010000
INFO - Training [14][  180/  391]   Loss 0.127980   Top1 95.486111   Top5 99.978299   BatchTime 0.107963   LR 0.010000
INFO - Training [14][  200/  391]   Loss 0.129432   Top1 95.457031   Top5 99.976562   BatchTime 0.107128   LR 0.010000
INFO - Training [14][  220/  391]   Loss 0.130252   Top1 95.440341   Top5 99.978693   BatchTime 0.106535   LR 0.010000
INFO - Training [14][  240/  391]   Loss 0.130451   Top1 95.432943   Top5 99.973958   BatchTime 0.106026   LR 0.010000
INFO - Training [14][  260/  391]   Loss 0.130835   Top1 95.426683   Top5 99.972957   BatchTime 0.105606   LR 0.010000
INFO - Training [14][  280/  391]   Loss 0.130958   Top1 95.401786   Top5 99.974888   BatchTime 0.105153   LR 0.010000
INFO - Training [14][  300/  391]   Loss 0.131141   Top1 95.385417   Top5 99.973958   BatchTime 0.104843   LR 0.010000
INFO - Training [14][  320/  391]   Loss 0.131432   Top1 95.375977   Top5 99.973145   BatchTime 0.104574   LR 0.010000
INFO - Training [14][  340/  391]   Loss 0.132207   Top1 95.358456   Top5 99.963235   BatchTime 0.104267   LR 0.010000
INFO - Training [14][  360/  391]   Loss 0.132990   Top1 95.342882   Top5 99.960938   BatchTime 0.103952   LR 0.010000
INFO - Training [14][  380/  391]   Loss 0.133225   Top1 95.349507   Top5 99.958882   BatchTime 0.103618   LR 0.010000
INFO - ==> Top1: 95.336    Top5: 99.958    Loss: 0.133
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [14][   20/   79]   Loss 0.371416   Top1 89.179688   Top5 99.570312   BatchTime 0.126011
INFO - Validation [14][   40/   79]   Loss 0.378811   Top1 89.101562   Top5 99.531250   BatchTime 0.076097
INFO - Validation [14][   60/   79]   Loss 0.376586   Top1 89.218750   Top5 99.531250   BatchTime 0.059364
INFO - ==> Top1: 89.110    Top5: 99.520    Loss: 0.377
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 2 ==> Epoch [14][Top1: 89.110   Top5: 99.520] Sparsity : 0.819
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.030   Top5: 99.530] Sparsity : 0.750
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  15
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [15][   20/  391]   Loss 0.110254   Top1 95.820312   Top5 99.960938   BatchTime 0.178931   LR 0.010000
INFO - Training [15][   40/  391]   Loss 0.114375   Top1 95.605469   Top5 99.980469   BatchTime 0.137876   LR 0.010000
INFO - Training [15][   60/  391]   Loss 0.116606   Top1 95.559896   Top5 99.986979   BatchTime 0.125302   LR 0.010000
INFO - Training [15][   80/  391]   Loss 0.120426   Top1 95.517578   Top5 99.980469   BatchTime 0.118987   LR 0.010000
INFO - Training [15][  100/  391]   Loss 0.121809   Top1 95.570312   Top5 99.984375   BatchTime 0.115128   LR 0.010000
INFO - Training [15][  120/  391]   Loss 0.120167   Top1 95.631510   Top5 99.986979   BatchTime 0.112720   LR 0.010000
INFO - Training [15][  140/  391]   Loss 0.121642   Top1 95.513393   Top5 99.988839   BatchTime 0.110998   LR 0.010000
INFO - Training [15][  160/  391]   Loss 0.121170   Top1 95.576172   Top5 99.985352   BatchTime 0.109714   LR 0.010000
INFO - Training [15][  180/  391]   Loss 0.123266   Top1 95.503472   Top5 99.986979   BatchTime 0.108730   LR 0.010000
INFO - Training [15][  200/  391]   Loss 0.122664   Top1 95.566406   Top5 99.988281   BatchTime 0.107891   LR 0.010000
INFO - Training [15][  220/  391]   Loss 0.124378   Top1 95.511364   Top5 99.978693   BatchTime 0.107189   LR 0.010000
INFO - Training [15][  240/  391]   Loss 0.124914   Top1 95.524089   Top5 99.970703   BatchTime 0.106631   LR 0.010000
INFO - Training [15][  260/  391]   Loss 0.123591   Top1 95.591947   Top5 99.972957   BatchTime 0.106157   LR 0.010000
INFO - Training [15][  280/  391]   Loss 0.125164   Top1 95.516183   Top5 99.972098   BatchTime 0.105713   LR 0.010000
INFO - Training [15][  300/  391]   Loss 0.125114   Top1 95.505208   Top5 99.971354   BatchTime 0.105309   LR 0.010000
INFO - Training [15][  320/  391]   Loss 0.125402   Top1 95.507812   Top5 99.968262   BatchTime 0.105463   LR 0.010000
INFO - Training [15][  340/  391]   Loss 0.125272   Top1 95.535386   Top5 99.967831   BatchTime 0.104909   LR 0.010000
INFO - Training [15][  360/  391]   Loss 0.125348   Top1 95.527344   Top5 99.965278   BatchTime 0.104551   LR 0.010000
INFO - Training [15][  380/  391]   Loss 0.125494   Top1 95.540707   Top5 99.962993   BatchTime 0.104213   LR 0.010000
INFO - ==> Top1: 95.550    Top5: 99.964    Loss: 0.126
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [15][   20/   79]   Loss 0.369048   Top1 89.570312   Top5 99.687500   BatchTime 0.124590
INFO - Validation [15][   40/   79]   Loss 0.378139   Top1 89.550781   Top5 99.589844   BatchTime 0.075410
INFO - Validation [15][   60/   79]   Loss 0.375007   Top1 89.557292   Top5 99.622396   BatchTime 0.058967
INFO - ==> Top1: 89.430    Top5: 99.640    Loss: 0.373
INFO - Scoreboard best 1 ==> Epoch [15][Top1: 89.430   Top5: 99.640] Sparsity : 0.820
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 3 ==> Epoch [14][Top1: 89.110   Top5: 99.520] Sparsity : 0.819
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  16
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [16][   20/  391]   Loss 0.126831   Top1 95.468750   Top5 99.960938   BatchTime 0.175715   LR 0.010000
INFO - Training [16][   40/  391]   Loss 0.112586   Top1 95.996094   Top5 99.980469   BatchTime 0.141977   LR 0.010000
INFO - Training [16][   60/  391]   Loss 0.114667   Top1 95.976562   Top5 99.960938   BatchTime 0.128325   LR 0.010000
INFO - Training [16][   80/  391]   Loss 0.117616   Top1 95.888672   Top5 99.970703   BatchTime 0.121153   LR 0.010000
INFO - Training [16][  100/  391]   Loss 0.117095   Top1 95.898438   Top5 99.976562   BatchTime 0.116752   LR 0.010000
INFO - Training [16][  120/  391]   Loss 0.119673   Top1 95.768229   Top5 99.980469   BatchTime 0.113965   LR 0.010000
INFO - Training [16][  140/  391]   Loss 0.119644   Top1 95.786830   Top5 99.977679   BatchTime 0.112067   LR 0.010000
INFO - Training [16][  160/  391]   Loss 0.118961   Top1 95.820312   Top5 99.980469   BatchTime 0.110734   LR 0.010000
INFO - Training [16][  180/  391]   Loss 0.120762   Top1 95.768229   Top5 99.982639   BatchTime 0.109563   LR 0.010000
INFO - Training [16][  200/  391]   Loss 0.119797   Top1 95.769531   Top5 99.984375   BatchTime 0.108720   LR 0.010000
INFO - Training [16][  220/  391]   Loss 0.120924   Top1 95.759943   Top5 99.985795   BatchTime 0.107964   LR 0.010000
INFO - Training [16][  240/  391]   Loss 0.121200   Top1 95.755208   Top5 99.977214   BatchTime 0.107308   LR 0.010000
INFO - Training [16][  260/  391]   Loss 0.121646   Top1 95.754207   Top5 99.972957   BatchTime 0.106741   LR 0.010000
INFO - Training [16][  280/  391]   Loss 0.122505   Top1 95.731027   Top5 99.972098   BatchTime 0.106238   LR 0.010000
INFO - Training [16][  300/  391]   Loss 0.122316   Top1 95.716146   Top5 99.971354   BatchTime 0.105884   LR 0.010000
INFO - Training [16][  320/  391]   Loss 0.122641   Top1 95.698242   Top5 99.963379   BatchTime 0.105507   LR 0.010000
INFO - Training [16][  340/  391]   Loss 0.122857   Top1 95.703125   Top5 99.963235   BatchTime 0.105110   LR 0.010000
INFO - Training [16][  360/  391]   Loss 0.122956   Top1 95.683594   Top5 99.963108   BatchTime 0.104753   LR 0.010000
INFO - Training [16][  380/  391]   Loss 0.122125   Top1 95.723684   Top5 99.962993   BatchTime 0.104380   LR 0.010000
INFO - ==> Top1: 95.708    Top5: 99.964    Loss: 0.122
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [16][   20/   79]   Loss 0.374897   Top1 89.140625   Top5 99.570312   BatchTime 0.125133
INFO - Validation [16][   40/   79]   Loss 0.384250   Top1 89.218750   Top5 99.375000   BatchTime 0.075657
INFO - Validation [16][   60/   79]   Loss 0.379784   Top1 89.244792   Top5 99.427083   BatchTime 0.062057
INFO - ==> Top1: 89.290    Top5: 99.460    Loss: 0.379
INFO - Scoreboard best 1 ==> Epoch [15][Top1: 89.430   Top5: 99.640] Sparsity : 0.820
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 89.290   Top5: 99.460] Sparsity : 0.821
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  17
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [17][   20/  391]   Loss 0.090450   Top1 96.875000   Top5 99.960938   BatchTime 0.186191   LR 0.010000
INFO - Training [17][   40/  391]   Loss 0.104303   Top1 96.171875   Top5 99.980469   BatchTime 0.143425   LR 0.010000
INFO - Training [17][   60/  391]   Loss 0.101624   Top1 96.367188   Top5 99.973958   BatchTime 0.129041   LR 0.010000
INFO - Training [17][   80/  391]   Loss 0.106218   Top1 96.318359   Top5 99.970703   BatchTime 0.121966   LR 0.010000
INFO - Training [17][  100/  391]   Loss 0.106939   Top1 96.273438   Top5 99.968750   BatchTime 0.117476   LR 0.010000
INFO - Training [17][  120/  391]   Loss 0.109976   Top1 96.158854   Top5 99.973958   BatchTime 0.114622   LR 0.010000
INFO - Training [17][  140/  391]   Loss 0.110460   Top1 96.155134   Top5 99.972098   BatchTime 0.112643   LR 0.010000
INFO - Training [17][  160/  391]   Loss 0.110475   Top1 96.171875   Top5 99.975586   BatchTime 0.111096   LR 0.010000
INFO - Training [17][  180/  391]   Loss 0.111414   Top1 96.141493   Top5 99.973958   BatchTime 0.109932   LR 0.010000
INFO - Training [17][  200/  391]   Loss 0.111367   Top1 96.128906   Top5 99.976562   BatchTime 0.109117   LR 0.010000
INFO - Training [17][  220/  391]   Loss 0.112610   Top1 96.051136   Top5 99.975142   BatchTime 0.108335   LR 0.010000
INFO - Training [17][  240/  391]   Loss 0.113309   Top1 96.031901   Top5 99.970703   BatchTime 0.107686   LR 0.010000
INFO - Training [17][  260/  391]   Loss 0.114143   Top1 96.009615   Top5 99.966947   BatchTime 0.107180   LR 0.010000
INFO - Training [17][  280/  391]   Loss 0.114119   Top1 96.001674   Top5 99.966518   BatchTime 0.106691   LR 0.010000
INFO - Training [17][  300/  391]   Loss 0.114055   Top1 96.010417   Top5 99.968750   BatchTime 0.106299   LR 0.010000
INFO - Training [17][  320/  391]   Loss 0.114328   Top1 96.022949   Top5 99.968262   BatchTime 0.105936   LR 0.010000
INFO - Training [17][  340/  391]   Loss 0.115375   Top1 95.969669   Top5 99.965533   BatchTime 0.105639   LR 0.010000
INFO - Training [17][  360/  391]   Loss 0.114889   Top1 96.006944   Top5 99.965278   BatchTime 0.105210   LR 0.010000
INFO - Training [17][  380/  391]   Loss 0.114425   Top1 96.021793   Top5 99.967105   BatchTime 0.104979   LR 0.010000
INFO - ==> Top1: 96.012    Top5: 99.966    Loss: 0.115
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [17][   20/   79]   Loss 0.367631   Top1 89.960938   Top5 99.648438   BatchTime 0.126222
INFO - Validation [17][   40/   79]   Loss 0.378620   Top1 89.570312   Top5 99.570312   BatchTime 0.079160
INFO - Validation [17][   60/   79]   Loss 0.365660   Top1 89.934896   Top5 99.583333   BatchTime 0.064545
INFO - ==> Top1: 89.900    Top5: 99.600    Loss: 0.363
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.430   Top5: 99.640] Sparsity : 0.820
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 89.430   Top5: 99.550] Sparsity : 0.769
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  18
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [18][   20/  391]   Loss 0.099519   Top1 96.406250   Top5 100.000000   BatchTime 0.192823   LR 0.010000
INFO - Training [18][   40/  391]   Loss 0.107396   Top1 96.074219   Top5 100.000000   BatchTime 0.147083   LR 0.010000
INFO - Training [18][   60/  391]   Loss 0.100388   Top1 96.380208   Top5 100.000000   BatchTime 0.132230   LR 0.010000
INFO - Training [18][   80/  391]   Loss 0.101542   Top1 96.250000   Top5 100.000000   BatchTime 0.124531   LR 0.010000
INFO - Training [18][  100/  391]   Loss 0.100274   Top1 96.281250   Top5 100.000000   BatchTime 0.119759   LR 0.010000
INFO - Training [18][  120/  391]   Loss 0.100845   Top1 96.282552   Top5 100.000000   BatchTime 0.116451   LR 0.010000
INFO - Training [18][  140/  391]   Loss 0.101237   Top1 96.261161   Top5 100.000000   BatchTime 0.114248   LR 0.010000
INFO - Training [18][  160/  391]   Loss 0.101812   Top1 96.274414   Top5 99.995117   BatchTime 0.112538   LR 0.010000
INFO - Training [18][  180/  391]   Loss 0.100750   Top1 96.323785   Top5 99.995660   BatchTime 0.111202   LR 0.010000
INFO - Training [18][  200/  391]   Loss 0.102394   Top1 96.289062   Top5 99.988281   BatchTime 0.110078   LR 0.010000
INFO - Training [18][  220/  391]   Loss 0.105091   Top1 96.214489   Top5 99.989347   BatchTime 0.109252   LR 0.010000
INFO - Training [18][  240/  391]   Loss 0.107016   Top1 96.149089   Top5 99.977214   BatchTime 0.108579   LR 0.010000
INFO - Training [18][  260/  391]   Loss 0.108190   Top1 96.120793   Top5 99.978966   BatchTime 0.107894   LR 0.010000
INFO - Training [18][  280/  391]   Loss 0.107910   Top1 96.138393   Top5 99.977679   BatchTime 0.107296   LR 0.010000
INFO - Training [18][  300/  391]   Loss 0.106923   Top1 96.195312   Top5 99.976562   BatchTime 0.106800   LR 0.010000
INFO - Training [18][  320/  391]   Loss 0.106539   Top1 96.193848   Top5 99.975586   BatchTime 0.106403   LR 0.010000
INFO - Training [18][  340/  391]   Loss 0.107050   Top1 96.187960   Top5 99.972426   BatchTime 0.105958   LR 0.010000
INFO - Training [18][  360/  391]   Loss 0.107579   Top1 96.161024   Top5 99.973958   BatchTime 0.105477   LR 0.010000
INFO - Training [18][  380/  391]   Loss 0.108097   Top1 96.163651   Top5 99.975329   BatchTime 0.105058   LR 0.010000
INFO - ==> Top1: 96.150    Top5: 99.976    Loss: 0.109
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [18][   20/   79]   Loss 0.380180   Top1 89.687500   Top5 99.492188   BatchTime 0.130947
INFO - Validation [18][   40/   79]   Loss 0.385141   Top1 89.589844   Top5 99.472656   BatchTime 0.082834
INFO - Validation [18][   60/   79]   Loss 0.379948   Top1 89.674479   Top5 99.492188   BatchTime 0.066936
INFO - ==> Top1: 89.660    Top5: 99.540    Loss: 0.374
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.430   Top5: 99.640] Sparsity : 0.820
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  19
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [19][   20/  391]   Loss 0.118269   Top1 95.976562   Top5 100.000000   BatchTime 0.199221   LR 0.010000
INFO - Training [19][   40/  391]   Loss 0.104916   Top1 96.503906   Top5 100.000000   BatchTime 0.149761   LR 0.010000
INFO - Training [19][   60/  391]   Loss 0.103059   Top1 96.471354   Top5 100.000000   BatchTime 0.133921   LR 0.010000
INFO - Training [19][   80/  391]   Loss 0.101717   Top1 96.474609   Top5 100.000000   BatchTime 0.125676   LR 0.010000
INFO - Training [19][  100/  391]   Loss 0.101782   Top1 96.460938   Top5 100.000000   BatchTime 0.120763   LR 0.010000
INFO - Training [19][  120/  391]   Loss 0.102622   Top1 96.451823   Top5 100.000000   BatchTime 0.117357   LR 0.010000
INFO - Training [19][  140/  391]   Loss 0.102986   Top1 96.434152   Top5 100.000000   BatchTime 0.115101   LR 0.010000
INFO - Training [19][  160/  391]   Loss 0.102717   Top1 96.435547   Top5 100.000000   BatchTime 0.113271   LR 0.010000
INFO - Training [19][  180/  391]   Loss 0.101861   Top1 96.432292   Top5 100.000000   BatchTime 0.111931   LR 0.010000
INFO - Training [19][  200/  391]   Loss 0.101642   Top1 96.453125   Top5 100.000000   BatchTime 0.110843   LR 0.010000
INFO - Training [19][  220/  391]   Loss 0.101706   Top1 96.448864   Top5 99.996449   BatchTime 0.109856   LR 0.010000
INFO - Training [19][  240/  391]   Loss 0.102989   Top1 96.419271   Top5 99.996745   BatchTime 0.109061   LR 0.010000
INFO - Training [19][  260/  391]   Loss 0.103600   Top1 96.400240   Top5 99.993990   BatchTime 0.108409   LR 0.010000
INFO - Training [19][  280/  391]   Loss 0.102670   Top1 96.420201   Top5 99.991629   BatchTime 0.107848   LR 0.010000
INFO - Training [19][  300/  391]   Loss 0.103206   Top1 96.419271   Top5 99.992188   BatchTime 0.107351   LR 0.010000
INFO - Training [19][  320/  391]   Loss 0.103527   Top1 96.420898   Top5 99.992676   BatchTime 0.106945   LR 0.010000
INFO - Training [19][  340/  391]   Loss 0.104956   Top1 96.341912   Top5 99.990809   BatchTime 0.106600   LR 0.010000
INFO - Training [19][  360/  391]   Loss 0.105526   Top1 96.293403   Top5 99.991319   BatchTime 0.106107   LR 0.010000
INFO - Training [19][  380/  391]   Loss 0.105721   Top1 96.262336   Top5 99.991776   BatchTime 0.105652   LR 0.010000
INFO - ==> Top1: 96.250    Top5: 99.992    Loss: 0.106
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [19][   20/   79]   Loss 0.383158   Top1 89.609375   Top5 99.414062   BatchTime 0.136707
INFO - Validation [19][   40/   79]   Loss 0.382092   Top1 89.707031   Top5 99.492188   BatchTime 0.085561
INFO - Validation [19][   60/   79]   Loss 0.377486   Top1 89.726562   Top5 99.557292   BatchTime 0.068578
INFO - ==> Top1: 89.600    Top5: 99.580    Loss: 0.373
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  20
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [20][   20/  391]   Loss 0.096073   Top1 96.562500   Top5 99.960938   BatchTime 0.207062   LR 0.010000
INFO - Training [20][   40/  391]   Loss 0.089877   Top1 96.738281   Top5 99.980469   BatchTime 0.153703   LR 0.010000
INFO - Training [20][   60/  391]   Loss 0.089325   Top1 96.783854   Top5 99.986979   BatchTime 0.135972   LR 0.010000
INFO - Training [20][   80/  391]   Loss 0.094950   Top1 96.533203   Top5 99.980469   BatchTime 0.126941   LR 0.010000
INFO - Training [20][  100/  391]   Loss 0.096510   Top1 96.500000   Top5 99.984375   BatchTime 0.121758   LR 0.010000
INFO - Training [20][  120/  391]   Loss 0.096808   Top1 96.536458   Top5 99.973958   BatchTime 0.118210   LR 0.010000
INFO - Training [20][  140/  391]   Loss 0.099379   Top1 96.456473   Top5 99.977679   BatchTime 0.115780   LR 0.010000
INFO - Training [20][  160/  391]   Loss 0.099440   Top1 96.459961   Top5 99.970703   BatchTime 0.113881   LR 0.010000
INFO - Training [20][  180/  391]   Loss 0.099350   Top1 96.471354   Top5 99.969618   BatchTime 0.112443   LR 0.010000
INFO - Training [20][  200/  391]   Loss 0.101031   Top1 96.363281   Top5 99.972656   BatchTime 0.111234   LR 0.010000
INFO - Training [20][  220/  391]   Loss 0.101646   Top1 96.360085   Top5 99.971591   BatchTime 0.110244   LR 0.010000
INFO - Training [20][  240/  391]   Loss 0.102487   Top1 96.324870   Top5 99.973958   BatchTime 0.109481   LR 0.010000
INFO - Training [20][  260/  391]   Loss 0.104406   Top1 96.256010   Top5 99.975962   BatchTime 0.108851   LR 0.010000
INFO - Training [20][  280/  391]   Loss 0.105941   Top1 96.196987   Top5 99.974888   BatchTime 0.108221   LR 0.010000
INFO - Training [20][  300/  391]   Loss 0.106121   Top1 96.171875   Top5 99.973958   BatchTime 0.107747   LR 0.010000
INFO - Training [20][  320/  391]   Loss 0.106459   Top1 96.174316   Top5 99.975586   BatchTime 0.107277   LR 0.010000
INFO - Training [20][  340/  391]   Loss 0.106608   Top1 96.171875   Top5 99.977022   BatchTime 0.106765   LR 0.010000
INFO - Training [20][  360/  391]   Loss 0.107831   Top1 96.134983   Top5 99.978299   BatchTime 0.106251   LR 0.010000
INFO - Training [20][  380/  391]   Loss 0.108989   Top1 96.114309   Top5 99.979441   BatchTime 0.105794   LR 0.010000
INFO - ==> Top1: 96.102    Top5: 99.980    Loss: 0.109
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [20][   20/   79]   Loss 0.390113   Top1 89.335938   Top5 99.531250   BatchTime 0.138307
INFO - Validation [20][   40/   79]   Loss 0.393693   Top1 89.375000   Top5 99.453125   BatchTime 0.086541
INFO - Validation [20][   60/   79]   Loss 0.387343   Top1 89.557292   Top5 99.505208   BatchTime 0.069212
INFO - ==> Top1: 89.510    Top5: 99.540    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  21
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [21][   20/  391]   Loss 0.120251   Top1 95.898438   Top5 99.921875   BatchTime 0.202490   LR 0.010000
INFO - Training [21][   40/  391]   Loss 0.114677   Top1 96.093750   Top5 99.902344   BatchTime 0.151335   LR 0.010000
INFO - Training [21][   60/  391]   Loss 0.119173   Top1 95.872396   Top5 99.921875   BatchTime 0.134686   LR 0.010000
INFO - Training [21][   80/  391]   Loss 0.123724   Top1 95.556641   Top5 99.941406   BatchTime 0.126165   LR 0.010000
INFO - Training [21][  100/  391]   Loss 0.124232   Top1 95.539062   Top5 99.953125   BatchTime 0.120955   LR 0.010000
INFO - Training [21][  120/  391]   Loss 0.126075   Top1 95.488281   Top5 99.941406   BatchTime 0.117763   LR 0.010000
INFO - Training [21][  140/  391]   Loss 0.125115   Top1 95.552455   Top5 99.944196   BatchTime 0.115149   LR 0.010000
INFO - Training [21][  160/  391]   Loss 0.126277   Top1 95.498047   Top5 99.946289   BatchTime 0.113387   LR 0.010000
INFO - Training [21][  180/  391]   Loss 0.125193   Top1 95.551215   Top5 99.947917   BatchTime 0.111914   LR 0.010000
INFO - Training [21][  200/  391]   Loss 0.124669   Top1 95.578125   Top5 99.949219   BatchTime 0.110716   LR 0.010000
INFO - Training [21][  220/  391]   Loss 0.126007   Top1 95.546875   Top5 99.946733   BatchTime 0.109714   LR 0.010000
INFO - Training [21][  240/  391]   Loss 0.125256   Top1 95.556641   Top5 99.951172   BatchTime 0.108923   LR 0.010000
INFO - Training [21][  260/  391]   Loss 0.124993   Top1 95.573918   Top5 99.951923   BatchTime 0.108267   LR 0.010000
INFO - Training [21][  280/  391]   Loss 0.125571   Top1 95.541295   Top5 99.955357   BatchTime 0.107667   LR 0.010000
INFO - Training [21][  300/  391]   Loss 0.126599   Top1 95.497396   Top5 99.953125   BatchTime 0.107227   LR 0.010000
INFO - Training [21][  320/  391]   Loss 0.127121   Top1 95.463867   Top5 99.956055   BatchTime 0.106832   LR 0.010000
INFO - Training [21][  340/  391]   Loss 0.127794   Top1 95.448070   Top5 99.956342   BatchTime 0.106417   LR 0.010000
INFO - Training [21][  360/  391]   Loss 0.128554   Top1 95.392795   Top5 99.958767   BatchTime 0.105930   LR 0.010000
INFO - Training [21][  380/  391]   Loss 0.129610   Top1 95.351562   Top5 99.956826   BatchTime 0.105501   LR 0.010000
INFO - ==> Top1: 95.336    Top5: 99.954    Loss: 0.130
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [21][   20/   79]   Loss 0.392954   Top1 89.101562   Top5 99.414062   BatchTime 0.138387
INFO - Validation [21][   40/   79]   Loss 0.400266   Top1 89.003906   Top5 99.453125   BatchTime 0.086657
INFO - Validation [21][   60/   79]   Loss 0.396929   Top1 89.127604   Top5 99.531250   BatchTime 0.069332
INFO - ==> Top1: 89.190    Top5: 99.530    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  22
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [22][   20/  391]   Loss 0.105967   Top1 96.171875   Top5 100.000000   BatchTime 0.204649   LR 0.010000
INFO - Training [22][   40/  391]   Loss 0.114012   Top1 95.996094   Top5 99.980469   BatchTime 0.152406   LR 0.010000
INFO - Training [22][   60/  391]   Loss 0.117316   Top1 95.833333   Top5 99.973958   BatchTime 0.136882   LR 0.010000
INFO - Training [22][   80/  391]   Loss 0.122460   Top1 95.605469   Top5 99.960938   BatchTime 0.128428   LR 0.010000
INFO - Training [22][  100/  391]   Loss 0.124624   Top1 95.562500   Top5 99.968750   BatchTime 0.122831   LR 0.010000
INFO - Training [22][  120/  391]   Loss 0.125845   Top1 95.540365   Top5 99.973958   BatchTime 0.119301   LR 0.010000
INFO - Training [22][  140/  391]   Loss 0.127382   Top1 95.502232   Top5 99.966518   BatchTime 0.116670   LR 0.010000
INFO - Training [22][  160/  391]   Loss 0.127290   Top1 95.527344   Top5 99.965820   BatchTime 0.114700   LR 0.010000
INFO - Training [22][  180/  391]   Loss 0.127530   Top1 95.499132   Top5 99.969618   BatchTime 0.113142   LR 0.010000
INFO - Training [22][  200/  391]   Loss 0.128223   Top1 95.480469   Top5 99.968750   BatchTime 0.111959   LR 0.010000
INFO - Training [22][  220/  391]   Loss 0.127981   Top1 95.486506   Top5 99.968040   BatchTime 0.110931   LR 0.010000
INFO - Training [22][  240/  391]   Loss 0.126619   Top1 95.524089   Top5 99.970703   BatchTime 0.110081   LR 0.010000
INFO - Training [22][  260/  391]   Loss 0.126935   Top1 95.507812   Top5 99.969952   BatchTime 0.109347   LR 0.010000
INFO - Training [22][  280/  391]   Loss 0.126496   Top1 95.532924   Top5 99.969308   BatchTime 0.108731   LR 0.010000
INFO - Training [22][  300/  391]   Loss 0.127213   Top1 95.476562   Top5 99.971354   BatchTime 0.108144   LR 0.010000
INFO - Training [22][  320/  391]   Loss 0.126889   Top1 95.485840   Top5 99.973145   BatchTime 0.107634   LR 0.010000
INFO - Training [22][  340/  391]   Loss 0.126629   Top1 95.461857   Top5 99.972426   BatchTime 0.107120   LR 0.010000
INFO - Training [22][  360/  391]   Loss 0.126947   Top1 95.442708   Top5 99.971788   BatchTime 0.106640   LR 0.010000
INFO - Training [22][  380/  391]   Loss 0.128146   Top1 95.411184   Top5 99.973273   BatchTime 0.106154   LR 0.010000
INFO - ==> Top1: 95.414    Top5: 99.974    Loss: 0.128
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [22][   20/   79]   Loss 0.393435   Top1 89.062500   Top5 99.492188   BatchTime 0.130729
INFO - Validation [22][   40/   79]   Loss 0.394641   Top1 88.769531   Top5 99.472656   BatchTime 0.082887
INFO - Validation [22][   60/   79]   Loss 0.383484   Top1 89.283854   Top5 99.531250   BatchTime 0.066698
INFO - ==> Top1: 89.060    Top5: 99.530    Loss: 0.385
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  23
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [23][   20/  391]   Loss 0.117790   Top1 95.703125   Top5 99.960938   BatchTime 0.205315   LR 0.010000
INFO - Training [23][   40/  391]   Loss 0.115982   Top1 95.917969   Top5 99.960938   BatchTime 0.153584   LR 0.010000
INFO - Training [23][   60/  391]   Loss 0.117073   Top1 95.963542   Top5 99.973958   BatchTime 0.136166   LR 0.010000
INFO - Training [23][   80/  391]   Loss 0.115760   Top1 95.898438   Top5 99.960938   BatchTime 0.127509   LR 0.010000
INFO - Training [23][  100/  391]   Loss 0.114543   Top1 95.914062   Top5 99.968750   BatchTime 0.122327   LR 0.010000
INFO - Training [23][  120/  391]   Loss 0.113398   Top1 96.022135   Top5 99.973958   BatchTime 0.118680   LR 0.010000
INFO - Training [23][  140/  391]   Loss 0.114011   Top1 95.970982   Top5 99.966518   BatchTime 0.116088   LR 0.010000
INFO - Training [23][  160/  391]   Loss 0.116474   Top1 95.849609   Top5 99.965820   BatchTime 0.114141   LR 0.010000
INFO - Training [23][  180/  391]   Loss 0.116353   Top1 95.881076   Top5 99.965278   BatchTime 0.112516   LR 0.010000
INFO - Training [23][  200/  391]   Loss 0.117882   Top1 95.820312   Top5 99.964844   BatchTime 0.111283   LR 0.010000
INFO - Training [23][  220/  391]   Loss 0.118061   Top1 95.777699   Top5 99.968040   BatchTime 0.110336   LR 0.010000
INFO - Training [23][  240/  391]   Loss 0.118968   Top1 95.751953   Top5 99.967448   BatchTime 0.109458   LR 0.010000
INFO - Training [23][  260/  391]   Loss 0.120195   Top1 95.706130   Top5 99.966947   BatchTime 0.108815   LR 0.010000
INFO - Training [23][  280/  391]   Loss 0.120957   Top1 95.700335   Top5 99.963728   BatchTime 0.108239   LR 0.010000
INFO - Training [23][  300/  391]   Loss 0.121875   Top1 95.653646   Top5 99.958333   BatchTime 0.107686   LR 0.010000
INFO - Training [23][  320/  391]   Loss 0.121303   Top1 95.688477   Top5 99.956055   BatchTime 0.107295   LR 0.010000
INFO - Training [23][  340/  391]   Loss 0.120942   Top1 95.710018   Top5 99.956342   BatchTime 0.106786   LR 0.010000
INFO - Training [23][  360/  391]   Loss 0.121038   Top1 95.683594   Top5 99.958767   BatchTime 0.106278   LR 0.010000
INFO - Training [23][  380/  391]   Loss 0.120500   Top1 95.711349   Top5 99.958882   BatchTime 0.105827   LR 0.010000
INFO - ==> Top1: 95.720    Top5: 99.960    Loss: 0.120
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [23][   20/   79]   Loss 0.380415   Top1 89.570312   Top5 99.687500   BatchTime 0.131299
INFO - Validation [23][   40/   79]   Loss 0.388949   Top1 89.414062   Top5 99.570312   BatchTime 0.082960
INFO - Validation [23][   60/   79]   Loss 0.394303   Top1 89.361979   Top5 99.557292   BatchTime 0.066952
INFO - ==> Top1: 89.510    Top5: 99.620    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  24
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [24][   20/  391]   Loss 0.110423   Top1 96.210938   Top5 100.000000   BatchTime 0.199273   LR 0.010000
INFO - Training [24][   40/  391]   Loss 0.108991   Top1 96.269531   Top5 99.980469   BatchTime 0.150154   LR 0.010000
INFO - Training [24][   60/  391]   Loss 0.111035   Top1 96.054688   Top5 99.986979   BatchTime 0.133646   LR 0.010000
INFO - Training [24][   80/  391]   Loss 0.108348   Top1 96.162109   Top5 99.990234   BatchTime 0.125259   LR 0.010000
INFO - Training [24][  100/  391]   Loss 0.108823   Top1 96.164062   Top5 99.992188   BatchTime 0.120240   LR 0.010000
INFO - Training [24][  120/  391]   Loss 0.109618   Top1 96.093750   Top5 99.993490   BatchTime 0.116925   LR 0.010000
INFO - Training [24][  140/  391]   Loss 0.108766   Top1 96.049107   Top5 99.988839   BatchTime 0.115109   LR 0.010000
INFO - Training [24][  160/  391]   Loss 0.108097   Top1 96.088867   Top5 99.990234   BatchTime 0.113244   LR 0.010000
INFO - Training [24][  180/  391]   Loss 0.109493   Top1 96.002604   Top5 99.991319   BatchTime 0.111770   LR 0.010000
INFO - Training [24][  200/  391]   Loss 0.113306   Top1 95.867188   Top5 99.984375   BatchTime 0.110705   LR 0.010000
INFO - Training [24][  220/  391]   Loss 0.112854   Top1 95.862926   Top5 99.985795   BatchTime 0.109841   LR 0.010000
INFO - Training [24][  240/  391]   Loss 0.113082   Top1 95.872396   Top5 99.983724   BatchTime 0.109076   LR 0.010000
INFO - Training [24][  260/  391]   Loss 0.112247   Top1 95.940505   Top5 99.981971   BatchTime 0.108393   LR 0.010000
INFO - Training [24][  280/  391]   Loss 0.111692   Top1 95.979353   Top5 99.980469   BatchTime 0.107812   LR 0.010000
INFO - Training [24][  300/  391]   Loss 0.113587   Top1 95.914062   Top5 99.981771   BatchTime 0.107305   LR 0.010000
INFO - Training [24][  320/  391]   Loss 0.114471   Top1 95.888672   Top5 99.980469   BatchTime 0.106870   LR 0.010000
INFO - Training [24][  340/  391]   Loss 0.114144   Top1 95.903033   Top5 99.981618   BatchTime 0.106369   LR 0.010000
INFO - Training [24][  360/  391]   Loss 0.115124   Top1 95.852865   Top5 99.976128   BatchTime 0.105907   LR 0.010000
INFO - Training [24][  380/  391]   Loss 0.115787   Top1 95.824424   Top5 99.977385   BatchTime 0.105472   LR 0.010000
INFO - ==> Top1: 95.824    Top5: 99.978    Loss: 0.116
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [24][   20/   79]   Loss 0.379836   Top1 89.570312   Top5 99.570312   BatchTime 0.132071
INFO - Validation [24][   40/   79]   Loss 0.376584   Top1 89.492188   Top5 99.492188   BatchTime 0.083472
INFO - Validation [24][   60/   79]   Loss 0.374031   Top1 89.583333   Top5 99.544271   BatchTime 0.067262
INFO - ==> Top1: 89.460    Top5: 99.560    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  25
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [25][   20/  391]   Loss 0.112501   Top1 95.703125   Top5 100.000000   BatchTime 0.202393   LR 0.010000
INFO - Training [25][   40/  391]   Loss 0.110183   Top1 95.878906   Top5 99.980469   BatchTime 0.152138   LR 0.010000
INFO - Training [25][   60/  391]   Loss 0.108459   Top1 95.989583   Top5 99.986979   BatchTime 0.134451   LR 0.010000
INFO - Training [25][   80/  391]   Loss 0.109082   Top1 95.888672   Top5 99.990234   BatchTime 0.126045   LR 0.010000
INFO - Training [25][  100/  391]   Loss 0.110052   Top1 95.875000   Top5 99.984375   BatchTime 0.120774   LR 0.010000
INFO - Training [25][  120/  391]   Loss 0.109395   Top1 95.885417   Top5 99.980469   BatchTime 0.117435   LR 0.010000
INFO - Training [25][  140/  391]   Loss 0.108390   Top1 95.959821   Top5 99.977679   BatchTime 0.114998   LR 0.010000
INFO - Training [25][  160/  391]   Loss 0.108424   Top1 95.996094   Top5 99.965820   BatchTime 0.113390   LR 0.010000
INFO - Training [25][  180/  391]   Loss 0.108881   Top1 96.006944   Top5 99.965278   BatchTime 0.111929   LR 0.010000
INFO - Training [25][  200/  391]   Loss 0.108678   Top1 96.011719   Top5 99.953125   BatchTime 0.110829   LR 0.010000
INFO - Training [25][  220/  391]   Loss 0.109685   Top1 95.962358   Top5 99.950284   BatchTime 0.109911   LR 0.010000
INFO - Training [25][  240/  391]   Loss 0.110584   Top1 95.966797   Top5 99.944661   BatchTime 0.109073   LR 0.010000
INFO - Training [25][  260/  391]   Loss 0.110551   Top1 96.000601   Top5 99.948918   BatchTime 0.108410   LR 0.010000
INFO - Training [25][  280/  391]   Loss 0.109813   Top1 96.021205   Top5 99.952567   BatchTime 0.107851   LR 0.010000
INFO - Training [25][  300/  391]   Loss 0.110106   Top1 96.002604   Top5 99.955729   BatchTime 0.107335   LR 0.010000
INFO - Training [25][  320/  391]   Loss 0.110683   Top1 95.969238   Top5 99.956055   BatchTime 0.106917   LR 0.010000
INFO - Training [25][  340/  391]   Loss 0.110450   Top1 95.994945   Top5 99.956342   BatchTime 0.106432   LR 0.010000
INFO - Training [25][  360/  391]   Loss 0.111760   Top1 95.950521   Top5 99.956597   BatchTime 0.105959   LR 0.010000
INFO - Training [25][  380/  391]   Loss 0.112285   Top1 95.929276   Top5 99.958882   BatchTime 0.105531   LR 0.010000
INFO - ==> Top1: 95.924    Top5: 99.960    Loss: 0.113
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [25][   20/   79]   Loss 0.381800   Top1 90.039062   Top5 99.492188   BatchTime 0.131559
INFO - Validation [25][   40/   79]   Loss 0.387236   Top1 89.589844   Top5 99.414062   BatchTime 0.083132
INFO - Validation [25][   60/   79]   Loss 0.384880   Top1 89.622396   Top5 99.505208   BatchTime 0.067069
INFO - ==> Top1: 89.580    Top5: 99.510    Loss: 0.380
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  26
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [26][   20/  391]   Loss 0.110082   Top1 95.976562   Top5 100.000000   BatchTime 0.204200   LR 0.010000
INFO - Training [26][   40/  391]   Loss 0.116854   Top1 95.664062   Top5 100.000000   BatchTime 0.152462   LR 0.010000
INFO - Training [26][   60/  391]   Loss 0.118566   Top1 95.638021   Top5 100.000000   BatchTime 0.135491   LR 0.010000
INFO - Training [26][   80/  391]   Loss 0.121848   Top1 95.556641   Top5 100.000000   BatchTime 0.126679   LR 0.010000
INFO - Training [26][  100/  391]   Loss 0.122135   Top1 95.562500   Top5 100.000000   BatchTime 0.121535   LR 0.010000
INFO - Training [26][  120/  391]   Loss 0.121597   Top1 95.572917   Top5 100.000000   BatchTime 0.117965   LR 0.010000
INFO - Training [26][  140/  391]   Loss 0.123082   Top1 95.507812   Top5 100.000000   BatchTime 0.115400   LR 0.010000
INFO - Training [26][  160/  391]   Loss 0.126008   Top1 95.395508   Top5 100.000000   BatchTime 0.114118   LR 0.010000
INFO - Training [26][  180/  391]   Loss 0.126646   Top1 95.416667   Top5 99.991319   BatchTime 0.112259   LR 0.010000
INFO - Training [26][  200/  391]   Loss 0.125159   Top1 95.492188   Top5 99.992188   BatchTime 0.111153   LR 0.010000
INFO - Training [26][  220/  391]   Loss 0.125933   Top1 95.475852   Top5 99.992898   BatchTime 0.110036   LR 0.010000
INFO - Training [26][  240/  391]   Loss 0.127368   Top1 95.445964   Top5 99.986979   BatchTime 0.109244   LR 0.010000
INFO - Training [26][  260/  391]   Loss 0.127627   Top1 95.423678   Top5 99.978966   BatchTime 0.108516   LR 0.010000
INFO - Training [26][  280/  391]   Loss 0.127917   Top1 95.429688   Top5 99.980469   BatchTime 0.107935   LR 0.010000
INFO - Training [26][  300/  391]   Loss 0.127743   Top1 95.432292   Top5 99.981771   BatchTime 0.107391   LR 0.010000
INFO - Training [26][  320/  391]   Loss 0.129128   Top1 95.378418   Top5 99.982910   BatchTime 0.106963   LR 0.010000
INFO - Training [26][  340/  391]   Loss 0.128731   Top1 95.411305   Top5 99.981618   BatchTime 0.106470   LR 0.010000
INFO - Training [26][  360/  391]   Loss 0.129579   Top1 95.390625   Top5 99.978299   BatchTime 0.105987   LR 0.010000
INFO - Training [26][  380/  391]   Loss 0.130219   Top1 95.388569   Top5 99.977385   BatchTime 0.105534   LR 0.010000
INFO - ==> Top1: 95.374    Top5: 99.974    Loss: 0.131
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [26][   20/   79]   Loss 0.389176   Top1 89.375000   Top5 99.453125   BatchTime 0.133713
INFO - Validation [26][   40/   79]   Loss 0.388449   Top1 89.335938   Top5 99.355469   BatchTime 0.084448
INFO - Validation [26][   60/   79]   Loss 0.388167   Top1 89.427083   Top5 99.453125   BatchTime 0.067808
INFO - ==> Top1: 89.350    Top5: 99.510    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  27
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [27][   20/  391]   Loss 0.138827   Top1 94.609375   Top5 99.960938   BatchTime 0.199743   LR 0.010000
INFO - Training [27][   40/  391]   Loss 0.135858   Top1 94.687500   Top5 99.980469   BatchTime 0.150474   LR 0.010000
INFO - Training [27][   60/  391]   Loss 0.137132   Top1 94.921875   Top5 99.973958   BatchTime 0.134045   LR 0.010000
INFO - Training [27][   80/  391]   Loss 0.133830   Top1 95.126953   Top5 99.980469   BatchTime 0.125638   LR 0.010000
INFO - Training [27][  100/  391]   Loss 0.132823   Top1 95.234375   Top5 99.960938   BatchTime 0.120731   LR 0.010000
INFO - Training [27][  120/  391]   Loss 0.133808   Top1 95.247396   Top5 99.967448   BatchTime 0.117317   LR 0.010000
INFO - Training [27][  140/  391]   Loss 0.132896   Top1 95.262277   Top5 99.972098   BatchTime 0.115024   LR 0.010000
INFO - Training [27][  160/  391]   Loss 0.131523   Top1 95.317383   Top5 99.975586   BatchTime 0.113160   LR 0.010000
INFO - Training [27][  180/  391]   Loss 0.131141   Top1 95.342882   Top5 99.978299   BatchTime 0.111765   LR 0.010000
INFO - Training [27][  200/  391]   Loss 0.132240   Top1 95.304688   Top5 99.980469   BatchTime 0.110674   LR 0.010000
INFO - Training [27][  220/  391]   Loss 0.131088   Top1 95.362216   Top5 99.982244   BatchTime 0.109711   LR 0.010000
INFO - Training [27][  240/  391]   Loss 0.130275   Top1 95.345052   Top5 99.980469   BatchTime 0.108830   LR 0.010000
INFO - Training [27][  260/  391]   Loss 0.130096   Top1 95.342548   Top5 99.981971   BatchTime 0.108227   LR 0.010000
INFO - Training [27][  280/  391]   Loss 0.130043   Top1 95.334821   Top5 99.983259   BatchTime 0.107723   LR 0.010000
INFO - Training [27][  300/  391]   Loss 0.130491   Top1 95.286458   Top5 99.984375   BatchTime 0.107276   LR 0.010000
INFO - Training [27][  320/  391]   Loss 0.130680   Top1 95.270996   Top5 99.982910   BatchTime 0.106918   LR 0.010000
INFO - Training [27][  340/  391]   Loss 0.130430   Top1 95.284926   Top5 99.983915   BatchTime 0.106402   LR 0.010000
INFO - Training [27][  360/  391]   Loss 0.130485   Top1 95.284288   Top5 99.984809   BatchTime 0.105892   LR 0.010000
INFO - Training [27][  380/  391]   Loss 0.131007   Top1 95.271382   Top5 99.983553   BatchTime 0.105447   LR 0.010000
INFO - ==> Top1: 95.274    Top5: 99.982    Loss: 0.131
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [27][   20/   79]   Loss 0.409836   Top1 88.632812   Top5 99.531250   BatchTime 0.132783
INFO - Validation [27][   40/   79]   Loss 0.407618   Top1 88.593750   Top5 99.453125   BatchTime 0.083836
INFO - Validation [27][   60/   79]   Loss 0.402655   Top1 88.958333   Top5 99.518229   BatchTime 0.067444
INFO - ==> Top1: 88.940    Top5: 99.570    Loss: 0.396
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  28
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [28][   20/  391]   Loss 0.125441   Top1 95.820312   Top5 99.960938   BatchTime 0.202287   LR 0.010000
INFO - Training [28][   40/  391]   Loss 0.119644   Top1 95.839844   Top5 99.980469   BatchTime 0.151308   LR 0.010000
INFO - Training [28][   60/  391]   Loss 0.128358   Top1 95.390625   Top5 99.973958   BatchTime 0.134239   LR 0.010000
INFO - Training [28][   80/  391]   Loss 0.126359   Top1 95.556641   Top5 99.970703   BatchTime 0.125907   LR 0.010000
INFO - Training [28][  100/  391]   Loss 0.123654   Top1 95.656250   Top5 99.976562   BatchTime 0.120721   LR 0.010000
INFO - Training [28][  120/  391]   Loss 0.123769   Top1 95.631510   Top5 99.973958   BatchTime 0.117397   LR 0.010000
INFO - Training [28][  140/  391]   Loss 0.120832   Top1 95.809152   Top5 99.972098   BatchTime 0.115014   LR 0.010000
INFO - Training [28][  160/  391]   Loss 0.121700   Top1 95.742188   Top5 99.970703   BatchTime 0.113142   LR 0.010000
INFO - Training [28][  180/  391]   Loss 0.121841   Top1 95.750868   Top5 99.969618   BatchTime 0.111721   LR 0.010000
INFO - Training [28][  200/  391]   Loss 0.122254   Top1 95.738281   Top5 99.968750   BatchTime 0.110598   LR 0.010000
INFO - Training [28][  220/  391]   Loss 0.121719   Top1 95.788352   Top5 99.968040   BatchTime 0.110032   LR 0.010000
INFO - Training [28][  240/  391]   Loss 0.120252   Top1 95.846354   Top5 99.960938   BatchTime 0.108968   LR 0.010000
INFO - Training [28][  260/  391]   Loss 0.119779   Top1 95.847356   Top5 99.960938   BatchTime 0.108308   LR 0.010000
INFO - Training [28][  280/  391]   Loss 0.120009   Top1 95.831473   Top5 99.960938   BatchTime 0.107664   LR 0.010000
INFO - Training [28][  300/  391]   Loss 0.119817   Top1 95.830729   Top5 99.963542   BatchTime 0.107125   LR 0.010000
INFO - Training [28][  320/  391]   Loss 0.121110   Top1 95.781250   Top5 99.960938   BatchTime 0.106692   LR 0.010000
INFO - Training [28][  340/  391]   Loss 0.121666   Top1 95.769761   Top5 99.960938   BatchTime 0.106204   LR 0.010000
INFO - Training [28][  360/  391]   Loss 0.121162   Top1 95.766059   Top5 99.958767   BatchTime 0.105751   LR 0.010000
INFO - Training [28][  380/  391]   Loss 0.121753   Top1 95.762747   Top5 99.958882   BatchTime 0.105354   LR 0.010000
INFO - ==> Top1: 95.746    Top5: 99.960    Loss: 0.122
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [28][   20/   79]   Loss 0.373312   Top1 89.179688   Top5 99.609375   BatchTime 0.133117
INFO - Validation [28][   40/   79]   Loss 0.375815   Top1 89.296875   Top5 99.609375   BatchTime 0.083900
INFO - Validation [28][   60/   79]   Loss 0.369120   Top1 89.700521   Top5 99.648438   BatchTime 0.067413
INFO - ==> Top1: 89.560    Top5: 99.660    Loss: 0.368
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Scoreboard best 3 ==> Epoch [19][Top1: 89.600   Top5: 99.580] Sparsity : 0.825
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  29
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [29][   20/  391]   Loss 0.104757   Top1 96.171875   Top5 100.000000   BatchTime 0.204206   LR 0.010000
INFO - Training [29][   40/  391]   Loss 0.102313   Top1 96.289062   Top5 100.000000   BatchTime 0.152692   LR 0.010000
INFO - Training [29][   60/  391]   Loss 0.103839   Top1 96.184896   Top5 99.986979   BatchTime 0.135332   LR 0.010000
INFO - Training [29][   80/  391]   Loss 0.108060   Top1 95.996094   Top5 99.990234   BatchTime 0.127013   LR 0.010000
INFO - Training [29][  100/  391]   Loss 0.109127   Top1 96.023438   Top5 99.976562   BatchTime 0.121954   LR 0.010000
INFO - Training [29][  120/  391]   Loss 0.108320   Top1 96.041667   Top5 99.980469   BatchTime 0.118463   LR 0.010000
INFO - Training [29][  140/  391]   Loss 0.109587   Top1 96.010045   Top5 99.977679   BatchTime 0.115923   LR 0.010000
INFO - Training [29][  160/  391]   Loss 0.112263   Top1 95.942383   Top5 99.975586   BatchTime 0.114050   LR 0.010000
INFO - Training [29][  180/  391]   Loss 0.113195   Top1 95.885417   Top5 99.978299   BatchTime 0.112653   LR 0.010000
INFO - Training [29][  200/  391]   Loss 0.114340   Top1 95.859375   Top5 99.972656   BatchTime 0.111434   LR 0.010000
INFO - Training [29][  220/  391]   Loss 0.114383   Top1 95.880682   Top5 99.975142   BatchTime 0.110432   LR 0.010000
INFO - Training [29][  240/  391]   Loss 0.114426   Top1 95.878906   Top5 99.977214   BatchTime 0.109609   LR 0.010000
INFO - Training [29][  260/  391]   Loss 0.115146   Top1 95.841346   Top5 99.972957   BatchTime 0.108965   LR 0.010000
INFO - Training [29][  280/  391]   Loss 0.115378   Top1 95.862165   Top5 99.966518   BatchTime 0.108366   LR 0.010000
INFO - Training [29][  300/  391]   Loss 0.113996   Top1 95.929688   Top5 99.968750   BatchTime 0.107863   LR 0.010000
INFO - Training [29][  320/  391]   Loss 0.115180   Top1 95.910645   Top5 99.970703   BatchTime 0.107406   LR 0.010000
INFO - Training [29][  340/  391]   Loss 0.115386   Top1 95.914522   Top5 99.970129   BatchTime 0.106888   LR 0.010000
INFO - Training [29][  360/  391]   Loss 0.115499   Top1 95.922309   Top5 99.971788   BatchTime 0.106379   LR 0.010000
INFO - Training [29][  380/  391]   Loss 0.115794   Top1 95.875822   Top5 99.971217   BatchTime 0.105943   LR 0.010000
INFO - ==> Top1: 95.854    Top5: 99.972    Loss: 0.116
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [29][   20/   79]   Loss 0.376043   Top1 89.414062   Top5 99.648438   BatchTime 0.132212
INFO - Validation [29][   40/   79]   Loss 0.378178   Top1 89.414062   Top5 99.570312   BatchTime 0.083736
INFO - Validation [29][   60/   79]   Loss 0.368832   Top1 89.648438   Top5 99.609375   BatchTime 0.067436
INFO - ==> Top1: 89.740    Top5: 99.620    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 2 ==> Epoch [29][Top1: 89.740   Top5: 99.620] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [18][Top1: 89.660   Top5: 99.540] Sparsity : 0.823
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  30
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [30][   20/  391]   Loss 0.102626   Top1 96.406250   Top5 99.960938   BatchTime 0.204837   LR 0.001000
INFO - Training [30][   40/  391]   Loss 0.101392   Top1 96.328125   Top5 99.980469   BatchTime 0.152673   LR 0.001000
INFO - Training [30][   60/  391]   Loss 0.095439   Top1 96.523438   Top5 99.973958   BatchTime 0.135327   LR 0.001000
INFO - Training [30][   80/  391]   Loss 0.098490   Top1 96.337891   Top5 99.980469   BatchTime 0.126796   LR 0.001000
INFO - Training [30][  100/  391]   Loss 0.099320   Top1 96.289062   Top5 99.984375   BatchTime 0.121748   LR 0.001000
INFO - Training [30][  120/  391]   Loss 0.098273   Top1 96.360677   Top5 99.986979   BatchTime 0.118396   LR 0.001000
INFO - Training [30][  140/  391]   Loss 0.097471   Top1 96.406250   Top5 99.983259   BatchTime 0.115954   LR 0.001000
INFO - Training [30][  160/  391]   Loss 0.096620   Top1 96.489258   Top5 99.985352   BatchTime 0.113987   LR 0.001000
INFO - Training [30][  180/  391]   Loss 0.096748   Top1 96.436632   Top5 99.982639   BatchTime 0.112428   LR 0.001000
INFO - Training [30][  200/  391]   Loss 0.097240   Top1 96.449219   Top5 99.984375   BatchTime 0.111207   LR 0.001000
INFO - Training [30][  220/  391]   Loss 0.097090   Top1 96.473722   Top5 99.985795   BatchTime 0.110271   LR 0.001000
INFO - Training [30][  240/  391]   Loss 0.097201   Top1 96.474609   Top5 99.986979   BatchTime 0.109416   LR 0.001000
INFO - Training [30][  260/  391]   Loss 0.098203   Top1 96.460337   Top5 99.981971   BatchTime 0.108740   LR 0.001000
INFO - Training [30][  280/  391]   Loss 0.098468   Top1 96.459263   Top5 99.983259   BatchTime 0.108621   LR 0.001000
INFO - Training [30][  300/  391]   Loss 0.097634   Top1 96.502604   Top5 99.981771   BatchTime 0.108135   LR 0.001000
INFO - Training [30][  320/  391]   Loss 0.097275   Top1 96.523438   Top5 99.982910   BatchTime 0.107618   LR 0.001000
INFO - Training [30][  340/  391]   Loss 0.096734   Top1 96.534926   Top5 99.983915   BatchTime 0.107130   LR 0.001000
INFO - Training [30][  360/  391]   Loss 0.096851   Top1 96.527778   Top5 99.984809   BatchTime 0.106612   LR 0.001000
INFO - Training [30][  380/  391]   Loss 0.096038   Top1 96.581003   Top5 99.983553   BatchTime 0.106166   LR 0.001000
INFO - ==> Top1: 96.582    Top5: 99.982    Loss: 0.096
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [30][   20/   79]   Loss 0.358104   Top1 90.546875   Top5 99.648438   BatchTime 0.132111
INFO - Validation [30][   40/   79]   Loss 0.366437   Top1 90.253906   Top5 99.628906   BatchTime 0.083479
INFO - Validation [30][   60/   79]   Loss 0.358299   Top1 90.468750   Top5 99.648438   BatchTime 0.067324
INFO - ==> Top1: 90.260    Top5: 99.690    Loss: 0.356
INFO - Scoreboard best 1 ==> Epoch [30][Top1: 90.260   Top5: 99.690] Sparsity : 0.848
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Scoreboard best 3 ==> Epoch [29][Top1: 89.740   Top5: 99.620] Sparsity : 0.848
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  31
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [31][   20/  391]   Loss 0.091271   Top1 96.757812   Top5 100.000000   BatchTime 0.199002   LR 0.001000
INFO - Training [31][   40/  391]   Loss 0.091410   Top1 96.835938   Top5 100.000000   BatchTime 0.149899   LR 0.001000
INFO - Training [31][   60/  391]   Loss 0.092435   Top1 96.757812   Top5 100.000000   BatchTime 0.133752   LR 0.001000
INFO - Training [31][   80/  391]   Loss 0.090692   Top1 96.845703   Top5 99.990234   BatchTime 0.125339   LR 0.001000
INFO - Training [31][  100/  391]   Loss 0.091235   Top1 96.804688   Top5 99.992188   BatchTime 0.120395   LR 0.001000
INFO - Training [31][  120/  391]   Loss 0.090322   Top1 96.855469   Top5 99.986979   BatchTime 0.117032   LR 0.001000
INFO - Training [31][  140/  391]   Loss 0.089372   Top1 96.852679   Top5 99.988839   BatchTime 0.114628   LR 0.001000
INFO - Training [31][  160/  391]   Loss 0.089936   Top1 96.767578   Top5 99.990234   BatchTime 0.112915   LR 0.001000
INFO - Training [31][  180/  391]   Loss 0.088930   Top1 96.818576   Top5 99.991319   BatchTime 0.111565   LR 0.001000
INFO - Training [31][  200/  391]   Loss 0.087869   Top1 96.882812   Top5 99.992188   BatchTime 0.110650   LR 0.001000
INFO - Training [31][  220/  391]   Loss 0.088090   Top1 96.832386   Top5 99.992898   BatchTime 0.109904   LR 0.001000
INFO - Training [31][  240/  391]   Loss 0.087536   Top1 96.858724   Top5 99.993490   BatchTime 0.109111   LR 0.001000
INFO - Training [31][  260/  391]   Loss 0.086963   Top1 96.878005   Top5 99.993990   BatchTime 0.108477   LR 0.001000
INFO - Training [31][  280/  391]   Loss 0.086142   Top1 96.902902   Top5 99.994420   BatchTime 0.107922   LR 0.001000
INFO - Training [31][  300/  391]   Loss 0.086189   Top1 96.898438   Top5 99.994792   BatchTime 0.107389   LR 0.001000
INFO - Training [31][  320/  391]   Loss 0.085463   Top1 96.931152   Top5 99.995117   BatchTime 0.106989   LR 0.001000
INFO - Training [31][  340/  391]   Loss 0.085438   Top1 96.918658   Top5 99.995404   BatchTime 0.106476   LR 0.001000
INFO - Training [31][  360/  391]   Loss 0.085847   Top1 96.922743   Top5 99.995660   BatchTime 0.105994   LR 0.001000
INFO - Training [31][  380/  391]   Loss 0.086627   Top1 96.889391   Top5 99.993832   BatchTime 0.105534   LR 0.001000
INFO - ==> Top1: 96.896    Top5: 99.994    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [31][   20/   79]   Loss 0.357445   Top1 90.546875   Top5 99.609375   BatchTime 0.132848
INFO - Validation [31][   40/   79]   Loss 0.367186   Top1 90.371094   Top5 99.511719   BatchTime 0.083864
INFO - Validation [31][   60/   79]   Loss 0.356979   Top1 90.742188   Top5 99.570312   BatchTime 0.067473
INFO - ==> Top1: 90.440    Top5: 99.590    Loss: 0.358
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Scoreboard best 2 ==> Epoch [30][Top1: 90.260   Top5: 99.690] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 89.900   Top5: 99.600] Sparsity : 0.822
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  32
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [32][   20/  391]   Loss 0.075228   Top1 97.460938   Top5 100.000000   BatchTime 0.198363   LR 0.001000
INFO - Training [32][   40/  391]   Loss 0.083002   Top1 97.285156   Top5 99.980469   BatchTime 0.150213   LR 0.001000
INFO - Training [32][   60/  391]   Loss 0.082473   Top1 97.330729   Top5 99.986979   BatchTime 0.133751   LR 0.001000
INFO - Training [32][   80/  391]   Loss 0.082722   Top1 97.304688   Top5 99.990234   BatchTime 0.125580   LR 0.001000
INFO - Training [32][  100/  391]   Loss 0.085095   Top1 97.195312   Top5 99.992188   BatchTime 0.120683   LR 0.001000
INFO - Training [32][  120/  391]   Loss 0.084569   Top1 97.200521   Top5 99.993490   BatchTime 0.117295   LR 0.001000
INFO - Training [32][  140/  391]   Loss 0.084346   Top1 97.209821   Top5 99.988839   BatchTime 0.114750   LR 0.001000
INFO - Training [32][  160/  391]   Loss 0.085313   Top1 97.143555   Top5 99.990234   BatchTime 0.112931   LR 0.001000
INFO - Training [32][  180/  391]   Loss 0.084787   Top1 97.122396   Top5 99.982639   BatchTime 0.111593   LR 0.001000
INFO - Training [32][  200/  391]   Loss 0.086148   Top1 97.062500   Top5 99.980469   BatchTime 0.110449   LR 0.001000
INFO - Training [32][  220/  391]   Loss 0.086823   Top1 97.024148   Top5 99.982244   BatchTime 0.109628   LR 0.001000
INFO - Training [32][  240/  391]   Loss 0.086066   Top1 97.047526   Top5 99.983724   BatchTime 0.108972   LR 0.001000
INFO - Training [32][  260/  391]   Loss 0.086224   Top1 97.058293   Top5 99.981971   BatchTime 0.108383   LR 0.001000
INFO - Training [32][  280/  391]   Loss 0.085650   Top1 97.092634   Top5 99.983259   BatchTime 0.107904   LR 0.001000
INFO - Training [32][  300/  391]   Loss 0.085537   Top1 97.080729   Top5 99.981771   BatchTime 0.107471   LR 0.001000
INFO - Training [32][  320/  391]   Loss 0.085025   Top1 97.084961   Top5 99.982910   BatchTime 0.107360   LR 0.001000
INFO - Training [32][  340/  391]   Loss 0.085528   Top1 97.058824   Top5 99.979320   BatchTime 0.106860   LR 0.001000
INFO - Training [32][  360/  391]   Loss 0.085712   Top1 97.057292   Top5 99.978299   BatchTime 0.106368   LR 0.001000
INFO - Training [32][  380/  391]   Loss 0.086259   Top1 97.045641   Top5 99.979441   BatchTime 0.105894   LR 0.001000
INFO - ==> Top1: 97.040    Top5: 99.980    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [32][   20/   79]   Loss 0.362169   Top1 90.781250   Top5 99.570312   BatchTime 0.132254
INFO - Validation [32][   40/   79]   Loss 0.365793   Top1 90.410156   Top5 99.589844   BatchTime 0.083588
INFO - Validation [32][   60/   79]   Loss 0.356662   Top1 90.625000   Top5 99.622396   BatchTime 0.067278
INFO - ==> Top1: 90.400    Top5: 99.690    Loss: 0.354
INFO - Scoreboard best 1 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Scoreboard best 2 ==> Epoch [32][Top1: 90.400   Top5: 99.690] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [30][Top1: 90.260   Top5: 99.690] Sparsity : 0.848
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  33
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [33][   20/  391]   Loss 0.073816   Top1 97.421875   Top5 99.960938   BatchTime 0.202368   LR 0.001000
INFO - Training [33][   40/  391]   Loss 0.079508   Top1 97.304688   Top5 99.960938   BatchTime 0.151607   LR 0.001000
INFO - Training [33][   60/  391]   Loss 0.075772   Top1 97.421875   Top5 99.973958   BatchTime 0.134662   LR 0.001000
INFO - Training [33][   80/  391]   Loss 0.077262   Top1 97.324219   Top5 99.980469   BatchTime 0.126290   LR 0.001000
INFO - Training [33][  100/  391]   Loss 0.079943   Top1 97.250000   Top5 99.984375   BatchTime 0.121285   LR 0.001000
INFO - Training [33][  120/  391]   Loss 0.080814   Top1 97.213542   Top5 99.986979   BatchTime 0.117876   LR 0.001000
INFO - Training [33][  140/  391]   Loss 0.081288   Top1 97.176339   Top5 99.988839   BatchTime 0.115544   LR 0.001000
INFO - Training [33][  160/  391]   Loss 0.080766   Top1 97.226562   Top5 99.990234   BatchTime 0.113597   LR 0.001000
INFO - Training [33][  180/  391]   Loss 0.081881   Top1 97.183160   Top5 99.991319   BatchTime 0.112084   LR 0.001000
INFO - Training [33][  200/  391]   Loss 0.080579   Top1 97.238281   Top5 99.984375   BatchTime 0.110923   LR 0.001000
INFO - Training [33][  220/  391]   Loss 0.081109   Top1 97.240767   Top5 99.982244   BatchTime 0.109919   LR 0.001000
INFO - Training [33][  240/  391]   Loss 0.080481   Top1 97.239583   Top5 99.983724   BatchTime 0.109137   LR 0.001000
INFO - Training [33][  260/  391]   Loss 0.079705   Top1 97.271635   Top5 99.984976   BatchTime 0.108478   LR 0.001000
INFO - Training [33][  280/  391]   Loss 0.080152   Top1 97.260045   Top5 99.986049   BatchTime 0.107877   LR 0.001000
INFO - Training [33][  300/  391]   Loss 0.080564   Top1 97.247396   Top5 99.986979   BatchTime 0.107360   LR 0.001000
INFO - Training [33][  320/  391]   Loss 0.080164   Top1 97.253418   Top5 99.987793   BatchTime 0.106931   LR 0.001000
INFO - Training [33][  340/  391]   Loss 0.080655   Top1 97.208180   Top5 99.988511   BatchTime 0.106468   LR 0.001000
INFO - Training [33][  360/  391]   Loss 0.080980   Top1 97.200521   Top5 99.986979   BatchTime 0.105984   LR 0.001000
INFO - Training [33][  380/  391]   Loss 0.081515   Top1 97.185444   Top5 99.987664   BatchTime 0.105575   LR 0.001000
INFO - ==> Top1: 97.168    Top5: 99.986    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [33][   20/   79]   Loss 0.360233   Top1 90.507812   Top5 99.570312   BatchTime 0.131634
INFO - Validation [33][   40/   79]   Loss 0.365863   Top1 90.195312   Top5 99.550781   BatchTime 0.083327
INFO - Validation [33][   60/   79]   Loss 0.357655   Top1 90.651042   Top5 99.609375   BatchTime 0.067136
INFO - ==> Top1: 90.440    Top5: 99.650    Loss: 0.356
INFO - Scoreboard best 1 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.400   Top5: 99.690] Sparsity : 0.848
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  34
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [34][   20/  391]   Loss 0.076739   Top1 97.421875   Top5 99.960938   BatchTime 0.202715   LR 0.001000
INFO - Training [34][   40/  391]   Loss 0.080317   Top1 97.050781   Top5 99.980469   BatchTime 0.151761   LR 0.001000
INFO - Training [34][   60/  391]   Loss 0.078539   Top1 97.161458   Top5 99.986979   BatchTime 0.134831   LR 0.001000
INFO - Training [34][   80/  391]   Loss 0.077994   Top1 97.285156   Top5 99.990234   BatchTime 0.126567   LR 0.001000
INFO - Training [34][  100/  391]   Loss 0.076886   Top1 97.359375   Top5 99.992188   BatchTime 0.121492   LR 0.001000
INFO - Training [34][  120/  391]   Loss 0.078719   Top1 97.291667   Top5 99.993490   BatchTime 0.118137   LR 0.001000
INFO - Training [34][  140/  391]   Loss 0.079090   Top1 97.243304   Top5 99.994420   BatchTime 0.115723   LR 0.001000
INFO - Training [34][  160/  391]   Loss 0.078179   Top1 97.275391   Top5 99.995117   BatchTime 0.113956   LR 0.001000
INFO - Training [34][  180/  391]   Loss 0.078900   Top1 97.296007   Top5 99.995660   BatchTime 0.112416   LR 0.001000
INFO - Training [34][  200/  391]   Loss 0.079275   Top1 97.269531   Top5 99.996094   BatchTime 0.111254   LR 0.001000
INFO - Training [34][  220/  391]   Loss 0.079617   Top1 97.240767   Top5 99.996449   BatchTime 0.110219   LR 0.001000
INFO - Training [34][  240/  391]   Loss 0.079652   Top1 97.236328   Top5 99.996745   BatchTime 0.109404   LR 0.001000
INFO - Training [34][  260/  391]   Loss 0.079936   Top1 97.220553   Top5 99.996995   BatchTime 0.108721   LR 0.001000
INFO - Training [34][  280/  391]   Loss 0.080223   Top1 97.209821   Top5 99.997210   BatchTime 0.108088   LR 0.001000
INFO - Training [34][  300/  391]   Loss 0.080516   Top1 97.190104   Top5 99.997396   BatchTime 0.107573   LR 0.001000
INFO - Training [34][  320/  391]   Loss 0.080288   Top1 97.182617   Top5 99.995117   BatchTime 0.107113   LR 0.001000
INFO - Training [34][  340/  391]   Loss 0.080386   Top1 97.180607   Top5 99.990809   BatchTime 0.106602   LR 0.001000
INFO - Training [34][  360/  391]   Loss 0.080279   Top1 97.180990   Top5 99.989149   BatchTime 0.106474   LR 0.001000
INFO - Training [34][  380/  391]   Loss 0.080431   Top1 97.173109   Top5 99.987664   BatchTime 0.106002   LR 0.001000
INFO - ==> Top1: 97.172    Top5: 99.988    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [34][   20/   79]   Loss 0.355543   Top1 90.625000   Top5 99.531250   BatchTime 0.129432
INFO - Validation [34][   40/   79]   Loss 0.369412   Top1 90.351562   Top5 99.589844   BatchTime 0.082019
INFO - Validation [34][   60/   79]   Loss 0.361004   Top1 90.585938   Top5 99.609375   BatchTime 0.066492
INFO - ==> Top1: 90.320    Top5: 99.640    Loss: 0.360
INFO - Scoreboard best 1 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [32][Top1: 90.400   Top5: 99.690] Sparsity : 0.848
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  35
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [35][   20/  391]   Loss 0.072903   Top1 97.539062   Top5 100.000000   BatchTime 0.199351   LR 0.001000
INFO - Training [35][   40/  391]   Loss 0.073404   Top1 97.617188   Top5 100.000000   BatchTime 0.149849   LR 0.001000
INFO - Training [35][   60/  391]   Loss 0.073129   Top1 97.500000   Top5 100.000000   BatchTime 0.133388   LR 0.001000
INFO - Training [35][   80/  391]   Loss 0.071626   Top1 97.548828   Top5 100.000000   BatchTime 0.125220   LR 0.001000
INFO - Training [35][  100/  391]   Loss 0.074306   Top1 97.484375   Top5 100.000000   BatchTime 0.120462   LR 0.001000
INFO - Training [35][  120/  391]   Loss 0.077965   Top1 97.343750   Top5 100.000000   BatchTime 0.117254   LR 0.001000
INFO - Training [35][  140/  391]   Loss 0.076533   Top1 97.360491   Top5 100.000000   BatchTime 0.114962   LR 0.001000
INFO - Training [35][  160/  391]   Loss 0.075025   Top1 97.412109   Top5 100.000000   BatchTime 0.113184   LR 0.001000
INFO - Training [35][  180/  391]   Loss 0.075401   Top1 97.378472   Top5 100.000000   BatchTime 0.111779   LR 0.001000
INFO - Training [35][  200/  391]   Loss 0.075307   Top1 97.398438   Top5 99.996094   BatchTime 0.110731   LR 0.001000
INFO - Training [35][  220/  391]   Loss 0.074970   Top1 97.400568   Top5 99.996449   BatchTime 0.109738   LR 0.001000
INFO - Training [35][  240/  391]   Loss 0.075011   Top1 97.360026   Top5 99.993490   BatchTime 0.109045   LR 0.001000
INFO - Training [35][  260/  391]   Loss 0.074826   Top1 97.358774   Top5 99.993990   BatchTime 0.108393   LR 0.001000
INFO - Training [35][  280/  391]   Loss 0.074655   Top1 97.352121   Top5 99.994420   BatchTime 0.107856   LR 0.001000
INFO - Training [35][  300/  391]   Loss 0.074979   Top1 97.346354   Top5 99.994792   BatchTime 0.107376   LR 0.001000
INFO - Training [35][  320/  391]   Loss 0.075387   Top1 97.316895   Top5 99.995117   BatchTime 0.106937   LR 0.001000
INFO - Training [35][  340/  391]   Loss 0.075397   Top1 97.323070   Top5 99.995404   BatchTime 0.106437   LR 0.001000
INFO - Training [35][  360/  391]   Loss 0.075641   Top1 97.313368   Top5 99.995660   BatchTime 0.105958   LR 0.001000
INFO - Training [35][  380/  391]   Loss 0.075800   Top1 97.304688   Top5 99.995888   BatchTime 0.105540   LR 0.001000
INFO - ==> Top1: 97.284    Top5: 99.996    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [35][   20/   79]   Loss 0.354805   Top1 90.781250   Top5 99.648438   BatchTime 0.132663
INFO - Validation [35][   40/   79]   Loss 0.362829   Top1 90.527344   Top5 99.667969   BatchTime 0.083998
INFO - Validation [35][   60/   79]   Loss 0.356523   Top1 90.559896   Top5 99.674479   BatchTime 0.068272
INFO - ==> Top1: 90.400    Top5: 99.700    Loss: 0.354
INFO - Scoreboard best 1 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [35][Top1: 90.400   Top5: 99.700] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  36
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [36][   20/  391]   Loss 0.077367   Top1 97.421875   Top5 100.000000   BatchTime 0.200250   LR 0.001000
INFO - Training [36][   40/  391]   Loss 0.077182   Top1 97.382812   Top5 100.000000   BatchTime 0.150629   LR 0.001000
INFO - Training [36][   60/  391]   Loss 0.073254   Top1 97.500000   Top5 100.000000   BatchTime 0.133990   LR 0.001000
INFO - Training [36][   80/  391]   Loss 0.072357   Top1 97.490234   Top5 100.000000   BatchTime 0.126167   LR 0.001000
INFO - Training [36][  100/  391]   Loss 0.073579   Top1 97.476562   Top5 100.000000   BatchTime 0.121246   LR 0.001000
INFO - Training [36][  120/  391]   Loss 0.074493   Top1 97.363281   Top5 100.000000   BatchTime 0.117881   LR 0.001000
INFO - Training [36][  140/  391]   Loss 0.075617   Top1 97.304688   Top5 100.000000   BatchTime 0.115532   LR 0.001000
INFO - Training [36][  160/  391]   Loss 0.077181   Top1 97.290039   Top5 100.000000   BatchTime 0.113725   LR 0.001000
INFO - Training [36][  180/  391]   Loss 0.076156   Top1 97.291667   Top5 100.000000   BatchTime 0.112326   LR 0.001000
INFO - Training [36][  200/  391]   Loss 0.075296   Top1 97.320312   Top5 100.000000   BatchTime 0.111154   LR 0.001000
INFO - Training [36][  220/  391]   Loss 0.075290   Top1 97.318892   Top5 100.000000   BatchTime 0.110304   LR 0.001000
INFO - Training [36][  240/  391]   Loss 0.075207   Top1 97.314453   Top5 99.993490   BatchTime 0.109461   LR 0.001000
INFO - Training [36][  260/  391]   Loss 0.075520   Top1 97.304688   Top5 99.993990   BatchTime 0.108734   LR 0.001000
INFO - Training [36][  280/  391]   Loss 0.075449   Top1 97.324219   Top5 99.994420   BatchTime 0.108175   LR 0.001000
INFO - Training [36][  300/  391]   Loss 0.075672   Top1 97.320312   Top5 99.994792   BatchTime 0.107651   LR 0.001000
INFO - Training [36][  320/  391]   Loss 0.075948   Top1 97.319336   Top5 99.995117   BatchTime 0.107204   LR 0.001000
INFO - Training [36][  340/  391]   Loss 0.076112   Top1 97.316176   Top5 99.995404   BatchTime 0.106682   LR 0.001000
INFO - Training [36][  360/  391]   Loss 0.076287   Top1 97.304688   Top5 99.995660   BatchTime 0.106199   LR 0.001000
INFO - Training [36][  380/  391]   Loss 0.076214   Top1 97.308799   Top5 99.995888   BatchTime 0.105743   LR 0.001000
INFO - ==> Top1: 97.298    Top5: 99.996    Loss: 0.076
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [36][   20/   79]   Loss 0.360272   Top1 90.820312   Top5 99.531250   BatchTime 0.133004
INFO - Validation [36][   40/   79]   Loss 0.376147   Top1 90.117188   Top5 99.589844   BatchTime 0.083926
INFO - Validation [36][   60/   79]   Loss 0.367095   Top1 90.273438   Top5 99.609375   BatchTime 0.067291
INFO - ==> Top1: 90.180    Top5: 99.650    Loss: 0.364
INFO - Scoreboard best 1 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Scoreboard best 3 ==> Epoch [35][Top1: 90.400   Top5: 99.700] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  37
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [37][   20/  391]   Loss 0.069375   Top1 97.421875   Top5 100.000000   BatchTime 0.205652   LR 0.001000
INFO - Training [37][   40/  391]   Loss 0.067241   Top1 97.558594   Top5 100.000000   BatchTime 0.153194   LR 0.001000
INFO - Training [37][   60/  391]   Loss 0.067841   Top1 97.513021   Top5 100.000000   BatchTime 0.135927   LR 0.001000
INFO - Training [37][   80/  391]   Loss 0.068049   Top1 97.636719   Top5 100.000000   BatchTime 0.127304   LR 0.001000
INFO - Training [37][  100/  391]   Loss 0.070801   Top1 97.484375   Top5 100.000000   BatchTime 0.121831   LR 0.001000
INFO - Training [37][  120/  391]   Loss 0.070804   Top1 97.454427   Top5 100.000000   BatchTime 0.118309   LR 0.001000
INFO - Training [37][  140/  391]   Loss 0.071600   Top1 97.460938   Top5 100.000000   BatchTime 0.115834   LR 0.001000
INFO - Training [37][  160/  391]   Loss 0.071557   Top1 97.480469   Top5 100.000000   BatchTime 0.114041   LR 0.001000
INFO - Training [37][  180/  391]   Loss 0.072652   Top1 97.434896   Top5 100.000000   BatchTime 0.112673   LR 0.001000
INFO - Training [37][  200/  391]   Loss 0.071757   Top1 97.480469   Top5 100.000000   BatchTime 0.111571   LR 0.001000
INFO - Training [37][  220/  391]   Loss 0.071818   Top1 97.475142   Top5 100.000000   BatchTime 0.110598   LR 0.001000
INFO - Training [37][  240/  391]   Loss 0.072503   Top1 97.467448   Top5 100.000000   BatchTime 0.109798   LR 0.001000
INFO - Training [37][  260/  391]   Loss 0.072696   Top1 97.478966   Top5 100.000000   BatchTime 0.109132   LR 0.001000
INFO - Training [37][  280/  391]   Loss 0.073426   Top1 97.441406   Top5 100.000000   BatchTime 0.108507   LR 0.001000
INFO - Training [37][  300/  391]   Loss 0.074463   Top1 97.388021   Top5 100.000000   BatchTime 0.108017   LR 0.001000
INFO - Training [37][  320/  391]   Loss 0.074825   Top1 97.377930   Top5 100.000000   BatchTime 0.107588   LR 0.001000
INFO - Training [37][  340/  391]   Loss 0.074274   Top1 97.398897   Top5 100.000000   BatchTime 0.107062   LR 0.001000
INFO - Training [37][  360/  391]   Loss 0.074259   Top1 97.413194   Top5 99.997830   BatchTime 0.106554   LR 0.001000
INFO - Training [37][  380/  391]   Loss 0.074186   Top1 97.425987   Top5 99.997944   BatchTime 0.106110   LR 0.001000
INFO - ==> Top1: 97.424    Top5: 99.998    Loss: 0.074
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [37][   20/   79]   Loss 0.368340   Top1 90.781250   Top5 99.531250   BatchTime 0.131018
INFO - Validation [37][   40/   79]   Loss 0.369846   Top1 90.488281   Top5 99.550781   BatchTime 0.082832
INFO - Validation [37][   60/   79]   Loss 0.364491   Top1 90.638021   Top5 99.596354   BatchTime 0.067650
INFO - ==> Top1: 90.450    Top5: 99.640    Loss: 0.361
INFO - Scoreboard best 1 ==> Epoch [37][Top1: 90.450   Top5: 99.640] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Scoreboard best 3 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  38
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [38][   20/  391]   Loss 0.075869   Top1 97.656250   Top5 99.960938   BatchTime 0.199980   LR 0.001000
INFO - Training [38][   40/  391]   Loss 0.078355   Top1 97.460938   Top5 99.960938   BatchTime 0.150560   LR 0.001000
INFO - Training [38][   60/  391]   Loss 0.073710   Top1 97.617188   Top5 99.973958   BatchTime 0.134075   LR 0.001000
INFO - Training [38][   80/  391]   Loss 0.074980   Top1 97.617188   Top5 99.980469   BatchTime 0.126227   LR 0.001000
INFO - Training [38][  100/  391]   Loss 0.074012   Top1 97.570312   Top5 99.984375   BatchTime 0.121096   LR 0.001000
INFO - Training [38][  120/  391]   Loss 0.074221   Top1 97.571615   Top5 99.986979   BatchTime 0.117685   LR 0.001000
INFO - Training [38][  140/  391]   Loss 0.074413   Top1 97.555804   Top5 99.988839   BatchTime 0.115248   LR 0.001000
INFO - Training [38][  160/  391]   Loss 0.074648   Top1 97.558594   Top5 99.990234   BatchTime 0.113441   LR 0.001000
INFO - Training [38][  180/  391]   Loss 0.073167   Top1 97.591146   Top5 99.991319   BatchTime 0.112016   LR 0.001000
INFO - Training [38][  200/  391]   Loss 0.073662   Top1 97.566406   Top5 99.992188   BatchTime 0.110888   LR 0.001000
INFO - Training [38][  220/  391]   Loss 0.073439   Top1 97.556818   Top5 99.992898   BatchTime 0.109991   LR 0.001000
INFO - Training [38][  240/  391]   Loss 0.072971   Top1 97.545573   Top5 99.993490   BatchTime 0.109235   LR 0.001000
INFO - Training [38][  260/  391]   Loss 0.073033   Top1 97.518029   Top5 99.990986   BatchTime 0.108579   LR 0.001000
INFO - Training [38][  280/  391]   Loss 0.073155   Top1 97.513951   Top5 99.991629   BatchTime 0.107987   LR 0.001000
INFO - Training [38][  300/  391]   Loss 0.073557   Top1 97.476562   Top5 99.992188   BatchTime 0.107498   LR 0.001000
INFO - Training [38][  320/  391]   Loss 0.073528   Top1 97.478027   Top5 99.990234   BatchTime 0.107044   LR 0.001000
INFO - Training [38][  340/  391]   Loss 0.073221   Top1 97.477022   Top5 99.990809   BatchTime 0.106552   LR 0.001000
INFO - Training [38][  360/  391]   Loss 0.072489   Top1 97.504340   Top5 99.991319   BatchTime 0.106063   LR 0.001000
INFO - Training [38][  380/  391]   Loss 0.072905   Top1 97.500000   Top5 99.991776   BatchTime 0.105623   LR 0.001000
INFO - ==> Top1: 97.496    Top5: 99.990    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [38][   20/   79]   Loss 0.362071   Top1 90.703125   Top5 99.453125   BatchTime 0.130809
INFO - Validation [38][   40/   79]   Loss 0.368346   Top1 90.449219   Top5 99.531250   BatchTime 0.082775
INFO - Validation [38][   60/   79]   Loss 0.366382   Top1 90.481771   Top5 99.570312   BatchTime 0.066373
INFO - ==> Top1: 90.310    Top5: 99.630    Loss: 0.363
INFO - Scoreboard best 1 ==> Epoch [37][Top1: 90.450   Top5: 99.640] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Scoreboard best 3 ==> Epoch [31][Top1: 90.440   Top5: 99.590] Sparsity : 0.848
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  39
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [39][   20/  391]   Loss 0.081867   Top1 97.265625   Top5 100.000000   BatchTime 0.200766   LR 0.001000
INFO - Training [39][   40/  391]   Loss 0.079860   Top1 97.167969   Top5 100.000000   BatchTime 0.150896   LR 0.001000
INFO - Training [39][   60/  391]   Loss 0.076133   Top1 97.278646   Top5 99.986979   BatchTime 0.136543   LR 0.001000
INFO - Training [39][   80/  391]   Loss 0.076205   Top1 97.324219   Top5 99.990234   BatchTime 0.127563   LR 0.001000
INFO - Training [39][  100/  391]   Loss 0.076336   Top1 97.304688   Top5 99.992188   BatchTime 0.122307   LR 0.001000
INFO - Training [39][  120/  391]   Loss 0.075364   Top1 97.402344   Top5 99.993490   BatchTime 0.118644   LR 0.001000
INFO - Training [39][  140/  391]   Loss 0.074189   Top1 97.427455   Top5 99.988839   BatchTime 0.116098   LR 0.001000
INFO - Training [39][  160/  391]   Loss 0.072768   Top1 97.480469   Top5 99.990234   BatchTime 0.113695   LR 0.001000
INFO - Training [39][  180/  391]   Loss 0.072792   Top1 97.473958   Top5 99.991319   BatchTime 0.112200   LR 0.001000
INFO - Training [39][  200/  391]   Loss 0.071713   Top1 97.500000   Top5 99.992188   BatchTime 0.111056   LR 0.001000
INFO - Training [39][  220/  391]   Loss 0.071995   Top1 97.492898   Top5 99.992898   BatchTime 0.110095   LR 0.001000
INFO - Training [39][  240/  391]   Loss 0.071643   Top1 97.509766   Top5 99.993490   BatchTime 0.109330   LR 0.001000
INFO - Training [39][  260/  391]   Loss 0.072811   Top1 97.484976   Top5 99.993990   BatchTime 0.108654   LR 0.001000
INFO - Training [39][  280/  391]   Loss 0.072404   Top1 97.486049   Top5 99.994420   BatchTime 0.108021   LR 0.001000
INFO - Training [39][  300/  391]   Loss 0.072716   Top1 97.484375   Top5 99.994792   BatchTime 0.107568   LR 0.001000
INFO - Training [39][  320/  391]   Loss 0.072366   Top1 97.492676   Top5 99.995117   BatchTime 0.107148   LR 0.001000
INFO - Training [39][  340/  391]   Loss 0.072401   Top1 97.504596   Top5 99.995404   BatchTime 0.106630   LR 0.001000
INFO - Training [39][  360/  391]   Loss 0.072639   Top1 97.491319   Top5 99.995660   BatchTime 0.106134   LR 0.001000
INFO - Training [39][  380/  391]   Loss 0.073077   Top1 97.458882   Top5 99.995888   BatchTime 0.105732   LR 0.001000
INFO - ==> Top1: 97.458    Top5: 99.996    Loss: 0.073
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [39][   20/   79]   Loss 0.363924   Top1 90.898438   Top5 99.453125   BatchTime 0.131708
INFO - Validation [39][   40/   79]   Loss 0.371665   Top1 90.625000   Top5 99.472656   BatchTime 0.083336
INFO - Validation [39][   60/   79]   Loss 0.365510   Top1 90.664062   Top5 99.557292   BatchTime 0.067518
INFO - ==> Top1: 90.520    Top5: 99.580    Loss: 0.361
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 90.520   Top5: 99.580] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 90.450   Top5: 99.640] Sparsity : 0.849
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  40
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [40][   20/  391]   Loss 0.066279   Top1 97.929688   Top5 100.000000   BatchTime 0.197244   LR 0.001000
INFO - Training [40][   40/  391]   Loss 0.066393   Top1 97.871094   Top5 100.000000   BatchTime 0.149401   LR 0.001000
INFO - Training [40][   60/  391]   Loss 0.067616   Top1 97.812500   Top5 100.000000   BatchTime 0.132754   LR 0.001000
INFO - Training [40][   80/  391]   Loss 0.071455   Top1 97.597656   Top5 99.970703   BatchTime 0.124492   LR 0.001000
INFO - Training [40][  100/  391]   Loss 0.072899   Top1 97.554688   Top5 99.976562   BatchTime 0.119631   LR 0.001000
INFO - Training [40][  120/  391]   Loss 0.071541   Top1 97.584635   Top5 99.980469   BatchTime 0.116574   LR 0.001000
INFO - Training [40][  140/  391]   Loss 0.070724   Top1 97.611607   Top5 99.983259   BatchTime 0.114309   LR 0.001000
INFO - Training [40][  160/  391]   Loss 0.070967   Top1 97.592773   Top5 99.985352   BatchTime 0.112580   LR 0.001000
INFO - Training [40][  180/  391]   Loss 0.070769   Top1 97.599826   Top5 99.986979   BatchTime 0.111231   LR 0.001000
INFO - Training [40][  200/  391]   Loss 0.072411   Top1 97.496094   Top5 99.984375   BatchTime 0.110215   LR 0.001000
INFO - Training [40][  220/  391]   Loss 0.072364   Top1 97.485795   Top5 99.985795   BatchTime 0.109392   LR 0.001000
INFO - Training [40][  240/  391]   Loss 0.072497   Top1 97.483724   Top5 99.986979   BatchTime 0.108623   LR 0.001000
INFO - Training [40][  260/  391]   Loss 0.072415   Top1 97.472957   Top5 99.987981   BatchTime 0.107969   LR 0.001000
INFO - Training [40][  280/  391]   Loss 0.072473   Top1 97.494420   Top5 99.986049   BatchTime 0.107438   LR 0.001000
INFO - Training [40][  300/  391]   Loss 0.072411   Top1 97.515625   Top5 99.984375   BatchTime 0.106945   LR 0.001000
INFO - Training [40][  320/  391]   Loss 0.072533   Top1 97.512207   Top5 99.985352   BatchTime 0.106492   LR 0.001000
INFO - Training [40][  340/  391]   Loss 0.072525   Top1 97.502298   Top5 99.986213   BatchTime 0.105992   LR 0.001000
INFO - Training [40][  360/  391]   Loss 0.072713   Top1 97.493490   Top5 99.986979   BatchTime 0.105547   LR 0.001000
INFO - Training [40][  380/  391]   Loss 0.072878   Top1 97.487664   Top5 99.987664   BatchTime 0.105123   LR 0.001000
INFO - ==> Top1: 97.512    Top5: 99.988    Loss: 0.072
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [40][   20/   79]   Loss 0.367969   Top1 90.703125   Top5 99.609375   BatchTime 0.131465
INFO - Validation [40][   40/   79]   Loss 0.374579   Top1 90.429688   Top5 99.609375   BatchTime 0.083069
INFO - Validation [40][   60/   79]   Loss 0.369517   Top1 90.585938   Top5 99.635417   BatchTime 0.066541
INFO - ==> Top1: 90.380    Top5: 99.650    Loss: 0.366
INFO - Scoreboard best 1 ==> Epoch [39][Top1: 90.520   Top5: 99.580] Sparsity : 0.849
INFO - Scoreboard best 2 ==> Epoch [37][Top1: 90.450   Top5: 99.640] Sparsity : 0.849
INFO - Scoreboard best 3 ==> Epoch [33][Top1: 90.440   Top5: 99.650] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  41
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [41][   20/  391]   Loss 0.073304   Top1 97.343750   Top5 99.960938   BatchTime 0.200648   LR 0.001000
INFO - Training [41][   40/  391]   Loss 0.069472   Top1 97.539062   Top5 99.980469   BatchTime 0.151039   LR 0.001000
INFO - Training [41][   60/  391]   Loss 0.073645   Top1 97.356771   Top5 99.986979   BatchTime 0.134417   LR 0.001000
INFO - Training [41][   80/  391]   Loss 0.071474   Top1 97.441406   Top5 99.990234   BatchTime 0.126009   LR 0.001000
INFO - Training [41][  100/  391]   Loss 0.074241   Top1 97.335938   Top5 99.992188   BatchTime 0.120959   LR 0.001000
INFO - Training [41][  120/  391]   Loss 0.073265   Top1 97.389323   Top5 99.993490   BatchTime 0.118417   LR 0.001000
INFO - Training [41][  140/  391]   Loss 0.072581   Top1 97.405134   Top5 99.994420   BatchTime 0.115865   LR 0.001000
INFO - Training [41][  160/  391]   Loss 0.071543   Top1 97.485352   Top5 99.995117   BatchTime 0.113946   LR 0.001000
INFO - Training [41][  180/  391]   Loss 0.072798   Top1 97.447917   Top5 99.995660   BatchTime 0.112589   LR 0.001000
INFO - Training [41][  200/  391]   Loss 0.073073   Top1 97.402344   Top5 99.992188   BatchTime 0.111313   LR 0.001000
INFO - Training [41][  220/  391]   Loss 0.075212   Top1 97.318892   Top5 99.989347   BatchTime 0.110312   LR 0.001000
INFO - Training [41][  240/  391]   Loss 0.074245   Top1 97.356771   Top5 99.990234   BatchTime 0.109474   LR 0.001000
INFO - Training [41][  260/  391]   Loss 0.074227   Top1 97.340745   Top5 99.990986   BatchTime 0.108744   LR 0.001000
INFO - Training [41][  280/  391]   Loss 0.073968   Top1 97.354911   Top5 99.988839   BatchTime 0.108254   LR 0.001000
INFO - Training [41][  300/  391]   Loss 0.072958   Top1 97.419271   Top5 99.989583   BatchTime 0.107756   LR 0.001000
INFO - Training [41][  320/  391]   Loss 0.072348   Top1 97.453613   Top5 99.990234   BatchTime 0.107264   LR 0.001000
INFO - Training [41][  340/  391]   Loss 0.071866   Top1 97.479320   Top5 99.990809   BatchTime 0.106726   LR 0.001000
INFO - Training [41][  360/  391]   Loss 0.071863   Top1 97.495660   Top5 99.991319   BatchTime 0.106235   LR 0.001000
INFO - Training [41][  380/  391]   Loss 0.071774   Top1 97.516447   Top5 99.991776   BatchTime 0.105982   LR 0.001000
INFO - ==> Top1: 97.506    Top5: 99.988    Loss: 0.072
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [41][   20/   79]   Loss 0.358972   Top1 90.781250   Top5 99.531250   BatchTime 0.132083
INFO - Validation [41][   40/   79]   Loss 0.368704   Top1 90.488281   Top5 99.550781   BatchTime 0.083541
INFO - Validation [41][   60/   79]   Loss 0.359144   Top1 90.677083   Top5 99.609375   BatchTime 0.066800
INFO - ==> Top1: 90.530    Top5: 99.650    Loss: 0.358
INFO - Scoreboard best 1 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.520   Top5: 99.580] Sparsity : 0.849
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 90.450   Top5: 99.640] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  42
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [42][   20/  391]   Loss 0.073607   Top1 97.031250   Top5 100.000000   BatchTime 0.199174   LR 0.001000
INFO - Training [42][   40/  391]   Loss 0.069207   Top1 97.265625   Top5 100.000000   BatchTime 0.150189   LR 0.001000
INFO - Training [42][   60/  391]   Loss 0.072219   Top1 97.330729   Top5 100.000000   BatchTime 0.133754   LR 0.001000
INFO - Training [42][   80/  391]   Loss 0.072614   Top1 97.246094   Top5 100.000000   BatchTime 0.125583   LR 0.001000
INFO - Training [42][  100/  391]   Loss 0.072073   Top1 97.335938   Top5 100.000000   BatchTime 0.120660   LR 0.001000
INFO - Training [42][  120/  391]   Loss 0.071428   Top1 97.408854   Top5 100.000000   BatchTime 0.117397   LR 0.001000
INFO - Training [42][  140/  391]   Loss 0.070753   Top1 97.433036   Top5 100.000000   BatchTime 0.114972   LR 0.001000
INFO - Training [42][  160/  391]   Loss 0.070405   Top1 97.441406   Top5 100.000000   BatchTime 0.113308   LR 0.001000
INFO - Training [42][  180/  391]   Loss 0.071156   Top1 97.447917   Top5 99.991319   BatchTime 0.111958   LR 0.001000
INFO - Training [42][  200/  391]   Loss 0.070767   Top1 97.457031   Top5 99.992188   BatchTime 0.110865   LR 0.001000
INFO - Training [42][  220/  391]   Loss 0.070694   Top1 97.468040   Top5 99.989347   BatchTime 0.109841   LR 0.001000
INFO - Training [42][  240/  391]   Loss 0.071032   Top1 97.428385   Top5 99.986979   BatchTime 0.109087   LR 0.001000
INFO - Training [42][  260/  391]   Loss 0.070368   Top1 97.469952   Top5 99.987981   BatchTime 0.108449   LR 0.001000
INFO - Training [42][  280/  391]   Loss 0.070121   Top1 97.477679   Top5 99.988839   BatchTime 0.107904   LR 0.001000
INFO - Training [42][  300/  391]   Loss 0.070691   Top1 97.447917   Top5 99.989583   BatchTime 0.107385   LR 0.001000
INFO - Training [42][  320/  391]   Loss 0.070729   Top1 97.451172   Top5 99.987793   BatchTime 0.106932   LR 0.001000
INFO - Training [42][  340/  391]   Loss 0.070371   Top1 97.467831   Top5 99.988511   BatchTime 0.106444   LR 0.001000
INFO - Training [42][  360/  391]   Loss 0.069997   Top1 97.489149   Top5 99.989149   BatchTime 0.105973   LR 0.001000
INFO - Training [42][  380/  391]   Loss 0.069791   Top1 97.483553   Top5 99.989720   BatchTime 0.105587   LR 0.001000
INFO - ==> Top1: 97.468    Top5: 99.990    Loss: 0.070
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [42][   20/   79]   Loss 0.369959   Top1 90.585938   Top5 99.570312   BatchTime 0.132095
INFO - Validation [42][   40/   79]   Loss 0.374799   Top1 90.390625   Top5 99.550781   BatchTime 0.083154
INFO - Validation [42][   60/   79]   Loss 0.367347   Top1 90.598958   Top5 99.596354   BatchTime 0.065851
INFO - ==> Top1: 90.410    Top5: 99.630    Loss: 0.363
INFO - Scoreboard best 1 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Scoreboard best 2 ==> Epoch [39][Top1: 90.520   Top5: 99.580] Sparsity : 0.849
INFO - Scoreboard best 3 ==> Epoch [37][Top1: 90.450   Top5: 99.640] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  43
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [43][   20/  391]   Loss 0.065655   Top1 97.812500   Top5 100.000000   BatchTime 0.198380   LR 0.001000
INFO - Training [43][   40/  391]   Loss 0.068245   Top1 97.675781   Top5 99.980469   BatchTime 0.149613   LR 0.001000
INFO - Training [43][   60/  391]   Loss 0.069396   Top1 97.604167   Top5 99.986979   BatchTime 0.133204   LR 0.001000
INFO - Training [43][   80/  391]   Loss 0.067765   Top1 97.675781   Top5 99.980469   BatchTime 0.124984   LR 0.001000
INFO - Training [43][  100/  391]   Loss 0.068114   Top1 97.617188   Top5 99.984375   BatchTime 0.120059   LR 0.001000
INFO - Training [43][  120/  391]   Loss 0.067372   Top1 97.636719   Top5 99.986979   BatchTime 0.116908   LR 0.001000
INFO - Training [43][  140/  391]   Loss 0.066760   Top1 97.689732   Top5 99.988839   BatchTime 0.114565   LR 0.001000
INFO - Training [43][  160/  391]   Loss 0.066637   Top1 97.709961   Top5 99.985352   BatchTime 0.112743   LR 0.001000
INFO - Training [43][  180/  391]   Loss 0.067275   Top1 97.690972   Top5 99.986979   BatchTime 0.111462   LR 0.001000
INFO - Training [43][  200/  391]   Loss 0.067929   Top1 97.656250   Top5 99.984375   BatchTime 0.110812   LR 0.001000
INFO - Training [43][  220/  391]   Loss 0.067896   Top1 97.638494   Top5 99.982244   BatchTime 0.109904   LR 0.001000
INFO - Training [43][  240/  391]   Loss 0.068444   Top1 97.636719   Top5 99.983724   BatchTime 0.109221   LR 0.001000
INFO - Training [43][  260/  391]   Loss 0.068427   Top1 97.644231   Top5 99.984976   BatchTime 0.108639   LR 0.001000
INFO - Training [43][  280/  391]   Loss 0.068016   Top1 97.645089   Top5 99.986049   BatchTime 0.108043   LR 0.001000
INFO - Training [43][  300/  391]   Loss 0.068336   Top1 97.645833   Top5 99.984375   BatchTime 0.107561   LR 0.001000
INFO - Training [43][  320/  391]   Loss 0.068903   Top1 97.626953   Top5 99.985352   BatchTime 0.107117   LR 0.001000
INFO - Training [43][  340/  391]   Loss 0.068894   Top1 97.619485   Top5 99.986213   BatchTime 0.106616   LR 0.001000
INFO - Training [43][  360/  391]   Loss 0.069623   Top1 97.593316   Top5 99.986979   BatchTime 0.106127   LR 0.001000
INFO - Training [43][  380/  391]   Loss 0.069609   Top1 97.584293   Top5 99.985609   BatchTime 0.105859   LR 0.001000
INFO - ==> Top1: 97.566    Top5: 99.986    Loss: 0.070
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [43][   20/   79]   Loss 0.359828   Top1 90.820312   Top5 99.570312   BatchTime 0.131476
INFO - Validation [43][   40/   79]   Loss 0.367332   Top1 90.546875   Top5 99.609375   BatchTime 0.083277
INFO - Validation [43][   60/   79]   Loss 0.361535   Top1 90.638021   Top5 99.635417   BatchTime 0.065567
INFO - ==> Top1: 90.520    Top5: 99.680    Loss: 0.359
INFO - Scoreboard best 1 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Scoreboard best 2 ==> Epoch [43][Top1: 90.520   Top5: 99.680] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [39][Top1: 90.520   Top5: 99.580] Sparsity : 0.849
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  44
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [44][   20/  391]   Loss 0.074185   Top1 97.226562   Top5 100.000000   BatchTime 0.200689   LR 0.001000
INFO - Training [44][   40/  391]   Loss 0.069336   Top1 97.578125   Top5 100.000000   BatchTime 0.150965   LR 0.001000
INFO - Training [44][   60/  391]   Loss 0.072109   Top1 97.330729   Top5 100.000000   BatchTime 0.133872   LR 0.001000
INFO - Training [44][   80/  391]   Loss 0.074232   Top1 97.246094   Top5 99.990234   BatchTime 0.125687   LR 0.001000
INFO - Training [44][  100/  391]   Loss 0.072378   Top1 97.375000   Top5 99.992188   BatchTime 0.120728   LR 0.001000
INFO - Training [44][  120/  391]   Loss 0.072539   Top1 97.363281   Top5 99.993490   BatchTime 0.117431   LR 0.001000
INFO - Training [44][  140/  391]   Loss 0.070951   Top1 97.427455   Top5 99.994420   BatchTime 0.114983   LR 0.001000
INFO - Training [44][  160/  391]   Loss 0.070420   Top1 97.451172   Top5 99.995117   BatchTime 0.113198   LR 0.001000
INFO - Training [44][  180/  391]   Loss 0.072561   Top1 97.361111   Top5 99.995660   BatchTime 0.111905   LR 0.001000
INFO - Training [44][  200/  391]   Loss 0.071333   Top1 97.425781   Top5 99.996094   BatchTime 0.110813   LR 0.001000
INFO - Training [44][  220/  391]   Loss 0.070598   Top1 97.443182   Top5 99.996449   BatchTime 0.109959   LR 0.001000
INFO - Training [44][  240/  391]   Loss 0.070121   Top1 97.441406   Top5 99.996745   BatchTime 0.109138   LR 0.001000
INFO - Training [44][  260/  391]   Loss 0.070016   Top1 97.454928   Top5 99.996995   BatchTime 0.108558   LR 0.001000
INFO - Training [44][  280/  391]   Loss 0.070731   Top1 97.419085   Top5 99.994420   BatchTime 0.107951   LR 0.001000
INFO - Training [44][  300/  391]   Loss 0.070183   Top1 97.460938   Top5 99.994792   BatchTime 0.107424   LR 0.001000
INFO - Training [44][  320/  391]   Loss 0.069832   Top1 97.475586   Top5 99.995117   BatchTime 0.106997   LR 0.001000
INFO - Training [44][  340/  391]   Loss 0.069796   Top1 97.479320   Top5 99.995404   BatchTime 0.106462   LR 0.001000
INFO - Training [44][  360/  391]   Loss 0.069399   Top1 97.489149   Top5 99.993490   BatchTime 0.105979   LR 0.001000
INFO - Training [44][  380/  391]   Loss 0.069634   Top1 97.487664   Top5 99.993832   BatchTime 0.105622   LR 0.001000
INFO - ==> Top1: 97.500    Top5: 99.994    Loss: 0.069
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [44][   20/   79]   Loss 0.370398   Top1 90.937500   Top5 99.492188   BatchTime 0.130655
INFO - Validation [44][   40/   79]   Loss 0.372600   Top1 90.820312   Top5 99.511719   BatchTime 0.082638
INFO - Validation [44][   60/   79]   Loss 0.369124   Top1 90.820312   Top5 99.583333   BatchTime 0.066041
INFO - ==> Top1: 90.560    Top5: 99.630    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.560   Top5: 99.630] Sparsity : 0.850
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.520   Top5: 99.680] Sparsity : 0.850
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  45
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [45][   20/  391]   Loss 0.064613   Top1 97.617188   Top5 100.000000   BatchTime 0.196685   LR 0.001000
INFO - Training [45][   40/  391]   Loss 0.067171   Top1 97.597656   Top5 100.000000   BatchTime 0.148098   LR 0.001000
INFO - Training [45][   60/  391]   Loss 0.066226   Top1 97.643229   Top5 100.000000   BatchTime 0.132272   LR 0.001000
INFO - Training [45][   80/  391]   Loss 0.064189   Top1 97.763672   Top5 100.000000   BatchTime 0.124434   LR 0.001000
INFO - Training [45][  100/  391]   Loss 0.067680   Top1 97.640625   Top5 100.000000   BatchTime 0.119530   LR 0.001000
INFO - Training [45][  120/  391]   Loss 0.065219   Top1 97.721354   Top5 100.000000   BatchTime 0.116339   LR 0.001000
INFO - Training [45][  140/  391]   Loss 0.066164   Top1 97.689732   Top5 99.994420   BatchTime 0.114120   LR 0.001000
INFO - Training [45][  160/  391]   Loss 0.067137   Top1 97.695312   Top5 99.985352   BatchTime 0.112437   LR 0.001000
INFO - Training [45][  180/  391]   Loss 0.067733   Top1 97.630208   Top5 99.986979   BatchTime 0.111069   LR 0.001000
INFO - Training [45][  200/  391]   Loss 0.067126   Top1 97.652344   Top5 99.988281   BatchTime 0.110020   LR 0.001000
INFO - Training [45][  220/  391]   Loss 0.066933   Top1 97.652699   Top5 99.989347   BatchTime 0.109561   LR 0.001000
INFO - Training [45][  240/  391]   Loss 0.067328   Top1 97.643229   Top5 99.990234   BatchTime 0.108831   LR 0.001000
INFO - Training [45][  260/  391]   Loss 0.067085   Top1 97.650240   Top5 99.990986   BatchTime 0.108149   LR 0.001000
INFO - Training [45][  280/  391]   Loss 0.066909   Top1 97.661830   Top5 99.991629   BatchTime 0.107563   LR 0.001000
INFO - Training [45][  300/  391]   Loss 0.067693   Top1 97.611979   Top5 99.992188   BatchTime 0.107161   LR 0.001000
INFO - Training [45][  320/  391]   Loss 0.068198   Top1 97.622070   Top5 99.992676   BatchTime 0.106718   LR 0.001000
INFO - Training [45][  340/  391]   Loss 0.067802   Top1 97.644761   Top5 99.993107   BatchTime 0.106058   LR 0.001000
INFO - Training [45][  360/  391]   Loss 0.067843   Top1 97.621528   Top5 99.993490   BatchTime 0.105557   LR 0.001000
INFO - Training [45][  380/  391]   Loss 0.068127   Top1 97.608964   Top5 99.991776   BatchTime 0.105194   LR 0.001000
INFO - ==> Top1: 97.596    Top5: 99.992    Loss: 0.068
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [45][   20/   79]   Loss 0.367265   Top1 90.625000   Top5 99.531250   BatchTime 0.130020
INFO - Validation [45][   40/   79]   Loss 0.375052   Top1 90.429688   Top5 99.570312   BatchTime 0.082199
INFO - Validation [45][   60/   79]   Loss 0.369131   Top1 90.664062   Top5 99.596354   BatchTime 0.064977
INFO - ==> Top1: 90.450    Top5: 99.620    Loss: 0.368
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.560   Top5: 99.630] Sparsity : 0.850
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.520   Top5: 99.680] Sparsity : 0.850
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  46
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [46][   20/  391]   Loss 0.057877   Top1 98.007812   Top5 100.000000   BatchTime 0.198288   LR 0.001000
INFO - Training [46][   40/  391]   Loss 0.066527   Top1 97.695312   Top5 100.000000   BatchTime 0.149904   LR 0.001000
INFO - Training [46][   60/  391]   Loss 0.067444   Top1 97.734375   Top5 99.986979   BatchTime 0.133590   LR 0.001000
INFO - Training [46][   80/  391]   Loss 0.066527   Top1 97.744141   Top5 99.990234   BatchTime 0.125274   LR 0.001000
INFO - Training [46][  100/  391]   Loss 0.066495   Top1 97.742188   Top5 99.992188   BatchTime 0.120267   LR 0.001000
INFO - Training [46][  120/  391]   Loss 0.065572   Top1 97.779948   Top5 99.993490   BatchTime 0.117045   LR 0.001000
INFO - Training [46][  140/  391]   Loss 0.064734   Top1 97.790179   Top5 99.994420   BatchTime 0.114721   LR 0.001000
INFO - Training [46][  160/  391]   Loss 0.064469   Top1 97.783203   Top5 99.995117   BatchTime 0.112849   LR 0.001000
INFO - Training [46][  180/  391]   Loss 0.064640   Top1 97.782118   Top5 99.995660   BatchTime 0.111534   LR 0.001000
INFO - Training [46][  200/  391]   Loss 0.064107   Top1 97.816406   Top5 99.996094   BatchTime 0.110450   LR 0.001000
INFO - Training [46][  220/  391]   Loss 0.065013   Top1 97.755682   Top5 99.996449   BatchTime 0.109475   LR 0.001000
INFO - Training [46][  240/  391]   Loss 0.064867   Top1 97.757161   Top5 99.996745   BatchTime 0.108652   LR 0.001000
INFO - Training [46][  260/  391]   Loss 0.064594   Top1 97.755409   Top5 99.996995   BatchTime 0.107957   LR 0.001000
INFO - Training [46][  280/  391]   Loss 0.063928   Top1 97.756696   Top5 99.994420   BatchTime 0.107469   LR 0.001000
INFO - Training [46][  300/  391]   Loss 0.064765   Top1 97.721354   Top5 99.994792   BatchTime 0.107012   LR 0.001000
INFO - Training [46][  320/  391]   Loss 0.065274   Top1 97.714844   Top5 99.995117   BatchTime 0.106547   LR 0.001000
INFO - Training [46][  340/  391]   Loss 0.065847   Top1 97.660846   Top5 99.993107   BatchTime 0.106048   LR 0.001000
INFO - Training [46][  360/  391]   Loss 0.066125   Top1 97.643229   Top5 99.993490   BatchTime 0.105566   LR 0.001000
INFO - Training [46][  380/  391]   Loss 0.066435   Top1 97.623355   Top5 99.991776   BatchTime 0.105324   LR 0.001000
INFO - ==> Top1: 97.626    Top5: 99.992    Loss: 0.066
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [46][   20/   79]   Loss 0.363241   Top1 90.703125   Top5 99.531250   BatchTime 0.129523
INFO - Validation [46][   40/   79]   Loss 0.374514   Top1 90.566406   Top5 99.570312   BatchTime 0.082045
INFO - Validation [46][   60/   79]   Loss 0.370686   Top1 90.625000   Top5 99.596354   BatchTime 0.065520
INFO - ==> Top1: 90.390    Top5: 99.620    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.560   Top5: 99.630] Sparsity : 0.850
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.520   Top5: 99.680] Sparsity : 0.850
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  47
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [47][   20/  391]   Loss 0.058174   Top1 98.242188   Top5 100.000000   BatchTime 0.199095   LR 0.001000
INFO - Training [47][   40/  391]   Loss 0.058955   Top1 98.027344   Top5 100.000000   BatchTime 0.149896   LR 0.001000
INFO - Training [47][   60/  391]   Loss 0.063916   Top1 97.864583   Top5 100.000000   BatchTime 0.133148   LR 0.001000
INFO - Training [47][   80/  391]   Loss 0.065807   Top1 97.724609   Top5 99.990234   BatchTime 0.124970   LR 0.001000
INFO - Training [47][  100/  391]   Loss 0.066029   Top1 97.679688   Top5 99.992188   BatchTime 0.120095   LR 0.001000
INFO - Training [47][  120/  391]   Loss 0.067216   Top1 97.604167   Top5 99.993490   BatchTime 0.116744   LR 0.001000
INFO - Training [47][  140/  391]   Loss 0.066675   Top1 97.606027   Top5 99.994420   BatchTime 0.114474   LR 0.001000
INFO - Training [47][  160/  391]   Loss 0.067119   Top1 97.568359   Top5 99.995117   BatchTime 0.112684   LR 0.001000
INFO - Training [47][  180/  391]   Loss 0.066651   Top1 97.621528   Top5 99.995660   BatchTime 0.111298   LR 0.001000
INFO - Training [47][  200/  391]   Loss 0.066642   Top1 97.613281   Top5 99.992188   BatchTime 0.110214   LR 0.001000
INFO - Training [47][  220/  391]   Loss 0.067106   Top1 97.599432   Top5 99.992898   BatchTime 0.109292   LR 0.001000
INFO - Training [47][  240/  391]   Loss 0.066760   Top1 97.639974   Top5 99.993490   BatchTime 0.108538   LR 0.001000
INFO - Training [47][  260/  391]   Loss 0.066654   Top1 97.635216   Top5 99.993990   BatchTime 0.108260   LR 0.001000
INFO - Training [47][  280/  391]   Loss 0.067016   Top1 97.628348   Top5 99.994420   BatchTime 0.107647   LR 0.001000
INFO - Training [47][  300/  391]   Loss 0.067718   Top1 97.601562   Top5 99.994792   BatchTime 0.107141   LR 0.001000
INFO - Training [47][  320/  391]   Loss 0.067326   Top1 97.631836   Top5 99.995117   BatchTime 0.106647   LR 0.001000
INFO - Training [47][  340/  391]   Loss 0.067120   Top1 97.621783   Top5 99.995404   BatchTime 0.106154   LR 0.001000
INFO - Training [47][  360/  391]   Loss 0.067257   Top1 97.621528   Top5 99.995660   BatchTime 0.105674   LR 0.001000
INFO - Training [47][  380/  391]   Loss 0.067473   Top1 97.619243   Top5 99.995888   BatchTime 0.105325   LR 0.001000
INFO - ==> Top1: 97.618    Top5: 99.996    Loss: 0.067
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [47][   20/   79]   Loss 0.383604   Top1 90.390625   Top5 99.609375   BatchTime 0.129072
INFO - Validation [47][   40/   79]   Loss 0.384480   Top1 90.312500   Top5 99.628906   BatchTime 0.081669
INFO - Validation [47][   60/   79]   Loss 0.373734   Top1 90.664062   Top5 99.635417   BatchTime 0.064093
INFO - ==> Top1: 90.470    Top5: 99.650    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [44][Top1: 90.560   Top5: 99.630] Sparsity : 0.850
INFO - Scoreboard best 2 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [43][Top1: 90.520   Top5: 99.680] Sparsity : 0.850
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  48
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [48][   20/  391]   Loss 0.064459   Top1 97.578125   Top5 99.960938   BatchTime 0.197673   LR 0.001000
INFO - Training [48][   40/  391]   Loss 0.063557   Top1 97.597656   Top5 99.980469   BatchTime 0.149157   LR 0.001000
INFO - Training [48][   60/  391]   Loss 0.060891   Top1 97.773438   Top5 99.973958   BatchTime 0.132928   LR 0.001000
INFO - Training [48][   80/  391]   Loss 0.062806   Top1 97.773438   Top5 99.980469   BatchTime 0.124649   LR 0.001000
INFO - Training [48][  100/  391]   Loss 0.064014   Top1 97.757812   Top5 99.984375   BatchTime 0.119774   LR 0.001000
INFO - Training [48][  120/  391]   Loss 0.063303   Top1 97.792969   Top5 99.986979   BatchTime 0.116518   LR 0.001000
INFO - Training [48][  140/  391]   Loss 0.064175   Top1 97.762277   Top5 99.988839   BatchTime 0.114171   LR 0.001000
INFO - Training [48][  160/  391]   Loss 0.064979   Top1 97.739258   Top5 99.990234   BatchTime 0.112495   LR 0.001000
INFO - Training [48][  180/  391]   Loss 0.065692   Top1 97.747396   Top5 99.986979   BatchTime 0.111186   LR 0.001000
INFO - Training [48][  200/  391]   Loss 0.065773   Top1 97.734375   Top5 99.988281   BatchTime 0.109985   LR 0.001000
INFO - Training [48][  220/  391]   Loss 0.065892   Top1 97.737926   Top5 99.989347   BatchTime 0.109067   LR 0.001000
INFO - Training [48][  240/  391]   Loss 0.064848   Top1 97.783203   Top5 99.990234   BatchTime 0.108258   LR 0.001000
INFO - Training [48][  260/  391]   Loss 0.065223   Top1 97.761418   Top5 99.990986   BatchTime 0.107625   LR 0.001000
INFO - Training [48][  280/  391]   Loss 0.065321   Top1 97.765067   Top5 99.988839   BatchTime 0.107108   LR 0.001000
INFO - Training [48][  300/  391]   Loss 0.065382   Top1 97.770833   Top5 99.986979   BatchTime 0.106599   LR 0.001000
INFO - Training [48][  320/  391]   Loss 0.064996   Top1 97.790527   Top5 99.987793   BatchTime 0.106206   LR 0.001000
INFO - Training [48][  340/  391]   Loss 0.064915   Top1 97.791820   Top5 99.988511   BatchTime 0.105721   LR 0.001000
INFO - Training [48][  360/  391]   Loss 0.064999   Top1 97.775608   Top5 99.989149   BatchTime 0.105240   LR 0.001000
INFO - Training [48][  380/  391]   Loss 0.064976   Top1 97.765214   Top5 99.989720   BatchTime 0.104975   LR 0.001000
INFO - ==> Top1: 97.748    Top5: 99.990    Loss: 0.066
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [48][   20/   79]   Loss 0.378735   Top1 90.820312   Top5 99.570312   BatchTime 0.129776
INFO - Validation [48][   40/   79]   Loss 0.383453   Top1 90.488281   Top5 99.589844   BatchTime 0.082232
INFO - Validation [48][   60/   79]   Loss 0.372962   Top1 90.651042   Top5 99.609375   BatchTime 0.065210
INFO - ==> Top1: 90.560    Top5: 99.640    Loss: 0.368
INFO - Scoreboard best 1 ==> Epoch [48][Top1: 90.560   Top5: 99.640] Sparsity : 0.851
INFO - Scoreboard best 2 ==> Epoch [44][Top1: 90.560   Top5: 99.630] Sparsity : 0.850
INFO - Scoreboard best 3 ==> Epoch [41][Top1: 90.530   Top5: 99.650] Sparsity : 0.850
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  49
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [49][   20/  391]   Loss 0.068837   Top1 97.343750   Top5 100.000000   BatchTime 0.198640   LR 0.001000
INFO - Training [49][   40/  391]   Loss 0.065183   Top1 97.656250   Top5 100.000000   BatchTime 0.149866   LR 0.001000
INFO - Training [49][   60/  391]   Loss 0.066788   Top1 97.526042   Top5 100.000000   BatchTime 0.133404   LR 0.001000
INFO - Training [49][   80/  391]   Loss 0.066472   Top1 97.587891   Top5 99.990234   BatchTime 0.125057   LR 0.001000
INFO - Training [49][  100/  391]   Loss 0.066282   Top1 97.609375   Top5 99.992188   BatchTime 0.120202   LR 0.001000
INFO - Training [49][  120/  391]   Loss 0.066109   Top1 97.578125   Top5 99.993490   BatchTime 0.116968   LR 0.001000
INFO - Training [49][  140/  391]   Loss 0.065167   Top1 97.633929   Top5 99.994420   BatchTime 0.114658   LR 0.001000
INFO - Training [49][  160/  391]   Loss 0.065914   Top1 97.626953   Top5 99.995117   BatchTime 0.112876   LR 0.001000
INFO - Training [49][  180/  391]   Loss 0.066204   Top1 97.608507   Top5 99.995660   BatchTime 0.111543   LR 0.001000
INFO - Training [49][  200/  391]   Loss 0.066504   Top1 97.582031   Top5 99.996094   BatchTime 0.110446   LR 0.001000
INFO - Training [49][  220/  391]   Loss 0.067167   Top1 97.553267   Top5 99.996449   BatchTime 0.109466   LR 0.001000
INFO - Training [49][  240/  391]   Loss 0.066857   Top1 97.555339   Top5 99.996745   BatchTime 0.108666   LR 0.001000
INFO - Training [49][  260/  391]   Loss 0.066065   Top1 97.602163   Top5 99.996995   BatchTime 0.108075   LR 0.001000
INFO - Training [49][  280/  391]   Loss 0.066557   Top1 97.580915   Top5 99.997210   BatchTime 0.107495   LR 0.001000
INFO - Training [49][  300/  391]   Loss 0.066572   Top1 97.596354   Top5 99.997396   BatchTime 0.107449   LR 0.001000
INFO - Training [49][  320/  391]   Loss 0.066472   Top1 97.609863   Top5 99.997559   BatchTime 0.106941   LR 0.001000
INFO - Training [49][  340/  391]   Loss 0.066418   Top1 97.612592   Top5 99.997702   BatchTime 0.106416   LR 0.001000
INFO - Training [49][  360/  391]   Loss 0.066750   Top1 97.601997   Top5 99.997830   BatchTime 0.105947   LR 0.001000
INFO - Training [49][  380/  391]   Loss 0.066759   Top1 97.608964   Top5 99.997944   BatchTime 0.105619   LR 0.001000
INFO - ==> Top1: 97.606    Top5: 99.998    Loss: 0.067
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [49][   20/   79]   Loss 0.369294   Top1 90.742188   Top5 99.609375   BatchTime 0.129465
INFO - Validation [49][   40/   79]   Loss 0.372537   Top1 90.644531   Top5 99.628906   BatchTime 0.082286
INFO - Validation [49][   60/   79]   Loss 0.365538   Top1 90.794271   Top5 99.622396   BatchTime 0.064662
INFO - ==> Top1: 90.590    Top5: 99.650    Loss: 0.365
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 2 ==> Epoch [48][Top1: 90.560   Top5: 99.640] Sparsity : 0.851
INFO - Scoreboard best 3 ==> Epoch [44][Top1: 90.560   Top5: 99.630] Sparsity : 0.850
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  50
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [50][   20/  391]   Loss 0.064400   Top1 97.968750   Top5 100.000000   BatchTime 0.199269   LR 0.001000
INFO - Training [50][   40/  391]   Loss 0.063121   Top1 97.871094   Top5 100.000000   BatchTime 0.150563   LR 0.001000
INFO - Training [50][   60/  391]   Loss 0.062566   Top1 97.916667   Top5 100.000000   BatchTime 0.134066   LR 0.001000
INFO - Training [50][   80/  391]   Loss 0.064154   Top1 97.802734   Top5 99.990234   BatchTime 0.125464   LR 0.001000
INFO - Training [50][  100/  391]   Loss 0.062520   Top1 97.875000   Top5 99.992188   BatchTime 0.120461   LR 0.001000
INFO - Training [50][  120/  391]   Loss 0.063204   Top1 97.851562   Top5 99.993490   BatchTime 0.117030   LR 0.001000
INFO - Training [50][  140/  391]   Loss 0.063717   Top1 97.890625   Top5 99.994420   BatchTime 0.114578   LR 0.001000
INFO - Training [50][  160/  391]   Loss 0.063895   Top1 97.875977   Top5 99.995117   BatchTime 0.112821   LR 0.001000
INFO - Training [50][  180/  391]   Loss 0.064169   Top1 97.890625   Top5 99.995660   BatchTime 0.111447   LR 0.001000
INFO - Training [50][  200/  391]   Loss 0.063674   Top1 97.890625   Top5 99.996094   BatchTime 0.110349   LR 0.001000
INFO - Training [50][  220/  391]   Loss 0.063683   Top1 97.876420   Top5 99.996449   BatchTime 0.109431   LR 0.001000
INFO - Training [50][  240/  391]   Loss 0.062994   Top1 97.887370   Top5 99.993490   BatchTime 0.108667   LR 0.001000
INFO - Training [50][  260/  391]   Loss 0.062779   Top1 97.866587   Top5 99.993990   BatchTime 0.108033   LR 0.001000
INFO - Training [50][  280/  391]   Loss 0.062420   Top1 97.868304   Top5 99.994420   BatchTime 0.107461   LR 0.001000
INFO - Training [50][  300/  391]   Loss 0.062941   Top1 97.846354   Top5 99.994792   BatchTime 0.107024   LR 0.001000
INFO - Training [50][  320/  391]   Loss 0.063387   Top1 97.829590   Top5 99.995117   BatchTime 0.106625   LR 0.001000
INFO - Training [50][  340/  391]   Loss 0.063796   Top1 97.798713   Top5 99.995404   BatchTime 0.106163   LR 0.001000
INFO - Training [50][  360/  391]   Loss 0.063865   Top1 97.795139   Top5 99.995660   BatchTime 0.105677   LR 0.001000
INFO - Training [50][  380/  391]   Loss 0.063455   Top1 97.802220   Top5 99.995888   BatchTime 0.105291   LR 0.001000
INFO - ==> Top1: 97.782    Top5: 99.994    Loss: 0.064
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [50][   20/   79]   Loss 0.381714   Top1 90.664062   Top5 99.609375   BatchTime 0.132181
INFO - Validation [50][   40/   79]   Loss 0.384129   Top1 90.507812   Top5 99.628906   BatchTime 0.083454
INFO - Validation [50][   60/   79]   Loss 0.371914   Top1 90.703125   Top5 99.661458   BatchTime 0.065979
INFO - ==> Top1: 90.570    Top5: 99.690    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 90.560   Top5: 99.640] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  51
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [51][   20/  391]   Loss 0.054286   Top1 98.203125   Top5 100.000000   BatchTime 0.196519   LR 0.001000
INFO - Training [51][   40/  391]   Loss 0.058610   Top1 97.988281   Top5 100.000000   BatchTime 0.148645   LR 0.001000
INFO - Training [51][   60/  391]   Loss 0.058851   Top1 98.059896   Top5 100.000000   BatchTime 0.132684   LR 0.001000
INFO - Training [51][   80/  391]   Loss 0.060980   Top1 98.007812   Top5 100.000000   BatchTime 0.124949   LR 0.001000
INFO - Training [51][  100/  391]   Loss 0.061270   Top1 98.007812   Top5 100.000000   BatchTime 0.120109   LR 0.001000
INFO - Training [51][  120/  391]   Loss 0.061338   Top1 98.020833   Top5 100.000000   BatchTime 0.116760   LR 0.001000
INFO - Training [51][  140/  391]   Loss 0.061826   Top1 97.985491   Top5 100.000000   BatchTime 0.114382   LR 0.001000
INFO - Training [51][  160/  391]   Loss 0.061638   Top1 97.949219   Top5 99.995117   BatchTime 0.112586   LR 0.001000
INFO - Training [51][  180/  391]   Loss 0.062092   Top1 97.929688   Top5 99.995660   BatchTime 0.111259   LR 0.001000
INFO - Training [51][  200/  391]   Loss 0.062829   Top1 97.910156   Top5 99.996094   BatchTime 0.110193   LR 0.001000
INFO - Training [51][  220/  391]   Loss 0.064156   Top1 97.865767   Top5 99.996449   BatchTime 0.109255   LR 0.001000
INFO - Training [51][  240/  391]   Loss 0.063679   Top1 97.871094   Top5 99.996745   BatchTime 0.108557   LR 0.001000
INFO - Training [51][  260/  391]   Loss 0.063596   Top1 97.860577   Top5 99.996995   BatchTime 0.107926   LR 0.001000
INFO - Training [51][  280/  391]   Loss 0.063991   Top1 97.848772   Top5 99.997210   BatchTime 0.107386   LR 0.001000
INFO - Training [51][  300/  391]   Loss 0.064564   Top1 97.807292   Top5 99.997396   BatchTime 0.106952   LR 0.001000
INFO - Training [51][  320/  391]   Loss 0.064697   Top1 97.778320   Top5 99.997559   BatchTime 0.106468   LR 0.001000
INFO - Training [51][  340/  391]   Loss 0.064545   Top1 97.794118   Top5 99.997702   BatchTime 0.105964   LR 0.001000
INFO - Training [51][  360/  391]   Loss 0.064483   Top1 97.799479   Top5 99.997830   BatchTime 0.105485   LR 0.001000
INFO - Training [51][  380/  391]   Loss 0.064763   Top1 97.777549   Top5 99.997944   BatchTime 0.105420   LR 0.001000
INFO - ==> Top1: 97.780    Top5: 99.998    Loss: 0.065
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [51][   20/   79]   Loss 0.372411   Top1 90.898438   Top5 99.570312   BatchTime 0.129470
INFO - Validation [51][   40/   79]   Loss 0.380835   Top1 90.566406   Top5 99.589844   BatchTime 0.081638
INFO - Validation [51][   60/   79]   Loss 0.372672   Top1 90.664062   Top5 99.583333   BatchTime 0.064320
INFO - ==> Top1: 90.520    Top5: 99.630    Loss: 0.371
INFO - Scoreboard best 1 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 2 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [48][Top1: 90.560   Top5: 99.640] Sparsity : 0.851
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  52
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [52][   20/  391]   Loss 0.061428   Top1 97.890625   Top5 100.000000   BatchTime 0.199894   LR 0.001000
INFO - Training [52][   40/  391]   Loss 0.063770   Top1 97.597656   Top5 100.000000   BatchTime 0.150334   LR 0.001000
INFO - Training [52][   60/  391]   Loss 0.065520   Top1 97.656250   Top5 100.000000   BatchTime 0.133652   LR 0.001000
INFO - Training [52][   80/  391]   Loss 0.064157   Top1 97.675781   Top5 100.000000   BatchTime 0.125460   LR 0.001000
INFO - Training [52][  100/  391]   Loss 0.065347   Top1 97.726562   Top5 100.000000   BatchTime 0.120548   LR 0.001000
INFO - Training [52][  120/  391]   Loss 0.065364   Top1 97.734375   Top5 100.000000   BatchTime 0.117731   LR 0.001000
INFO - Training [52][  140/  391]   Loss 0.064551   Top1 97.767857   Top5 100.000000   BatchTime 0.115334   LR 0.001000
INFO - Training [52][  160/  391]   Loss 0.064311   Top1 97.763672   Top5 99.995117   BatchTime 0.113126   LR 0.001000
INFO - Training [52][  180/  391]   Loss 0.063962   Top1 97.777778   Top5 99.995660   BatchTime 0.111668   LR 0.001000
INFO - Training [52][  200/  391]   Loss 0.064153   Top1 97.773438   Top5 99.996094   BatchTime 0.110511   LR 0.001000
INFO - Training [52][  220/  391]   Loss 0.064051   Top1 97.745028   Top5 99.996449   BatchTime 0.109720   LR 0.001000
INFO - Training [52][  240/  391]   Loss 0.063684   Top1 97.763672   Top5 99.996745   BatchTime 0.109017   LR 0.001000
INFO - Training [52][  260/  391]   Loss 0.064031   Top1 97.749399   Top5 99.996995   BatchTime 0.108324   LR 0.001000
INFO - Training [52][  280/  391]   Loss 0.063455   Top1 97.739955   Top5 99.997210   BatchTime 0.107733   LR 0.001000
INFO - Training [52][  300/  391]   Loss 0.063564   Top1 97.731771   Top5 99.997396   BatchTime 0.107199   LR 0.001000
INFO - Training [52][  320/  391]   Loss 0.063797   Top1 97.731934   Top5 99.995117   BatchTime 0.106769   LR 0.001000
INFO - Training [52][  340/  391]   Loss 0.063610   Top1 97.750460   Top5 99.995404   BatchTime 0.106309   LR 0.001000
INFO - Training [52][  360/  391]   Loss 0.063664   Top1 97.749566   Top5 99.995660   BatchTime 0.105836   LR 0.001000
INFO - Training [52][  380/  391]   Loss 0.063872   Top1 97.759046   Top5 99.995888   BatchTime 0.105604   LR 0.001000
INFO - ==> Top1: 97.760    Top5: 99.996    Loss: 0.064
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [52][   20/   79]   Loss 0.389020   Top1 90.468750   Top5 99.648438   BatchTime 0.130013
INFO - Validation [52][   40/   79]   Loss 0.377783   Top1 90.703125   Top5 99.609375   BatchTime 0.082355
INFO - Validation [52][   60/   79]   Loss 0.370771   Top1 90.729167   Top5 99.635417   BatchTime 0.065030
INFO - ==> Top1: 90.590    Top5: 99.670    Loss: 0.367
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_best.pth.tar
INFO - >>>>>>>> Epoch  53
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [53][   20/  391]   Loss 0.074182   Top1 97.539062   Top5 100.000000   BatchTime 0.198764   LR 0.001000
INFO - Training [53][   40/  391]   Loss 0.065411   Top1 97.792969   Top5 100.000000   BatchTime 0.149459   LR 0.001000
INFO - Training [53][   60/  391]   Loss 0.063883   Top1 97.760417   Top5 100.000000   BatchTime 0.133356   LR 0.001000
INFO - Training [53][   80/  391]   Loss 0.061551   Top1 97.880859   Top5 99.990234   BatchTime 0.125129   LR 0.001000
INFO - Training [53][  100/  391]   Loss 0.062441   Top1 97.789062   Top5 99.992188   BatchTime 0.120164   LR 0.001000
INFO - Training [53][  120/  391]   Loss 0.062036   Top1 97.838542   Top5 99.993490   BatchTime 0.116796   LR 0.001000
INFO - Training [53][  140/  391]   Loss 0.062910   Top1 97.840402   Top5 99.994420   BatchTime 0.114478   LR 0.001000
INFO - Training [53][  160/  391]   Loss 0.063435   Top1 97.836914   Top5 99.995117   BatchTime 0.112857   LR 0.001000
INFO - Training [53][  180/  391]   Loss 0.062633   Top1 97.899306   Top5 99.995660   BatchTime 0.111613   LR 0.001000
INFO - Training [53][  200/  391]   Loss 0.062164   Top1 97.933594   Top5 99.992188   BatchTime 0.110523   LR 0.001000
INFO - Training [53][  220/  391]   Loss 0.061957   Top1 97.936790   Top5 99.992898   BatchTime 0.109740   LR 0.001000
INFO - Training [53][  240/  391]   Loss 0.062565   Top1 97.926432   Top5 99.993490   BatchTime 0.109050   LR 0.001000
INFO - Training [53][  260/  391]   Loss 0.063542   Top1 97.869591   Top5 99.993990   BatchTime 0.108400   LR 0.001000
INFO - Training [53][  280/  391]   Loss 0.062774   Top1 97.910156   Top5 99.991629   BatchTime 0.107851   LR 0.001000
INFO - Training [53][  300/  391]   Loss 0.063299   Top1 97.888021   Top5 99.989583   BatchTime 0.107361   LR 0.001000
INFO - Training [53][  320/  391]   Loss 0.063320   Top1 97.861328   Top5 99.990234   BatchTime 0.106956   LR 0.001000
INFO - Training [53][  340/  391]   Loss 0.063031   Top1 97.872243   Top5 99.990809   BatchTime 0.106434   LR 0.001000
INFO - Training [53][  360/  391]   Loss 0.063104   Top1 97.860243   Top5 99.991319   BatchTime 0.105970   LR 0.001000
INFO - Training [53][  380/  391]   Loss 0.063181   Top1 97.843339   Top5 99.991776   BatchTime 0.105726   LR 0.001000
INFO - ==> Top1: 97.830    Top5: 99.990    Loss: 0.064
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [53][   20/   79]   Loss 0.389278   Top1 90.507812   Top5 99.570312   BatchTime 0.129334
INFO - Validation [53][   40/   79]   Loss 0.391162   Top1 90.449219   Top5 99.531250   BatchTime 0.081656
INFO - Validation [53][   60/   79]   Loss 0.379665   Top1 90.638021   Top5 99.583333   BatchTime 0.064405
INFO - ==> Top1: 90.560    Top5: 99.620    Loss: 0.376
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  54
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [54][   20/  391]   Loss 0.067180   Top1 97.500000   Top5 100.000000   BatchTime 0.205335   LR 0.001000
INFO - Training [54][   40/  391]   Loss 0.069323   Top1 97.441406   Top5 100.000000   BatchTime 0.152673   LR 0.001000
INFO - Training [54][   60/  391]   Loss 0.067295   Top1 97.500000   Top5 100.000000   BatchTime 0.135526   LR 0.001000
INFO - Training [54][   80/  391]   Loss 0.068458   Top1 97.519531   Top5 100.000000   BatchTime 0.126705   LR 0.001000
INFO - Training [54][  100/  391]   Loss 0.069411   Top1 97.476562   Top5 100.000000   BatchTime 0.121518   LR 0.001000
INFO - Training [54][  120/  391]   Loss 0.069359   Top1 97.460938   Top5 100.000000   BatchTime 0.118178   LR 0.001000
INFO - Training [54][  140/  391]   Loss 0.069594   Top1 97.483259   Top5 100.000000   BatchTime 0.115705   LR 0.001000
INFO - Training [54][  160/  391]   Loss 0.068024   Top1 97.548828   Top5 100.000000   BatchTime 0.113807   LR 0.001000
INFO - Training [54][  180/  391]   Loss 0.069174   Top1 97.521701   Top5 100.000000   BatchTime 0.112313   LR 0.001000
INFO - Training [54][  200/  391]   Loss 0.068639   Top1 97.558594   Top5 100.000000   BatchTime 0.111067   LR 0.001000
INFO - Training [54][  220/  391]   Loss 0.067962   Top1 97.574574   Top5 100.000000   BatchTime 0.110135   LR 0.001000
INFO - Training [54][  240/  391]   Loss 0.067275   Top1 97.620443   Top5 100.000000   BatchTime 0.109274   LR 0.001000
INFO - Training [54][  260/  391]   Loss 0.067208   Top1 97.611178   Top5 100.000000   BatchTime 0.108639   LR 0.001000
INFO - Training [54][  280/  391]   Loss 0.066980   Top1 97.619978   Top5 100.000000   BatchTime 0.108093   LR 0.001000
INFO - Training [54][  300/  391]   Loss 0.066476   Top1 97.666667   Top5 99.997396   BatchTime 0.107526   LR 0.001000
INFO - Training [54][  320/  391]   Loss 0.066340   Top1 97.670898   Top5 99.997559   BatchTime 0.107063   LR 0.001000
INFO - Training [54][  340/  391]   Loss 0.066169   Top1 97.683824   Top5 99.997702   BatchTime 0.106524   LR 0.001000
INFO - Training [54][  360/  391]   Loss 0.066574   Top1 97.675781   Top5 99.997830   BatchTime 0.106116   LR 0.001000
INFO - Training [54][  380/  391]   Loss 0.066736   Top1 97.680921   Top5 99.997944   BatchTime 0.105773   LR 0.001000
INFO - ==> Top1: 97.672    Top5: 99.998    Loss: 0.067
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [54][   20/   79]   Loss 0.386461   Top1 90.937500   Top5 99.648438   BatchTime 0.130620
INFO - Validation [54][   40/   79]   Loss 0.387357   Top1 90.488281   Top5 99.550781   BatchTime 0.082365
INFO - Validation [54][   60/   79]   Loss 0.378443   Top1 90.807292   Top5 99.609375   BatchTime 0.064114
INFO - ==> Top1: 90.570    Top5: 99.650    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  55
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [55][   20/  391]   Loss 0.062834   Top1 97.617188   Top5 100.000000   BatchTime 0.198409   LR 0.001000
INFO - Training [55][   40/  391]   Loss 0.060159   Top1 97.812500   Top5 100.000000   BatchTime 0.149798   LR 0.001000
INFO - Training [55][   60/  391]   Loss 0.061062   Top1 97.812500   Top5 100.000000   BatchTime 0.133798   LR 0.001000
INFO - Training [55][   80/  391]   Loss 0.063540   Top1 97.753906   Top5 99.990234   BatchTime 0.125642   LR 0.001000
INFO - Training [55][  100/  391]   Loss 0.064841   Top1 97.710938   Top5 99.992188   BatchTime 0.120814   LR 0.001000
INFO - Training [55][  120/  391]   Loss 0.063754   Top1 97.825521   Top5 99.993490   BatchTime 0.117405   LR 0.001000
INFO - Training [55][  140/  391]   Loss 0.064370   Top1 97.779018   Top5 99.988839   BatchTime 0.114972   LR 0.001000
INFO - Training [55][  160/  391]   Loss 0.063018   Top1 97.827148   Top5 99.990234   BatchTime 0.113099   LR 0.001000
INFO - Training [55][  180/  391]   Loss 0.063269   Top1 97.795139   Top5 99.986979   BatchTime 0.111681   LR 0.001000
INFO - Training [55][  200/  391]   Loss 0.063693   Top1 97.789062   Top5 99.988281   BatchTime 0.110521   LR 0.001000
INFO - Training [55][  220/  391]   Loss 0.063907   Top1 97.776989   Top5 99.989347   BatchTime 0.109524   LR 0.001000
INFO - Training [55][  240/  391]   Loss 0.064628   Top1 97.734375   Top5 99.990234   BatchTime 0.108760   LR 0.001000
INFO - Training [55][  260/  391]   Loss 0.064642   Top1 97.743389   Top5 99.990986   BatchTime 0.108170   LR 0.001000
INFO - Training [55][  280/  391]   Loss 0.064980   Top1 97.734375   Top5 99.991629   BatchTime 0.107591   LR 0.001000
INFO - Training [55][  300/  391]   Loss 0.065355   Top1 97.703125   Top5 99.992188   BatchTime 0.107057   LR 0.001000
INFO - Training [55][  320/  391]   Loss 0.065432   Top1 97.690430   Top5 99.992676   BatchTime 0.106607   LR 0.001000
INFO - Training [55][  340/  391]   Loss 0.065754   Top1 97.672335   Top5 99.993107   BatchTime 0.106114   LR 0.001000
INFO - Training [55][  360/  391]   Loss 0.066091   Top1 97.649740   Top5 99.993490   BatchTime 0.105632   LR 0.001000
INFO - Training [55][  380/  391]   Loss 0.065779   Top1 97.662418   Top5 99.993832   BatchTime 0.105260   LR 0.001000
INFO - ==> Top1: 97.676    Top5: 99.994    Loss: 0.066
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [55][   20/   79]   Loss 0.386875   Top1 90.273438   Top5 99.648438   BatchTime 0.130215
INFO - Validation [55][   40/   79]   Loss 0.389513   Top1 90.234375   Top5 99.609375   BatchTime 0.082354
INFO - Validation [55][   60/   79]   Loss 0.379481   Top1 90.533854   Top5 99.622396   BatchTime 0.065705
INFO - ==> Top1: 90.390    Top5: 99.660    Loss: 0.375
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  56
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [56][   20/  391]   Loss 0.070760   Top1 97.226562   Top5 100.000000   BatchTime 0.198321   LR 0.001000
INFO - Training [56][   40/  391]   Loss 0.069164   Top1 97.421875   Top5 100.000000   BatchTime 0.149326   LR 0.001000
INFO - Training [56][   60/  391]   Loss 0.061764   Top1 97.721354   Top5 99.986979   BatchTime 0.134230   LR 0.001000
INFO - Training [56][   80/  391]   Loss 0.060267   Top1 97.841797   Top5 99.990234   BatchTime 0.125808   LR 0.001000
INFO - Training [56][  100/  391]   Loss 0.060934   Top1 97.851562   Top5 99.992188   BatchTime 0.120628   LR 0.001000
INFO - Training [56][  120/  391]   Loss 0.059950   Top1 97.884115   Top5 99.993490   BatchTime 0.117181   LR 0.001000
INFO - Training [56][  140/  391]   Loss 0.061083   Top1 97.823661   Top5 99.994420   BatchTime 0.114647   LR 0.001000
INFO - Training [56][  160/  391]   Loss 0.062711   Top1 97.729492   Top5 99.995117   BatchTime 0.112850   LR 0.001000
INFO - Training [56][  180/  391]   Loss 0.063230   Top1 97.747396   Top5 99.995660   BatchTime 0.111410   LR 0.001000
INFO - Training [56][  200/  391]   Loss 0.062288   Top1 97.785156   Top5 99.996094   BatchTime 0.110330   LR 0.001000
INFO - Training [56][  220/  391]   Loss 0.061898   Top1 97.812500   Top5 99.996449   BatchTime 0.109394   LR 0.001000
INFO - Training [56][  240/  391]   Loss 0.062258   Top1 97.832031   Top5 99.996745   BatchTime 0.108677   LR 0.001000
INFO - Training [56][  260/  391]   Loss 0.063546   Top1 97.785457   Top5 99.996995   BatchTime 0.108018   LR 0.001000
INFO - Training [56][  280/  391]   Loss 0.063882   Top1 97.762277   Top5 99.997210   BatchTime 0.107396   LR 0.001000
INFO - Training [56][  300/  391]   Loss 0.063574   Top1 97.778646   Top5 99.997396   BatchTime 0.106748   LR 0.001000
INFO - Training [56][  320/  391]   Loss 0.063862   Top1 97.746582   Top5 99.997559   BatchTime 0.106312   LR 0.001000
INFO - Training [56][  340/  391]   Loss 0.064593   Top1 97.715993   Top5 99.997702   BatchTime 0.105797   LR 0.001000
INFO - Training [56][  360/  391]   Loss 0.064523   Top1 97.736545   Top5 99.997830   BatchTime 0.105315   LR 0.001000
INFO - Training [56][  380/  391]   Loss 0.064524   Top1 97.748766   Top5 99.997944   BatchTime 0.105077   LR 0.001000
INFO - ==> Top1: 97.734    Top5: 99.998    Loss: 0.065
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [56][   20/   79]   Loss 0.383922   Top1 90.234375   Top5 99.609375   BatchTime 0.129999
INFO - Validation [56][   40/   79]   Loss 0.380741   Top1 90.000000   Top5 99.589844   BatchTime 0.082307
INFO - Validation [56][   60/   79]   Loss 0.374438   Top1 90.208333   Top5 99.622396   BatchTime 0.065686
INFO - ==> Top1: 90.140    Top5: 99.630    Loss: 0.369
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  57
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [57][   20/  391]   Loss 0.063825   Top1 97.617188   Top5 100.000000   BatchTime 0.198575   LR 0.001000
INFO - Training [57][   40/  391]   Loss 0.063380   Top1 97.636719   Top5 100.000000   BatchTime 0.149309   LR 0.001000
INFO - Training [57][   60/  391]   Loss 0.063818   Top1 97.643229   Top5 100.000000   BatchTime 0.132793   LR 0.001000
INFO - Training [57][   80/  391]   Loss 0.065019   Top1 97.578125   Top5 100.000000   BatchTime 0.124770   LR 0.001000
INFO - Training [57][  100/  391]   Loss 0.064958   Top1 97.609375   Top5 100.000000   BatchTime 0.119947   LR 0.001000
INFO - Training [57][  120/  391]   Loss 0.065385   Top1 97.597656   Top5 100.000000   BatchTime 0.116671   LR 0.001000
INFO - Training [57][  140/  391]   Loss 0.066977   Top1 97.522321   Top5 100.000000   BatchTime 0.114391   LR 0.001000
INFO - Training [57][  160/  391]   Loss 0.066781   Top1 97.539062   Top5 100.000000   BatchTime 0.112932   LR 0.001000
INFO - Training [57][  180/  391]   Loss 0.066582   Top1 97.552083   Top5 100.000000   BatchTime 0.111714   LR 0.001000
INFO - Training [57][  200/  391]   Loss 0.066941   Top1 97.546875   Top5 100.000000   BatchTime 0.110616   LR 0.001000
INFO - Training [57][  220/  391]   Loss 0.067236   Top1 97.549716   Top5 100.000000   BatchTime 0.109819   LR 0.001000
INFO - Training [57][  240/  391]   Loss 0.068491   Top1 97.506510   Top5 100.000000   BatchTime 0.109143   LR 0.001000
INFO - Training [57][  260/  391]   Loss 0.068416   Top1 97.518029   Top5 100.000000   BatchTime 0.108529   LR 0.001000
INFO - Training [57][  280/  391]   Loss 0.068365   Top1 97.525112   Top5 99.997210   BatchTime 0.107952   LR 0.001000
INFO - Training [57][  300/  391]   Loss 0.068612   Top1 97.520833   Top5 99.997396   BatchTime 0.107421   LR 0.001000
INFO - Training [57][  320/  391]   Loss 0.068763   Top1 97.517090   Top5 99.995117   BatchTime 0.106956   LR 0.001000
INFO - Training [57][  340/  391]   Loss 0.068783   Top1 97.518382   Top5 99.995404   BatchTime 0.106438   LR 0.001000
INFO - Training [57][  360/  391]   Loss 0.069177   Top1 97.500000   Top5 99.995660   BatchTime 0.105931   LR 0.001000
INFO - Training [57][  380/  391]   Loss 0.069220   Top1 97.500000   Top5 99.993832   BatchTime 0.105651   LR 0.001000
INFO - ==> Top1: 97.482    Top5: 99.994    Loss: 0.070
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [57][   20/   79]   Loss 0.376960   Top1 90.234375   Top5 99.609375   BatchTime 0.131613
INFO - Validation [57][   40/   79]   Loss 0.381780   Top1 90.097656   Top5 99.609375   BatchTime 0.083160
INFO - Validation [57][   60/   79]   Loss 0.376309   Top1 90.208333   Top5 99.622396   BatchTime 0.065783
INFO - ==> Top1: 90.100    Top5: 99.680    Loss: 0.373
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  58
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [58][   20/  391]   Loss 0.069362   Top1 97.421875   Top5 100.000000   BatchTime 0.199014   LR 0.001000
INFO - Training [58][   40/  391]   Loss 0.076392   Top1 97.148438   Top5 100.000000   BatchTime 0.149689   LR 0.001000
INFO - Training [58][   60/  391]   Loss 0.074807   Top1 97.317708   Top5 100.000000   BatchTime 0.133316   LR 0.001000
INFO - Training [58][   80/  391]   Loss 0.075482   Top1 97.275391   Top5 100.000000   BatchTime 0.125225   LR 0.001000
INFO - Training [58][  100/  391]   Loss 0.073097   Top1 97.320312   Top5 100.000000   BatchTime 0.121664   LR 0.001000
INFO - Training [58][  120/  391]   Loss 0.072443   Top1 97.389323   Top5 100.000000   BatchTime 0.118030   LR 0.001000
INFO - Training [58][  140/  391]   Loss 0.074432   Top1 97.343750   Top5 99.994420   BatchTime 0.115444   LR 0.001000
INFO - Training [58][  160/  391]   Loss 0.074569   Top1 97.348633   Top5 99.995117   BatchTime 0.113600   LR 0.001000
INFO - Training [58][  180/  391]   Loss 0.072431   Top1 97.417535   Top5 99.995660   BatchTime 0.112195   LR 0.001000
INFO - Training [58][  200/  391]   Loss 0.071855   Top1 97.417969   Top5 99.996094   BatchTime 0.110992   LR 0.001000
INFO - Training [58][  220/  391]   Loss 0.071160   Top1 97.439631   Top5 99.996449   BatchTime 0.110013   LR 0.001000
INFO - Training [58][  240/  391]   Loss 0.070783   Top1 97.425130   Top5 99.996745   BatchTime 0.109220   LR 0.001000
INFO - Training [58][  260/  391]   Loss 0.071159   Top1 97.445913   Top5 99.996995   BatchTime 0.108575   LR 0.001000
INFO - Training [58][  280/  391]   Loss 0.072354   Top1 97.385603   Top5 99.997210   BatchTime 0.108015   LR 0.001000
INFO - Training [58][  300/  391]   Loss 0.072437   Top1 97.377604   Top5 99.997396   BatchTime 0.107547   LR 0.001000
INFO - Training [58][  320/  391]   Loss 0.072721   Top1 97.355957   Top5 99.997559   BatchTime 0.107162   LR 0.001000
INFO - Training [58][  340/  391]   Loss 0.072265   Top1 97.385110   Top5 99.997702   BatchTime 0.106615   LR 0.001000
INFO - Training [58][  360/  391]   Loss 0.072223   Top1 97.367622   Top5 99.997830   BatchTime 0.106058   LR 0.001000
INFO - Training [58][  380/  391]   Loss 0.073397   Top1 97.329359   Top5 99.997944   BatchTime 0.105696   LR 0.001000
INFO - ==> Top1: 97.328    Top5: 99.996    Loss: 0.074
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [58][   20/   79]   Loss 0.391171   Top1 90.078125   Top5 99.492188   BatchTime 0.130564
INFO - Validation [58][   40/   79]   Loss 0.391591   Top1 89.609375   Top5 99.550781   BatchTime 0.082551
INFO - Validation [58][   60/   79]   Loss 0.385253   Top1 89.830729   Top5 99.557292   BatchTime 0.065343
INFO - ==> Top1: 89.770    Top5: 99.580    Loss: 0.382
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  59
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [59][   20/  391]   Loss 0.070130   Top1 97.617188   Top5 100.000000   BatchTime 0.199093   LR 0.001000
INFO - Training [59][   40/  391]   Loss 0.077027   Top1 97.324219   Top5 99.980469   BatchTime 0.150288   LR 0.001000
INFO - Training [59][   60/  391]   Loss 0.075679   Top1 97.291667   Top5 99.986979   BatchTime 0.133500   LR 0.001000
INFO - Training [59][   80/  391]   Loss 0.074006   Top1 97.392578   Top5 99.990234   BatchTime 0.125358   LR 0.001000
INFO - Training [59][  100/  391]   Loss 0.074103   Top1 97.414062   Top5 99.992188   BatchTime 0.120376   LR 0.001000
INFO - Training [59][  120/  391]   Loss 0.073451   Top1 97.408854   Top5 99.993490   BatchTime 0.116997   LR 0.001000
INFO - Training [59][  140/  391]   Loss 0.072110   Top1 97.483259   Top5 99.983259   BatchTime 0.114715   LR 0.001000
INFO - Training [59][  160/  391]   Loss 0.072331   Top1 97.485352   Top5 99.980469   BatchTime 0.113023   LR 0.001000
INFO - Training [59][  180/  391]   Loss 0.074217   Top1 97.404514   Top5 99.982639   BatchTime 0.111703   LR 0.001000
INFO - Training [59][  200/  391]   Loss 0.075516   Top1 97.351562   Top5 99.984375   BatchTime 0.110576   LR 0.001000
INFO - Training [59][  220/  391]   Loss 0.075668   Top1 97.311790   Top5 99.982244   BatchTime 0.109684   LR 0.001000
INFO - Training [59][  240/  391]   Loss 0.075886   Top1 97.288411   Top5 99.983724   BatchTime 0.108946   LR 0.001000
INFO - Training [59][  260/  391]   Loss 0.075691   Top1 97.292668   Top5 99.984976   BatchTime 0.108352   LR 0.001000
INFO - Training [59][  280/  391]   Loss 0.075612   Top1 97.296317   Top5 99.986049   BatchTime 0.107866   LR 0.001000
INFO - Training [59][  300/  391]   Loss 0.075565   Top1 97.304688   Top5 99.986979   BatchTime 0.107410   LR 0.001000
INFO - Training [59][  320/  391]   Loss 0.076483   Top1 97.263184   Top5 99.985352   BatchTime 0.106988   LR 0.001000
INFO - Training [59][  340/  391]   Loss 0.077774   Top1 97.226562   Top5 99.983915   BatchTime 0.106459   LR 0.001000
INFO - Training [59][  360/  391]   Loss 0.078486   Top1 97.209201   Top5 99.984809   BatchTime 0.105949   LR 0.001000
INFO - Training [59][  380/  391]   Loss 0.078719   Top1 97.206003   Top5 99.985609   BatchTime 0.105757   LR 0.001000
INFO - ==> Top1: 97.192    Top5: 99.986    Loss: 0.079
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [59][   20/   79]   Loss 0.409734   Top1 89.296875   Top5 99.570312   BatchTime 0.130489
INFO - Validation [59][   40/   79]   Loss 0.404216   Top1 89.687500   Top5 99.550781   BatchTime 0.082700
INFO - Validation [59][   60/   79]   Loss 0.392416   Top1 90.026042   Top5 99.570312   BatchTime 0.065881
INFO - ==> Top1: 89.910    Top5: 99.620    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  60
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [60][   20/  391]   Loss 0.094001   Top1 96.289062   Top5 100.000000   BatchTime 0.180712   LR 0.000100
INFO - Training [60][   40/  391]   Loss 0.088271   Top1 96.523438   Top5 100.000000   BatchTime 0.131454   LR 0.000100
INFO - Training [60][   60/  391]   Loss 0.085925   Top1 96.705729   Top5 100.000000   BatchTime 0.112956   LR 0.000100
INFO - Training [60][   80/  391]   Loss 0.083312   Top1 96.865234   Top5 100.000000   BatchTime 0.103614   LR 0.000100
INFO - Training [60][  100/  391]   Loss 0.085273   Top1 96.781250   Top5 100.000000   BatchTime 0.097952   LR 0.000100
INFO - Training [60][  120/  391]   Loss 0.083586   Top1 96.835938   Top5 100.000000   BatchTime 0.094173   LR 0.000100
INFO - Training [60][  140/  391]   Loss 0.083095   Top1 96.891741   Top5 100.000000   BatchTime 0.091496   LR 0.000100
INFO - Training [60][  160/  391]   Loss 0.081949   Top1 96.953125   Top5 100.000000   BatchTime 0.090052   LR 0.000100
INFO - Training [60][  180/  391]   Loss 0.080504   Top1 97.026910   Top5 100.000000   BatchTime 0.088241   LR 0.000100
INFO - Training [60][  200/  391]   Loss 0.080566   Top1 96.992188   Top5 100.000000   BatchTime 0.086834   LR 0.000100
INFO - Training [60][  220/  391]   Loss 0.081327   Top1 96.995739   Top5 99.996449   BatchTime 0.085634   LR 0.000100
INFO - Training [60][  240/  391]   Loss 0.079880   Top1 97.063802   Top5 99.996745   BatchTime 0.084661   LR 0.000100
INFO - Training [60][  260/  391]   Loss 0.080653   Top1 97.025240   Top5 99.996995   BatchTime 0.083756   LR 0.000100
INFO - Training [60][  280/  391]   Loss 0.080195   Top1 97.064732   Top5 99.994420   BatchTime 0.083097   LR 0.000100
INFO - Training [60][  300/  391]   Loss 0.080393   Top1 97.075521   Top5 99.992188   BatchTime 0.082287   LR 0.000100
INFO - Training [60][  320/  391]   Loss 0.080629   Top1 97.062988   Top5 99.992676   BatchTime 0.081555   LR 0.000100
INFO - Training [60][  340/  391]   Loss 0.080022   Top1 97.088695   Top5 99.993107   BatchTime 0.080906   LR 0.000100
INFO - Training [60][  360/  391]   Loss 0.080199   Top1 97.068142   Top5 99.993490   BatchTime 0.080278   LR 0.000100
INFO - Training [60][  380/  391]   Loss 0.080058   Top1 97.090872   Top5 99.993832   BatchTime 0.079676   LR 0.000100
INFO - ==> Top1: 97.062    Top5: 99.994    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [60][   20/   79]   Loss 0.410387   Top1 89.804688   Top5 99.492188   BatchTime 0.119784
INFO - Validation [60][   40/   79]   Loss 0.400739   Top1 89.921875   Top5 99.511719   BatchTime 0.072893
INFO - Validation [60][   60/   79]   Loss 0.390302   Top1 90.156250   Top5 99.557292   BatchTime 0.057236
INFO - ==> Top1: 90.060    Top5: 99.590    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  61
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [61][   20/  391]   Loss 0.095730   Top1 96.718750   Top5 100.000000   BatchTime 0.169966   LR 0.000100
INFO - Training [61][   40/  391]   Loss 0.092534   Top1 96.796875   Top5 100.000000   BatchTime 0.123733   LR 0.000100
INFO - Training [61][   60/  391]   Loss 0.087419   Top1 96.979167   Top5 100.000000   BatchTime 0.111028   LR 0.000100
INFO - Training [61][   80/  391]   Loss 0.085001   Top1 97.080078   Top5 100.000000   BatchTime 0.105130   LR 0.000100
INFO - Training [61][  100/  391]   Loss 0.082521   Top1 97.125000   Top5 100.000000   BatchTime 0.101802   LR 0.000100
INFO - Training [61][  120/  391]   Loss 0.081980   Top1 97.115885   Top5 100.000000   BatchTime 0.099842   LR 0.000100
INFO - Training [61][  140/  391]   Loss 0.081527   Top1 97.092634   Top5 100.000000   BatchTime 0.103085   LR 0.000100
INFO - Training [61][  160/  391]   Loss 0.081566   Top1 97.133789   Top5 100.000000   BatchTime 0.105728   LR 0.000100
INFO - Training [61][  180/  391]   Loss 0.081844   Top1 97.113715   Top5 100.000000   BatchTime 0.107832   LR 0.000100
INFO - Training [61][  200/  391]   Loss 0.083295   Top1 97.054688   Top5 100.000000   BatchTime 0.109542   LR 0.000100
INFO - Training [61][  220/  391]   Loss 0.083648   Top1 97.034801   Top5 99.996449   BatchTime 0.110820   LR 0.000100
INFO - Training [61][  240/  391]   Loss 0.081938   Top1 97.106120   Top5 99.996745   BatchTime 0.111939   LR 0.000100
INFO - Training [61][  260/  391]   Loss 0.081984   Top1 97.127404   Top5 99.990986   BatchTime 0.112863   LR 0.000100
INFO - Training [61][  280/  391]   Loss 0.081985   Top1 97.109375   Top5 99.991629   BatchTime 0.113654   LR 0.000100
INFO - Training [61][  300/  391]   Loss 0.081932   Top1 97.101562   Top5 99.989583   BatchTime 0.114320   LR 0.000100
INFO - Training [61][  320/  391]   Loss 0.081829   Top1 97.099609   Top5 99.990234   BatchTime 0.114375   LR 0.000100
INFO - Training [61][  340/  391]   Loss 0.081754   Top1 97.116268   Top5 99.990809   BatchTime 0.112189   LR 0.000100
INFO - Training [61][  360/  391]   Loss 0.081963   Top1 97.115885   Top5 99.991319   BatchTime 0.110739   LR 0.000100
INFO - Training [61][  380/  391]   Loss 0.081625   Top1 97.129934   Top5 99.991776   BatchTime 0.109531   LR 0.000100
INFO - ==> Top1: 97.132    Top5: 99.992    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [61][   20/   79]   Loss 0.411574   Top1 89.765625   Top5 99.531250   BatchTime 0.163407
INFO - Validation [61][   40/   79]   Loss 0.401459   Top1 89.824219   Top5 99.550781   BatchTime 0.114072
INFO - Validation [61][   60/   79]   Loss 0.391786   Top1 90.039062   Top5 99.609375   BatchTime 0.097314
INFO - ==> Top1: 89.870    Top5: 99.650    Loss: 0.385
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  62
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [62][   20/  391]   Loss 0.077349   Top1 97.304688   Top5 99.921875   BatchTime 0.215004   LR 0.000100
INFO - Training [62][   40/  391]   Loss 0.078215   Top1 97.246094   Top5 99.960938   BatchTime 0.169555   LR 0.000100
INFO - Training [62][   60/  391]   Loss 0.081598   Top1 97.005208   Top5 99.973958   BatchTime 0.154373   LR 0.000100
INFO - Training [62][   80/  391]   Loss 0.080466   Top1 97.021484   Top5 99.970703   BatchTime 0.146672   LR 0.000100
INFO - Training [62][  100/  391]   Loss 0.079878   Top1 97.062500   Top5 99.976562   BatchTime 0.142093   LR 0.000100
INFO - Training [62][  120/  391]   Loss 0.079625   Top1 97.089844   Top5 99.980469   BatchTime 0.136250   LR 0.000100
INFO - Training [62][  140/  391]   Loss 0.081293   Top1 97.075893   Top5 99.977679   BatchTime 0.128294   LR 0.000100
INFO - Training [62][  160/  391]   Loss 0.081536   Top1 97.084961   Top5 99.980469   BatchTime 0.123308   LR 0.000100
INFO - Training [62][  180/  391]   Loss 0.080526   Top1 97.131076   Top5 99.978299   BatchTime 0.119079   LR 0.000100
INFO - Training [62][  200/  391]   Loss 0.082098   Top1 97.070312   Top5 99.976562   BatchTime 0.118724   LR 0.000100
INFO - Training [62][  220/  391]   Loss 0.081335   Top1 97.098722   Top5 99.978693   BatchTime 0.119648   LR 0.000100
INFO - Training [62][  240/  391]   Loss 0.081019   Top1 97.093099   Top5 99.980469   BatchTime 0.120141   LR 0.000100
INFO - Training [62][  260/  391]   Loss 0.080544   Top1 97.133413   Top5 99.981971   BatchTime 0.120484   LR 0.000100
INFO - Training [62][  280/  391]   Loss 0.079626   Top1 97.198661   Top5 99.983259   BatchTime 0.120711   LR 0.000100
INFO - Training [62][  300/  391]   Loss 0.080364   Top1 97.210938   Top5 99.981771   BatchTime 0.120974   LR 0.000100
INFO - Training [62][  320/  391]   Loss 0.080125   Top1 97.219238   Top5 99.982910   BatchTime 0.121233   LR 0.000100
INFO - Training [62][  340/  391]   Loss 0.079787   Top1 97.233456   Top5 99.983915   BatchTime 0.121354   LR 0.000100
INFO - Training [62][  360/  391]   Loss 0.079454   Top1 97.246094   Top5 99.984809   BatchTime 0.121420   LR 0.000100
INFO - Training [62][  380/  391]   Loss 0.079310   Top1 97.243010   Top5 99.985609   BatchTime 0.121521   LR 0.000100
INFO - ==> Top1: 97.226    Top5: 99.986    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [62][   20/   79]   Loss 0.400381   Top1 89.375000   Top5 99.570312   BatchTime 0.142115
INFO - Validation [62][   40/   79]   Loss 0.394769   Top1 89.667969   Top5 99.570312   BatchTime 0.084752
INFO - Validation [62][   60/   79]   Loss 0.386269   Top1 89.986979   Top5 99.583333   BatchTime 0.065136
INFO - ==> Top1: 89.870    Top5: 99.620    Loss: 0.380
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  63
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [63][   20/  391]   Loss 0.080428   Top1 97.070312   Top5 99.960938   BatchTime 0.224095   LR 0.000100
INFO - Training [63][   40/  391]   Loss 0.078188   Top1 97.304688   Top5 99.980469   BatchTime 0.173977   LR 0.000100
INFO - Training [63][   60/  391]   Loss 0.083805   Top1 97.122396   Top5 99.986979   BatchTime 0.157396   LR 0.000100
INFO - Training [63][   80/  391]   Loss 0.084203   Top1 97.080078   Top5 99.990234   BatchTime 0.149056   LR 0.000100
INFO - Training [63][  100/  391]   Loss 0.082801   Top1 97.132812   Top5 99.984375   BatchTime 0.144075   LR 0.000100
INFO - Training [63][  120/  391]   Loss 0.082504   Top1 97.122396   Top5 99.986979   BatchTime 0.140731   LR 0.000100
INFO - Training [63][  140/  391]   Loss 0.082833   Top1 97.142857   Top5 99.988839   BatchTime 0.138250   LR 0.000100
INFO - Training [63][  160/  391]   Loss 0.083101   Top1 97.104492   Top5 99.990234   BatchTime 0.136405   LR 0.000100
INFO - Training [63][  180/  391]   Loss 0.082836   Top1 97.118056   Top5 99.986979   BatchTime 0.134590   LR 0.000100
INFO - Training [63][  200/  391]   Loss 0.083130   Top1 97.093750   Top5 99.984375   BatchTime 0.128920   LR 0.000100
INFO - Training [63][  220/  391]   Loss 0.082852   Top1 97.109375   Top5 99.985795   BatchTime 0.125555   LR 0.000100
INFO - Training [63][  240/  391]   Loss 0.082919   Top1 97.073568   Top5 99.986979   BatchTime 0.122156   LR 0.000100
INFO - Training [63][  260/  391]   Loss 0.081808   Top1 97.133413   Top5 99.987981   BatchTime 0.121083   LR 0.000100
INFO - Training [63][  280/  391]   Loss 0.082427   Top1 97.089844   Top5 99.988839   BatchTime 0.121379   LR 0.000100
INFO - Training [63][  300/  391]   Loss 0.081794   Top1 97.125000   Top5 99.989583   BatchTime 0.121634   LR 0.000100
INFO - Training [63][  320/  391]   Loss 0.081986   Top1 97.126465   Top5 99.987793   BatchTime 0.121818   LR 0.000100
INFO - Training [63][  340/  391]   Loss 0.082359   Top1 97.093290   Top5 99.988511   BatchTime 0.121923   LR 0.000100
INFO - Training [63][  360/  391]   Loss 0.082471   Top1 97.089844   Top5 99.986979   BatchTime 0.122064   LR 0.000100
INFO - Training [63][  380/  391]   Loss 0.082109   Top1 97.092928   Top5 99.987664   BatchTime 0.122151   LR 0.000100
INFO - ==> Top1: 97.106    Top5: 99.986    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [63][   20/   79]   Loss 0.412017   Top1 89.257812   Top5 99.570312   BatchTime 0.155089
INFO - Validation [63][   40/   79]   Loss 0.397687   Top1 89.609375   Top5 99.570312   BatchTime 0.109349
INFO - Validation [63][   60/   79]   Loss 0.388443   Top1 89.804688   Top5 99.596354   BatchTime 0.091188
INFO - ==> Top1: 89.830    Top5: 99.640    Loss: 0.382
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  64
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [64][   20/  391]   Loss 0.068319   Top1 97.851562   Top5 100.000000   BatchTime 0.186160   LR 0.000100
INFO - Training [64][   40/  391]   Loss 0.073697   Top1 97.500000   Top5 100.000000   BatchTime 0.142032   LR 0.000100
INFO - Training [64][   60/  391]   Loss 0.076505   Top1 97.317708   Top5 100.000000   BatchTime 0.136225   LR 0.000100
INFO - Training [64][   80/  391]   Loss 0.080137   Top1 97.109375   Top5 100.000000   BatchTime 0.133293   LR 0.000100
INFO - Training [64][  100/  391]   Loss 0.079763   Top1 97.140625   Top5 100.000000   BatchTime 0.131456   LR 0.000100
INFO - Training [64][  120/  391]   Loss 0.078093   Top1 97.194010   Top5 100.000000   BatchTime 0.129947   LR 0.000100
INFO - Training [64][  140/  391]   Loss 0.077848   Top1 97.204241   Top5 100.000000   BatchTime 0.129160   LR 0.000100
INFO - Training [64][  160/  391]   Loss 0.078727   Top1 97.153320   Top5 100.000000   BatchTime 0.128530   LR 0.000100
INFO - Training [64][  180/  391]   Loss 0.078405   Top1 97.222222   Top5 99.995660   BatchTime 0.128011   LR 0.000100
INFO - Training [64][  200/  391]   Loss 0.078922   Top1 97.191406   Top5 99.996094   BatchTime 0.127546   LR 0.000100
INFO - Training [64][  220/  391]   Loss 0.079023   Top1 97.201705   Top5 99.996449   BatchTime 0.127175   LR 0.000100
INFO - Training [64][  240/  391]   Loss 0.079798   Top1 97.190755   Top5 99.996745   BatchTime 0.125853   LR 0.000100
INFO - Training [64][  260/  391]   Loss 0.080719   Top1 97.130409   Top5 99.996995   BatchTime 0.122642   LR 0.000100
INFO - Training [64][  280/  391]   Loss 0.080208   Top1 97.145647   Top5 99.997210   BatchTime 0.120655   LR 0.000100
INFO - Training [64][  300/  391]   Loss 0.080149   Top1 97.143229   Top5 99.994792   BatchTime 0.118399   LR 0.000100
INFO - Training [64][  320/  391]   Loss 0.080459   Top1 97.109375   Top5 99.995117   BatchTime 0.118910   LR 0.000100
INFO - Training [64][  340/  391]   Loss 0.080987   Top1 97.086397   Top5 99.995404   BatchTime 0.119220   LR 0.000100
INFO - Training [64][  360/  391]   Loss 0.081276   Top1 97.083333   Top5 99.995660   BatchTime 0.119474   LR 0.000100
INFO - Training [64][  380/  391]   Loss 0.081969   Top1 97.047697   Top5 99.995888   BatchTime 0.119686   LR 0.000100
INFO - ==> Top1: 97.048    Top5: 99.996    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [64][   20/   79]   Loss 0.407902   Top1 89.609375   Top5 99.570312   BatchTime 0.156090
INFO - Validation [64][   40/   79]   Loss 0.401425   Top1 89.628906   Top5 99.550781   BatchTime 0.110100
INFO - Validation [64][   60/   79]   Loss 0.390699   Top1 89.856771   Top5 99.609375   BatchTime 0.094744
INFO - ==> Top1: 89.760    Top5: 99.650    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  65
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [65][   20/  391]   Loss 0.073666   Top1 97.460938   Top5 100.000000   BatchTime 0.197385   LR 0.000100
INFO - Training [65][   40/  391]   Loss 0.072429   Top1 97.636719   Top5 100.000000   BatchTime 0.140470   LR 0.000100
INFO - Training [65][   60/  391]   Loss 0.071566   Top1 97.669271   Top5 100.000000   BatchTime 0.123009   LR 0.000100
INFO - Training [65][   80/  391]   Loss 0.072653   Top1 97.617188   Top5 100.000000   BatchTime 0.113305   LR 0.000100
INFO - Training [65][  100/  391]   Loss 0.075655   Top1 97.476562   Top5 99.992188   BatchTime 0.114368   LR 0.000100
INFO - Training [65][  120/  391]   Loss 0.075863   Top1 97.434896   Top5 99.993490   BatchTime 0.115968   LR 0.000100
INFO - Training [65][  140/  391]   Loss 0.077235   Top1 97.405134   Top5 99.983259   BatchTime 0.117120   LR 0.000100
INFO - Training [65][  160/  391]   Loss 0.076275   Top1 97.421875   Top5 99.985352   BatchTime 0.117971   LR 0.000100
INFO - Training [65][  180/  391]   Loss 0.077806   Top1 97.343750   Top5 99.982639   BatchTime 0.118676   LR 0.000100
INFO - Training [65][  200/  391]   Loss 0.078099   Top1 97.332031   Top5 99.984375   BatchTime 0.119179   LR 0.000100
INFO - Training [65][  220/  391]   Loss 0.078268   Top1 97.318892   Top5 99.982244   BatchTime 0.119670   LR 0.000100
INFO - Training [65][  240/  391]   Loss 0.078396   Top1 97.317708   Top5 99.980469   BatchTime 0.120005   LR 0.000100
INFO - Training [65][  260/  391]   Loss 0.078313   Top1 97.310697   Top5 99.981971   BatchTime 0.120281   LR 0.000100
INFO - Training [65][  280/  391]   Loss 0.078587   Top1 97.301897   Top5 99.980469   BatchTime 0.120539   LR 0.000100
INFO - Training [65][  300/  391]   Loss 0.078893   Top1 97.283854   Top5 99.981771   BatchTime 0.118094   LR 0.000100
INFO - Training [65][  320/  391]   Loss 0.078285   Top1 97.307129   Top5 99.980469   BatchTime 0.116422   LR 0.000100
INFO - Training [65][  340/  391]   Loss 0.079378   Top1 97.265625   Top5 99.981618   BatchTime 0.114920   LR 0.000100
INFO - Training [65][  360/  391]   Loss 0.079749   Top1 97.237413   Top5 99.982639   BatchTime 0.113169   LR 0.000100
INFO - Training [65][  380/  391]   Loss 0.080037   Top1 97.240954   Top5 99.983553   BatchTime 0.113757   LR 0.000100
INFO - ==> Top1: 97.248    Top5: 99.984    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [65][   20/   79]   Loss 0.407568   Top1 89.062500   Top5 99.492188   BatchTime 0.157145
INFO - Validation [65][   40/   79]   Loss 0.401628   Top1 89.335938   Top5 99.511719   BatchTime 0.110378
INFO - Validation [65][   60/   79]   Loss 0.391870   Top1 89.674479   Top5 99.544271   BatchTime 0.094326
INFO - ==> Top1: 89.710    Top5: 99.600    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  66
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [66][   20/  391]   Loss 0.078908   Top1 97.226562   Top5 100.000000   BatchTime 0.215277   LR 0.000100
INFO - Training [66][   40/  391]   Loss 0.077560   Top1 97.167969   Top5 99.980469   BatchTime 0.169264   LR 0.000100
INFO - Training [66][   60/  391]   Loss 0.076442   Top1 97.278646   Top5 99.986979   BatchTime 0.154012   LR 0.000100
INFO - Training [66][   80/  391]   Loss 0.079294   Top1 97.255859   Top5 99.980469   BatchTime 0.140551   LR 0.000100
INFO - Training [66][  100/  391]   Loss 0.079013   Top1 97.296875   Top5 99.984375   BatchTime 0.129428   LR 0.000100
INFO - Training [66][  120/  391]   Loss 0.079367   Top1 97.311198   Top5 99.980469   BatchTime 0.123232   LR 0.000100
INFO - Training [66][  140/  391]   Loss 0.081402   Top1 97.226562   Top5 99.983259   BatchTime 0.116184   LR 0.000100
INFO - Training [66][  160/  391]   Loss 0.081653   Top1 97.202148   Top5 99.985352   BatchTime 0.116550   LR 0.000100
INFO - Training [66][  180/  391]   Loss 0.081826   Top1 97.161458   Top5 99.986979   BatchTime 0.117355   LR 0.000100
INFO - Training [66][  200/  391]   Loss 0.081919   Top1 97.167969   Top5 99.988281   BatchTime 0.118102   LR 0.000100
INFO - Training [66][  220/  391]   Loss 0.082931   Top1 97.144886   Top5 99.989347   BatchTime 0.118578   LR 0.000100
INFO - Training [66][  240/  391]   Loss 0.082022   Top1 97.161458   Top5 99.990234   BatchTime 0.118541   LR 0.000100
INFO - Training [66][  260/  391]   Loss 0.081951   Top1 97.154447   Top5 99.990986   BatchTime 0.118958   LR 0.000100
INFO - Training [66][  280/  391]   Loss 0.081687   Top1 97.140067   Top5 99.991629   BatchTime 0.119290   LR 0.000100
INFO - Training [66][  300/  391]   Loss 0.081944   Top1 97.135417   Top5 99.986979   BatchTime 0.119535   LR 0.000100
INFO - Training [66][  320/  391]   Loss 0.082298   Top1 97.119141   Top5 99.987793   BatchTime 0.119769   LR 0.000100
INFO - Training [66][  340/  391]   Loss 0.081694   Top1 97.143842   Top5 99.988511   BatchTime 0.120242   LR 0.000100
INFO - Training [66][  360/  391]   Loss 0.081657   Top1 97.146267   Top5 99.989149   BatchTime 0.118107   LR 0.000100
INFO - Training [66][  380/  391]   Loss 0.081374   Top1 97.146382   Top5 99.989720   BatchTime 0.116460   LR 0.000100
INFO - ==> Top1: 97.144    Top5: 99.990    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [66][   20/   79]   Loss 0.406823   Top1 89.921875   Top5 99.531250   BatchTime 0.138165
INFO - Validation [66][   40/   79]   Loss 0.399425   Top1 89.980469   Top5 99.570312   BatchTime 0.100229
INFO - Validation [66][   60/   79]   Loss 0.389689   Top1 90.182292   Top5 99.609375   BatchTime 0.088484
INFO - ==> Top1: 90.030    Top5: 99.650    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  67
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [67][   20/  391]   Loss 0.073114   Top1 97.304688   Top5 100.000000   BatchTime 0.217223   LR 0.000100
INFO - Training [67][   40/  391]   Loss 0.077298   Top1 97.265625   Top5 100.000000   BatchTime 0.171129   LR 0.000100
INFO - Training [67][   60/  391]   Loss 0.077038   Top1 97.265625   Top5 100.000000   BatchTime 0.155756   LR 0.000100
INFO - Training [67][   80/  391]   Loss 0.075871   Top1 97.285156   Top5 99.990234   BatchTime 0.147838   LR 0.000100
INFO - Training [67][  100/  391]   Loss 0.079002   Top1 97.171875   Top5 99.992188   BatchTime 0.142940   LR 0.000100
INFO - Training [67][  120/  391]   Loss 0.078572   Top1 97.161458   Top5 99.986979   BatchTime 0.139809   LR 0.000100
INFO - Training [67][  140/  391]   Loss 0.079177   Top1 97.148438   Top5 99.988839   BatchTime 0.135345   LR 0.000100
INFO - Training [67][  160/  391]   Loss 0.080647   Top1 97.114258   Top5 99.990234   BatchTime 0.128742   LR 0.000100
INFO - Training [67][  180/  391]   Loss 0.081722   Top1 97.070312   Top5 99.991319   BatchTime 0.124191   LR 0.000100
INFO - Training [67][  200/  391]   Loss 0.081502   Top1 97.085938   Top5 99.992188   BatchTime 0.120253   LR 0.000100
INFO - Training [67][  220/  391]   Loss 0.080079   Top1 97.105824   Top5 99.992898   BatchTime 0.119632   LR 0.000100
INFO - Training [67][  240/  391]   Loss 0.080310   Top1 97.119141   Top5 99.993490   BatchTime 0.120045   LR 0.000100
INFO - Training [67][  260/  391]   Loss 0.079837   Top1 97.121394   Top5 99.993990   BatchTime 0.120361   LR 0.000100
INFO - Training [67][  280/  391]   Loss 0.079789   Top1 97.140067   Top5 99.988839   BatchTime 0.120638   LR 0.000100
INFO - Training [67][  300/  391]   Loss 0.079376   Top1 97.143229   Top5 99.989583   BatchTime 0.120868   LR 0.000100
INFO - Training [67][  320/  391]   Loss 0.078845   Top1 97.155762   Top5 99.990234   BatchTime 0.121063   LR 0.000100
INFO - Training [67][  340/  391]   Loss 0.079068   Top1 97.134651   Top5 99.988511   BatchTime 0.121222   LR 0.000100
INFO - Training [67][  360/  391]   Loss 0.079154   Top1 97.133247   Top5 99.989149   BatchTime 0.121330   LR 0.000100
INFO - Training [67][  380/  391]   Loss 0.079153   Top1 97.134046   Top5 99.989720   BatchTime 0.121432   LR 0.000100
INFO - ==> Top1: 97.120    Top5: 99.990    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [67][   20/   79]   Loss 0.413673   Top1 89.335938   Top5 99.492188   BatchTime 0.125506
INFO - Validation [67][   40/   79]   Loss 0.405974   Top1 89.589844   Top5 99.550781   BatchTime 0.079276
INFO - Validation [67][   60/   79]   Loss 0.392296   Top1 90.000000   Top5 99.609375   BatchTime 0.066878
INFO - ==> Top1: 89.840    Top5: 99.630    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  68
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [68][   20/  391]   Loss 0.083829   Top1 97.031250   Top5 100.000000   BatchTime 0.232498   LR 0.000100
INFO - Training [68][   40/  391]   Loss 0.073683   Top1 97.539062   Top5 100.000000   BatchTime 0.178749   LR 0.000100
INFO - Training [68][   60/  391]   Loss 0.077966   Top1 97.434896   Top5 100.000000   BatchTime 0.160914   LR 0.000100
INFO - Training [68][   80/  391]   Loss 0.082743   Top1 97.158203   Top5 100.000000   BatchTime 0.151686   LR 0.000100
INFO - Training [68][  100/  391]   Loss 0.082928   Top1 97.062500   Top5 100.000000   BatchTime 0.146244   LR 0.000100
INFO - Training [68][  120/  391]   Loss 0.084175   Top1 96.992188   Top5 100.000000   BatchTime 0.142542   LR 0.000100
INFO - Training [68][  140/  391]   Loss 0.082508   Top1 97.053571   Top5 100.000000   BatchTime 0.139856   LR 0.000100
INFO - Training [68][  160/  391]   Loss 0.082291   Top1 97.065430   Top5 100.000000   BatchTime 0.137830   LR 0.000100
INFO - Training [68][  180/  391]   Loss 0.081477   Top1 97.078993   Top5 100.000000   BatchTime 0.136257   LR 0.000100
INFO - Training [68][  200/  391]   Loss 0.080940   Top1 97.093750   Top5 100.000000   BatchTime 0.134144   LR 0.000100
INFO - Training [68][  220/  391]   Loss 0.081672   Top1 97.077415   Top5 99.996449   BatchTime 0.129408   LR 0.000100
INFO - Training [68][  240/  391]   Loss 0.080583   Top1 97.093099   Top5 99.996745   BatchTime 0.126068   LR 0.000100
INFO - Training [68][  260/  391]   Loss 0.080642   Top1 97.103365   Top5 99.993990   BatchTime 0.122813   LR 0.000100
INFO - Training [68][  280/  391]   Loss 0.081035   Top1 97.081473   Top5 99.994420   BatchTime 0.122151   LR 0.000100
INFO - Training [68][  300/  391]   Loss 0.081155   Top1 97.091146   Top5 99.986979   BatchTime 0.122422   LR 0.000100
INFO - Training [68][  320/  391]   Loss 0.081148   Top1 97.097168   Top5 99.987793   BatchTime 0.122534   LR 0.000100
INFO - Training [68][  340/  391]   Loss 0.082184   Top1 97.090993   Top5 99.983915   BatchTime 0.122606   LR 0.000100
INFO - Training [68][  360/  391]   Loss 0.081980   Top1 97.113715   Top5 99.984809   BatchTime 0.122310   LR 0.000100
INFO - Training [68][  380/  391]   Loss 0.082653   Top1 97.080592   Top5 99.985609   BatchTime 0.122402   LR 0.000100
INFO - ==> Top1: 97.090    Top5: 99.986    Loss: 0.083
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [68][   20/   79]   Loss 0.401344   Top1 89.921875   Top5 99.453125   BatchTime 0.161928
INFO - Validation [68][   40/   79]   Loss 0.397445   Top1 89.746094   Top5 99.531250   BatchTime 0.112554
INFO - Validation [68][   60/   79]   Loss 0.388437   Top1 90.026042   Top5 99.505208   BatchTime 0.096621
INFO - ==> Top1: 89.840    Top5: 99.550    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  69
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [69][   20/  391]   Loss 0.077121   Top1 96.914062   Top5 100.000000   BatchTime 0.190547   LR 0.000100
INFO - Training [69][   40/  391]   Loss 0.084150   Top1 96.796875   Top5 99.980469   BatchTime 0.137286   LR 0.000100
INFO - Training [69][   60/  391]   Loss 0.081890   Top1 96.875000   Top5 99.986979   BatchTime 0.128496   LR 0.000100
INFO - Training [69][   80/  391]   Loss 0.080964   Top1 96.962891   Top5 99.990234   BatchTime 0.127343   LR 0.000100
INFO - Training [69][  100/  391]   Loss 0.080344   Top1 96.976562   Top5 99.992188   BatchTime 0.126690   LR 0.000100
INFO - Training [69][  120/  391]   Loss 0.081846   Top1 96.920573   Top5 99.993490   BatchTime 0.126454   LR 0.000100
INFO - Training [69][  140/  391]   Loss 0.078960   Top1 97.070312   Top5 99.994420   BatchTime 0.126156   LR 0.000100
INFO - Training [69][  160/  391]   Loss 0.079169   Top1 97.070312   Top5 99.995117   BatchTime 0.126033   LR 0.000100
INFO - Training [69][  180/  391]   Loss 0.078635   Top1 97.113715   Top5 99.995660   BatchTime 0.125714   LR 0.000100
INFO - Training [69][  200/  391]   Loss 0.078172   Top1 97.128906   Top5 99.996094   BatchTime 0.125540   LR 0.000100
INFO - Training [69][  220/  391]   Loss 0.077154   Top1 97.169744   Top5 99.996449   BatchTime 0.125403   LR 0.000100
INFO - Training [69][  240/  391]   Loss 0.077479   Top1 97.190755   Top5 99.993490   BatchTime 0.125344   LR 0.000100
INFO - Training [69][  260/  391]   Loss 0.079028   Top1 97.133413   Top5 99.993990   BatchTime 0.122941   LR 0.000100
INFO - Training [69][  280/  391]   Loss 0.079552   Top1 97.112165   Top5 99.994420   BatchTime 0.120383   LR 0.000100
INFO - Training [69][  300/  391]   Loss 0.079672   Top1 97.104167   Top5 99.989583   BatchTime 0.118428   LR 0.000100
INFO - Training [69][  320/  391]   Loss 0.080150   Top1 97.087402   Top5 99.987793   BatchTime 0.115598   LR 0.000100
INFO - Training [69][  340/  391]   Loss 0.080415   Top1 97.088695   Top5 99.986213   BatchTime 0.116333   LR 0.000100
INFO - Training [69][  360/  391]   Loss 0.080346   Top1 97.085503   Top5 99.986979   BatchTime 0.116835   LR 0.000100
INFO - Training [69][  380/  391]   Loss 0.080201   Top1 97.097039   Top5 99.987664   BatchTime 0.117173   LR 0.000100
INFO - ==> Top1: 97.082    Top5: 99.988    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [69][   20/   79]   Loss 0.404696   Top1 89.648438   Top5 99.414062   BatchTime 0.156358
INFO - Validation [69][   40/   79]   Loss 0.399608   Top1 89.746094   Top5 99.492188   BatchTime 0.109254
INFO - Validation [69][   60/   79]   Loss 0.391037   Top1 89.934896   Top5 99.518229   BatchTime 0.093824
INFO - ==> Top1: 89.890    Top5: 99.570    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  70
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [70][   20/  391]   Loss 0.079065   Top1 96.914062   Top5 100.000000   BatchTime 0.215764   LR 0.000010
INFO - Training [70][   40/  391]   Loss 0.085914   Top1 96.835938   Top5 100.000000   BatchTime 0.162203   LR 0.000010
INFO - Training [70][   60/  391]   Loss 0.084845   Top1 96.992188   Top5 100.000000   BatchTime 0.135413   LR 0.000010
INFO - Training [70][   80/  391]   Loss 0.088023   Top1 96.923828   Top5 99.990234   BatchTime 0.123925   LR 0.000010
INFO - Training [70][  100/  391]   Loss 0.087471   Top1 96.945312   Top5 99.992188   BatchTime 0.116438   LR 0.000010
INFO - Training [70][  120/  391]   Loss 0.085518   Top1 97.063802   Top5 99.993490   BatchTime 0.117315   LR 0.000010
INFO - Training [70][  140/  391]   Loss 0.086356   Top1 96.986607   Top5 99.988839   BatchTime 0.118334   LR 0.000010
INFO - Training [70][  160/  391]   Loss 0.084990   Top1 97.036133   Top5 99.990234   BatchTime 0.119058   LR 0.000010
INFO - Training [70][  180/  391]   Loss 0.083645   Top1 97.109375   Top5 99.991319   BatchTime 0.119679   LR 0.000010
INFO - Training [70][  200/  391]   Loss 0.082784   Top1 97.144531   Top5 99.992188   BatchTime 0.120149   LR 0.000010
INFO - Training [70][  220/  391]   Loss 0.083120   Top1 97.112926   Top5 99.992898   BatchTime 0.120490   LR 0.000010
INFO - Training [70][  240/  391]   Loss 0.082949   Top1 97.089844   Top5 99.993490   BatchTime 0.120817   LR 0.000010
INFO - Training [70][  260/  391]   Loss 0.083340   Top1 97.058293   Top5 99.993990   BatchTime 0.121057   LR 0.000010
INFO - Training [70][  280/  391]   Loss 0.082593   Top1 97.075893   Top5 99.994420   BatchTime 0.121229   LR 0.000010
INFO - Training [70][  300/  391]   Loss 0.082230   Top1 97.080729   Top5 99.994792   BatchTime 0.121447   LR 0.000010
INFO - Training [70][  320/  391]   Loss 0.082767   Top1 97.077637   Top5 99.990234   BatchTime 0.119163   LR 0.000010
INFO - Training [70][  340/  391]   Loss 0.082483   Top1 97.081801   Top5 99.988511   BatchTime 0.117395   LR 0.000010
INFO - Training [70][  360/  391]   Loss 0.082224   Top1 97.105035   Top5 99.986979   BatchTime 0.115828   LR 0.000010
INFO - Training [70][  380/  391]   Loss 0.082136   Top1 97.111431   Top5 99.987664   BatchTime 0.114322   LR 0.000010
INFO - ==> Top1: 97.100    Top5: 99.988    Loss: 0.082
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [70][   20/   79]   Loss 0.411574   Top1 89.609375   Top5 99.531250   BatchTime 0.154163
INFO - Validation [70][   40/   79]   Loss 0.401439   Top1 89.609375   Top5 99.550781   BatchTime 0.108754
INFO - Validation [70][   60/   79]   Loss 0.391523   Top1 89.817708   Top5 99.583333   BatchTime 0.093912
INFO - ==> Top1: 89.740    Top5: 99.600    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  71
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [71][   20/  391]   Loss 0.076016   Top1 96.992188   Top5 100.000000   BatchTime 0.222020   LR 0.000010
INFO - Training [71][   40/  391]   Loss 0.080509   Top1 97.031250   Top5 99.980469   BatchTime 0.173134   LR 0.000010
INFO - Training [71][   60/  391]   Loss 0.080456   Top1 97.135417   Top5 99.973958   BatchTime 0.156887   LR 0.000010
INFO - Training [71][   80/  391]   Loss 0.079209   Top1 97.148438   Top5 99.980469   BatchTime 0.148545   LR 0.000010
INFO - Training [71][  100/  391]   Loss 0.080395   Top1 97.093750   Top5 99.984375   BatchTime 0.138944   LR 0.000010
INFO - Training [71][  120/  391]   Loss 0.082391   Top1 97.050781   Top5 99.973958   BatchTime 0.129870   LR 0.000010
INFO - Training [71][  140/  391]   Loss 0.082677   Top1 97.047991   Top5 99.977679   BatchTime 0.124790   LR 0.000010
INFO - Training [71][  160/  391]   Loss 0.083664   Top1 97.006836   Top5 99.980469   BatchTime 0.118744   LR 0.000010
INFO - Training [71][  180/  391]   Loss 0.082813   Top1 97.048611   Top5 99.982639   BatchTime 0.119125   LR 0.000010
INFO - Training [71][  200/  391]   Loss 0.082136   Top1 97.082031   Top5 99.984375   BatchTime 0.119639   LR 0.000010
INFO - Training [71][  220/  391]   Loss 0.082053   Top1 97.077415   Top5 99.985795   BatchTime 0.120084   LR 0.000010
INFO - Training [71][  240/  391]   Loss 0.083310   Top1 97.060547   Top5 99.983724   BatchTime 0.120482   LR 0.000010
INFO - Training [71][  260/  391]   Loss 0.082383   Top1 97.097356   Top5 99.984976   BatchTime 0.120857   LR 0.000010
INFO - Training [71][  280/  391]   Loss 0.081812   Top1 97.128906   Top5 99.986049   BatchTime 0.121058   LR 0.000010
INFO - Training [71][  300/  391]   Loss 0.081745   Top1 97.125000   Top5 99.986979   BatchTime 0.121264   LR 0.000010
INFO - Training [71][  320/  391]   Loss 0.081612   Top1 97.124023   Top5 99.987793   BatchTime 0.121406   LR 0.000010
INFO - Training [71][  340/  391]   Loss 0.081505   Top1 97.127757   Top5 99.988511   BatchTime 0.121506   LR 0.000010
INFO - Training [71][  360/  391]   Loss 0.081823   Top1 97.135417   Top5 99.986979   BatchTime 0.121587   LR 0.000010
INFO - Training [71][  380/  391]   Loss 0.081295   Top1 97.160773   Top5 99.987664   BatchTime 0.119274   LR 0.000010
INFO - ==> Top1: 97.152    Top5: 99.988    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [71][   20/   79]   Loss 0.412221   Top1 89.609375   Top5 99.570312   BatchTime 0.127521
INFO - Validation [71][   40/   79]   Loss 0.399800   Top1 89.785156   Top5 99.589844   BatchTime 0.079545
INFO - Validation [71][   60/   79]   Loss 0.389675   Top1 89.882812   Top5 99.583333   BatchTime 0.074071
INFO - ==> Top1: 89.770    Top5: 99.620    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  72
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [72][   20/  391]   Loss 0.084030   Top1 97.031250   Top5 100.000000   BatchTime 0.216139   LR 0.000010
INFO - Training [72][   40/  391]   Loss 0.084545   Top1 96.953125   Top5 100.000000   BatchTime 0.170694   LR 0.000010
INFO - Training [72][   60/  391]   Loss 0.085195   Top1 96.992188   Top5 99.986979   BatchTime 0.154978   LR 0.000010
INFO - Training [72][   80/  391]   Loss 0.084617   Top1 96.972656   Top5 99.980469   BatchTime 0.147596   LR 0.000010
INFO - Training [72][  100/  391]   Loss 0.081551   Top1 97.054688   Top5 99.984375   BatchTime 0.143091   LR 0.000010
INFO - Training [72][  120/  391]   Loss 0.080623   Top1 97.089844   Top5 99.986979   BatchTime 0.139981   LR 0.000010
INFO - Training [72][  140/  391]   Loss 0.080662   Top1 97.064732   Top5 99.983259   BatchTime 0.137683   LR 0.000010
INFO - Training [72][  160/  391]   Loss 0.079970   Top1 97.099609   Top5 99.985352   BatchTime 0.133395   LR 0.000010
INFO - Training [72][  180/  391]   Loss 0.081903   Top1 97.057292   Top5 99.986979   BatchTime 0.128060   LR 0.000010
INFO - Training [72][  200/  391]   Loss 0.081461   Top1 97.082031   Top5 99.988281   BatchTime 0.124457   LR 0.000010
INFO - Training [72][  220/  391]   Loss 0.081734   Top1 97.066761   Top5 99.989347   BatchTime 0.119814   LR 0.000010
INFO - Training [72][  240/  391]   Loss 0.081808   Top1 97.070312   Top5 99.983724   BatchTime 0.119963   LR 0.000010
INFO - Training [72][  260/  391]   Loss 0.081461   Top1 97.079327   Top5 99.984976   BatchTime 0.120343   LR 0.000010
INFO - Training [72][  280/  391]   Loss 0.081401   Top1 97.078683   Top5 99.986049   BatchTime 0.120625   LR 0.000010
INFO - Training [72][  300/  391]   Loss 0.081462   Top1 97.091146   Top5 99.986979   BatchTime 0.120816   LR 0.000010
INFO - Training [72][  320/  391]   Loss 0.082181   Top1 97.092285   Top5 99.985352   BatchTime 0.121049   LR 0.000010
INFO - Training [72][  340/  391]   Loss 0.081898   Top1 97.079504   Top5 99.986213   BatchTime 0.121171   LR 0.000010
INFO - Training [72][  360/  391]   Loss 0.082219   Top1 97.083333   Top5 99.984809   BatchTime 0.121377   LR 0.000010
INFO - Training [72][  380/  391]   Loss 0.082975   Top1 97.060033   Top5 99.985609   BatchTime 0.121440   LR 0.000010
INFO - ==> Top1: 97.056    Top5: 99.986    Loss: 0.083
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [72][   20/   79]   Loss 0.406944   Top1 89.960938   Top5 99.570312   BatchTime 0.141562
INFO - Validation [72][   40/   79]   Loss 0.400457   Top1 89.843750   Top5 99.492188   BatchTime 0.083827
INFO - Validation [72][   60/   79]   Loss 0.392144   Top1 90.026042   Top5 99.544271   BatchTime 0.064694
INFO - ==> Top1: 89.840    Top5: 99.560    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  73
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [73][   20/  391]   Loss 0.083244   Top1 97.070312   Top5 100.000000   BatchTime 0.186928   LR 0.000010
INFO - Training [73][   40/  391]   Loss 0.082953   Top1 96.933594   Top5 100.000000   BatchTime 0.155367   LR 0.000010
INFO - Training [73][   60/  391]   Loss 0.079743   Top1 97.200521   Top5 99.986979   BatchTime 0.144869   LR 0.000010
INFO - Training [73][   80/  391]   Loss 0.080505   Top1 97.158203   Top5 99.990234   BatchTime 0.139745   LR 0.000010
INFO - Training [73][  100/  391]   Loss 0.079658   Top1 97.210938   Top5 99.992188   BatchTime 0.137318   LR 0.000010
INFO - Training [73][  120/  391]   Loss 0.080396   Top1 97.180990   Top5 99.993490   BatchTime 0.135227   LR 0.000010
INFO - Training [73][  140/  391]   Loss 0.080304   Top1 97.209821   Top5 99.994420   BatchTime 0.133627   LR 0.000010
INFO - Training [73][  160/  391]   Loss 0.080476   Top1 97.187500   Top5 99.995117   BatchTime 0.132158   LR 0.000010
INFO - Training [73][  180/  391]   Loss 0.081620   Top1 97.105035   Top5 99.995660   BatchTime 0.131169   LR 0.000010
INFO - Training [73][  200/  391]   Loss 0.080896   Top1 97.132812   Top5 99.996094   BatchTime 0.130421   LR 0.000010
INFO - Training [73][  220/  391]   Loss 0.081146   Top1 97.141335   Top5 99.996449   BatchTime 0.129312   LR 0.000010
INFO - Training [73][  240/  391]   Loss 0.080876   Top1 97.145182   Top5 99.996745   BatchTime 0.125112   LR 0.000010
INFO - Training [73][  260/  391]   Loss 0.081083   Top1 97.154447   Top5 99.993990   BatchTime 0.122404   LR 0.000010
INFO - Training [73][  280/  391]   Loss 0.080682   Top1 97.159598   Top5 99.994420   BatchTime 0.119546   LR 0.000010
INFO - Training [73][  300/  391]   Loss 0.080449   Top1 97.151042   Top5 99.992188   BatchTime 0.118501   LR 0.000010
INFO - Training [73][  320/  391]   Loss 0.080128   Top1 97.170410   Top5 99.990234   BatchTime 0.118935   LR 0.000010
INFO - Training [73][  340/  391]   Loss 0.079780   Top1 97.182904   Top5 99.990809   BatchTime 0.119185   LR 0.000010
INFO - Training [73][  360/  391]   Loss 0.079689   Top1 97.196181   Top5 99.989149   BatchTime 0.119413   LR 0.000010
INFO - Training [73][  380/  391]   Loss 0.079874   Top1 97.187500   Top5 99.989720   BatchTime 0.119656   LR 0.000010
INFO - ==> Top1: 97.174    Top5: 99.988    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [73][   20/   79]   Loss 0.403673   Top1 89.648438   Top5 99.531250   BatchTime 0.154407
INFO - Validation [73][   40/   79]   Loss 0.400976   Top1 89.648438   Top5 99.570312   BatchTime 0.110044
INFO - Validation [73][   60/   79]   Loss 0.393098   Top1 89.986979   Top5 99.596354   BatchTime 0.095339
INFO - ==> Top1: 89.920    Top5: 99.620    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  74
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [74][   20/  391]   Loss 0.094870   Top1 96.875000   Top5 99.960938   BatchTime 0.182782   LR 0.000010
INFO - Training [74][   40/  391]   Loss 0.082515   Top1 97.246094   Top5 99.980469   BatchTime 0.135871   LR 0.000010
INFO - Training [74][   60/  391]   Loss 0.083070   Top1 97.200521   Top5 99.986979   BatchTime 0.119406   LR 0.000010
INFO - Training [74][   80/  391]   Loss 0.083649   Top1 97.167969   Top5 99.970703   BatchTime 0.112711   LR 0.000010
INFO - Training [74][  100/  391]   Loss 0.083103   Top1 97.132812   Top5 99.976562   BatchTime 0.115155   LR 0.000010
INFO - Training [74][  120/  391]   Loss 0.085804   Top1 96.972656   Top5 99.967448   BatchTime 0.116541   LR 0.000010
INFO - Training [74][  140/  391]   Loss 0.084693   Top1 97.014509   Top5 99.972098   BatchTime 0.117585   LR 0.000010
INFO - Training [74][  160/  391]   Loss 0.084683   Top1 97.011719   Top5 99.975586   BatchTime 0.118506   LR 0.000010
INFO - Training [74][  180/  391]   Loss 0.084223   Top1 97.035590   Top5 99.978299   BatchTime 0.119142   LR 0.000010
INFO - Training [74][  200/  391]   Loss 0.083687   Top1 97.054688   Top5 99.980469   BatchTime 0.119632   LR 0.000010
INFO - Training [74][  220/  391]   Loss 0.083995   Top1 97.056108   Top5 99.982244   BatchTime 0.119980   LR 0.000010
INFO - Training [74][  240/  391]   Loss 0.084000   Top1 97.044271   Top5 99.980469   BatchTime 0.120281   LR 0.000010
INFO - Training [74][  260/  391]   Loss 0.083869   Top1 97.049279   Top5 99.981971   BatchTime 0.120489   LR 0.000010
INFO - Training [74][  280/  391]   Loss 0.083854   Top1 97.042411   Top5 99.980469   BatchTime 0.119737   LR 0.000010
INFO - Training [74][  300/  391]   Loss 0.084427   Top1 97.010417   Top5 99.979167   BatchTime 0.117307   LR 0.000010
INFO - Training [74][  320/  391]   Loss 0.084382   Top1 97.016602   Top5 99.975586   BatchTime 0.115775   LR 0.000010
INFO - Training [74][  340/  391]   Loss 0.084296   Top1 97.012868   Top5 99.974724   BatchTime 0.113650   LR 0.000010
INFO - Training [74][  360/  391]   Loss 0.083992   Top1 97.013889   Top5 99.976128   BatchTime 0.113650   LR 0.000010
INFO - Training [74][  380/  391]   Loss 0.084382   Top1 97.004523   Top5 99.977385   BatchTime 0.114164   LR 0.000010
INFO - ==> Top1: 97.006    Top5: 99.978    Loss: 0.085
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [74][   20/   79]   Loss 0.411191   Top1 89.687500   Top5 99.531250   BatchTime 0.154875
INFO - Validation [74][   40/   79]   Loss 0.400808   Top1 89.843750   Top5 99.609375   BatchTime 0.109697
INFO - Validation [74][   60/   79]   Loss 0.395595   Top1 89.947917   Top5 99.622396   BatchTime 0.094128
INFO - ==> Top1: 89.750    Top5: 99.640    Loss: 0.388
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  75
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [75][   20/  391]   Loss 0.079173   Top1 97.343750   Top5 100.000000   BatchTime 0.215038   LR 0.000010
INFO - Training [75][   40/  391]   Loss 0.085137   Top1 97.128906   Top5 100.000000   BatchTime 0.169010   LR 0.000010
INFO - Training [75][   60/  391]   Loss 0.083107   Top1 97.122396   Top5 99.986979   BatchTime 0.152145   LR 0.000010
INFO - Training [75][   80/  391]   Loss 0.080153   Top1 97.187500   Top5 99.980469   BatchTime 0.134130   LR 0.000010
INFO - Training [75][  100/  391]   Loss 0.082203   Top1 97.101562   Top5 99.984375   BatchTime 0.125106   LR 0.000010
INFO - Training [75][  120/  391]   Loss 0.081661   Top1 97.076823   Top5 99.980469   BatchTime 0.118465   LR 0.000010
INFO - Training [75][  140/  391]   Loss 0.082430   Top1 97.059152   Top5 99.983259   BatchTime 0.116579   LR 0.000010
INFO - Training [75][  160/  391]   Loss 0.082954   Top1 97.080078   Top5 99.985352   BatchTime 0.118237   LR 0.000010
INFO - Training [75][  180/  391]   Loss 0.083318   Top1 97.074653   Top5 99.978299   BatchTime 0.118796   LR 0.000010
INFO - Training [75][  200/  391]   Loss 0.083577   Top1 97.062500   Top5 99.980469   BatchTime 0.119313   LR 0.000010
INFO - Training [75][  220/  391]   Loss 0.082470   Top1 97.088068   Top5 99.982244   BatchTime 0.119727   LR 0.000010
INFO - Training [75][  240/  391]   Loss 0.082492   Top1 97.086589   Top5 99.983724   BatchTime 0.120084   LR 0.000010
INFO - Training [75][  260/  391]   Loss 0.082565   Top1 97.091346   Top5 99.984976   BatchTime 0.120416   LR 0.000010
INFO - Training [75][  280/  391]   Loss 0.082108   Top1 97.081473   Top5 99.986049   BatchTime 0.120654   LR 0.000010
INFO - Training [75][  300/  391]   Loss 0.082154   Top1 97.059896   Top5 99.986979   BatchTime 0.120449   LR 0.000010
INFO - Training [75][  320/  391]   Loss 0.082783   Top1 97.009277   Top5 99.987793   BatchTime 0.120599   LR 0.000010
INFO - Training [75][  340/  391]   Loss 0.083207   Top1 97.022059   Top5 99.986213   BatchTime 0.119334   LR 0.000010
INFO - Training [75][  360/  391]   Loss 0.083883   Top1 96.983507   Top5 99.986979   BatchTime 0.117265   LR 0.000010
INFO - Training [75][  380/  391]   Loss 0.084034   Top1 96.963405   Top5 99.987664   BatchTime 0.115998   LR 0.000010
INFO - ==> Top1: 96.968    Top5: 99.988    Loss: 0.084
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [75][   20/   79]   Loss 0.413476   Top1 89.648438   Top5 99.531250   BatchTime 0.164381
INFO - Validation [75][   40/   79]   Loss 0.405262   Top1 89.570312   Top5 99.511719   BatchTime 0.115106
INFO - Validation [75][   60/   79]   Loss 0.397134   Top1 89.674479   Top5 99.570312   BatchTime 0.098709
INFO - ==> Top1: 89.580    Top5: 99.610    Loss: 0.388
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  76
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [76][   20/  391]   Loss 0.083504   Top1 96.992188   Top5 100.000000   BatchTime 0.215020   LR 0.000010
INFO - Training [76][   40/  391]   Loss 0.079211   Top1 97.128906   Top5 100.000000   BatchTime 0.169880   LR 0.000010
INFO - Training [76][   60/  391]   Loss 0.077435   Top1 97.278646   Top5 100.000000   BatchTime 0.154192   LR 0.000010
INFO - Training [76][   80/  391]   Loss 0.075582   Top1 97.324219   Top5 100.000000   BatchTime 0.146582   LR 0.000010
INFO - Training [76][  100/  391]   Loss 0.075839   Top1 97.265625   Top5 100.000000   BatchTime 0.141842   LR 0.000010
INFO - Training [76][  120/  391]   Loss 0.075056   Top1 97.311198   Top5 100.000000   BatchTime 0.135392   LR 0.000010
INFO - Training [76][  140/  391]   Loss 0.076316   Top1 97.321429   Top5 99.994420   BatchTime 0.128000   LR 0.000010
INFO - Training [76][  160/  391]   Loss 0.078239   Top1 97.216797   Top5 99.995117   BatchTime 0.123096   LR 0.000010
INFO - Training [76][  180/  391]   Loss 0.078382   Top1 97.213542   Top5 99.991319   BatchTime 0.118723   LR 0.000010
INFO - Training [76][  200/  391]   Loss 0.079307   Top1 97.156250   Top5 99.992188   BatchTime 0.118091   LR 0.000010
INFO - Training [76][  220/  391]   Loss 0.079650   Top1 97.144886   Top5 99.992898   BatchTime 0.118695   LR 0.000010
INFO - Training [76][  240/  391]   Loss 0.079005   Top1 97.161458   Top5 99.986979   BatchTime 0.119126   LR 0.000010
INFO - Training [76][  260/  391]   Loss 0.080339   Top1 97.112380   Top5 99.984976   BatchTime 0.119509   LR 0.000010
INFO - Training [76][  280/  391]   Loss 0.080019   Top1 97.131696   Top5 99.983259   BatchTime 0.119890   LR 0.000010
INFO - Training [76][  300/  391]   Loss 0.079566   Top1 97.158854   Top5 99.984375   BatchTime 0.120167   LR 0.000010
INFO - Training [76][  320/  391]   Loss 0.079449   Top1 97.158203   Top5 99.982910   BatchTime 0.120396   LR 0.000010
INFO - Training [76][  340/  391]   Loss 0.079551   Top1 97.159926   Top5 99.981618   BatchTime 0.120544   LR 0.000010
INFO - Training [76][  360/  391]   Loss 0.079885   Top1 97.154948   Top5 99.982639   BatchTime 0.120683   LR 0.000010
INFO - Training [76][  380/  391]   Loss 0.080125   Top1 97.140214   Top5 99.983553   BatchTime 0.120801   LR 0.000010
INFO - ==> Top1: 97.136    Top5: 99.984    Loss: 0.080
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [76][   20/   79]   Loss 0.401423   Top1 89.843750   Top5 99.570312   BatchTime 0.136298
INFO - Validation [76][   40/   79]   Loss 0.394330   Top1 89.863281   Top5 99.589844   BatchTime 0.086236
INFO - Validation [76][   60/   79]   Loss 0.388743   Top1 90.078125   Top5 99.596354   BatchTime 0.066094
INFO - ==> Top1: 89.970    Top5: 99.620    Loss: 0.380
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  77
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [77][   20/  391]   Loss 0.076117   Top1 96.914062   Top5 100.000000   BatchTime 0.226812   LR 0.000010
INFO - Training [77][   40/  391]   Loss 0.082097   Top1 96.718750   Top5 99.980469   BatchTime 0.175244   LR 0.000010
INFO - Training [77][   60/  391]   Loss 0.076864   Top1 97.070312   Top5 99.986979   BatchTime 0.158265   LR 0.000010
INFO - Training [77][   80/  391]   Loss 0.080297   Top1 97.021484   Top5 99.990234   BatchTime 0.149733   LR 0.000010
INFO - Training [77][  100/  391]   Loss 0.082613   Top1 97.007812   Top5 99.984375   BatchTime 0.144434   LR 0.000010
INFO - Training [77][  120/  391]   Loss 0.083266   Top1 96.933594   Top5 99.986979   BatchTime 0.141028   LR 0.000010
INFO - Training [77][  140/  391]   Loss 0.081131   Top1 97.047991   Top5 99.983259   BatchTime 0.138582   LR 0.000010
INFO - Training [77][  160/  391]   Loss 0.082060   Top1 97.055664   Top5 99.985352   BatchTime 0.136681   LR 0.000010
INFO - Training [77][  180/  391]   Loss 0.082253   Top1 97.074653   Top5 99.978299   BatchTime 0.135106   LR 0.000010
INFO - Training [77][  200/  391]   Loss 0.081357   Top1 97.117188   Top5 99.976562   BatchTime 0.129540   LR 0.000010
INFO - Training [77][  220/  391]   Loss 0.081214   Top1 97.151989   Top5 99.978693   BatchTime 0.125949   LR 0.000010
INFO - Training [77][  240/  391]   Loss 0.081720   Top1 97.132161   Top5 99.977214   BatchTime 0.123224   LR 0.000010
INFO - Training [77][  260/  391]   Loss 0.081776   Top1 97.106370   Top5 99.978966   BatchTime 0.121781   LR 0.000010
INFO - Training [77][  280/  391]   Loss 0.081654   Top1 97.112165   Top5 99.977679   BatchTime 0.121973   LR 0.000010
INFO - Training [77][  300/  391]   Loss 0.081868   Top1 97.101562   Top5 99.976562   BatchTime 0.122072   LR 0.000010
INFO - Training [77][  320/  391]   Loss 0.082416   Top1 97.087402   Top5 99.975586   BatchTime 0.122225   LR 0.000010
INFO - Training [77][  340/  391]   Loss 0.082799   Top1 97.058824   Top5 99.977022   BatchTime 0.122287   LR 0.000010
INFO - Training [77][  360/  391]   Loss 0.083093   Top1 97.039931   Top5 99.978299   BatchTime 0.122380   LR 0.000010
INFO - Training [77][  380/  391]   Loss 0.083094   Top1 97.027138   Top5 99.979441   BatchTime 0.122403   LR 0.000010
INFO - ==> Top1: 97.018    Top5: 99.980    Loss: 0.083
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [77][   20/   79]   Loss 0.405994   Top1 89.570312   Top5 99.453125   BatchTime 0.151626
INFO - Validation [77][   40/   79]   Loss 0.402932   Top1 89.609375   Top5 99.492188   BatchTime 0.105974
INFO - Validation [77][   60/   79]   Loss 0.396574   Top1 89.791667   Top5 99.518229   BatchTime 0.091834
INFO - ==> Top1: 89.690    Top5: 99.570    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  78
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [78][   20/  391]   Loss 0.070390   Top1 97.500000   Top5 100.000000   BatchTime 0.189312   LR 0.000010
INFO - Training [78][   40/  391]   Loss 0.076580   Top1 97.324219   Top5 100.000000   BatchTime 0.135691   LR 0.000010
INFO - Training [78][   60/  391]   Loss 0.080860   Top1 97.122396   Top5 100.000000   BatchTime 0.131792   LR 0.000010
INFO - Training [78][   80/  391]   Loss 0.081214   Top1 97.041016   Top5 100.000000   BatchTime 0.130027   LR 0.000010
INFO - Training [78][  100/  391]   Loss 0.079462   Top1 97.156250   Top5 100.000000   BatchTime 0.128797   LR 0.000010
INFO - Training [78][  120/  391]   Loss 0.080510   Top1 97.187500   Top5 99.993490   BatchTime 0.127973   LR 0.000010
INFO - Training [78][  140/  391]   Loss 0.081474   Top1 97.103795   Top5 99.994420   BatchTime 0.127428   LR 0.000010
INFO - Training [78][  160/  391]   Loss 0.082047   Top1 97.089844   Top5 99.995117   BatchTime 0.127011   LR 0.000010
INFO - Training [78][  180/  391]   Loss 0.081915   Top1 97.100694   Top5 99.995660   BatchTime 0.126694   LR 0.000010
INFO - Training [78][  200/  391]   Loss 0.081670   Top1 97.117188   Top5 99.996094   BatchTime 0.126318   LR 0.000010
INFO - Training [78][  220/  391]   Loss 0.082403   Top1 97.073864   Top5 99.996449   BatchTime 0.126090   LR 0.000010
INFO - Training [78][  240/  391]   Loss 0.082324   Top1 97.067057   Top5 99.996745   BatchTime 0.125572   LR 0.000010
INFO - Training [78][  260/  391]   Loss 0.082505   Top1 97.058293   Top5 99.996995   BatchTime 0.122014   LR 0.000010
INFO - Training [78][  280/  391]   Loss 0.082296   Top1 97.059152   Top5 99.997210   BatchTime 0.119808   LR 0.000010
INFO - Training [78][  300/  391]   Loss 0.083027   Top1 97.033854   Top5 99.994792   BatchTime 0.117608   LR 0.000010
INFO - Training [78][  320/  391]   Loss 0.083022   Top1 97.036133   Top5 99.995117   BatchTime 0.117056   LR 0.000010
INFO - Training [78][  340/  391]   Loss 0.083105   Top1 97.026654   Top5 99.995404   BatchTime 0.117460   LR 0.000010
INFO - Training [78][  360/  391]   Loss 0.083780   Top1 97.024740   Top5 99.993490   BatchTime 0.117800   LR 0.000010
INFO - Training [78][  380/  391]   Loss 0.083481   Top1 97.045641   Top5 99.991776   BatchTime 0.118098   LR 0.000010
INFO - ==> Top1: 97.038    Top5: 99.992    Loss: 0.084
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [78][   20/   79]   Loss 0.403743   Top1 89.726562   Top5 99.492188   BatchTime 0.154210
INFO - Validation [78][   40/   79]   Loss 0.398660   Top1 89.804688   Top5 99.570312   BatchTime 0.108514
INFO - Validation [78][   60/   79]   Loss 0.393921   Top1 89.973958   Top5 99.583333   BatchTime 0.093758
INFO - ==> Top1: 89.810    Top5: 99.630    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  79
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [79][   20/  391]   Loss 0.084740   Top1 97.304688   Top5 100.000000   BatchTime 0.216613   LR 0.000010
INFO - Training [79][   40/  391]   Loss 0.087967   Top1 97.089844   Top5 99.980469   BatchTime 0.146377   LR 0.000010
INFO - Training [79][   60/  391]   Loss 0.088797   Top1 97.044271   Top5 99.986979   BatchTime 0.128381   LR 0.000010
INFO - Training [79][   80/  391]   Loss 0.082944   Top1 97.216797   Top5 99.990234   BatchTime 0.117627   LR 0.000010
INFO - Training [79][  100/  391]   Loss 0.083611   Top1 97.117188   Top5 99.992188   BatchTime 0.113717   LR 0.000010
INFO - Training [79][  120/  391]   Loss 0.082677   Top1 97.109375   Top5 99.993490   BatchTime 0.115506   LR 0.000010
INFO - Training [79][  140/  391]   Loss 0.083289   Top1 97.092634   Top5 99.988839   BatchTime 0.116770   LR 0.000010
INFO - Training [79][  160/  391]   Loss 0.082961   Top1 97.114258   Top5 99.990234   BatchTime 0.117668   LR 0.000010
INFO - Training [79][  180/  391]   Loss 0.084604   Top1 97.052951   Top5 99.991319   BatchTime 0.118385   LR 0.000010
INFO - Training [79][  200/  391]   Loss 0.083143   Top1 97.078125   Top5 99.992188   BatchTime 0.119003   LR 0.000010
INFO - Training [79][  220/  391]   Loss 0.084437   Top1 97.020597   Top5 99.992898   BatchTime 0.119508   LR 0.000010
INFO - Training [79][  240/  391]   Loss 0.084933   Top1 96.998698   Top5 99.990234   BatchTime 0.119828   LR 0.000010
INFO - Training [79][  260/  391]   Loss 0.085388   Top1 96.986178   Top5 99.990986   BatchTime 0.120130   LR 0.000010
INFO - Training [79][  280/  391]   Loss 0.085795   Top1 96.969866   Top5 99.991629   BatchTime 0.120390   LR 0.000010
INFO - Training [79][  300/  391]   Loss 0.085831   Top1 96.950521   Top5 99.989583   BatchTime 0.119499   LR 0.000010
INFO - Training [79][  320/  391]   Loss 0.085209   Top1 96.965332   Top5 99.987793   BatchTime 0.117200   LR 0.000010
INFO - Training [79][  340/  391]   Loss 0.085046   Top1 96.973805   Top5 99.988511   BatchTime 0.115583   LR 0.000010
INFO - Training [79][  360/  391]   Loss 0.085462   Top1 96.972656   Top5 99.986979   BatchTime 0.113328   LR 0.000010
INFO - Training [79][  380/  391]   Loss 0.085245   Top1 96.981908   Top5 99.985609   BatchTime 0.113536   LR 0.000010
INFO - ==> Top1: 96.982    Top5: 99.986    Loss: 0.085
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [79][   20/   79]   Loss 0.415541   Top1 89.414062   Top5 99.531250   BatchTime 0.153645
INFO - Validation [79][   40/   79]   Loss 0.402268   Top1 89.667969   Top5 99.511719   BatchTime 0.108280
INFO - Validation [79][   60/   79]   Loss 0.393444   Top1 89.739583   Top5 99.544271   BatchTime 0.094099
INFO - ==> Top1: 89.610    Top5: 99.610    Loss: 0.385
INFO - Scoreboard best 1 ==> Epoch [52][Top1: 90.590   Top5: 99.670] Sparsity : 0.854
INFO - Scoreboard best 2 ==> Epoch [49][Top1: 90.590   Top5: 99.650] Sparsity : 0.852
INFO - Scoreboard best 3 ==> Epoch [50][Top1: 90.570   Top5: 99.690] Sparsity : 0.852
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_15_epoch80_20221103-224102/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - >>>>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 0.415541   Top1 89.414062   Top5 99.531250   BatchTime 0.153967
INFO - Validation [   40/   79]   Loss 0.402268   Top1 89.667969   Top5 99.511719   BatchTime 0.109472
INFO - Validation [   60/   79]   Loss 0.393444   Top1 89.739583   Top5 99.544271   BatchTime 0.094713
INFO - ==> Top1: 89.610    Top5: 99.610    Loss: 0.385
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/pruned_model/MobileNetv2_cifar10_a8w8_15_epoch80_checkpoint.pth.tar
INFO - Program completed successfully ... exiting ...
INFO - If you have any questions or suggestions, please visit: github.com/zhutmost/lsq-net