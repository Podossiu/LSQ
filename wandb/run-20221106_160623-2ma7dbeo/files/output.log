INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_imagenet_a8w8_30_epoch80_20221106-160624/MobileNetv2_imagenet_a8w8_30_epoch80_20221106-160624.log
2022-11-06 16:06:24.248161: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-06 16:06:24.370644: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-06 16:06:24.744370: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-06 16:06:24.744415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-06 16:06:24.744421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_imagenet_a8w8_30_epoch80_20221106-160624/tb_runs
********************pre-trained*****************
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
INFO - Created `MobileNetv2` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:128: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               initial_lr: 0.05
               lr: 0.05
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: <torch.optim.lr_scheduler.CosineAnnealingWarmRestarts object at 0x7f21605e0fd0>
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): QuanConv2d(
          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): IdentityQuan()
          (quan_a_fn): IdentityQuan()
        )
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv): Sequential(
      (0): QuanConv2d(
        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
        (quan_w_fn): SLsqQuan()
        (quan_a_fn): LsqQuan()
      )
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): QuanLinear(
      in_features=1280, out_features=1000, bias=True
      (quan_w_fn): IdentityQuan()
      (quan_a_fn): IdentityQuan()
    )
  )
)
Epoch 00000: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Validation [   20/  391]   Loss 27.609890   Top1 0.000000   Top5 0.117188   BatchTime 0.618808
INFO - Validation [   40/  391]   Loss 28.591004   Top1 0.000000   Top5 0.058594   BatchTime 0.397531
INFO - Validation [   60/  391]   Loss 27.567062   Top1 0.000000   Top5 0.403646   BatchTime 0.324291
INFO - Validation [   80/  391]   Loss 24.924523   Top1 0.000000   Top5 0.419922   BatchTime 0.287487
INFO - Validation [  100/  391]   Loss 23.307600   Top1 0.000000   Top5 0.335938   BatchTime 0.265279
INFO - Validation [  120/  391]   Loss 23.457049   Top1 0.312500   Top5 0.657552   BatchTime 0.250520
INFO - Validation [  140/  391]   Loss 24.179314   Top1 0.267857   Top5 0.563616   BatchTime 0.240092
INFO - Validation [  160/  391]   Loss 24.201449   Top1 0.234375   Top5 0.493164   BatchTime 0.232220
INFO - Validation [  180/  391]   Loss 24.038137   Top1 0.208333   Top5 0.438368   BatchTime 0.226156
INFO - Validation [  200/  391]   Loss 24.115841   Top1 0.187500   Top5 0.394531   BatchTime 0.221282
INFO - Validation [  220/  391]   Loss 24.233748   Top1 0.184659   Top5 0.529119   BatchTime 0.217403
INFO - Validation [  240/  391]   Loss 24.254343   Top1 0.169271   Top5 0.488281   BatchTime 0.213975
INFO - Validation [  260/  391]   Loss 24.264850   Top1 0.156250   Top5 0.450721   BatchTime 0.211303
INFO - Validation [  280/  391]   Loss 24.328493   Top1 0.145089   Top5 0.502232   BatchTime 0.208866
INFO - Validation [  300/  391]   Loss 24.268197   Top1 0.135417   Top5 0.611979   BatchTime 0.206761
INFO - Validation [  320/  391]   Loss 24.314753   Top1 0.126953   Top5 0.573730   BatchTime 0.204911
INFO - Validation [  340/  391]   Loss 24.194326   Top1 0.119485   Top5 0.562960   BatchTime 0.202908
INFO - Validation [  360/  391]   Loss 24.101134   Top1 0.112847   Top5 0.531684   BatchTime 0.200897
INFO - Validation [  380/  391]   Loss 24.285498   Top1 0.106908   Top5 0.503701   BatchTime 0.199091
INFO - ==> Top1: 0.104    Top5: 0.490    Loss: 24.380
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.104   Top5: 0.490] Sparsity : 0.060
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][   20/10010]   Loss 7.282508   Top1 1.054688   Top5 3.750000   BatchTime 0.736554   LR 0.050000
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][   40/10010]   Loss 7.104260   Top1 0.683594   Top5 2.910156   BatchTime 0.590837   LR 0.050000
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.00: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][   60/10010]   Loss 6.975539   Top1 0.611979   Top5 2.591146   BatchTime 0.542375   LR 0.050000
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][   80/10010]   Loss 6.866246   Top1 0.664062   Top5 2.656250   BatchTime 0.518199   LR 0.050000
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  100/10010]   Loss 6.778104   Top1 0.679688   Top5 2.812500   BatchTime 0.503528   LR 0.050000
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  120/10010]   Loss 6.707448   Top1 0.807292   Top5 3.177083   BatchTime 0.493806   LR 0.050000
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  140/10010]   Loss 6.646622   Top1 0.859375   Top5 3.404018   BatchTime 0.486898   LR 0.050000
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.01: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  160/10010]   Loss 6.594379   Top1 0.947266   Top5 3.671875   BatchTime 0.481696   LR 0.050000
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  180/10010]   Loss 6.551073   Top1 1.050347   Top5 3.953993   BatchTime 0.478825   LR 0.050000
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  200/10010]   Loss 6.511810   Top1 1.113281   Top5 4.207031   BatchTime 0.475513   LR 0.050000
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  220/10010]   Loss 6.477096   Top1 1.164773   Top5 4.410511   BatchTime 0.472837   LR 0.050000
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  240/10010]   Loss 6.440491   Top1 1.233724   Top5 4.661458   BatchTime 0.470695   LR 0.050000
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.02: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  260/10010]   Loss 6.414406   Top1 1.286058   Top5 4.810697   BatchTime 0.468989   LR 0.050000
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  280/10010]   Loss 6.388227   Top1 1.297433   Top5 4.952567   BatchTime 0.467329   LR 0.050000
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  300/10010]   Loss 6.357495   Top1 1.356771   Top5 5.187500   BatchTime 0.465908   LR 0.050000
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  320/10010]   Loss 6.327235   Top1 1.430664   Top5 5.427246   BatchTime 0.464660   LR 0.050000
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  340/10010]   Loss 6.298789   Top1 1.530331   Top5 5.749081   BatchTime 0.463667   LR 0.050000
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.03: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  360/10010]   Loss 6.272755   Top1 1.610243   Top5 5.987413   BatchTime 0.462717   LR 0.050000
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  380/10010]   Loss 6.252367   Top1 1.669408   Top5 6.165707   BatchTime 0.461955   LR 0.050000
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
INFO - Training [0][  400/10010]   Loss 6.230449   Top1 1.748047   Top5 6.394531   BatchTime 0.461175   LR 0.050000
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0000e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  420/10010]   Loss 6.206876   Top1 1.813616   Top5 6.670387   BatchTime 0.460483   LR 0.050001
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  440/10010]   Loss 6.187065   Top1 1.887429   Top5 6.853693   BatchTime 0.459824   LR 0.050001
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.04: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  460/10010]   Loss 6.167913   Top1 1.937840   Top5 7.019361   BatchTime 0.459241   LR 0.050001
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  480/10010]   Loss 6.150301   Top1 1.998698   Top5 7.224935   BatchTime 0.458679   LR 0.050001
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  500/10010]   Loss 6.132817   Top1 2.071875   Top5 7.425000   BatchTime 0.458210   LR 0.050001
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  520/10010]   Loss 6.114248   Top1 2.160457   Top5 7.651743   BatchTime 0.457784   LR 0.050001
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  540/10010]   Loss 6.095819   Top1 2.233796   Top5 7.861690   BatchTime 0.457359   LR 0.050001
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.05: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  560/10010]   Loss 6.080513   Top1 2.280971   Top5 8.010603   BatchTime 0.457036   LR 0.050001
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
INFO - Training [0][  580/10010]   Loss 6.063968   Top1 2.359914   Top5 8.221983   BatchTime 0.456707   LR 0.050001
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
Epoch 0.06: adjusting learning rate of group 0 to 5.0001e-02.
