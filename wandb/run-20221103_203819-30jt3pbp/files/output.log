
Files already downloaded and verified
INFO - Log file for this run: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820.log
2022-11-03 20:38:20.646514: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-03 20:38:20.764988: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-11-03 20:38:21.136752: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-11-03 20:38:21.136798: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-11-03 20:38:21.136803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
INFO - TensorBoard data directory: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/tb_runs
Files already downloaded and verified
hello
********************pre-trained*****************
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
INFO - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
INFO - Created `MobileNetv2` model for `cifar10` dataset
          Use pre-trained model = True
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/slsq/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
DataParallel(
  (module): MobileNetV2(
    (features): Sequential(
      (0): Sequential(
        (0): QuanConv2d(
          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): IdentityQuan()
          (quan_a_fn): IdentityQuan()
        )
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): InvertedResidual(
        (conv): Sequential(
          (0): QuanConv2d(
            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): QuanConv2d(
            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): QuanConv2d(
            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
            (quan_w_fn): SLsqQuan()
            (quan_a_fn): LsqQuan()
          )
          (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (conv): Sequential(
      (0): QuanConv2d(
        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
        (quan_w_fn): SLsqQuan()
        (quan_a_fn): LsqQuan()
      )
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (classifier): QuanLinear(
      in_features=1280, out_features=10, bias=True
      (quan_w_fn): IdentityQuan()
      (quan_a_fn): IdentityQuan()
    )
  )
)
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 2.545371   Top1 10.429688   Top5 49.101562   BatchTime 0.152936
INFO - Validation [   40/   79]   Loss 2.549466   Top1 10.175781   Top5 49.941406   BatchTime 0.088738
INFO - Validation [   60/   79]   Loss 2.541519   Top1 10.117188   Top5 50.377604   BatchTime 0.067291
INFO - ==> Top1: 10.000    Top5: 50.000    Loss: 2.546
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - >>>>>>>> Epoch   0
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [0][   20/  391]   Loss 1.617645   Top1 67.929688   Top5 96.523438   BatchTime 0.173694   LR 0.010000
INFO - Training [0][   40/  391]   Loss 1.342365   Top1 69.277344   Top5 97.343750   BatchTime 0.122323   LR 0.010000
INFO - Training [0][   60/  391]   Loss 1.154562   Top1 71.119792   Top5 97.682292   BatchTime 0.105074   LR 0.010000
INFO - Training [0][   80/  391]   Loss 1.035326   Top1 72.773438   Top5 97.998047   BatchTime 0.096487   LR 0.010000
INFO - Training [0][  100/  391]   Loss 0.941873   Top1 74.242188   Top5 98.187500   BatchTime 0.091129   LR 0.010000
INFO - Training [0][  120/  391]   Loss 0.868743   Top1 75.572917   Top5 98.417969   BatchTime 0.087466   LR 0.010000
INFO - Training [0][  140/  391]   Loss 0.811275   Top1 76.785714   Top5 98.565848   BatchTime 0.085182   LR 0.010000
INFO - Training [0][  160/  391]   Loss 0.768765   Top1 77.617188   Top5 98.662109   BatchTime 0.083505   LR 0.010000
INFO - Training [0][  180/  391]   Loss 0.731386   Top1 78.424479   Top5 98.754340   BatchTime 0.082214   LR 0.010000
INFO - Training [0][  200/  391]   Loss 0.704414   Top1 78.937500   Top5 98.792969   BatchTime 0.081278   LR 0.010000
INFO - Training [0][  220/  391]   Loss 0.681659   Top1 79.399858   Top5 98.831676   BatchTime 0.080349   LR 0.010000
INFO - Training [0][  240/  391]   Loss 0.658928   Top1 79.921875   Top5 98.893229   BatchTime 0.079505   LR 0.010000
INFO - Training [0][  260/  391]   Loss 0.639124   Top1 80.393630   Top5 98.960337   BatchTime 0.078984   LR 0.010000
INFO - Training [0][  280/  391]   Loss 0.621343   Top1 80.764509   Top5 99.012277   BatchTime 0.078705   LR 0.010000
INFO - Training [0][  300/  391]   Loss 0.606617   Top1 81.057292   Top5 99.049479   BatchTime 0.078544   LR 0.010000
INFO - Training [0][  320/  391]   Loss 0.590840   Top1 81.477051   Top5 99.091797   BatchTime 0.078271   LR 0.010000
INFO - Training [0][  340/  391]   Loss 0.576930   Top1 81.858915   Top5 99.131434   BatchTime 0.077893   LR 0.010000
INFO - Training [0][  360/  391]   Loss 0.563446   Top1 82.211372   Top5 99.164497   BatchTime 0.077743   LR 0.010000
INFO - Training [0][  380/  391]   Loss 0.551706   Top1 82.547286   Top5 99.187911   BatchTime 0.077632   LR 0.010000
INFO - ==> Top1: 82.700    Top5: 99.202    Loss: 0.546
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [0][   20/   79]   Loss 0.454939   Top1 84.726562   Top5 99.257812   BatchTime 0.131615
INFO - Validation [0][   40/   79]   Loss 0.458713   Top1 84.667969   Top5 99.199219   BatchTime 0.082834
INFO - Validation [0][   60/   79]   Loss 0.459039   Top1 84.947917   Top5 99.244792   BatchTime 0.066558
INFO - ==> Top1: 84.870    Top5: 99.290    Loss: 0.461
INFO - Scoreboard best 1 ==> Epoch [0][Top1: 84.870   Top5: 99.290] Sparsity : 0.292
INFO - Scoreboard best 2 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   1
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [1][   20/  391]   Loss 0.293181   Top1 89.531250   Top5 99.843750   BatchTime 0.198231   LR 0.010000
INFO - Training [1][   40/  391]   Loss 0.288222   Top1 89.882812   Top5 99.726562   BatchTime 0.148224   LR 0.010000
INFO - Training [1][   60/  391]   Loss 0.294849   Top1 89.648438   Top5 99.778646   BatchTime 0.131750   LR 0.010000
INFO - Training [1][   80/  391]   Loss 0.295424   Top1 89.462891   Top5 99.746094   BatchTime 0.123157   LR 0.010000
INFO - Training [1][  100/  391]   Loss 0.291575   Top1 89.679688   Top5 99.781250   BatchTime 0.118327   LR 0.010000
INFO - Training [1][  120/  391]   Loss 0.289980   Top1 89.674479   Top5 99.778646   BatchTime 0.114881   LR 0.010000
INFO - Training [1][  140/  391]   Loss 0.286326   Top1 89.754464   Top5 99.787946   BatchTime 0.112609   LR 0.010000
INFO - Training [1][  160/  391]   Loss 0.285892   Top1 89.794922   Top5 99.804688   BatchTime 0.110885   LR 0.010000
INFO - Training [1][  180/  391]   Loss 0.285560   Top1 89.835069   Top5 99.804688   BatchTime 0.109594   LR 0.010000
INFO - Training [1][  200/  391]   Loss 0.285912   Top1 89.855469   Top5 99.792969   BatchTime 0.108510   LR 0.010000
INFO - Training [1][  220/  391]   Loss 0.284330   Top1 89.939631   Top5 99.790483   BatchTime 0.107547   LR 0.010000
INFO - Training [1][  240/  391]   Loss 0.282419   Top1 89.980469   Top5 99.798177   BatchTime 0.106741   LR 0.010000
INFO - Training [1][  260/  391]   Loss 0.282503   Top1 89.990986   Top5 99.798678   BatchTime 0.106117   LR 0.010000
INFO - Training [1][  280/  391]   Loss 0.280001   Top1 90.097656   Top5 99.796317   BatchTime 0.105517   LR 0.010000
INFO - Training [1][  300/  391]   Loss 0.278553   Top1 90.171875   Top5 99.804688   BatchTime 0.105007   LR 0.010000
INFO - Training [1][  320/  391]   Loss 0.276452   Top1 90.253906   Top5 99.802246   BatchTime 0.104610   LR 0.010000
INFO - Training [1][  340/  391]   Loss 0.275485   Top1 90.317096   Top5 99.800092   BatchTime 0.104123   LR 0.010000
INFO - Training [1][  360/  391]   Loss 0.273378   Top1 90.414497   Top5 99.802517   BatchTime 0.103637   LR 0.010000
INFO - Training [1][  380/  391]   Loss 0.271763   Top1 90.493421   Top5 99.806743   BatchTime 0.103049   LR 0.010000
INFO - ==> Top1: 90.548    Top5: 99.812    Loss: 0.270
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [1][   20/   79]   Loss 0.403013   Top1 86.367188   Top5 99.531250   BatchTime 0.138233
INFO - Validation [1][   40/   79]   Loss 0.414152   Top1 86.445312   Top5 99.394531   BatchTime 0.086030
INFO - Validation [1][   60/   79]   Loss 0.410035   Top1 86.718750   Top5 99.427083   BatchTime 0.068660
INFO - ==> Top1: 86.880    Top5: 99.480    Loss: 0.408
INFO - Scoreboard best 1 ==> Epoch [1][Top1: 86.880   Top5: 99.480] Sparsity : 0.550
INFO - Scoreboard best 2 ==> Epoch [0][Top1: 84.870   Top5: 99.290] Sparsity : 0.292
INFO - Scoreboard best 3 ==> Epoch [-1][Top1: 10.000   Top5: 50.000] Sparsity : 0.062
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   2
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [2][   20/  391]   Loss 0.214333   Top1 92.539062   Top5 99.960938   BatchTime 0.198179   LR 0.010000
INFO - Training [2][   40/  391]   Loss 0.219838   Top1 92.285156   Top5 99.902344   BatchTime 0.148095   LR 0.010000
INFO - Training [2][   60/  391]   Loss 0.215578   Top1 92.382812   Top5 99.869792   BatchTime 0.131366   LR 0.010000
INFO - Training [2][   80/  391]   Loss 0.211351   Top1 92.519531   Top5 99.882812   BatchTime 0.122862   LR 0.010000
INFO - Training [2][  100/  391]   Loss 0.211096   Top1 92.531250   Top5 99.882812   BatchTime 0.117945   LR 0.010000
INFO - Training [2][  120/  391]   Loss 0.211632   Top1 92.532552   Top5 99.889323   BatchTime 0.114657   LR 0.010000
INFO - Training [2][  140/  391]   Loss 0.209424   Top1 92.656250   Top5 99.893973   BatchTime 0.112407   LR 0.010000
INFO - Training [2][  160/  391]   Loss 0.209126   Top1 92.675781   Top5 99.887695   BatchTime 0.110606   LR 0.010000
INFO - Training [2][  180/  391]   Loss 0.211690   Top1 92.578125   Top5 99.891493   BatchTime 0.109223   LR 0.010000
INFO - Training [2][  200/  391]   Loss 0.210294   Top1 92.644531   Top5 99.894531   BatchTime 0.108199   LR 0.010000
INFO - Training [2][  220/  391]   Loss 0.211144   Top1 92.606534   Top5 99.900568   BatchTime 0.107301   LR 0.010000
INFO - Training [2][  240/  391]   Loss 0.208945   Top1 92.669271   Top5 99.908854   BatchTime 0.106558   LR 0.010000
INFO - Training [2][  260/  391]   Loss 0.209080   Top1 92.668269   Top5 99.903846   BatchTime 0.105965   LR 0.010000
INFO - Training [2][  280/  391]   Loss 0.207481   Top1 92.745536   Top5 99.905134   BatchTime 0.105453   LR 0.010000
INFO - Training [2][  300/  391]   Loss 0.207153   Top1 92.781250   Top5 99.898438   BatchTime 0.104968   LR 0.010000
INFO - Training [2][  320/  391]   Loss 0.205696   Top1 92.861328   Top5 99.897461   BatchTime 0.104537   LR 0.010000
INFO - Training [2][  340/  391]   Loss 0.204844   Top1 92.874540   Top5 99.894301   BatchTime 0.104114   LR 0.010000
INFO - Training [2][  360/  391]   Loss 0.204291   Top1 92.877604   Top5 99.895833   BatchTime 0.103652   LR 0.010000
INFO - Training [2][  380/  391]   Loss 0.202977   Top1 92.921464   Top5 99.893092   BatchTime 0.103290   LR 0.010000
INFO - ==> Top1: 92.936    Top5: 99.896    Loss: 0.202
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [2][   20/   79]   Loss 0.403929   Top1 87.890625   Top5 99.570312   BatchTime 0.130877
INFO - Validation [2][   40/   79]   Loss 0.406900   Top1 87.812500   Top5 99.414062   BatchTime 0.082394
INFO - Validation [2][   60/   79]   Loss 0.407893   Top1 87.773438   Top5 99.414062   BatchTime 0.066199
INFO - ==> Top1: 87.930    Top5: 99.470    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [2][Top1: 87.930   Top5: 99.470] Sparsity : 0.573
INFO - Scoreboard best 2 ==> Epoch [1][Top1: 86.880   Top5: 99.480] Sparsity : 0.550
INFO - Scoreboard best 3 ==> Epoch [0][Top1: 84.870   Top5: 99.290] Sparsity : 0.292
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   3
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [3][   20/  391]   Loss 0.164901   Top1 94.023438   Top5 99.960938   BatchTime 0.198090   LR 0.010000
INFO - Training [3][   40/  391]   Loss 0.155328   Top1 94.414062   Top5 99.980469   BatchTime 0.148679   LR 0.010000
INFO - Training [3][   60/  391]   Loss 0.161082   Top1 94.140625   Top5 99.986979   BatchTime 0.132326   LR 0.010000
INFO - Training [3][   80/  391]   Loss 0.160035   Top1 94.160156   Top5 99.951172   BatchTime 0.124194   LR 0.010000
INFO - Training [3][  100/  391]   Loss 0.162194   Top1 94.046875   Top5 99.945312   BatchTime 0.119181   LR 0.010000
INFO - Training [3][  120/  391]   Loss 0.161902   Top1 94.153646   Top5 99.954427   BatchTime 0.116013   LR 0.010000
INFO - Training [3][  140/  391]   Loss 0.161602   Top1 94.168527   Top5 99.955357   BatchTime 0.113645   LR 0.010000
INFO - Training [3][  160/  391]   Loss 0.163865   Top1 94.121094   Top5 99.951172   BatchTime 0.111893   LR 0.010000
INFO - Training [3][  180/  391]   Loss 0.165406   Top1 94.023438   Top5 99.952257   BatchTime 0.110604   LR 0.010000
INFO - Training [3][  200/  391]   Loss 0.163955   Top1 94.117188   Top5 99.953125   BatchTime 0.109367   LR 0.010000
INFO - Training [3][  220/  391]   Loss 0.165238   Top1 94.066051   Top5 99.957386   BatchTime 0.108344   LR 0.010000
INFO - Training [3][  240/  391]   Loss 0.164554   Top1 94.098307   Top5 99.960938   BatchTime 0.107631   LR 0.010000
INFO - Training [3][  260/  391]   Loss 0.164997   Top1 94.068510   Top5 99.960938   BatchTime 0.106984   LR 0.010000
INFO - Training [3][  280/  391]   Loss 0.164509   Top1 94.090402   Top5 99.963728   BatchTime 0.106491   LR 0.010000
INFO - Training [3][  300/  391]   Loss 0.164353   Top1 94.111979   Top5 99.963542   BatchTime 0.106018   LR 0.010000
INFO - Training [3][  320/  391]   Loss 0.164303   Top1 94.130859   Top5 99.960938   BatchTime 0.105514   LR 0.010000
INFO - Training [3][  340/  391]   Loss 0.163976   Top1 94.156710   Top5 99.960938   BatchTime 0.105039   LR 0.010000
INFO - Training [3][  360/  391]   Loss 0.164176   Top1 94.140625   Top5 99.960938   BatchTime 0.104543   LR 0.010000
INFO - Training [3][  380/  391]   Loss 0.163377   Top1 94.194079   Top5 99.962993   BatchTime 0.104144   LR 0.010000
INFO - ==> Top1: 94.190    Top5: 99.964    Loss: 0.163
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [3][   20/   79]   Loss 0.399869   Top1 88.085938   Top5 99.453125   BatchTime 0.130683
INFO - Validation [3][   40/   79]   Loss 0.402916   Top1 88.320312   Top5 99.453125   BatchTime 0.082361
INFO - Validation [3][   60/   79]   Loss 0.394839   Top1 88.528646   Top5 99.518229   BatchTime 0.066493
INFO - ==> Top1: 88.520    Top5: 99.610    Loss: 0.390
INFO - Scoreboard best 1 ==> Epoch [3][Top1: 88.520   Top5: 99.610] Sparsity : 0.581
INFO - Scoreboard best 2 ==> Epoch [2][Top1: 87.930   Top5: 99.470] Sparsity : 0.573
INFO - Scoreboard best 3 ==> Epoch [1][Top1: 86.880   Top5: 99.480] Sparsity : 0.550
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   4
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [4][   20/  391]   Loss 0.137656   Top1 95.156250   Top5 100.000000   BatchTime 0.200479   LR 0.010000
INFO - Training [4][   40/  391]   Loss 0.140780   Top1 95.097656   Top5 99.960938   BatchTime 0.150102   LR 0.010000
INFO - Training [4][   60/  391]   Loss 0.136184   Top1 95.273438   Top5 99.973958   BatchTime 0.133757   LR 0.010000
INFO - Training [4][   80/  391]   Loss 0.134634   Top1 95.341797   Top5 99.970703   BatchTime 0.125218   LR 0.010000
INFO - Training [4][  100/  391]   Loss 0.137260   Top1 95.312500   Top5 99.968750   BatchTime 0.120052   LR 0.010000
INFO - Training [4][  120/  391]   Loss 0.135286   Top1 95.397135   Top5 99.973958   BatchTime 0.116592   LR 0.010000
INFO - Training [4][  140/  391]   Loss 0.136926   Top1 95.306920   Top5 99.977679   BatchTime 0.114178   LR 0.010000
INFO - Training [4][  160/  391]   Loss 0.136549   Top1 95.400391   Top5 99.980469   BatchTime 0.112289   LR 0.010000
INFO - Training [4][  180/  391]   Loss 0.137171   Top1 95.342882   Top5 99.982639   BatchTime 0.110883   LR 0.010000
INFO - Training [4][  200/  391]   Loss 0.138063   Top1 95.332031   Top5 99.984375   BatchTime 0.109715   LR 0.010000
INFO - Training [4][  220/  391]   Loss 0.137335   Top1 95.319602   Top5 99.978693   BatchTime 0.108683   LR 0.010000
INFO - Training [4][  240/  391]   Loss 0.136472   Top1 95.341797   Top5 99.977214   BatchTime 0.107869   LR 0.010000
INFO - Training [4][  260/  391]   Loss 0.136540   Top1 95.351562   Top5 99.978966   BatchTime 0.107196   LR 0.010000
INFO - Training [4][  280/  391]   Loss 0.137315   Top1 95.281808   Top5 99.977679   BatchTime 0.106561   LR 0.010000
INFO - Training [4][  300/  391]   Loss 0.136258   Top1 95.333333   Top5 99.976562   BatchTime 0.106047   LR 0.010000
INFO - Training [4][  320/  391]   Loss 0.135544   Top1 95.346680   Top5 99.975586   BatchTime 0.105656   LR 0.010000
INFO - Training [4][  340/  391]   Loss 0.135317   Top1 95.351562   Top5 99.977022   BatchTime 0.105140   LR 0.010000
INFO - Training [4][  360/  391]   Loss 0.135539   Top1 95.325521   Top5 99.976128   BatchTime 0.104653   LR 0.010000
INFO - Training [4][  380/  391]   Loss 0.135048   Top1 95.333059   Top5 99.975329   BatchTime 0.104214   LR 0.010000
INFO - ==> Top1: 95.320    Top5: 99.976    Loss: 0.135
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [4][   20/   79]   Loss 0.404774   Top1 88.242188   Top5 99.453125   BatchTime 0.131135
INFO - Validation [4][   40/   79]   Loss 0.420828   Top1 88.457031   Top5 99.375000   BatchTime 0.082830
INFO - Validation [4][   60/   79]   Loss 0.402808   Top1 88.593750   Top5 99.492188   BatchTime 0.066619
INFO - ==> Top1: 88.610    Top5: 99.550    Loss: 0.398
INFO - Scoreboard best 1 ==> Epoch [4][Top1: 88.610   Top5: 99.550] Sparsity : 0.586
INFO - Scoreboard best 2 ==> Epoch [3][Top1: 88.520   Top5: 99.610] Sparsity : 0.581
INFO - Scoreboard best 3 ==> Epoch [2][Top1: 87.930   Top5: 99.470] Sparsity : 0.573
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   5
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [5][   20/  391]   Loss 0.139052   Top1 95.390625   Top5 99.921875   BatchTime 0.202432   LR 0.010000
INFO - Training [5][   40/  391]   Loss 0.125676   Top1 95.800781   Top5 99.960938   BatchTime 0.150860   LR 0.010000
INFO - Training [5][   60/  391]   Loss 0.124715   Top1 95.729167   Top5 99.973958   BatchTime 0.133639   LR 0.010000
INFO - Training [5][   80/  391]   Loss 0.122142   Top1 95.703125   Top5 99.970703   BatchTime 0.124851   LR 0.010000
INFO - Training [5][  100/  391]   Loss 0.121870   Top1 95.726562   Top5 99.968750   BatchTime 0.119709   LR 0.010000
INFO - Training [5][  120/  391]   Loss 0.120992   Top1 95.735677   Top5 99.973958   BatchTime 0.116162   LR 0.010000
INFO - Training [5][  140/  391]   Loss 0.119347   Top1 95.803571   Top5 99.977679   BatchTime 0.113685   LR 0.010000
INFO - Training [5][  160/  391]   Loss 0.119745   Top1 95.766602   Top5 99.980469   BatchTime 0.111918   LR 0.010000
INFO - Training [5][  180/  391]   Loss 0.119493   Top1 95.763889   Top5 99.982639   BatchTime 0.110607   LR 0.010000
INFO - Training [5][  200/  391]   Loss 0.118544   Top1 95.816406   Top5 99.972656   BatchTime 0.109466   LR 0.010000
INFO - Training [5][  220/  391]   Loss 0.119168   Top1 95.802557   Top5 99.971591   BatchTime 0.108546   LR 0.010000
INFO - Training [5][  240/  391]   Loss 0.117529   Top1 95.872396   Top5 99.973958   BatchTime 0.107724   LR 0.010000
INFO - Training [5][  260/  391]   Loss 0.115550   Top1 95.928486   Top5 99.975962   BatchTime 0.107105   LR 0.010000
INFO - Training [5][  280/  391]   Loss 0.115196   Top1 95.931920   Top5 99.977679   BatchTime 0.106513   LR 0.010000
INFO - Training [5][  300/  391]   Loss 0.114689   Top1 95.929688   Top5 99.979167   BatchTime 0.106103   LR 0.010000
INFO - Training [5][  320/  391]   Loss 0.114800   Top1 95.942383   Top5 99.978027   BatchTime 0.105644   LR 0.010000
INFO - Training [5][  340/  391]   Loss 0.115650   Top1 95.905331   Top5 99.977022   BatchTime 0.105159   LR 0.010000
INFO - Training [5][  360/  391]   Loss 0.114997   Top1 95.946181   Top5 99.978299   BatchTime 0.104694   LR 0.010000
INFO - Training [5][  380/  391]   Loss 0.115861   Top1 95.916941   Top5 99.979441   BatchTime 0.104491   LR 0.010000
INFO - ==> Top1: 95.920    Top5: 99.980    Loss: 0.116
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [5][   20/   79]   Loss 0.384320   Top1 88.671875   Top5 99.531250   BatchTime 0.130284
INFO - Validation [5][   40/   79]   Loss 0.393364   Top1 88.574219   Top5 99.453125   BatchTime 0.082426
INFO - Validation [5][   60/   79]   Loss 0.379531   Top1 88.919271   Top5 99.492188   BatchTime 0.064847
INFO - ==> Top1: 88.940    Top5: 99.530    Loss: 0.376
INFO - Scoreboard best 1 ==> Epoch [5][Top1: 88.940   Top5: 99.530] Sparsity : 0.591
INFO - Scoreboard best 2 ==> Epoch [4][Top1: 88.610   Top5: 99.550] Sparsity : 0.586
INFO - Scoreboard best 3 ==> Epoch [3][Top1: 88.520   Top5: 99.610] Sparsity : 0.581
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   6
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [6][   20/  391]   Loss 0.110725   Top1 95.898438   Top5 100.000000   BatchTime 0.198847   LR 0.010000
INFO - Training [6][   40/  391]   Loss 0.105334   Top1 96.191406   Top5 99.980469   BatchTime 0.149186   LR 0.010000
INFO - Training [6][   60/  391]   Loss 0.103093   Top1 96.119792   Top5 99.973958   BatchTime 0.132827   LR 0.010000
INFO - Training [6][   80/  391]   Loss 0.102947   Top1 96.093750   Top5 99.980469   BatchTime 0.124205   LR 0.010000
INFO - Training [6][  100/  391]   Loss 0.102429   Top1 96.070312   Top5 99.984375   BatchTime 0.119243   LR 0.010000
INFO - Training [6][  120/  391]   Loss 0.102028   Top1 96.165365   Top5 99.986979   BatchTime 0.116022   LR 0.010000
INFO - Training [6][  140/  391]   Loss 0.102863   Top1 96.166295   Top5 99.988839   BatchTime 0.113664   LR 0.010000
INFO - Training [6][  160/  391]   Loss 0.101475   Top1 96.225586   Top5 99.990234   BatchTime 0.112021   LR 0.010000
INFO - Training [6][  180/  391]   Loss 0.101102   Top1 96.267361   Top5 99.986979   BatchTime 0.110538   LR 0.010000
INFO - Training [6][  200/  391]   Loss 0.101343   Top1 96.304688   Top5 99.988281   BatchTime 0.109505   LR 0.010000
INFO - Training [6][  220/  391]   Loss 0.099850   Top1 96.345881   Top5 99.989347   BatchTime 0.108552   LR 0.010000
INFO - Training [6][  240/  391]   Loss 0.098942   Top1 96.357422   Top5 99.990234   BatchTime 0.107809   LR 0.010000
INFO - Training [6][  260/  391]   Loss 0.098016   Top1 96.382212   Top5 99.990986   BatchTime 0.107167   LR 0.010000
INFO - Training [6][  280/  391]   Loss 0.097634   Top1 96.420201   Top5 99.991629   BatchTime 0.106669   LR 0.010000
INFO - Training [6][  300/  391]   Loss 0.097737   Top1 96.432292   Top5 99.992188   BatchTime 0.106154   LR 0.010000
INFO - Training [6][  320/  391]   Loss 0.097856   Top1 96.430664   Top5 99.990234   BatchTime 0.105677   LR 0.010000
INFO - Training [6][  340/  391]   Loss 0.098776   Top1 96.399357   Top5 99.990809   BatchTime 0.105152   LR 0.010000
INFO - Training [6][  360/  391]   Loss 0.098516   Top1 96.410590   Top5 99.991319   BatchTime 0.104682   LR 0.010000
INFO - Training [6][  380/  391]   Loss 0.098254   Top1 96.424753   Top5 99.991776   BatchTime 0.104277   LR 0.010000
INFO - ==> Top1: 96.444    Top5: 99.992    Loss: 0.098
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [6][   20/   79]   Loss 0.391668   Top1 89.296875   Top5 99.570312   BatchTime 0.131260
INFO - Validation [6][   40/   79]   Loss 0.408603   Top1 89.140625   Top5 99.433594   BatchTime 0.083348
INFO - Validation [6][   60/   79]   Loss 0.395269   Top1 89.531250   Top5 99.505208   BatchTime 0.064292
INFO - ==> Top1: 89.530    Top5: 99.540    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 89.530   Top5: 99.540] Sparsity : 0.598
INFO - Scoreboard best 2 ==> Epoch [5][Top1: 88.940   Top5: 99.530] Sparsity : 0.591
INFO - Scoreboard best 3 ==> Epoch [4][Top1: 88.610   Top5: 99.550] Sparsity : 0.586
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   7
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [7][   20/  391]   Loss 0.081355   Top1 97.031250   Top5 100.000000   BatchTime 0.200608   LR 0.010000
INFO - Training [7][   40/  391]   Loss 0.083314   Top1 96.972656   Top5 100.000000   BatchTime 0.153551   LR 0.010000
INFO - Training [7][   60/  391]   Loss 0.088225   Top1 96.796875   Top5 99.986979   BatchTime 0.136022   LR 0.010000
INFO - Training [7][   80/  391]   Loss 0.086890   Top1 96.865234   Top5 99.990234   BatchTime 0.127173   LR 0.010000
INFO - Training [7][  100/  391]   Loss 0.087042   Top1 96.859375   Top5 99.992188   BatchTime 0.121850   LR 0.010000
INFO - Training [7][  120/  391]   Loss 0.086186   Top1 96.868490   Top5 99.986979   BatchTime 0.118104   LR 0.010000
INFO - Training [7][  140/  391]   Loss 0.086917   Top1 96.824777   Top5 99.988839   BatchTime 0.115376   LR 0.010000
INFO - Training [7][  160/  391]   Loss 0.085686   Top1 96.865234   Top5 99.990234   BatchTime 0.113467   LR 0.010000
INFO - Training [7][  180/  391]   Loss 0.085622   Top1 96.857639   Top5 99.991319   BatchTime 0.111803   LR 0.010000
INFO - Training [7][  200/  391]   Loss 0.086591   Top1 96.808594   Top5 99.988281   BatchTime 0.110442   LR 0.010000
INFO - Training [7][  220/  391]   Loss 0.086381   Top1 96.825284   Top5 99.989347   BatchTime 0.109442   LR 0.010000
INFO - Training [7][  240/  391]   Loss 0.086737   Top1 96.826172   Top5 99.990234   BatchTime 0.108590   LR 0.010000
INFO - Training [7][  260/  391]   Loss 0.087771   Top1 96.811899   Top5 99.987981   BatchTime 0.107855   LR 0.010000
INFO - Training [7][  280/  391]   Loss 0.088078   Top1 96.799665   Top5 99.988839   BatchTime 0.107339   LR 0.010000
INFO - Training [7][  300/  391]   Loss 0.088817   Top1 96.791667   Top5 99.989583   BatchTime 0.106741   LR 0.010000
INFO - Training [7][  320/  391]   Loss 0.089820   Top1 96.748047   Top5 99.990234   BatchTime 0.106160   LR 0.010000
INFO - Training [7][  340/  391]   Loss 0.090142   Top1 96.744026   Top5 99.990809   BatchTime 0.105628   LR 0.010000
INFO - Training [7][  360/  391]   Loss 0.089745   Top1 96.753472   Top5 99.991319   BatchTime 0.105131   LR 0.010000
INFO - Training [7][  380/  391]   Loss 0.089857   Top1 96.753701   Top5 99.991776   BatchTime 0.104425   LR 0.010000
INFO - ==> Top1: 96.752    Top5: 99.992    Loss: 0.090
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [7][   20/   79]   Loss 0.411906   Top1 89.257812   Top5 99.648438   BatchTime 0.129184
INFO - Validation [7][   40/   79]   Loss 0.421921   Top1 89.257812   Top5 99.550781   BatchTime 0.079948
INFO - Validation [7][   60/   79]   Loss 0.405108   Top1 89.348958   Top5 99.596354   BatchTime 0.062102
INFO - ==> Top1: 89.270    Top5: 99.630    Loss: 0.397
INFO - Scoreboard best 1 ==> Epoch [6][Top1: 89.530   Top5: 99.540] Sparsity : 0.598
INFO - Scoreboard best 2 ==> Epoch [7][Top1: 89.270   Top5: 99.630] Sparsity : 0.616
INFO - Scoreboard best 3 ==> Epoch [5][Top1: 88.940   Top5: 99.530] Sparsity : 0.591
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch   8
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [8][   20/  391]   Loss 0.075256   Top1 97.460938   Top5 100.000000   BatchTime 0.197958   LR 0.010000
INFO - Training [8][   40/  391]   Loss 0.081142   Top1 97.187500   Top5 99.980469   BatchTime 0.148748   LR 0.010000
INFO - Training [8][   60/  391]   Loss 0.085855   Top1 97.135417   Top5 99.986979   BatchTime 0.132266   LR 0.010000
INFO - Training [8][   80/  391]   Loss 0.085575   Top1 97.050781   Top5 99.990234   BatchTime 0.124143   LR 0.010000
INFO - Training [8][  100/  391]   Loss 0.084150   Top1 97.085938   Top5 99.992188   BatchTime 0.119301   LR 0.010000
INFO - Training [8][  120/  391]   Loss 0.084010   Top1 97.102865   Top5 99.993490   BatchTime 0.116198   LR 0.010000
INFO - Training [8][  140/  391]   Loss 0.083391   Top1 97.137277   Top5 99.994420   BatchTime 0.113841   LR 0.010000
INFO - Training [8][  160/  391]   Loss 0.083334   Top1 97.099609   Top5 99.995117   BatchTime 0.112113   LR 0.010000
INFO - Training [8][  180/  391]   Loss 0.083249   Top1 97.057292   Top5 99.995660   BatchTime 0.110776   LR 0.010000
INFO - Training [8][  200/  391]   Loss 0.082984   Top1 97.054688   Top5 99.996094   BatchTime 0.109757   LR 0.010000
INFO - Training [8][  220/  391]   Loss 0.083410   Top1 97.038352   Top5 99.996449   BatchTime 0.108815   LR 0.010000
INFO - Training [8][  240/  391]   Loss 0.083398   Top1 97.047526   Top5 99.996745   BatchTime 0.108084   LR 0.010000
INFO - Training [8][  260/  391]   Loss 0.083834   Top1 97.028245   Top5 99.996995   BatchTime 0.107408   LR 0.010000
INFO - Training [8][  280/  391]   Loss 0.084180   Top1 97.022879   Top5 99.997210   BatchTime 0.106840   LR 0.010000
INFO - Training [8][  300/  391]   Loss 0.084159   Top1 97.026042   Top5 99.994792   BatchTime 0.106321   LR 0.010000
INFO - Training [8][  320/  391]   Loss 0.084439   Top1 96.997070   Top5 99.995117   BatchTime 0.105862   LR 0.010000
INFO - Training [8][  340/  391]   Loss 0.084332   Top1 97.019761   Top5 99.995404   BatchTime 0.105365   LR 0.010000
INFO - Training [8][  360/  391]   Loss 0.085208   Top1 96.992188   Top5 99.995660   BatchTime 0.104873   LR 0.010000
INFO - Training [8][  380/  391]   Loss 0.085323   Top1 96.983964   Top5 99.995888   BatchTime 0.104285   LR 0.010000
INFO - ==> Top1: 96.976    Top5: 99.996    Loss: 0.085
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [8][   20/   79]   Loss 0.388328   Top1 89.414062   Top5 99.492188   BatchTime 0.129709
INFO - Validation [8][   40/   79]   Loss 0.403386   Top1 89.296875   Top5 99.453125   BatchTime 0.082494
INFO - Validation [8][   60/   79]   Loss 0.386894   Top1 89.583333   Top5 99.505208   BatchTime 0.063667
INFO - ==> Top1: 89.560    Top5: 99.560    Loss: 0.385
INFO - Scoreboard best 1 ==> Epoch [8][Top1: 89.560   Top5: 99.560] Sparsity : 0.624
INFO - Scoreboard best 2 ==> Epoch [6][Top1: 89.530   Top5: 99.540] Sparsity : 0.598
INFO - Scoreboard best 3 ==> Epoch [7][Top1: 89.270   Top5: 99.630] Sparsity : 0.616
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch   9
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [9][   20/  391]   Loss 0.077992   Top1 97.460938   Top5 100.000000   BatchTime 0.198567   LR 0.010000
INFO - Training [9][   40/  391]   Loss 0.076970   Top1 97.324219   Top5 100.000000   BatchTime 0.149081   LR 0.010000
INFO - Training [9][   60/  391]   Loss 0.074478   Top1 97.473958   Top5 100.000000   BatchTime 0.132722   LR 0.010000
INFO - Training [9][   80/  391]   Loss 0.076715   Top1 97.363281   Top5 100.000000   BatchTime 0.124332   LR 0.010000
INFO - Training [9][  100/  391]   Loss 0.076234   Top1 97.312500   Top5 99.992188   BatchTime 0.119531   LR 0.010000
INFO - Training [9][  120/  391]   Loss 0.076845   Top1 97.239583   Top5 99.993490   BatchTime 0.116926   LR 0.010000
INFO - Training [9][  140/  391]   Loss 0.076465   Top1 97.265625   Top5 99.988839   BatchTime 0.114524   LR 0.010000
INFO - Training [9][  160/  391]   Loss 0.075338   Top1 97.358398   Top5 99.990234   BatchTime 0.112702   LR 0.010000
INFO - Training [9][  180/  391]   Loss 0.076587   Top1 97.330729   Top5 99.991319   BatchTime 0.111142   LR 0.010000
INFO - Training [9][  200/  391]   Loss 0.075719   Top1 97.367188   Top5 99.992188   BatchTime 0.110051   LR 0.010000
INFO - Training [9][  220/  391]   Loss 0.074371   Top1 97.411222   Top5 99.992898   BatchTime 0.109161   LR 0.010000
INFO - Training [9][  240/  391]   Loss 0.074336   Top1 97.408854   Top5 99.993490   BatchTime 0.108357   LR 0.010000
INFO - Training [9][  260/  391]   Loss 0.074274   Top1 97.418870   Top5 99.993990   BatchTime 0.107684   LR 0.010000
INFO - Training [9][  280/  391]   Loss 0.074768   Top1 97.416295   Top5 99.994420   BatchTime 0.107146   LR 0.010000
INFO - Training [9][  300/  391]   Loss 0.074923   Top1 97.421875   Top5 99.994792   BatchTime 0.106674   LR 0.010000
INFO - Training [9][  320/  391]   Loss 0.075742   Top1 97.380371   Top5 99.995117   BatchTime 0.106216   LR 0.010000
INFO - Training [9][  340/  391]   Loss 0.076527   Top1 97.348346   Top5 99.995404   BatchTime 0.105679   LR 0.010000
INFO - Training [9][  360/  391]   Loss 0.076699   Top1 97.354601   Top5 99.995660   BatchTime 0.105215   LR 0.010000
INFO - Training [9][  380/  391]   Loss 0.076886   Top1 97.347862   Top5 99.995888   BatchTime 0.104647   LR 0.010000
INFO - ==> Top1: 97.336    Top5: 99.994    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [9][   20/   79]   Loss 0.407401   Top1 89.609375   Top5 99.492188   BatchTime 0.130138
INFO - Validation [9][   40/   79]   Loss 0.418494   Top1 89.589844   Top5 99.355469   BatchTime 0.079785
INFO - Validation [9][   60/   79]   Loss 0.404243   Top1 89.674479   Top5 99.492188   BatchTime 0.061860
INFO - ==> Top1: 89.680    Top5: 99.550    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.680   Top5: 99.550] Sparsity : 0.651
INFO - Scoreboard best 2 ==> Epoch [8][Top1: 89.560   Top5: 99.560] Sparsity : 0.624
INFO - Scoreboard best 3 ==> Epoch [6][Top1: 89.530   Top5: 99.540] Sparsity : 0.598
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  10
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [10][   20/  391]   Loss 0.068231   Top1 97.421875   Top5 100.000000   BatchTime 0.196949   LR 0.010000
INFO - Training [10][   40/  391]   Loss 0.068279   Top1 97.421875   Top5 100.000000   BatchTime 0.148671   LR 0.010000
INFO - Training [10][   60/  391]   Loss 0.070662   Top1 97.486979   Top5 100.000000   BatchTime 0.132539   LR 0.010000
INFO - Training [10][   80/  391]   Loss 0.068718   Top1 97.539062   Top5 100.000000   BatchTime 0.124008   LR 0.010000
INFO - Training [10][  100/  391]   Loss 0.068733   Top1 97.562500   Top5 99.992188   BatchTime 0.119227   LR 0.010000
INFO - Training [10][  120/  391]   Loss 0.069367   Top1 97.591146   Top5 99.993490   BatchTime 0.115897   LR 0.010000
INFO - Training [10][  140/  391]   Loss 0.070591   Top1 97.511161   Top5 99.994420   BatchTime 0.113670   LR 0.010000
INFO - Training [10][  160/  391]   Loss 0.071870   Top1 97.426758   Top5 99.995117   BatchTime 0.111995   LR 0.010000
INFO - Training [10][  180/  391]   Loss 0.070849   Top1 97.508681   Top5 99.991319   BatchTime 0.110715   LR 0.010000
INFO - Training [10][  200/  391]   Loss 0.070923   Top1 97.503906   Top5 99.992188   BatchTime 0.109576   LR 0.010000
INFO - Training [10][  220/  391]   Loss 0.073352   Top1 97.425426   Top5 99.992898   BatchTime 0.108707   LR 0.010000
INFO - Training [10][  240/  391]   Loss 0.073502   Top1 97.434896   Top5 99.993490   BatchTime 0.107999   LR 0.010000
INFO - Training [10][  260/  391]   Loss 0.074708   Top1 97.388822   Top5 99.990986   BatchTime 0.107280   LR 0.010000
INFO - Training [10][  280/  391]   Loss 0.076345   Top1 97.321429   Top5 99.991629   BatchTime 0.106816   LR 0.010000
INFO - Training [10][  300/  391]   Loss 0.076970   Top1 97.302083   Top5 99.992188   BatchTime 0.106344   LR 0.010000
INFO - Training [10][  320/  391]   Loss 0.077681   Top1 97.260742   Top5 99.992676   BatchTime 0.105831   LR 0.010000
INFO - Training [10][  340/  391]   Loss 0.078336   Top1 97.247243   Top5 99.993107   BatchTime 0.105302   LR 0.010000
INFO - Training [10][  360/  391]   Loss 0.079280   Top1 97.211372   Top5 99.991319   BatchTime 0.104834   LR 0.010000
INFO - Training [10][  380/  391]   Loss 0.080426   Top1 97.175164   Top5 99.991776   BatchTime 0.104080   LR 0.010000
INFO - ==> Top1: 97.166    Top5: 99.990    Loss: 0.081
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [10][   20/   79]   Loss 0.397392   Top1 89.570312   Top5 99.453125   BatchTime 0.129062
INFO - Validation [10][   40/   79]   Loss 0.411146   Top1 89.277344   Top5 99.355469   BatchTime 0.079874
INFO - Validation [10][   60/   79]   Loss 0.395390   Top1 89.739583   Top5 99.440104   BatchTime 0.062003
INFO - ==> Top1: 89.590    Top5: 99.460    Loss: 0.397
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.680   Top5: 99.550] Sparsity : 0.651
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 89.590   Top5: 99.460] Sparsity : 0.687
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 89.560   Top5: 99.560] Sparsity : 0.624
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  11
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [11][   20/  391]   Loss 0.078519   Top1 97.265625   Top5 100.000000   BatchTime 0.196867   LR 0.010000
INFO - Training [11][   40/  391]   Loss 0.079400   Top1 97.207031   Top5 100.000000   BatchTime 0.148553   LR 0.010000
INFO - Training [11][   60/  391]   Loss 0.078444   Top1 97.161458   Top5 100.000000   BatchTime 0.132294   LR 0.010000
INFO - Training [11][   80/  391]   Loss 0.080151   Top1 97.177734   Top5 100.000000   BatchTime 0.123999   LR 0.010000
INFO - Training [11][  100/  391]   Loss 0.081534   Top1 97.085938   Top5 99.992188   BatchTime 0.119202   LR 0.010000
INFO - Training [11][  120/  391]   Loss 0.081466   Top1 97.076823   Top5 99.993490   BatchTime 0.116014   LR 0.010000
INFO - Training [11][  140/  391]   Loss 0.080988   Top1 97.109375   Top5 99.994420   BatchTime 0.113710   LR 0.010000
INFO - Training [11][  160/  391]   Loss 0.083435   Top1 97.026367   Top5 99.995117   BatchTime 0.111918   LR 0.010000
INFO - Training [11][  180/  391]   Loss 0.084364   Top1 96.979167   Top5 99.986979   BatchTime 0.110998   LR 0.010000
INFO - Training [11][  200/  391]   Loss 0.084269   Top1 97.007812   Top5 99.988281   BatchTime 0.109785   LR 0.010000
INFO - Training [11][  220/  391]   Loss 0.084145   Top1 97.020597   Top5 99.989347   BatchTime 0.108849   LR 0.010000
INFO - Training [11][  240/  391]   Loss 0.084566   Top1 97.001953   Top5 99.990234   BatchTime 0.108102   LR 0.010000
INFO - Training [11][  260/  391]   Loss 0.085002   Top1 96.977163   Top5 99.990986   BatchTime 0.107493   LR 0.010000
INFO - Training [11][  280/  391]   Loss 0.084450   Top1 96.983817   Top5 99.991629   BatchTime 0.106979   LR 0.010000
INFO - Training [11][  300/  391]   Loss 0.084949   Top1 96.976562   Top5 99.992188   BatchTime 0.106483   LR 0.010000
INFO - Training [11][  320/  391]   Loss 0.084951   Top1 96.975098   Top5 99.990234   BatchTime 0.105994   LR 0.010000
INFO - Training [11][  340/  391]   Loss 0.085437   Top1 96.960018   Top5 99.990809   BatchTime 0.105511   LR 0.010000
INFO - Training [11][  360/  391]   Loss 0.085698   Top1 96.933594   Top5 99.991319   BatchTime 0.105061   LR 0.010000
INFO - Training [11][  380/  391]   Loss 0.086236   Top1 96.907895   Top5 99.991776   BatchTime 0.104201   LR 0.010000
INFO - ==> Top1: 96.934    Top5: 99.990    Loss: 0.086
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [11][   20/   79]   Loss 0.365460   Top1 89.726562   Top5 99.492188   BatchTime 0.128102
INFO - Validation [11][   40/   79]   Loss 0.393364   Top1 89.335938   Top5 99.453125   BatchTime 0.079945
INFO - Validation [11][   60/   79]   Loss 0.390832   Top1 89.531250   Top5 99.518229   BatchTime 0.061984
INFO - ==> Top1: 89.440    Top5: 99.570    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [9][Top1: 89.680   Top5: 99.550] Sparsity : 0.651
INFO - Scoreboard best 2 ==> Epoch [10][Top1: 89.590   Top5: 99.460] Sparsity : 0.687
INFO - Scoreboard best 3 ==> Epoch [8][Top1: 89.560   Top5: 99.560] Sparsity : 0.624
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  12
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [12][   20/  391]   Loss 0.084611   Top1 96.914062   Top5 100.000000   BatchTime 0.199045   LR 0.010000
INFO - Training [12][   40/  391]   Loss 0.078573   Top1 97.304688   Top5 100.000000   BatchTime 0.149669   LR 0.010000
INFO - Training [12][   60/  391]   Loss 0.077359   Top1 97.291667   Top5 100.000000   BatchTime 0.133188   LR 0.010000
INFO - Training [12][   80/  391]   Loss 0.073995   Top1 97.421875   Top5 100.000000   BatchTime 0.124729   LR 0.010000
INFO - Training [12][  100/  391]   Loss 0.075072   Top1 97.414062   Top5 100.000000   BatchTime 0.119953   LR 0.010000
INFO - Training [12][  120/  391]   Loss 0.075368   Top1 97.395833   Top5 100.000000   BatchTime 0.116685   LR 0.010000
INFO - Training [12][  140/  391]   Loss 0.076603   Top1 97.276786   Top5 100.000000   BatchTime 0.113803   LR 0.010000
INFO - Training [12][  160/  391]   Loss 0.076349   Top1 97.314453   Top5 100.000000   BatchTime 0.112047   LR 0.010000
INFO - Training [12][  180/  391]   Loss 0.076523   Top1 97.304688   Top5 100.000000   BatchTime 0.110875   LR 0.010000
INFO - Training [12][  200/  391]   Loss 0.077737   Top1 97.257812   Top5 100.000000   BatchTime 0.109759   LR 0.010000
INFO - Training [12][  220/  391]   Loss 0.078417   Top1 97.237216   Top5 100.000000   BatchTime 0.108917   LR 0.010000
INFO - Training [12][  240/  391]   Loss 0.077803   Top1 97.246094   Top5 100.000000   BatchTime 0.108194   LR 0.010000
INFO - Training [12][  260/  391]   Loss 0.078281   Top1 97.184495   Top5 100.000000   BatchTime 0.107614   LR 0.010000
INFO - Training [12][  280/  391]   Loss 0.079590   Top1 97.123326   Top5 100.000000   BatchTime 0.107065   LR 0.010000
INFO - Training [12][  300/  391]   Loss 0.078868   Top1 97.158854   Top5 100.000000   BatchTime 0.106583   LR 0.010000
INFO - Training [12][  320/  391]   Loss 0.078153   Top1 97.175293   Top5 100.000000   BatchTime 0.106203   LR 0.010000
INFO - Training [12][  340/  391]   Loss 0.077665   Top1 97.185202   Top5 100.000000   BatchTime 0.105706   LR 0.010000
INFO - Training [12][  360/  391]   Loss 0.077452   Top1 97.191840   Top5 100.000000   BatchTime 0.105226   LR 0.010000
INFO - Training [12][  380/  391]   Loss 0.077959   Top1 97.189556   Top5 99.995888   BatchTime 0.104609   LR 0.010000
INFO - ==> Top1: 97.182    Top5: 99.996    Loss: 0.078
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [12][   20/   79]   Loss 0.391800   Top1 89.765625   Top5 99.648438   BatchTime 0.130609
INFO - Validation [12][   40/   79]   Loss 0.401921   Top1 89.453125   Top5 99.570312   BatchTime 0.079868
INFO - Validation [12][   60/   79]   Loss 0.391652   Top1 89.895833   Top5 99.609375   BatchTime 0.061930
INFO - ==> Top1: 89.800    Top5: 99.640    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 89.680   Top5: 99.550] Sparsity : 0.651
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.590   Top5: 99.460] Sparsity : 0.687
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  13
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [13][   20/  391]   Loss 0.071947   Top1 97.382812   Top5 100.000000   BatchTime 0.195810   LR 0.010000
INFO - Training [13][   40/  391]   Loss 0.070696   Top1 97.363281   Top5 100.000000   BatchTime 0.148577   LR 0.010000
INFO - Training [13][   60/  391]   Loss 0.067211   Top1 97.565104   Top5 100.000000   BatchTime 0.132644   LR 0.010000
INFO - Training [13][   80/  391]   Loss 0.073061   Top1 97.451172   Top5 100.000000   BatchTime 0.124962   LR 0.010000
INFO - Training [13][  100/  391]   Loss 0.071419   Top1 97.546875   Top5 100.000000   BatchTime 0.120171   LR 0.010000
INFO - Training [13][  120/  391]   Loss 0.073590   Top1 97.473958   Top5 99.993490   BatchTime 0.116972   LR 0.010000
INFO - Training [13][  140/  391]   Loss 0.073187   Top1 97.483259   Top5 99.994420   BatchTime 0.114615   LR 0.010000
INFO - Training [13][  160/  391]   Loss 0.074024   Top1 97.465820   Top5 99.990234   BatchTime 0.112818   LR 0.010000
INFO - Training [13][  180/  391]   Loss 0.073916   Top1 97.460938   Top5 99.991319   BatchTime 0.111449   LR 0.010000
INFO - Training [13][  200/  391]   Loss 0.073997   Top1 97.460938   Top5 99.992188   BatchTime 0.110342   LR 0.010000
INFO - Training [13][  220/  391]   Loss 0.074663   Top1 97.418324   Top5 99.992898   BatchTime 0.109408   LR 0.010000
INFO - Training [13][  240/  391]   Loss 0.074841   Top1 97.441406   Top5 99.990234   BatchTime 0.108746   LR 0.010000
INFO - Training [13][  260/  391]   Loss 0.074643   Top1 97.445913   Top5 99.990986   BatchTime 0.108411   LR 0.010000
INFO - Training [13][  280/  391]   Loss 0.075403   Top1 97.421875   Top5 99.991629   BatchTime 0.107741   LR 0.010000
INFO - Training [13][  300/  391]   Loss 0.075809   Top1 97.403646   Top5 99.992188   BatchTime 0.107154   LR 0.010000
INFO - Training [13][  320/  391]   Loss 0.076526   Top1 97.368164   Top5 99.992676   BatchTime 0.106605   LR 0.010000
INFO - Training [13][  340/  391]   Loss 0.076713   Top1 97.366728   Top5 99.993107   BatchTime 0.106064   LR 0.010000
INFO - Training [13][  360/  391]   Loss 0.077080   Top1 97.363281   Top5 99.991319   BatchTime 0.105580   LR 0.010000
INFO - Training [13][  380/  391]   Loss 0.077033   Top1 97.358141   Top5 99.991776   BatchTime 0.104605   LR 0.010000
INFO - ==> Top1: 97.374    Top5: 99.992    Loss: 0.077
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [13][   20/   79]   Loss 0.402819   Top1 89.101562   Top5 99.531250   BatchTime 0.128367
INFO - Validation [13][   40/   79]   Loss 0.426153   Top1 88.925781   Top5 99.394531   BatchTime 0.077350
INFO - Validation [13][   60/   79]   Loss 0.414251   Top1 89.361979   Top5 99.453125   BatchTime 0.060123
INFO - ==> Top1: 89.390    Top5: 99.520    Loss: 0.413
INFO - Scoreboard best 1 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 89.680   Top5: 99.550] Sparsity : 0.651
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.590   Top5: 99.460] Sparsity : 0.687
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  14
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [14][   20/  391]   Loss 0.062849   Top1 97.695312   Top5 100.000000   BatchTime 0.195089   LR 0.010000
INFO - Training [14][   40/  391]   Loss 0.061147   Top1 97.890625   Top5 100.000000   BatchTime 0.147717   LR 0.010000
INFO - Training [14][   60/  391]   Loss 0.063986   Top1 97.747396   Top5 99.986979   BatchTime 0.132126   LR 0.010000
INFO - Training [14][   80/  391]   Loss 0.065122   Top1 97.724609   Top5 99.990234   BatchTime 0.124218   LR 0.010000
INFO - Training [14][  100/  391]   Loss 0.066084   Top1 97.804688   Top5 99.992188   BatchTime 0.119302   LR 0.010000
INFO - Training [14][  120/  391]   Loss 0.065748   Top1 97.740885   Top5 99.993490   BatchTime 0.116006   LR 0.010000
INFO - Training [14][  140/  391]   Loss 0.065696   Top1 97.723214   Top5 99.988839   BatchTime 0.113625   LR 0.010000
INFO - Training [14][  160/  391]   Loss 0.065457   Top1 97.749023   Top5 99.985352   BatchTime 0.111802   LR 0.010000
INFO - Training [14][  180/  391]   Loss 0.065405   Top1 97.708333   Top5 99.986979   BatchTime 0.110432   LR 0.010000
INFO - Training [14][  200/  391]   Loss 0.065780   Top1 97.695312   Top5 99.984375   BatchTime 0.109264   LR 0.010000
INFO - Training [14][  220/  391]   Loss 0.067246   Top1 97.649148   Top5 99.982244   BatchTime 0.108409   LR 0.010000
INFO - Training [14][  240/  391]   Loss 0.066836   Top1 97.685547   Top5 99.983724   BatchTime 0.107728   LR 0.010000
INFO - Training [14][  260/  391]   Loss 0.066933   Top1 97.710337   Top5 99.981971   BatchTime 0.107090   LR 0.010000
INFO - Training [14][  280/  391]   Loss 0.066410   Top1 97.731585   Top5 99.983259   BatchTime 0.106582   LR 0.010000
INFO - Training [14][  300/  391]   Loss 0.066413   Top1 97.721354   Top5 99.984375   BatchTime 0.106129   LR 0.010000
INFO - Training [14][  320/  391]   Loss 0.066443   Top1 97.707520   Top5 99.985352   BatchTime 0.105645   LR 0.010000
INFO - Training [14][  340/  391]   Loss 0.068171   Top1 97.672335   Top5 99.986213   BatchTime 0.105179   LR 0.010000
INFO - Training [14][  360/  391]   Loss 0.068511   Top1 97.647569   Top5 99.984809   BatchTime 0.104774   LR 0.010000
INFO - Training [14][  380/  391]   Loss 0.068482   Top1 97.645970   Top5 99.985609   BatchTime 0.103988   LR 0.010000
INFO - ==> Top1: 97.638    Top5: 99.986    Loss: 0.069
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [14][   20/   79]   Loss 0.393532   Top1 90.078125   Top5 99.648438   BatchTime 0.133672
INFO - Validation [14][   40/   79]   Loss 0.419510   Top1 89.355469   Top5 99.550781   BatchTime 0.080456
INFO - Validation [14][   60/   79]   Loss 0.416249   Top1 89.427083   Top5 99.570312   BatchTime 0.062236
INFO - ==> Top1: 89.480    Top5: 99.570    Loss: 0.410
INFO - Scoreboard best 1 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Scoreboard best 2 ==> Epoch [9][Top1: 89.680   Top5: 99.550] Sparsity : 0.651
INFO - Scoreboard best 3 ==> Epoch [10][Top1: 89.590   Top5: 99.460] Sparsity : 0.687
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  15
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [15][   20/  391]   Loss 0.060648   Top1 97.929688   Top5 100.000000   BatchTime 0.195184   LR 0.010000
INFO - Training [15][   40/  391]   Loss 0.057316   Top1 97.988281   Top5 100.000000   BatchTime 0.147582   LR 0.010000
INFO - Training [15][   60/  391]   Loss 0.057610   Top1 98.020833   Top5 100.000000   BatchTime 0.132011   LR 0.010000
INFO - Training [15][   80/  391]   Loss 0.060027   Top1 97.900391   Top5 100.000000   BatchTime 0.124178   LR 0.010000
INFO - Training [15][  100/  391]   Loss 0.059345   Top1 97.929688   Top5 100.000000   BatchTime 0.119429   LR 0.010000
INFO - Training [15][  120/  391]   Loss 0.059900   Top1 97.942708   Top5 100.000000   BatchTime 0.116291   LR 0.010000
INFO - Training [15][  140/  391]   Loss 0.061697   Top1 97.840402   Top5 100.000000   BatchTime 0.113990   LR 0.010000
INFO - Training [15][  160/  391]   Loss 0.061954   Top1 97.802734   Top5 100.000000   BatchTime 0.112290   LR 0.010000
INFO - Training [15][  180/  391]   Loss 0.063513   Top1 97.730035   Top5 100.000000   BatchTime 0.110884   LR 0.010000
INFO - Training [15][  200/  391]   Loss 0.063311   Top1 97.742188   Top5 100.000000   BatchTime 0.109705   LR 0.010000
INFO - Training [15][  220/  391]   Loss 0.064361   Top1 97.716619   Top5 100.000000   BatchTime 0.108795   LR 0.010000
INFO - Training [15][  240/  391]   Loss 0.064565   Top1 97.727865   Top5 99.996745   BatchTime 0.108079   LR 0.010000
INFO - Training [15][  260/  391]   Loss 0.064049   Top1 97.752404   Top5 99.993990   BatchTime 0.107514   LR 0.010000
INFO - Training [15][  280/  391]   Loss 0.065358   Top1 97.712054   Top5 99.994420   BatchTime 0.106969   LR 0.010000
INFO - Training [15][  300/  391]   Loss 0.065246   Top1 97.729167   Top5 99.992188   BatchTime 0.106730   LR 0.010000
INFO - Training [15][  320/  391]   Loss 0.065227   Top1 97.729492   Top5 99.992676   BatchTime 0.106298   LR 0.010000
INFO - Training [15][  340/  391]   Loss 0.065015   Top1 97.715993   Top5 99.993107   BatchTime 0.105842   LR 0.010000
INFO - Training [15][  360/  391]   Loss 0.065160   Top1 97.714844   Top5 99.993490   BatchTime 0.105363   LR 0.010000
INFO - Training [15][  380/  391]   Loss 0.066136   Top1 97.693257   Top5 99.991776   BatchTime 0.104225   LR 0.010000
INFO - ==> Top1: 97.698    Top5: 99.992    Loss: 0.066
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [15][   20/   79]   Loss 0.397485   Top1 90.195312   Top5 99.531250   BatchTime 0.125025
INFO - Validation [15][   40/   79]   Loss 0.415079   Top1 89.667969   Top5 99.511719   BatchTime 0.075746
INFO - Validation [15][   60/   79]   Loss 0.407975   Top1 89.856771   Top5 99.544271   BatchTime 0.059207
INFO - ==> Top1: 89.790    Top5: 99.570    Loss: 0.405
INFO - Scoreboard best 1 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Scoreboard best 2 ==> Epoch [15][Top1: 89.790   Top5: 99.570] Sparsity : 0.702
INFO - Scoreboard best 3 ==> Epoch [9][Top1: 89.680   Top5: 99.550] Sparsity : 0.651
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  16
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [16][   20/  391]   Loss 0.067243   Top1 97.500000   Top5 100.000000   BatchTime 0.194804   LR 0.010000
INFO - Training [16][   40/  391]   Loss 0.060092   Top1 97.753906   Top5 100.000000   BatchTime 0.147325   LR 0.010000
INFO - Training [16][   60/  391]   Loss 0.059683   Top1 97.851562   Top5 100.000000   BatchTime 0.131326   LR 0.010000
INFO - Training [16][   80/  391]   Loss 0.062013   Top1 97.832031   Top5 100.000000   BatchTime 0.123704   LR 0.010000
INFO - Training [16][  100/  391]   Loss 0.061678   Top1 97.859375   Top5 100.000000   BatchTime 0.119144   LR 0.010000
INFO - Training [16][  120/  391]   Loss 0.062010   Top1 97.877604   Top5 100.000000   BatchTime 0.116184   LR 0.010000
INFO - Training [16][  140/  391]   Loss 0.061306   Top1 97.857143   Top5 100.000000   BatchTime 0.113865   LR 0.010000
INFO - Training [16][  160/  391]   Loss 0.060343   Top1 97.905273   Top5 100.000000   BatchTime 0.112097   LR 0.010000
INFO - Training [16][  180/  391]   Loss 0.060063   Top1 97.916667   Top5 100.000000   BatchTime 0.110767   LR 0.010000
INFO - Training [16][  200/  391]   Loss 0.060587   Top1 97.906250   Top5 100.000000   BatchTime 0.109694   LR 0.010000
INFO - Training [16][  220/  391]   Loss 0.060574   Top1 97.890625   Top5 100.000000   BatchTime 0.108855   LR 0.010000
INFO - Training [16][  240/  391]   Loss 0.060131   Top1 97.887370   Top5 100.000000   BatchTime 0.108083   LR 0.010000
INFO - Training [16][  260/  391]   Loss 0.060987   Top1 97.827524   Top5 100.000000   BatchTime 0.107462   LR 0.010000
INFO - Training [16][  280/  391]   Loss 0.062007   Top1 97.759487   Top5 100.000000   BatchTime 0.106673   LR 0.010000
INFO - Training [16][  300/  391]   Loss 0.061662   Top1 97.778646   Top5 100.000000   BatchTime 0.106193   LR 0.010000
INFO - Training [16][  320/  391]   Loss 0.061511   Top1 97.775879   Top5 100.000000   BatchTime 0.105738   LR 0.010000
INFO - Training [16][  340/  391]   Loss 0.061985   Top1 97.761949   Top5 100.000000   BatchTime 0.105250   LR 0.010000
INFO - Training [16][  360/  391]   Loss 0.061771   Top1 97.773438   Top5 100.000000   BatchTime 0.104808   LR 0.010000
INFO - Training [16][  380/  391]   Loss 0.061218   Top1 97.791941   Top5 100.000000   BatchTime 0.103759   LR 0.010000
INFO - ==> Top1: 97.794    Top5: 100.000    Loss: 0.061
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [16][   20/   79]   Loss 0.380760   Top1 90.390625   Top5 99.492188   BatchTime 0.124643
INFO - Validation [16][   40/   79]   Loss 0.410524   Top1 89.902344   Top5 99.511719   BatchTime 0.075323
INFO - Validation [16][   60/   79]   Loss 0.403471   Top1 90.104167   Top5 99.609375   BatchTime 0.058813
INFO - ==> Top1: 90.200    Top5: 99.640    Loss: 0.400
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 90.200   Top5: 99.640] Sparsity : 0.706
INFO - Scoreboard best 2 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Scoreboard best 3 ==> Epoch [15][Top1: 89.790   Top5: 99.570] Sparsity : 0.702
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  17
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [17][   20/  391]   Loss 0.045891   Top1 98.593750   Top5 100.000000   BatchTime 0.194850   LR 0.010000
INFO - Training [17][   40/  391]   Loss 0.051818   Top1 98.222656   Top5 100.000000   BatchTime 0.147548   LR 0.010000
INFO - Training [17][   60/  391]   Loss 0.050809   Top1 98.359375   Top5 100.000000   BatchTime 0.131726   LR 0.010000
INFO - Training [17][   80/  391]   Loss 0.053827   Top1 98.105469   Top5 99.990234   BatchTime 0.123894   LR 0.010000
INFO - Training [17][  100/  391]   Loss 0.054854   Top1 98.078125   Top5 99.992188   BatchTime 0.119134   LR 0.010000
INFO - Training [17][  120/  391]   Loss 0.057355   Top1 98.001302   Top5 99.993490   BatchTime 0.115946   LR 0.010000
INFO - Training [17][  140/  391]   Loss 0.057306   Top1 98.030134   Top5 99.994420   BatchTime 0.113611   LR 0.010000
INFO - Training [17][  160/  391]   Loss 0.057989   Top1 98.007812   Top5 99.995117   BatchTime 0.111958   LR 0.010000
INFO - Training [17][  180/  391]   Loss 0.057671   Top1 98.020833   Top5 99.995660   BatchTime 0.110605   LR 0.010000
INFO - Training [17][  200/  391]   Loss 0.058539   Top1 97.996094   Top5 99.996094   BatchTime 0.109596   LR 0.010000
INFO - Training [17][  220/  391]   Loss 0.058773   Top1 97.990057   Top5 99.996449   BatchTime 0.108669   LR 0.010000
INFO - Training [17][  240/  391]   Loss 0.059382   Top1 97.991536   Top5 99.996745   BatchTime 0.107891   LR 0.010000
INFO - Training [17][  260/  391]   Loss 0.060566   Top1 97.929688   Top5 99.993990   BatchTime 0.107253   LR 0.010000
INFO - Training [17][  280/  391]   Loss 0.060942   Top1 97.918527   Top5 99.994420   BatchTime 0.106778   LR 0.010000
INFO - Training [17][  300/  391]   Loss 0.061392   Top1 97.906250   Top5 99.994792   BatchTime 0.106317   LR 0.010000
INFO - Training [17][  320/  391]   Loss 0.061846   Top1 97.880859   Top5 99.995117   BatchTime 0.105847   LR 0.010000
INFO - Training [17][  340/  391]   Loss 0.061613   Top1 97.874540   Top5 99.995404   BatchTime 0.105721   LR 0.010000
INFO - Training [17][  360/  391]   Loss 0.061389   Top1 97.866753   Top5 99.995660   BatchTime 0.105418   LR 0.010000
INFO - Training [17][  380/  391]   Loss 0.061618   Top1 97.843339   Top5 99.995888   BatchTime 0.103688   LR 0.010000
INFO - ==> Top1: 97.816    Top5: 99.996    Loss: 0.062
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [17][   20/   79]   Loss 0.385483   Top1 89.882812   Top5 99.648438   BatchTime 0.124937
INFO - Validation [17][   40/   79]   Loss 0.400359   Top1 89.785156   Top5 99.511719   BatchTime 0.075540
INFO - Validation [17][   60/   79]   Loss 0.397102   Top1 89.882812   Top5 99.570312   BatchTime 0.059113
INFO - ==> Top1: 89.950    Top5: 99.600    Loss: 0.395
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 90.200   Top5: 99.640] Sparsity : 0.706
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 89.950   Top5: 99.600] Sparsity : 0.724
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  18
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [18][   20/  391]   Loss 0.060093   Top1 97.578125   Top5 99.921875   BatchTime 0.193079   LR 0.010000
INFO - Training [18][   40/  391]   Loss 0.063658   Top1 97.675781   Top5 99.960938   BatchTime 0.146445   LR 0.010000
INFO - Training [18][   60/  391]   Loss 0.062703   Top1 97.760417   Top5 99.973958   BatchTime 0.130881   LR 0.010000
INFO - Training [18][   80/  391]   Loss 0.063037   Top1 97.714844   Top5 99.980469   BatchTime 0.123330   LR 0.010000
INFO - Training [18][  100/  391]   Loss 0.060857   Top1 97.851562   Top5 99.984375   BatchTime 0.118617   LR 0.010000
INFO - Training [18][  120/  391]   Loss 0.061387   Top1 97.832031   Top5 99.986979   BatchTime 0.115372   LR 0.010000
INFO - Training [18][  140/  391]   Loss 0.061214   Top1 97.818080   Top5 99.988839   BatchTime 0.113151   LR 0.010000
INFO - Training [18][  160/  391]   Loss 0.062640   Top1 97.739258   Top5 99.990234   BatchTime 0.111453   LR 0.010000
INFO - Training [18][  180/  391]   Loss 0.062635   Top1 97.734375   Top5 99.991319   BatchTime 0.110296   LR 0.010000
INFO - Training [18][  200/  391]   Loss 0.063030   Top1 97.707031   Top5 99.992188   BatchTime 0.109298   LR 0.010000
INFO - Training [18][  220/  391]   Loss 0.065413   Top1 97.620739   Top5 99.992898   BatchTime 0.108415   LR 0.010000
INFO - Training [18][  240/  391]   Loss 0.065371   Top1 97.584635   Top5 99.993490   BatchTime 0.107784   LR 0.010000
INFO - Training [18][  260/  391]   Loss 0.064987   Top1 97.626202   Top5 99.993990   BatchTime 0.107232   LR 0.010000
INFO - Training [18][  280/  391]   Loss 0.065450   Top1 97.611607   Top5 99.994420   BatchTime 0.106755   LR 0.010000
INFO - Training [18][  300/  391]   Loss 0.064738   Top1 97.640625   Top5 99.994792   BatchTime 0.106288   LR 0.010000
INFO - Training [18][  320/  391]   Loss 0.065104   Top1 97.641602   Top5 99.992676   BatchTime 0.105853   LR 0.010000
INFO - Training [18][  340/  391]   Loss 0.066185   Top1 97.605699   Top5 99.993107   BatchTime 0.105321   LR 0.010000
INFO - Training [18][  360/  391]   Loss 0.066571   Top1 97.575955   Top5 99.993490   BatchTime 0.104925   LR 0.010000
INFO - Training [18][  380/  391]   Loss 0.067068   Top1 97.553454   Top5 99.993832   BatchTime 0.103506   LR 0.010000
INFO - ==> Top1: 97.552    Top5: 99.994    Loss: 0.067
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [18][   20/   79]   Loss 0.384011   Top1 89.960938   Top5 99.570312   BatchTime 0.124825
INFO - Validation [18][   40/   79]   Loss 0.409215   Top1 89.667969   Top5 99.531250   BatchTime 0.075423
INFO - Validation [18][   60/   79]   Loss 0.402560   Top1 89.739583   Top5 99.531250   BatchTime 0.061329
INFO - ==> Top1: 89.660    Top5: 99.600    Loss: 0.403
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 90.200   Top5: 99.640] Sparsity : 0.706
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 89.950   Top5: 99.600] Sparsity : 0.724
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  19
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [19][   20/  391]   Loss 0.062819   Top1 97.968750   Top5 100.000000   BatchTime 0.194665   LR 0.010000
INFO - Training [19][   40/  391]   Loss 0.063331   Top1 97.851562   Top5 100.000000   BatchTime 0.147457   LR 0.010000
INFO - Training [19][   60/  391]   Loss 0.061164   Top1 97.747396   Top5 100.000000   BatchTime 0.131498   LR 0.010000
INFO - Training [19][   80/  391]   Loss 0.060575   Top1 97.714844   Top5 100.000000   BatchTime 0.123746   LR 0.010000
INFO - Training [19][  100/  391]   Loss 0.062655   Top1 97.664062   Top5 100.000000   BatchTime 0.118989   LR 0.010000
INFO - Training [19][  120/  391]   Loss 0.063950   Top1 97.565104   Top5 100.000000   BatchTime 0.115670   LR 0.010000
INFO - Training [19][  140/  391]   Loss 0.064695   Top1 97.578125   Top5 100.000000   BatchTime 0.113474   LR 0.010000
INFO - Training [19][  160/  391]   Loss 0.063587   Top1 97.602539   Top5 100.000000   BatchTime 0.111792   LR 0.010000
INFO - Training [19][  180/  391]   Loss 0.062665   Top1 97.647569   Top5 100.000000   BatchTime 0.110508   LR 0.010000
INFO - Training [19][  200/  391]   Loss 0.063383   Top1 97.640625   Top5 100.000000   BatchTime 0.109395   LR 0.010000
INFO - Training [19][  220/  391]   Loss 0.063323   Top1 97.656250   Top5 99.996449   BatchTime 0.108468   LR 0.010000
INFO - Training [19][  240/  391]   Loss 0.063360   Top1 97.669271   Top5 99.996745   BatchTime 0.107721   LR 0.010000
INFO - Training [19][  260/  391]   Loss 0.064076   Top1 97.653245   Top5 99.996995   BatchTime 0.107099   LR 0.010000
INFO - Training [19][  280/  391]   Loss 0.064644   Top1 97.631138   Top5 99.994420   BatchTime 0.106552   LR 0.010000
INFO - Training [19][  300/  391]   Loss 0.064909   Top1 97.638021   Top5 99.994792   BatchTime 0.105992   LR 0.010000
INFO - Training [19][  320/  391]   Loss 0.065638   Top1 97.614746   Top5 99.995117   BatchTime 0.105519   LR 0.010000
INFO - Training [19][  340/  391]   Loss 0.065725   Top1 97.635570   Top5 99.995404   BatchTime 0.105107   LR 0.010000
INFO - Training [19][  360/  391]   Loss 0.066114   Top1 97.615017   Top5 99.995660   BatchTime 0.104262   LR 0.010000
INFO - Training [19][  380/  391]   Loss 0.065403   Top1 97.639803   Top5 99.995888   BatchTime 0.102831   LR 0.010000
INFO - ==> Top1: 97.636    Top5: 99.996    Loss: 0.066
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [19][   20/   79]   Loss 0.399088   Top1 89.648438   Top5 99.648438   BatchTime 0.125365
INFO - Validation [19][   40/   79]   Loss 0.428623   Top1 89.414062   Top5 99.492188   BatchTime 0.076821
INFO - Validation [19][   60/   79]   Loss 0.418089   Top1 89.609375   Top5 99.557292   BatchTime 0.066792
INFO - ==> Top1: 89.630    Top5: 99.570    Loss: 0.411
INFO - Scoreboard best 1 ==> Epoch [16][Top1: 90.200   Top5: 99.640] Sparsity : 0.706
INFO - Scoreboard best 2 ==> Epoch [17][Top1: 89.950   Top5: 99.600] Sparsity : 0.724
INFO - Scoreboard best 3 ==> Epoch [12][Top1: 89.800   Top5: 99.640] Sparsity : 0.695
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  20
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [20][   20/  391]   Loss 0.063529   Top1 97.656250   Top5 100.000000   BatchTime 0.194561   LR 0.001000
INFO - Training [20][   40/  391]   Loss 0.066328   Top1 97.656250   Top5 100.000000   BatchTime 0.147154   LR 0.001000
INFO - Training [20][   60/  391]   Loss 0.059575   Top1 97.903646   Top5 100.000000   BatchTime 0.131344   LR 0.001000
INFO - Training [20][   80/  391]   Loss 0.058057   Top1 98.037109   Top5 100.000000   BatchTime 0.123891   LR 0.001000
INFO - Training [20][  100/  391]   Loss 0.059177   Top1 98.039062   Top5 100.000000   BatchTime 0.119212   LR 0.001000
INFO - Training [20][  120/  391]   Loss 0.059313   Top1 98.027344   Top5 100.000000   BatchTime 0.115962   LR 0.001000
INFO - Training [20][  140/  391]   Loss 0.059109   Top1 98.035714   Top5 100.000000   BatchTime 0.113612   LR 0.001000
INFO - Training [20][  160/  391]   Loss 0.058640   Top1 98.037109   Top5 100.000000   BatchTime 0.111914   LR 0.001000
INFO - Training [20][  180/  391]   Loss 0.057794   Top1 98.068576   Top5 100.000000   BatchTime 0.110595   LR 0.001000
INFO - Training [20][  200/  391]   Loss 0.057045   Top1 98.070312   Top5 100.000000   BatchTime 0.109475   LR 0.001000
INFO - Training [20][  220/  391]   Loss 0.056596   Top1 98.085938   Top5 100.000000   BatchTime 0.108620   LR 0.001000
INFO - Training [20][  240/  391]   Loss 0.055985   Top1 98.085938   Top5 100.000000   BatchTime 0.107942   LR 0.001000
INFO - Training [20][  260/  391]   Loss 0.055427   Top1 98.100962   Top5 100.000000   BatchTime 0.107274   LR 0.001000
INFO - Training [20][  280/  391]   Loss 0.054655   Top1 98.122210   Top5 100.000000   BatchTime 0.106734   LR 0.001000
INFO - Training [20][  300/  391]   Loss 0.053847   Top1 98.151042   Top5 100.000000   BatchTime 0.106141   LR 0.001000
INFO - Training [20][  320/  391]   Loss 0.053675   Top1 98.168945   Top5 100.000000   BatchTime 0.105642   LR 0.001000
INFO - Training [20][  340/  391]   Loss 0.053379   Top1 98.166360   Top5 100.000000   BatchTime 0.105140   LR 0.001000
INFO - Training [20][  360/  391]   Loss 0.053443   Top1 98.166233   Top5 100.000000   BatchTime 0.103583   LR 0.001000
INFO - Training [20][  380/  391]   Loss 0.053288   Top1 98.168174   Top5 100.000000   BatchTime 0.102390   LR 0.001000
INFO - ==> Top1: 98.176    Top5: 100.000    Loss: 0.053
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [20][   20/   79]   Loss 0.374127   Top1 90.234375   Top5 99.570312   BatchTime 0.126167
INFO - Validation [20][   40/   79]   Loss 0.395263   Top1 90.253906   Top5 99.511719   BatchTime 0.082270
INFO - Validation [20][   60/   79]   Loss 0.385775   Top1 90.351562   Top5 99.570312   BatchTime 0.068488
INFO - ==> Top1: 90.420    Top5: 99.580    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [20][Top1: 90.420   Top5: 99.580] Sparsity : 0.735
INFO - Scoreboard best 2 ==> Epoch [16][Top1: 90.200   Top5: 99.640] Sparsity : 0.706
INFO - Scoreboard best 3 ==> Epoch [17][Top1: 89.950   Top5: 99.600] Sparsity : 0.724
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  21
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [21][   20/  391]   Loss 0.045800   Top1 98.398438   Top5 100.000000   BatchTime 0.196930   LR 0.001000
INFO - Training [21][   40/  391]   Loss 0.044844   Top1 98.554688   Top5 100.000000   BatchTime 0.148574   LR 0.001000
INFO - Training [21][   60/  391]   Loss 0.046505   Top1 98.541667   Top5 100.000000   BatchTime 0.132682   LR 0.001000
INFO - Training [21][   80/  391]   Loss 0.046603   Top1 98.447266   Top5 100.000000   BatchTime 0.124529   LR 0.001000
INFO - Training [21][  100/  391]   Loss 0.046011   Top1 98.484375   Top5 100.000000   BatchTime 0.119617   LR 0.001000
INFO - Training [21][  120/  391]   Loss 0.047438   Top1 98.411458   Top5 100.000000   BatchTime 0.116323   LR 0.001000
INFO - Training [21][  140/  391]   Loss 0.045805   Top1 98.443080   Top5 100.000000   BatchTime 0.113943   LR 0.001000
INFO - Training [21][  160/  391]   Loss 0.045663   Top1 98.447266   Top5 100.000000   BatchTime 0.112251   LR 0.001000
INFO - Training [21][  180/  391]   Loss 0.045092   Top1 98.472222   Top5 100.000000   BatchTime 0.110927   LR 0.001000
INFO - Training [21][  200/  391]   Loss 0.044797   Top1 98.480469   Top5 100.000000   BatchTime 0.109755   LR 0.001000
INFO - Training [21][  220/  391]   Loss 0.046166   Top1 98.423295   Top5 100.000000   BatchTime 0.108873   LR 0.001000
INFO - Training [21][  240/  391]   Loss 0.045437   Top1 98.450521   Top5 100.000000   BatchTime 0.108143   LR 0.001000
INFO - Training [21][  260/  391]   Loss 0.044123   Top1 98.518630   Top5 100.000000   BatchTime 0.107498   LR 0.001000
INFO - Training [21][  280/  391]   Loss 0.043422   Top1 98.543527   Top5 100.000000   BatchTime 0.106904   LR 0.001000
INFO - Training [21][  300/  391]   Loss 0.043236   Top1 98.541667   Top5 99.997396   BatchTime 0.106367   LR 0.001000
INFO - Training [21][  320/  391]   Loss 0.043440   Top1 98.540039   Top5 99.997559   BatchTime 0.105878   LR 0.001000
INFO - Training [21][  340/  391]   Loss 0.043955   Top1 98.506434   Top5 99.997702   BatchTime 0.105371   LR 0.001000
INFO - Training [21][  360/  391]   Loss 0.044133   Top1 98.502604   Top5 99.997830   BatchTime 0.103754   LR 0.001000
INFO - Training [21][  380/  391]   Loss 0.044548   Top1 98.474507   Top5 99.997944   BatchTime 0.102575   LR 0.001000
INFO - ==> Top1: 98.456    Top5: 99.998    Loss: 0.045
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [21][   20/   79]   Loss 0.382407   Top1 90.468750   Top5 99.609375   BatchTime 0.125411
INFO - Validation [21][   40/   79]   Loss 0.395730   Top1 90.273438   Top5 99.511719   BatchTime 0.082977
INFO - Validation [21][   60/   79]   Loss 0.386637   Top1 90.507812   Top5 99.583333   BatchTime 0.070475
INFO - ==> Top1: 90.470    Top5: 99.600    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [21][Top1: 90.470   Top5: 99.600] Sparsity : 0.736
INFO - Scoreboard best 2 ==> Epoch [20][Top1: 90.420   Top5: 99.580] Sparsity : 0.735
INFO - Scoreboard best 3 ==> Epoch [16][Top1: 90.200   Top5: 99.640] Sparsity : 0.706
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  22
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [22][   20/  391]   Loss 0.038167   Top1 98.710938   Top5 100.000000   BatchTime 0.199714   LR 0.001000
INFO - Training [22][   40/  391]   Loss 0.043714   Top1 98.535156   Top5 100.000000   BatchTime 0.149813   LR 0.001000
INFO - Training [22][   60/  391]   Loss 0.041077   Top1 98.619792   Top5 100.000000   BatchTime 0.133095   LR 0.001000
INFO - Training [22][   80/  391]   Loss 0.041862   Top1 98.613281   Top5 99.990234   BatchTime 0.124657   LR 0.001000
INFO - Training [22][  100/  391]   Loss 0.042647   Top1 98.554688   Top5 99.992188   BatchTime 0.119939   LR 0.001000
INFO - Training [22][  120/  391]   Loss 0.043974   Top1 98.470052   Top5 99.993490   BatchTime 0.116511   LR 0.001000
INFO - Training [22][  140/  391]   Loss 0.044513   Top1 98.448661   Top5 99.994420   BatchTime 0.114371   LR 0.001000
INFO - Training [22][  160/  391]   Loss 0.044957   Top1 98.471680   Top5 99.995117   BatchTime 0.112777   LR 0.001000
INFO - Training [22][  180/  391]   Loss 0.045713   Top1 98.415799   Top5 99.995660   BatchTime 0.111334   LR 0.001000
INFO - Training [22][  200/  391]   Loss 0.046150   Top1 98.398438   Top5 99.996094   BatchTime 0.110227   LR 0.001000
INFO - Training [22][  220/  391]   Loss 0.045227   Top1 98.430398   Top5 99.996449   BatchTime 0.109347   LR 0.001000
INFO - Training [22][  240/  391]   Loss 0.044515   Top1 98.466797   Top5 99.996745   BatchTime 0.108545   LR 0.001000
INFO - Training [22][  260/  391]   Loss 0.044287   Top1 98.491587   Top5 99.996995   BatchTime 0.107894   LR 0.001000
INFO - Training [22][  280/  391]   Loss 0.044239   Top1 98.496094   Top5 99.997210   BatchTime 0.107301   LR 0.001000
INFO - Training [22][  300/  391]   Loss 0.044415   Top1 98.492188   Top5 99.992188   BatchTime 0.106726   LR 0.001000
INFO - Training [22][  320/  391]   Loss 0.043912   Top1 98.513184   Top5 99.992676   BatchTime 0.106225   LR 0.001000
INFO - Training [22][  340/  391]   Loss 0.043654   Top1 98.517923   Top5 99.993107   BatchTime 0.105700   LR 0.001000
INFO - Training [22][  360/  391]   Loss 0.043419   Top1 98.526476   Top5 99.993490   BatchTime 0.103814   LR 0.001000
INFO - Training [22][  380/  391]   Loss 0.043700   Top1 98.517681   Top5 99.993832   BatchTime 0.102850   LR 0.001000
INFO - ==> Top1: 98.512    Top5: 99.994    Loss: 0.044
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [22][   20/   79]   Loss 0.380970   Top1 90.156250   Top5 99.609375   BatchTime 0.135972
INFO - Validation [22][   40/   79]   Loss 0.395273   Top1 90.292969   Top5 99.492188   BatchTime 0.089718
INFO - Validation [22][   60/   79]   Loss 0.387075   Top1 90.494792   Top5 99.557292   BatchTime 0.074888
INFO - ==> Top1: 90.550    Top5: 99.580    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [22][Top1: 90.550   Top5: 99.580] Sparsity : 0.736
INFO - Scoreboard best 2 ==> Epoch [21][Top1: 90.470   Top5: 99.600] Sparsity : 0.736
INFO - Scoreboard best 3 ==> Epoch [20][Top1: 90.420   Top5: 99.580] Sparsity : 0.735
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  23
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [23][   20/  391]   Loss 0.042565   Top1 98.359375   Top5 100.000000   BatchTime 0.192261   LR 0.001000
INFO - Training [23][   40/  391]   Loss 0.041825   Top1 98.476562   Top5 100.000000   BatchTime 0.146688   LR 0.001000
INFO - Training [23][   60/  391]   Loss 0.042371   Top1 98.450521   Top5 100.000000   BatchTime 0.131431   LR 0.001000
INFO - Training [23][   80/  391]   Loss 0.043831   Top1 98.339844   Top5 100.000000   BatchTime 0.123984   LR 0.001000
INFO - Training [23][  100/  391]   Loss 0.041967   Top1 98.476562   Top5 100.000000   BatchTime 0.119242   LR 0.001000
INFO - Training [23][  120/  391]   Loss 0.041247   Top1 98.496094   Top5 100.000000   BatchTime 0.116022   LR 0.001000
INFO - Training [23][  140/  391]   Loss 0.041869   Top1 98.510045   Top5 100.000000   BatchTime 0.113799   LR 0.001000
INFO - Training [23][  160/  391]   Loss 0.042019   Top1 98.520508   Top5 100.000000   BatchTime 0.112099   LR 0.001000
INFO - Training [23][  180/  391]   Loss 0.042140   Top1 98.519965   Top5 100.000000   BatchTime 0.110876   LR 0.001000
INFO - Training [23][  200/  391]   Loss 0.042882   Top1 98.511719   Top5 100.000000   BatchTime 0.109912   LR 0.001000
INFO - Training [23][  220/  391]   Loss 0.041974   Top1 98.554688   Top5 100.000000   BatchTime 0.109048   LR 0.001000
INFO - Training [23][  240/  391]   Loss 0.042160   Top1 98.544922   Top5 100.000000   BatchTime 0.108295   LR 0.001000
INFO - Training [23][  260/  391]   Loss 0.041872   Top1 98.572716   Top5 100.000000   BatchTime 0.107722   LR 0.001000
INFO - Training [23][  280/  391]   Loss 0.042121   Top1 98.551897   Top5 100.000000   BatchTime 0.107179   LR 0.001000
INFO - Training [23][  300/  391]   Loss 0.042097   Top1 98.557292   Top5 100.000000   BatchTime 0.106646   LR 0.001000
INFO - Training [23][  320/  391]   Loss 0.042679   Top1 98.530273   Top5 100.000000   BatchTime 0.106232   LR 0.001000
INFO - Training [23][  340/  391]   Loss 0.042470   Top1 98.540901   Top5 100.000000   BatchTime 0.105556   LR 0.001000
INFO - Training [23][  360/  391]   Loss 0.042330   Top1 98.543837   Top5 100.000000   BatchTime 0.103985   LR 0.001000
INFO - Training [23][  380/  391]   Loss 0.041896   Top1 98.564967   Top5 100.000000   BatchTime 0.102813   LR 0.001000
INFO - ==> Top1: 98.570    Top5: 100.000    Loss: 0.042
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [23][   20/   79]   Loss 0.371429   Top1 90.468750   Top5 99.570312   BatchTime 0.142609
INFO - Validation [23][   40/   79]   Loss 0.392002   Top1 90.410156   Top5 99.531250   BatchTime 0.093556
INFO - Validation [23][   60/   79]   Loss 0.385353   Top1 90.533854   Top5 99.544271   BatchTime 0.077364
INFO - ==> Top1: 90.630    Top5: 99.580    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [23][Top1: 90.630   Top5: 99.580] Sparsity : 0.736
INFO - Scoreboard best 2 ==> Epoch [22][Top1: 90.550   Top5: 99.580] Sparsity : 0.736
INFO - Scoreboard best 3 ==> Epoch [21][Top1: 90.470   Top5: 99.600] Sparsity : 0.736
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  24
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [24][   20/  391]   Loss 0.045454   Top1 98.515625   Top5 100.000000   BatchTime 0.196458   LR 0.001000
INFO - Training [24][   40/  391]   Loss 0.036651   Top1 98.828125   Top5 100.000000   BatchTime 0.148768   LR 0.001000
INFO - Training [24][   60/  391]   Loss 0.039058   Top1 98.736979   Top5 100.000000   BatchTime 0.132751   LR 0.001000
INFO - Training [24][   80/  391]   Loss 0.038159   Top1 98.730469   Top5 100.000000   BatchTime 0.125682   LR 0.001000
INFO - Training [24][  100/  391]   Loss 0.038760   Top1 98.734375   Top5 100.000000   BatchTime 0.120762   LR 0.001000
INFO - Training [24][  120/  391]   Loss 0.038638   Top1 98.736979   Top5 100.000000   BatchTime 0.117308   LR 0.001000
INFO - Training [24][  140/  391]   Loss 0.037946   Top1 98.750000   Top5 100.000000   BatchTime 0.114738   LR 0.001000
INFO - Training [24][  160/  391]   Loss 0.038295   Top1 98.740234   Top5 100.000000   BatchTime 0.113083   LR 0.001000
INFO - Training [24][  180/  391]   Loss 0.037899   Top1 98.732639   Top5 100.000000   BatchTime 0.111756   LR 0.001000
INFO - Training [24][  200/  391]   Loss 0.038627   Top1 98.710938   Top5 100.000000   BatchTime 0.110662   LR 0.001000
INFO - Training [24][  220/  391]   Loss 0.038637   Top1 98.728693   Top5 100.000000   BatchTime 0.109762   LR 0.001000
INFO - Training [24][  240/  391]   Loss 0.038124   Top1 98.746745   Top5 100.000000   BatchTime 0.109007   LR 0.001000
INFO - Training [24][  260/  391]   Loss 0.037720   Top1 98.762019   Top5 100.000000   BatchTime 0.108371   LR 0.001000
INFO - Training [24][  280/  391]   Loss 0.038039   Top1 98.736049   Top5 100.000000   BatchTime 0.107737   LR 0.001000
INFO - Training [24][  300/  391]   Loss 0.038722   Top1 98.690104   Top5 100.000000   BatchTime 0.107127   LR 0.001000
INFO - Training [24][  320/  391]   Loss 0.039484   Top1 98.676758   Top5 100.000000   BatchTime 0.106615   LR 0.001000
INFO - Training [24][  340/  391]   Loss 0.039244   Top1 98.671875   Top5 100.000000   BatchTime 0.105314   LR 0.001000
INFO - Training [24][  360/  391]   Loss 0.039310   Top1 98.676215   Top5 100.000000   BatchTime 0.103900   LR 0.001000
INFO - Training [24][  380/  391]   Loss 0.039126   Top1 98.682155   Top5 100.000000   BatchTime 0.102534   LR 0.001000
INFO - ==> Top1: 98.682    Top5: 100.000    Loss: 0.039
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [24][   20/   79]   Loss 0.381637   Top1 90.546875   Top5 99.492188   BatchTime 0.144951
INFO - Validation [24][   40/   79]   Loss 0.393323   Top1 90.488281   Top5 99.433594   BatchTime 0.094912
INFO - Validation [24][   60/   79]   Loss 0.384404   Top1 90.716146   Top5 99.518229   BatchTime 0.077888
INFO - ==> Top1: 90.780    Top5: 99.570    Loss: 0.381
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 90.630   Top5: 99.580] Sparsity : 0.736
INFO - Scoreboard best 3 ==> Epoch [22][Top1: 90.550   Top5: 99.580] Sparsity : 0.736
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  25
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [25][   20/  391]   Loss 0.040426   Top1 98.828125   Top5 100.000000   BatchTime 0.197461   LR 0.001000
INFO - Training [25][   40/  391]   Loss 0.035711   Top1 98.867188   Top5 100.000000   BatchTime 0.148871   LR 0.001000
INFO - Training [25][   60/  391]   Loss 0.035714   Top1 98.828125   Top5 100.000000   BatchTime 0.132886   LR 0.001000
INFO - Training [25][   80/  391]   Loss 0.036886   Top1 98.818359   Top5 100.000000   BatchTime 0.123765   LR 0.001000
INFO - Training [25][  100/  391]   Loss 0.039356   Top1 98.726562   Top5 100.000000   BatchTime 0.119074   LR 0.001000
INFO - Training [25][  120/  391]   Loss 0.038243   Top1 98.769531   Top5 100.000000   BatchTime 0.115973   LR 0.001000
INFO - Training [25][  140/  391]   Loss 0.038980   Top1 98.688616   Top5 100.000000   BatchTime 0.113836   LR 0.001000
INFO - Training [25][  160/  391]   Loss 0.039661   Top1 98.666992   Top5 100.000000   BatchTime 0.112110   LR 0.001000
INFO - Training [25][  180/  391]   Loss 0.039282   Top1 98.689236   Top5 100.000000   BatchTime 0.110778   LR 0.001000
INFO - Training [25][  200/  391]   Loss 0.039619   Top1 98.691406   Top5 100.000000   BatchTime 0.109740   LR 0.001000
INFO - Training [25][  220/  391]   Loss 0.039545   Top1 98.689631   Top5 100.000000   BatchTime 0.108805   LR 0.001000
INFO - Training [25][  240/  391]   Loss 0.039573   Top1 98.665365   Top5 100.000000   BatchTime 0.108104   LR 0.001000
INFO - Training [25][  260/  391]   Loss 0.039098   Top1 98.674880   Top5 100.000000   BatchTime 0.107611   LR 0.001000
INFO - Training [25][  280/  391]   Loss 0.039091   Top1 98.677455   Top5 100.000000   BatchTime 0.107035   LR 0.001000
INFO - Training [25][  300/  391]   Loss 0.038967   Top1 98.692708   Top5 100.000000   BatchTime 0.106458   LR 0.001000
INFO - Training [25][  320/  391]   Loss 0.039265   Top1 98.684082   Top5 100.000000   BatchTime 0.105983   LR 0.001000
INFO - Training [25][  340/  391]   Loss 0.039746   Top1 98.676471   Top5 100.000000   BatchTime 0.104802   LR 0.001000
INFO - Training [25][  360/  391]   Loss 0.039817   Top1 98.663194   Top5 100.000000   BatchTime 0.103522   LR 0.001000
INFO - Training [25][  380/  391]   Loss 0.039970   Top1 98.649260   Top5 100.000000   BatchTime 0.102349   LR 0.001000
INFO - ==> Top1: 98.646    Top5: 100.000    Loss: 0.040
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [25][   20/   79]   Loss 0.376465   Top1 90.429688   Top5 99.648438   BatchTime 0.147710
INFO - Validation [25][   40/   79]   Loss 0.391892   Top1 90.371094   Top5 99.550781   BatchTime 0.096103
INFO - Validation [25][   60/   79]   Loss 0.385312   Top1 90.598958   Top5 99.609375   BatchTime 0.079108
INFO - ==> Top1: 90.610    Top5: 99.640    Loss: 0.384
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 2 ==> Epoch [23][Top1: 90.630   Top5: 99.580] Sparsity : 0.736
INFO - Scoreboard best 3 ==> Epoch [25][Top1: 90.610   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  26
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [26][   20/  391]   Loss 0.037706   Top1 98.671875   Top5 100.000000   BatchTime 0.195962   LR 0.001000
INFO - Training [26][   40/  391]   Loss 0.036214   Top1 98.828125   Top5 100.000000   BatchTime 0.148024   LR 0.001000
INFO - Training [26][   60/  391]   Loss 0.036160   Top1 98.776042   Top5 100.000000   BatchTime 0.132096   LR 0.001000
INFO - Training [26][   80/  391]   Loss 0.037974   Top1 98.730469   Top5 100.000000   BatchTime 0.124271   LR 0.001000
INFO - Training [26][  100/  391]   Loss 0.038063   Top1 98.726562   Top5 100.000000   BatchTime 0.119598   LR 0.001000
INFO - Training [26][  120/  391]   Loss 0.037209   Top1 98.710938   Top5 100.000000   BatchTime 0.116330   LR 0.001000
INFO - Training [26][  140/  391]   Loss 0.036677   Top1 98.727679   Top5 100.000000   BatchTime 0.115011   LR 0.001000
INFO - Training [26][  160/  391]   Loss 0.037218   Top1 98.710938   Top5 100.000000   BatchTime 0.113246   LR 0.001000
INFO - Training [26][  180/  391]   Loss 0.037418   Top1 98.706597   Top5 100.000000   BatchTime 0.111849   LR 0.001000
INFO - Training [26][  200/  391]   Loss 0.036944   Top1 98.718750   Top5 100.000000   BatchTime 0.110699   LR 0.001000
INFO - Training [26][  220/  391]   Loss 0.037656   Top1 98.703835   Top5 100.000000   BatchTime 0.109766   LR 0.001000
INFO - Training [26][  240/  391]   Loss 0.037208   Top1 98.723958   Top5 100.000000   BatchTime 0.108989   LR 0.001000
INFO - Training [26][  260/  391]   Loss 0.036995   Top1 98.725962   Top5 100.000000   BatchTime 0.108283   LR 0.001000
INFO - Training [26][  280/  391]   Loss 0.036449   Top1 98.747210   Top5 100.000000   BatchTime 0.107568   LR 0.001000
INFO - Training [26][  300/  391]   Loss 0.036217   Top1 98.755208   Top5 100.000000   BatchTime 0.106954   LR 0.001000
INFO - Training [26][  320/  391]   Loss 0.036753   Top1 98.740234   Top5 100.000000   BatchTime 0.106730   LR 0.001000
INFO - Training [26][  340/  391]   Loss 0.036991   Top1 98.720129   Top5 99.997702   BatchTime 0.104817   LR 0.001000
INFO - Training [26][  360/  391]   Loss 0.037176   Top1 98.706597   Top5 99.997830   BatchTime 0.103693   LR 0.001000
INFO - Training [26][  380/  391]   Loss 0.037524   Top1 98.692434   Top5 99.997944   BatchTime 0.102465   LR 0.001000
INFO - ==> Top1: 98.698    Top5: 99.998    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [26][   20/   79]   Loss 0.385890   Top1 90.390625   Top5 99.609375   BatchTime 0.144644
INFO - Validation [26][   40/   79]   Loss 0.396848   Top1 90.507812   Top5 99.550781   BatchTime 0.095172
INFO - Validation [26][   60/   79]   Loss 0.388705   Top1 90.690104   Top5 99.596354   BatchTime 0.078910
INFO - ==> Top1: 90.750    Top5: 99.640    Loss: 0.383
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 90.630   Top5: 99.580] Sparsity : 0.736
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  27
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [27][   20/  391]   Loss 0.034027   Top1 98.984375   Top5 100.000000   BatchTime 0.194581   LR 0.001000
INFO - Training [27][   40/  391]   Loss 0.038485   Top1 98.769531   Top5 100.000000   BatchTime 0.147540   LR 0.001000
INFO - Training [27][   60/  391]   Loss 0.037924   Top1 98.854167   Top5 100.000000   BatchTime 0.131797   LR 0.001000
INFO - Training [27][   80/  391]   Loss 0.039032   Top1 98.789062   Top5 100.000000   BatchTime 0.124077   LR 0.001000
INFO - Training [27][  100/  391]   Loss 0.038331   Top1 98.796875   Top5 100.000000   BatchTime 0.119132   LR 0.001000
INFO - Training [27][  120/  391]   Loss 0.037749   Top1 98.808594   Top5 100.000000   BatchTime 0.116059   LR 0.001000
INFO - Training [27][  140/  391]   Loss 0.037154   Top1 98.833705   Top5 99.994420   BatchTime 0.113776   LR 0.001000
INFO - Training [27][  160/  391]   Loss 0.036266   Top1 98.862305   Top5 99.995117   BatchTime 0.112103   LR 0.001000
INFO - Training [27][  180/  391]   Loss 0.036236   Top1 98.867188   Top5 99.995660   BatchTime 0.110805   LR 0.001000
INFO - Training [27][  200/  391]   Loss 0.037516   Top1 98.785156   Top5 99.996094   BatchTime 0.109723   LR 0.001000
INFO - Training [27][  220/  391]   Loss 0.037692   Top1 98.767756   Top5 99.996449   BatchTime 0.108875   LR 0.001000
INFO - Training [27][  240/  391]   Loss 0.037701   Top1 98.769531   Top5 99.996745   BatchTime 0.108260   LR 0.001000
INFO - Training [27][  260/  391]   Loss 0.037267   Top1 98.771034   Top5 99.996995   BatchTime 0.107609   LR 0.001000
INFO - Training [27][  280/  391]   Loss 0.037121   Top1 98.766741   Top5 99.997210   BatchTime 0.106999   LR 0.001000
INFO - Training [27][  300/  391]   Loss 0.037032   Top1 98.773438   Top5 99.997396   BatchTime 0.106453   LR 0.001000
INFO - Training [27][  320/  391]   Loss 0.036636   Top1 98.784180   Top5 99.997559   BatchTime 0.105961   LR 0.001000
INFO - Training [27][  340/  391]   Loss 0.036406   Top1 98.793658   Top5 99.997702   BatchTime 0.104292   LR 0.001000
INFO - Training [27][  360/  391]   Loss 0.036708   Top1 98.786892   Top5 99.997830   BatchTime 0.103114   LR 0.001000
INFO - Training [27][  380/  391]   Loss 0.036881   Top1 98.778783   Top5 99.997944   BatchTime 0.101938   LR 0.001000
INFO - ==> Top1: 98.776    Top5: 99.998    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [27][   20/   79]   Loss 0.383793   Top1 90.312500   Top5 99.570312   BatchTime 0.142734
INFO - Validation [27][   40/   79]   Loss 0.399553   Top1 90.273438   Top5 99.472656   BatchTime 0.094466
INFO - Validation [27][   60/   79]   Loss 0.388814   Top1 90.598958   Top5 99.531250   BatchTime 0.078193
INFO - ==> Top1: 90.520    Top5: 99.570    Loss: 0.387
INFO - Scoreboard best 1 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 2 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [23][Top1: 90.630   Top5: 99.580] Sparsity : 0.736
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  28
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [28][   20/  391]   Loss 0.032260   Top1 98.984375   Top5 100.000000   BatchTime 0.196560   LR 0.001000
INFO - Training [28][   40/  391]   Loss 0.037906   Top1 98.710938   Top5 100.000000   BatchTime 0.149110   LR 0.001000
INFO - Training [28][   60/  391]   Loss 0.038764   Top1 98.736979   Top5 100.000000   BatchTime 0.133309   LR 0.001000
INFO - Training [28][   80/  391]   Loss 0.038129   Top1 98.759766   Top5 100.000000   BatchTime 0.125112   LR 0.001000
INFO - Training [28][  100/  391]   Loss 0.038390   Top1 98.734375   Top5 100.000000   BatchTime 0.120034   LR 0.001000
INFO - Training [28][  120/  391]   Loss 0.038091   Top1 98.730469   Top5 100.000000   BatchTime 0.116797   LR 0.001000
INFO - Training [28][  140/  391]   Loss 0.037356   Top1 98.750000   Top5 100.000000   BatchTime 0.114446   LR 0.001000
INFO - Training [28][  160/  391]   Loss 0.037496   Top1 98.730469   Top5 100.000000   BatchTime 0.112793   LR 0.001000
INFO - Training [28][  180/  391]   Loss 0.036913   Top1 98.754340   Top5 100.000000   BatchTime 0.111343   LR 0.001000
INFO - Training [28][  200/  391]   Loss 0.036812   Top1 98.753906   Top5 100.000000   BatchTime 0.110633   LR 0.001000
INFO - Training [28][  220/  391]   Loss 0.036196   Top1 98.767756   Top5 100.000000   BatchTime 0.109702   LR 0.001000
INFO - Training [28][  240/  391]   Loss 0.035801   Top1 98.782552   Top5 100.000000   BatchTime 0.108890   LR 0.001000
INFO - Training [28][  260/  391]   Loss 0.036164   Top1 98.777043   Top5 100.000000   BatchTime 0.108223   LR 0.001000
INFO - Training [28][  280/  391]   Loss 0.036940   Top1 98.761161   Top5 100.000000   BatchTime 0.107560   LR 0.001000
INFO - Training [28][  300/  391]   Loss 0.036659   Top1 98.747396   Top5 100.000000   BatchTime 0.107026   LR 0.001000
INFO - Training [28][  320/  391]   Loss 0.037265   Top1 98.710938   Top5 100.000000   BatchTime 0.106086   LR 0.001000
INFO - Training [28][  340/  391]   Loss 0.037501   Top1 98.699449   Top5 100.000000   BatchTime 0.104544   LR 0.001000
INFO - Training [28][  360/  391]   Loss 0.037792   Top1 98.682726   Top5 99.997830   BatchTime 0.103112   LR 0.001000
INFO - Training [28][  380/  391]   Loss 0.038226   Top1 98.673931   Top5 99.997944   BatchTime 0.102107   LR 0.001000
INFO - ==> Top1: 98.670    Top5: 99.998    Loss: 0.038
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [28][   20/   79]   Loss 0.379617   Top1 90.390625   Top5 99.570312   BatchTime 0.144522
INFO - Validation [28][   40/   79]   Loss 0.399843   Top1 90.527344   Top5 99.492188   BatchTime 0.094494
INFO - Validation [28][   60/   79]   Loss 0.389656   Top1 90.820312   Top5 99.531250   BatchTime 0.077880
INFO - ==> Top1: 90.780    Top5: 99.570    Loss: 0.386
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
                Best: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_best.pth.tar
INFO - >>>>>>>> Epoch  29
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [29][   20/  391]   Loss 0.032033   Top1 98.828125   Top5 100.000000   BatchTime 0.197505   LR 0.001000
INFO - Training [29][   40/  391]   Loss 0.032505   Top1 98.828125   Top5 100.000000   BatchTime 0.149451   LR 0.001000
INFO - Training [29][   60/  391]   Loss 0.033419   Top1 98.710938   Top5 100.000000   BatchTime 0.132943   LR 0.001000
INFO - Training [29][   80/  391]   Loss 0.034821   Top1 98.662109   Top5 100.000000   BatchTime 0.124855   LR 0.001000
INFO - Training [29][  100/  391]   Loss 0.034754   Top1 98.703125   Top5 100.000000   BatchTime 0.120087   LR 0.001000
INFO - Training [29][  120/  391]   Loss 0.032939   Top1 98.802083   Top5 100.000000   BatchTime 0.116814   LR 0.001000
INFO - Training [29][  140/  391]   Loss 0.033494   Top1 98.805804   Top5 100.000000   BatchTime 0.114482   LR 0.001000
INFO - Training [29][  160/  391]   Loss 0.034510   Top1 98.779297   Top5 100.000000   BatchTime 0.112789   LR 0.001000
INFO - Training [29][  180/  391]   Loss 0.034723   Top1 98.780382   Top5 100.000000   BatchTime 0.111361   LR 0.001000
INFO - Training [29][  200/  391]   Loss 0.035739   Top1 98.757812   Top5 99.996094   BatchTime 0.110276   LR 0.001000
INFO - Training [29][  220/  391]   Loss 0.035438   Top1 98.781960   Top5 99.996449   BatchTime 0.109469   LR 0.001000
INFO - Training [29][  240/  391]   Loss 0.035127   Top1 98.802083   Top5 99.996745   BatchTime 0.108688   LR 0.001000
INFO - Training [29][  260/  391]   Loss 0.035057   Top1 98.822115   Top5 99.990986   BatchTime 0.107906   LR 0.001000
INFO - Training [29][  280/  391]   Loss 0.034897   Top1 98.828125   Top5 99.991629   BatchTime 0.107249   LR 0.001000
INFO - Training [29][  300/  391]   Loss 0.034299   Top1 98.854167   Top5 99.992188   BatchTime 0.106729   LR 0.001000
INFO - Training [29][  320/  391]   Loss 0.034605   Top1 98.835449   Top5 99.992676   BatchTime 0.105624   LR 0.001000
INFO - Training [29][  340/  391]   Loss 0.034450   Top1 98.839614   Top5 99.993107   BatchTime 0.104316   LR 0.001000
INFO - Training [29][  360/  391]   Loss 0.034761   Top1 98.825955   Top5 99.993490   BatchTime 0.102956   LR 0.001000
INFO - Training [29][  380/  391]   Loss 0.035366   Top1 98.801398   Top5 99.993832   BatchTime 0.101998   LR 0.001000
INFO - ==> Top1: 98.800    Top5: 99.994    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [29][   20/   79]   Loss 0.386714   Top1 90.390625   Top5 99.531250   BatchTime 0.140741
INFO - Validation [29][   40/   79]   Loss 0.399753   Top1 90.351562   Top5 99.511719   BatchTime 0.093687
INFO - Validation [29][   60/   79]   Loss 0.393321   Top1 90.481771   Top5 99.583333   BatchTime 0.077439
INFO - ==> Top1: 90.540    Top5: 99.590    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  30
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [30][   20/  391]   Loss 0.038427   Top1 98.632812   Top5 100.000000   BatchTime 0.193760   LR 0.001000
INFO - Training [30][   40/  391]   Loss 0.035250   Top1 98.886719   Top5 100.000000   BatchTime 0.147044   LR 0.001000
INFO - Training [30][   60/  391]   Loss 0.036385   Top1 98.789062   Top5 100.000000   BatchTime 0.131585   LR 0.001000
INFO - Training [30][   80/  391]   Loss 0.036403   Top1 98.720703   Top5 100.000000   BatchTime 0.123882   LR 0.001000
INFO - Training [30][  100/  391]   Loss 0.036430   Top1 98.703125   Top5 100.000000   BatchTime 0.119161   LR 0.001000
INFO - Training [30][  120/  391]   Loss 0.036900   Top1 98.684896   Top5 100.000000   BatchTime 0.116057   LR 0.001000
INFO - Training [30][  140/  391]   Loss 0.035770   Top1 98.733259   Top5 100.000000   BatchTime 0.113822   LR 0.001000
INFO - Training [30][  160/  391]   Loss 0.034745   Top1 98.769531   Top5 100.000000   BatchTime 0.111984   LR 0.001000
INFO - Training [30][  180/  391]   Loss 0.033982   Top1 98.789062   Top5 100.000000   BatchTime 0.110643   LR 0.001000
INFO - Training [30][  200/  391]   Loss 0.034508   Top1 98.777344   Top5 100.000000   BatchTime 0.109528   LR 0.001000
INFO - Training [30][  220/  391]   Loss 0.034181   Top1 98.789062   Top5 100.000000   BatchTime 0.108662   LR 0.001000
INFO - Training [30][  240/  391]   Loss 0.034002   Top1 98.798828   Top5 100.000000   BatchTime 0.108332   LR 0.001000
INFO - Training [30][  260/  391]   Loss 0.034428   Top1 98.792067   Top5 100.000000   BatchTime 0.107589   LR 0.001000
INFO - Training [30][  280/  391]   Loss 0.034719   Top1 98.777902   Top5 100.000000   BatchTime 0.106959   LR 0.001000
INFO - Training [30][  300/  391]   Loss 0.034662   Top1 98.781250   Top5 100.000000   BatchTime 0.106463   LR 0.001000
INFO - Training [30][  320/  391]   Loss 0.035177   Top1 98.759766   Top5 100.000000   BatchTime 0.105295   LR 0.001000
INFO - Training [30][  340/  391]   Loss 0.035554   Top1 98.747702   Top5 100.000000   BatchTime 0.104008   LR 0.001000
INFO - Training [30][  360/  391]   Loss 0.035655   Top1 98.739149   Top5 100.000000   BatchTime 0.102777   LR 0.001000
INFO - Training [30][  380/  391]   Loss 0.035390   Top1 98.754112   Top5 100.000000   BatchTime 0.101794   LR 0.001000
INFO - ==> Top1: 98.752    Top5: 100.000    Loss: 0.035
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [30][   20/   79]   Loss 0.391828   Top1 90.585938   Top5 99.531250   BatchTime 0.140869
INFO - Validation [30][   40/   79]   Loss 0.407528   Top1 90.429688   Top5 99.550781   BatchTime 0.092841
INFO - Validation [30][   60/   79]   Loss 0.398653   Top1 90.638021   Top5 99.609375   BatchTime 0.076928
INFO - ==> Top1: 90.620    Top5: 99.630    Loss: 0.393
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  31
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [31][   20/  391]   Loss 0.035206   Top1 98.828125   Top5 100.000000   BatchTime 0.195220   LR 0.001000
INFO - Training [31][   40/  391]   Loss 0.031414   Top1 99.062500   Top5 100.000000   BatchTime 0.148041   LR 0.001000
INFO - Training [31][   60/  391]   Loss 0.030863   Top1 99.036458   Top5 100.000000   BatchTime 0.132097   LR 0.001000
INFO - Training [31][   80/  391]   Loss 0.031569   Top1 99.003906   Top5 100.000000   BatchTime 0.124166   LR 0.001000
INFO - Training [31][  100/  391]   Loss 0.032447   Top1 98.929688   Top5 100.000000   BatchTime 0.119125   LR 0.001000
INFO - Training [31][  120/  391]   Loss 0.032790   Top1 98.925781   Top5 100.000000   BatchTime 0.115993   LR 0.001000
INFO - Training [31][  140/  391]   Loss 0.032642   Top1 98.934152   Top5 100.000000   BatchTime 0.113912   LR 0.001000
INFO - Training [31][  160/  391]   Loss 0.033090   Top1 98.920898   Top5 100.000000   BatchTime 0.112190   LR 0.001000
INFO - Training [31][  180/  391]   Loss 0.033242   Top1 98.914931   Top5 100.000000   BatchTime 0.110423   LR 0.001000
INFO - Training [31][  200/  391]   Loss 0.033654   Top1 98.886719   Top5 100.000000   BatchTime 0.109411   LR 0.001000
INFO - Training [31][  220/  391]   Loss 0.033864   Top1 98.874290   Top5 100.000000   BatchTime 0.108664   LR 0.001000
INFO - Training [31][  240/  391]   Loss 0.033984   Top1 98.870443   Top5 100.000000   BatchTime 0.108002   LR 0.001000
INFO - Training [31][  260/  391]   Loss 0.033943   Top1 98.888221   Top5 100.000000   BatchTime 0.107310   LR 0.001000
INFO - Training [31][  280/  391]   Loss 0.034370   Top1 98.861607   Top5 100.000000   BatchTime 0.106678   LR 0.001000
INFO - Training [31][  300/  391]   Loss 0.034135   Top1 98.869792   Top5 100.000000   BatchTime 0.106094   LR 0.001000
INFO - Training [31][  320/  391]   Loss 0.033930   Top1 98.864746   Top5 100.000000   BatchTime 0.104966   LR 0.001000
INFO - Training [31][  340/  391]   Loss 0.034101   Top1 98.844210   Top5 100.000000   BatchTime 0.103743   LR 0.001000
INFO - Training [31][  360/  391]   Loss 0.034172   Top1 98.845486   Top5 100.000000   BatchTime 0.102346   LR 0.001000
INFO - Training [31][  380/  391]   Loss 0.034440   Top1 98.836349   Top5 100.000000   BatchTime 0.101317   LR 0.001000
INFO - ==> Top1: 98.840    Top5: 100.000    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [31][   20/   79]   Loss 0.388808   Top1 90.585938   Top5 99.570312   BatchTime 0.142060
INFO - Validation [31][   40/   79]   Loss 0.404656   Top1 90.468750   Top5 99.472656   BatchTime 0.093603
INFO - Validation [31][   60/   79]   Loss 0.396383   Top1 90.677083   Top5 99.570312   BatchTime 0.077342
INFO - ==> Top1: 90.690    Top5: 99.590    Loss: 0.391
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  32
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [32][   20/  391]   Loss 0.039658   Top1 98.867188   Top5 100.000000   BatchTime 0.194255   LR 0.001000
INFO - Training [32][   40/  391]   Loss 0.036792   Top1 98.828125   Top5 100.000000   BatchTime 0.147169   LR 0.001000
INFO - Training [32][   60/  391]   Loss 0.034359   Top1 98.867188   Top5 100.000000   BatchTime 0.131301   LR 0.001000
INFO - Training [32][   80/  391]   Loss 0.034114   Top1 98.867188   Top5 100.000000   BatchTime 0.123576   LR 0.001000
INFO - Training [32][  100/  391]   Loss 0.037559   Top1 98.773438   Top5 100.000000   BatchTime 0.118982   LR 0.001000
INFO - Training [32][  120/  391]   Loss 0.036449   Top1 98.782552   Top5 100.000000   BatchTime 0.115919   LR 0.001000
INFO - Training [32][  140/  391]   Loss 0.036404   Top1 98.772321   Top5 100.000000   BatchTime 0.113717   LR 0.001000
INFO - Training [32][  160/  391]   Loss 0.036697   Top1 98.764648   Top5 100.000000   BatchTime 0.111949   LR 0.001000
INFO - Training [32][  180/  391]   Loss 0.036104   Top1 98.793403   Top5 100.000000   BatchTime 0.110614   LR 0.001000
INFO - Training [32][  200/  391]   Loss 0.035809   Top1 98.792969   Top5 100.000000   BatchTime 0.109554   LR 0.001000
INFO - Training [32][  220/  391]   Loss 0.035813   Top1 98.792614   Top5 100.000000   BatchTime 0.108765   LR 0.001000
INFO - Training [32][  240/  391]   Loss 0.035556   Top1 98.795573   Top5 100.000000   BatchTime 0.108108   LR 0.001000
INFO - Training [32][  260/  391]   Loss 0.035646   Top1 98.783053   Top5 100.000000   BatchTime 0.107433   LR 0.001000
INFO - Training [32][  280/  391]   Loss 0.035733   Top1 98.786272   Top5 100.000000   BatchTime 0.106831   LR 0.001000
INFO - Training [32][  300/  391]   Loss 0.035744   Top1 98.802083   Top5 100.000000   BatchTime 0.106768   LR 0.001000
INFO - Training [32][  320/  391]   Loss 0.035900   Top1 98.801270   Top5 100.000000   BatchTime 0.105169   LR 0.001000
INFO - Training [32][  340/  391]   Loss 0.036454   Top1 98.770680   Top5 99.997702   BatchTime 0.103998   LR 0.001000
INFO - Training [32][  360/  391]   Loss 0.036886   Top1 98.756510   Top5 99.997830   BatchTime 0.102656   LR 0.001000
INFO - Training [32][  380/  391]   Loss 0.037192   Top1 98.747944   Top5 99.997944   BatchTime 0.101641   LR 0.001000
INFO - ==> Top1: 98.746    Top5: 99.998    Loss: 0.037
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [32][   20/   79]   Loss 0.388069   Top1 90.703125   Top5 99.531250   BatchTime 0.141025
INFO - Validation [32][   40/   79]   Loss 0.410451   Top1 90.566406   Top5 99.531250   BatchTime 0.093386
INFO - Validation [32][   60/   79]   Loss 0.400792   Top1 90.598958   Top5 99.609375   BatchTime 0.077325
INFO - ==> Top1: 90.590    Top5: 99.620    Loss: 0.397
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  33
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [33][   20/  391]   Loss 0.032377   Top1 98.867188   Top5 100.000000   BatchTime 0.194394   LR 0.001000
INFO - Training [33][   40/  391]   Loss 0.032158   Top1 98.808594   Top5 100.000000   BatchTime 0.147206   LR 0.001000
INFO - Training [33][   60/  391]   Loss 0.029521   Top1 98.971354   Top5 100.000000   BatchTime 0.131938   LR 0.001000
INFO - Training [33][   80/  391]   Loss 0.031483   Top1 98.867188   Top5 100.000000   BatchTime 0.124050   LR 0.001000
INFO - Training [33][  100/  391]   Loss 0.032047   Top1 98.828125   Top5 100.000000   BatchTime 0.119344   LR 0.001000
INFO - Training [33][  120/  391]   Loss 0.032350   Top1 98.782552   Top5 100.000000   BatchTime 0.116100   LR 0.001000
INFO - Training [33][  140/  391]   Loss 0.032605   Top1 98.794643   Top5 100.000000   BatchTime 0.113880   LR 0.001000
INFO - Training [33][  160/  391]   Loss 0.032640   Top1 98.803711   Top5 100.000000   BatchTime 0.112188   LR 0.001000
INFO - Training [33][  180/  391]   Loss 0.032744   Top1 98.823785   Top5 100.000000   BatchTime 0.110883   LR 0.001000
INFO - Training [33][  200/  391]   Loss 0.033171   Top1 98.808594   Top5 100.000000   BatchTime 0.109857   LR 0.001000
INFO - Training [33][  220/  391]   Loss 0.033318   Top1 98.817472   Top5 100.000000   BatchTime 0.108960   LR 0.001000
INFO - Training [33][  240/  391]   Loss 0.033270   Top1 98.815104   Top5 100.000000   BatchTime 0.108186   LR 0.001000
INFO - Training [33][  260/  391]   Loss 0.033057   Top1 98.834135   Top5 100.000000   BatchTime 0.107475   LR 0.001000
INFO - Training [33][  280/  391]   Loss 0.032824   Top1 98.869978   Top5 100.000000   BatchTime 0.106877   LR 0.001000
INFO - Training [33][  300/  391]   Loss 0.033291   Top1 98.856771   Top5 100.000000   BatchTime 0.106730   LR 0.001000
INFO - Training [33][  320/  391]   Loss 0.033410   Top1 98.854980   Top5 99.997559   BatchTime 0.104928   LR 0.001000
INFO - Training [33][  340/  391]   Loss 0.033772   Top1 98.835018   Top5 99.997702   BatchTime 0.103768   LR 0.001000
INFO - Training [33][  360/  391]   Loss 0.033386   Top1 98.845486   Top5 99.997830   BatchTime 0.102533   LR 0.001000
INFO - Training [33][  380/  391]   Loss 0.033573   Top1 98.846628   Top5 99.997944   BatchTime 0.101364   LR 0.001000
INFO - ==> Top1: 98.846    Top5: 99.998    Loss: 0.034
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [33][   20/   79]   Loss 0.391723   Top1 90.976562   Top5 99.531250   BatchTime 0.140455
INFO - Validation [33][   40/   79]   Loss 0.405941   Top1 90.703125   Top5 99.570312   BatchTime 0.092189
INFO - Validation [33][   60/   79]   Loss 0.399896   Top1 90.638021   Top5 99.596354   BatchTime 0.076436
INFO - ==> Top1: 90.580    Top5: 99.590    Loss: 0.398
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  34
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [34][   20/  391]   Loss 0.025772   Top1 99.101562   Top5 100.000000   BatchTime 0.195094   LR 0.001000
INFO - Training [34][   40/  391]   Loss 0.029062   Top1 99.042969   Top5 100.000000   BatchTime 0.148394   LR 0.001000
INFO - Training [34][   60/  391]   Loss 0.030048   Top1 98.971354   Top5 100.000000   BatchTime 0.132339   LR 0.001000
INFO - Training [34][   80/  391]   Loss 0.030603   Top1 98.984375   Top5 100.000000   BatchTime 0.124428   LR 0.001000
INFO - Training [34][  100/  391]   Loss 0.030087   Top1 99.007812   Top5 100.000000   BatchTime 0.119627   LR 0.001000
INFO - Training [34][  120/  391]   Loss 0.030591   Top1 99.010417   Top5 100.000000   BatchTime 0.116398   LR 0.001000
INFO - Training [34][  140/  391]   Loss 0.029746   Top1 99.029018   Top5 100.000000   BatchTime 0.114103   LR 0.001000
INFO - Training [34][  160/  391]   Loss 0.030921   Top1 98.984375   Top5 100.000000   BatchTime 0.112373   LR 0.001000
INFO - Training [34][  180/  391]   Loss 0.031066   Top1 98.980035   Top5 99.995660   BatchTime 0.111086   LR 0.001000
INFO - Training [34][  200/  391]   Loss 0.031365   Top1 98.957031   Top5 99.996094   BatchTime 0.110099   LR 0.001000
INFO - Training [34][  220/  391]   Loss 0.031494   Top1 98.955966   Top5 99.996449   BatchTime 0.109274   LR 0.001000
INFO - Training [34][  240/  391]   Loss 0.031060   Top1 98.971354   Top5 99.996745   BatchTime 0.108461   LR 0.001000
INFO - Training [34][  260/  391]   Loss 0.031346   Top1 98.957332   Top5 99.996995   BatchTime 0.107761   LR 0.001000
INFO - Training [34][  280/  391]   Loss 0.031603   Top1 98.942522   Top5 99.997210   BatchTime 0.107087   LR 0.001000
INFO - Training [34][  300/  391]   Loss 0.031696   Top1 98.940104   Top5 99.997396   BatchTime 0.106703   LR 0.001000
INFO - Training [34][  320/  391]   Loss 0.032104   Top1 98.913574   Top5 99.997559   BatchTime 0.104924   LR 0.001000
INFO - Training [34][  340/  391]   Loss 0.032320   Top1 98.903952   Top5 99.997702   BatchTime 0.103680   LR 0.001000
INFO - Training [34][  360/  391]   Loss 0.032563   Top1 98.897569   Top5 99.997830   BatchTime 0.102653   LR 0.001000
INFO - Training [34][  380/  391]   Loss 0.032441   Top1 98.895970   Top5 99.997944   BatchTime 0.101499   LR 0.001000
INFO - ==> Top1: 98.890    Top5: 99.998    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [34][   20/   79]   Loss 0.396783   Top1 90.234375   Top5 99.570312   BatchTime 0.137339
INFO - Validation [34][   40/   79]   Loss 0.411371   Top1 90.332031   Top5 99.492188   BatchTime 0.090980
INFO - Validation [34][   60/   79]   Loss 0.398486   Top1 90.585938   Top5 99.583333   BatchTime 0.075382
INFO - ==> Top1: 90.620    Top5: 99.620    Loss: 0.394
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  35
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [35][   20/  391]   Loss 0.030283   Top1 98.906250   Top5 100.000000   BatchTime 0.196691   LR 0.001000
INFO - Training [35][   40/  391]   Loss 0.030730   Top1 98.906250   Top5 100.000000   BatchTime 0.149174   LR 0.001000
INFO - Training [35][   60/  391]   Loss 0.029723   Top1 99.010417   Top5 100.000000   BatchTime 0.133597   LR 0.001000
INFO - Training [35][   80/  391]   Loss 0.031079   Top1 98.974609   Top5 100.000000   BatchTime 0.125274   LR 0.001000
INFO - Training [35][  100/  391]   Loss 0.031371   Top1 98.945312   Top5 100.000000   BatchTime 0.120273   LR 0.001000
INFO - Training [35][  120/  391]   Loss 0.032398   Top1 98.860677   Top5 100.000000   BatchTime 0.117104   LR 0.001000
INFO - Training [35][  140/  391]   Loss 0.031524   Top1 98.878348   Top5 100.000000   BatchTime 0.114823   LR 0.001000
INFO - Training [35][  160/  391]   Loss 0.031866   Top1 98.862305   Top5 100.000000   BatchTime 0.113033   LR 0.001000
INFO - Training [35][  180/  391]   Loss 0.032868   Top1 98.832465   Top5 100.000000   BatchTime 0.111671   LR 0.001000
INFO - Training [35][  200/  391]   Loss 0.032121   Top1 98.855469   Top5 100.000000   BatchTime 0.110662   LR 0.001000
INFO - Training [35][  220/  391]   Loss 0.032990   Top1 98.845881   Top5 100.000000   BatchTime 0.109800   LR 0.001000
INFO - Training [35][  240/  391]   Loss 0.033270   Top1 98.834635   Top5 100.000000   BatchTime 0.108954   LR 0.001000
INFO - Training [35][  260/  391]   Loss 0.032978   Top1 98.849159   Top5 100.000000   BatchTime 0.108228   LR 0.001000
INFO - Training [35][  280/  391]   Loss 0.033159   Top1 98.850446   Top5 100.000000   BatchTime 0.107585   LR 0.001000
INFO - Training [35][  300/  391]   Loss 0.033209   Top1 98.835938   Top5 100.000000   BatchTime 0.107237   LR 0.001000
INFO - Training [35][  320/  391]   Loss 0.032844   Top1 98.854980   Top5 100.000000   BatchTime 0.105475   LR 0.001000
INFO - Training [35][  340/  391]   Loss 0.032730   Top1 98.844210   Top5 100.000000   BatchTime 0.104285   LR 0.001000
INFO - Training [35][  360/  391]   Loss 0.033050   Top1 98.830295   Top5 100.000000   BatchTime 0.102977   LR 0.001000
INFO - Training [35][  380/  391]   Loss 0.032973   Top1 98.842516   Top5 100.000000   BatchTime 0.101721   LR 0.001000
INFO - ==> Top1: 98.828    Top5: 100.000    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [35][   20/   79]   Loss 0.397426   Top1 90.546875   Top5 99.531250   BatchTime 0.141316
INFO - Validation [35][   40/   79]   Loss 0.413704   Top1 90.312500   Top5 99.492188   BatchTime 0.093609
INFO - Validation [35][   60/   79]   Loss 0.405502   Top1 90.559896   Top5 99.544271   BatchTime 0.078301
INFO - ==> Top1: 90.620    Top5: 99.590    Loss: 0.399
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  36
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [36][   20/  391]   Loss 0.034339   Top1 98.789062   Top5 100.000000   BatchTime 0.194226   LR 0.001000
INFO - Training [36][   40/  391]   Loss 0.034791   Top1 98.750000   Top5 100.000000   BatchTime 0.147273   LR 0.001000
INFO - Training [36][   60/  391]   Loss 0.031490   Top1 98.880208   Top5 100.000000   BatchTime 0.131956   LR 0.001000
INFO - Training [36][   80/  391]   Loss 0.032343   Top1 98.828125   Top5 100.000000   BatchTime 0.124152   LR 0.001000
INFO - Training [36][  100/  391]   Loss 0.031890   Top1 98.875000   Top5 100.000000   BatchTime 0.119393   LR 0.001000
INFO - Training [36][  120/  391]   Loss 0.032780   Top1 98.860677   Top5 100.000000   BatchTime 0.116317   LR 0.001000
INFO - Training [36][  140/  391]   Loss 0.033552   Top1 98.800223   Top5 100.000000   BatchTime 0.114096   LR 0.001000
INFO - Training [36][  160/  391]   Loss 0.033862   Top1 98.808594   Top5 100.000000   BatchTime 0.112298   LR 0.001000
INFO - Training [36][  180/  391]   Loss 0.033126   Top1 98.854167   Top5 100.000000   BatchTime 0.110952   LR 0.001000
INFO - Training [36][  200/  391]   Loss 0.032806   Top1 98.886719   Top5 100.000000   BatchTime 0.109911   LR 0.001000
INFO - Training [36][  220/  391]   Loss 0.032907   Top1 98.874290   Top5 100.000000   BatchTime 0.109053   LR 0.001000
INFO - Training [36][  240/  391]   Loss 0.032976   Top1 98.867188   Top5 100.000000   BatchTime 0.108226   LR 0.001000
INFO - Training [36][  260/  391]   Loss 0.032893   Top1 98.870192   Top5 100.000000   BatchTime 0.107561   LR 0.001000
INFO - Training [36][  280/  391]   Loss 0.032478   Top1 98.878348   Top5 100.000000   BatchTime 0.106952   LR 0.001000
INFO - Training [36][  300/  391]   Loss 0.032165   Top1 98.885417   Top5 100.000000   BatchTime 0.106217   LR 0.001000
INFO - Training [36][  320/  391]   Loss 0.032367   Top1 98.876953   Top5 100.000000   BatchTime 0.104824   LR 0.001000
INFO - Training [36][  340/  391]   Loss 0.032325   Top1 98.876379   Top5 100.000000   BatchTime 0.103511   LR 0.001000
INFO - Training [36][  360/  391]   Loss 0.032642   Top1 98.860677   Top5 100.000000   BatchTime 0.102488   LR 0.001000
INFO - Training [36][  380/  391]   Loss 0.032722   Top1 98.858964   Top5 100.000000   BatchTime 0.100835   LR 0.001000
INFO - ==> Top1: 98.856    Top5: 100.000    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [36][   20/   79]   Loss 0.394879   Top1 90.117188   Top5 99.492188   BatchTime 0.145855
INFO - Validation [36][   40/   79]   Loss 0.416797   Top1 90.273438   Top5 99.511719   BatchTime 0.095057
INFO - Validation [36][   60/   79]   Loss 0.406264   Top1 90.533854   Top5 99.557292   BatchTime 0.078398
INFO - ==> Top1: 90.580    Top5: 99.590    Loss: 0.402
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  37
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [37][   20/  391]   Loss 0.035495   Top1 98.828125   Top5 100.000000   BatchTime 0.195074   LR 0.001000
INFO - Training [37][   40/  391]   Loss 0.036561   Top1 98.710938   Top5 100.000000   BatchTime 0.148135   LR 0.001000
INFO - Training [37][   60/  391]   Loss 0.036798   Top1 98.750000   Top5 100.000000   BatchTime 0.132194   LR 0.001000
INFO - Training [37][   80/  391]   Loss 0.035519   Top1 98.808594   Top5 100.000000   BatchTime 0.124333   LR 0.001000
INFO - Training [37][  100/  391]   Loss 0.034073   Top1 98.820312   Top5 100.000000   BatchTime 0.119689   LR 0.001000
INFO - Training [37][  120/  391]   Loss 0.034229   Top1 98.782552   Top5 100.000000   BatchTime 0.116368   LR 0.001000
INFO - Training [37][  140/  391]   Loss 0.034853   Top1 98.783482   Top5 100.000000   BatchTime 0.114157   LR 0.001000
INFO - Training [37][  160/  391]   Loss 0.034461   Top1 98.808594   Top5 100.000000   BatchTime 0.112339   LR 0.001000
INFO - Training [37][  180/  391]   Loss 0.034041   Top1 98.845486   Top5 100.000000   BatchTime 0.111017   LR 0.001000
INFO - Training [37][  200/  391]   Loss 0.033401   Top1 98.863281   Top5 100.000000   BatchTime 0.109885   LR 0.001000
INFO - Training [37][  220/  391]   Loss 0.033659   Top1 98.856534   Top5 100.000000   BatchTime 0.109112   LR 0.001000
INFO - Training [37][  240/  391]   Loss 0.033303   Top1 98.867188   Top5 100.000000   BatchTime 0.108266   LR 0.001000
INFO - Training [37][  260/  391]   Loss 0.033283   Top1 98.873197   Top5 100.000000   BatchTime 0.107479   LR 0.001000
INFO - Training [37][  280/  391]   Loss 0.033407   Top1 98.881138   Top5 100.000000   BatchTime 0.106840   LR 0.001000
INFO - Training [37][  300/  391]   Loss 0.033312   Top1 98.872396   Top5 100.000000   BatchTime 0.105622   LR 0.001000
INFO - Training [37][  320/  391]   Loss 0.033147   Top1 98.874512   Top5 100.000000   BatchTime 0.104258   LR 0.001000
INFO - Training [37][  340/  391]   Loss 0.033131   Top1 98.871783   Top5 100.000000   BatchTime 0.102852   LR 0.001000
INFO - Training [37][  360/  391]   Loss 0.032960   Top1 98.880208   Top5 100.000000   BatchTime 0.101715   LR 0.001000
INFO - Training [37][  380/  391]   Loss 0.032899   Top1 98.889803   Top5 100.000000   BatchTime 0.100461   LR 0.001000
INFO - ==> Top1: 98.874    Top5: 100.000    Loss: 0.033
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [37][   20/   79]   Loss 0.394357   Top1 90.390625   Top5 99.570312   BatchTime 0.137077
INFO - Validation [37][   40/   79]   Loss 0.415549   Top1 90.312500   Top5 99.550781   BatchTime 0.091992
INFO - Validation [37][   60/   79]   Loss 0.404212   Top1 90.559896   Top5 99.635417   BatchTime 0.076641
INFO - ==> Top1: 90.600    Top5: 99.630    Loss: 0.401
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  38
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [38][   20/  391]   Loss 0.028199   Top1 99.023438   Top5 100.000000   BatchTime 0.195712   LR 0.001000
INFO - Training [38][   40/  391]   Loss 0.025189   Top1 99.218750   Top5 99.980469   BatchTime 0.147893   LR 0.001000
INFO - Training [38][   60/  391]   Loss 0.025581   Top1 99.179688   Top5 99.986979   BatchTime 0.131818   LR 0.001000
INFO - Training [38][   80/  391]   Loss 0.028029   Top1 99.091797   Top5 99.980469   BatchTime 0.123984   LR 0.001000
INFO - Training [38][  100/  391]   Loss 0.027954   Top1 99.078125   Top5 99.984375   BatchTime 0.119292   LR 0.001000
INFO - Training [38][  120/  391]   Loss 0.029008   Top1 99.016927   Top5 99.986979   BatchTime 0.116224   LR 0.001000
INFO - Training [38][  140/  391]   Loss 0.029865   Top1 98.967634   Top5 99.988839   BatchTime 0.114067   LR 0.001000
INFO - Training [38][  160/  391]   Loss 0.029520   Top1 98.984375   Top5 99.990234   BatchTime 0.112344   LR 0.001000
INFO - Training [38][  180/  391]   Loss 0.029542   Top1 98.980035   Top5 99.991319   BatchTime 0.110978   LR 0.001000
INFO - Training [38][  200/  391]   Loss 0.030446   Top1 98.980469   Top5 99.988281   BatchTime 0.109859   LR 0.001000
INFO - Training [38][  220/  391]   Loss 0.030617   Top1 98.973722   Top5 99.989347   BatchTime 0.108986   LR 0.001000
INFO - Training [38][  240/  391]   Loss 0.030529   Top1 98.948568   Top5 99.990234   BatchTime 0.108203   LR 0.001000
INFO - Training [38][  260/  391]   Loss 0.030426   Top1 98.960337   Top5 99.990986   BatchTime 0.107488   LR 0.001000
INFO - Training [38][  280/  391]   Loss 0.030718   Top1 98.942522   Top5 99.991629   BatchTime 0.106914   LR 0.001000
INFO - Training [38][  300/  391]   Loss 0.030836   Top1 98.932292   Top5 99.992188   BatchTime 0.105421   LR 0.001000
INFO - Training [38][  320/  391]   Loss 0.030647   Top1 98.942871   Top5 99.992676   BatchTime 0.103968   LR 0.001000
INFO - Training [38][  340/  391]   Loss 0.030479   Top1 98.943015   Top5 99.993107   BatchTime 0.102593   LR 0.001000
INFO - Training [38][  360/  391]   Loss 0.030110   Top1 98.956163   Top5 99.993490   BatchTime 0.101565   LR 0.001000
INFO - Training [38][  380/  391]   Loss 0.030141   Top1 98.953536   Top5 99.993832   BatchTime 0.100408   LR 0.001000
INFO - ==> Top1: 98.960    Top5: 99.994    Loss: 0.030
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [38][   20/   79]   Loss 0.389468   Top1 90.585938   Top5 99.531250   BatchTime 0.139405
INFO - Validation [38][   40/   79]   Loss 0.409496   Top1 90.449219   Top5 99.589844   BatchTime 0.092795
INFO - Validation [38][   60/   79]   Loss 0.401246   Top1 90.638021   Top5 99.635417   BatchTime 0.077222
INFO - ==> Top1: 90.670    Top5: 99.650    Loss: 0.398
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch  39
INFO - Training: 50000 samples (128 per mini-batch)
INFO - Training [39][   20/  391]   Loss 0.030784   Top1 98.945312   Top5 100.000000   BatchTime 0.204200   LR 0.001000
INFO - Training [39][   40/  391]   Loss 0.033732   Top1 98.847656   Top5 100.000000   BatchTime 0.152682   LR 0.001000
INFO - Training [39][   60/  391]   Loss 0.030750   Top1 98.971354   Top5 100.000000   BatchTime 0.135316   LR 0.001000
INFO - Training [39][   80/  391]   Loss 0.030493   Top1 98.945312   Top5 100.000000   BatchTime 0.126559   LR 0.001000
INFO - Training [39][  100/  391]   Loss 0.030784   Top1 98.929688   Top5 100.000000   BatchTime 0.121453   LR 0.001000
INFO - Training [39][  120/  391]   Loss 0.030700   Top1 98.912760   Top5 100.000000   BatchTime 0.118012   LR 0.001000
INFO - Training [39][  140/  391]   Loss 0.030323   Top1 98.962054   Top5 100.000000   BatchTime 0.115627   LR 0.001000
INFO - Training [39][  160/  391]   Loss 0.030182   Top1 98.950195   Top5 100.000000   BatchTime 0.113791   LR 0.001000
INFO - Training [39][  180/  391]   Loss 0.031874   Top1 98.867188   Top5 100.000000   BatchTime 0.112337   LR 0.001000
INFO - Training [39][  200/  391]   Loss 0.031905   Top1 98.882812   Top5 100.000000   BatchTime 0.111299   LR 0.001000
INFO - Training [39][  220/  391]   Loss 0.031973   Top1 98.899148   Top5 100.000000   BatchTime 0.110232   LR 0.001000
INFO - Training [39][  240/  391]   Loss 0.031240   Top1 98.922526   Top5 100.000000   BatchTime 0.109336   LR 0.001000
INFO - Training [39][  260/  391]   Loss 0.032192   Top1 98.903245   Top5 100.000000   BatchTime 0.108609   LR 0.001000
INFO - Training [39][  280/  391]   Loss 0.032573   Top1 98.889509   Top5 100.000000   BatchTime 0.108142   LR 0.001000
INFO - Training [39][  300/  391]   Loss 0.032562   Top1 98.888021   Top5 100.000000   BatchTime 0.106032   LR 0.001000
INFO - Training [39][  320/  391]   Loss 0.032284   Top1 98.894043   Top5 99.997559   BatchTime 0.104867   LR 0.001000
INFO - Training [39][  340/  391]   Loss 0.031992   Top1 98.899357   Top5 99.997702   BatchTime 0.103493   LR 0.001000
INFO - Training [39][  360/  391]   Loss 0.032082   Top1 98.891059   Top5 99.997830   BatchTime 0.102303   LR 0.001000
INFO - Training [39][  380/  391]   Loss 0.032034   Top1 98.889803   Top5 99.997944   BatchTime 0.101790   LR 0.001000
INFO - ==> Top1: 98.882    Top5: 99.998    Loss: 0.032
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [39][   20/   79]   Loss 0.387984   Top1 90.156250   Top5 99.609375   BatchTime 0.137255
INFO - Validation [39][   40/   79]   Loss 0.407468   Top1 90.410156   Top5 99.550781   BatchTime 0.091176
INFO - Validation [39][   60/   79]   Loss 0.403643   Top1 90.546875   Top5 99.583333   BatchTime 0.075810
INFO - ==> Top1: 90.640    Top5: 99.610    Loss: 0.400
INFO - Scoreboard best 1 ==> Epoch [28][Top1: 90.780   Top5: 99.570] Sparsity : 0.738
INFO - Scoreboard best 2 ==> Epoch [24][Top1: 90.780   Top5: 99.570] Sparsity : 0.737
INFO - Scoreboard best 3 ==> Epoch [26][Top1: 90.750   Top5: 99.640] Sparsity : 0.737
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/out/MobileNetv2_cifar10_a8w8_5_epoch60_20221103-203820/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - >>>>>>>> Epoch -1 (final model evaluation)
INFO - Validation: 10000 samples (128 per mini-batch)
INFO - Validation [   20/   79]   Loss 0.387984   Top1 90.156250   Top5 99.609375   BatchTime 0.138634
INFO - Validation [   40/   79]   Loss 0.407468   Top1 90.410156   Top5 99.550781   BatchTime 0.091991
INFO - Validation [   60/   79]   Loss 0.403643   Top1 90.546875   Top5 99.583333   BatchTime 0.076112
INFO - ==> Top1: 90.640    Top5: 99.610    Loss: 0.400
INFO - Saving checkpoint to:
             Current: /home/ilena7440/slsq/LSQ/pruned_model/MobileNetv2_cifar10_a8w8_5_epoch60_checkpoint.pth.tar
INFO - Program completed successfully ... exiting ...
INFO - If you have any questions or suggestions, please visit: github.com/zhutmost/lsq-net