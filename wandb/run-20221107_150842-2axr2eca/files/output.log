INFO - Log file for this run: /home/ilena7440/LSQ/out/resnet18_imagenet_a8w8_20_epoch80_20221107-150843/resnet18_imagenet_a8w8_20_epoch80_20221107-150843.log
INFO - TensorBoard data directory: /home/ilena7440/LSQ/out/resnet18_imagenet_a8w8_20_epoch80_20221107-150843/tb_runs
INFO - Dataset `imagenet` size:
          Training Set = 1281167 (10010)
        Validation Set = 50000 (391)
              Test Set = 50000 (391)
Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to /home/ilena7440/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth
 38%|███████████████████████████████████████████████████████████████████████████▍                                                                                                                        | 17.2M/44.7M [00:00<00:00, 86.3MB/s]
*******************pre-trained****************
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
127
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:00<00:00, 78.4MB/s]
INFO - Created `resnet18` model for `imagenet` dataset
          Use pre-trained model = True
/home/ilena7440/LSQ/quan/quantizer/lsq.py:128: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (len(x.shape) == 4 and x.shape[1] != 1):
/home/ilena7440/LSQ/quan/quantizer/lsq.py:94: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  x_reshape = x.reshape(co // self.block_size, self.block_size, ci, kh, kw)
/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
INFO - Inserted quantizers into the original model
INFO - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.01
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 0.0001
           )
INFO - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.01
INFO - >>>>>>>> Epoch -1 (pre-trained model evaluation)
INFO - Validation: 50000 samples (128 per mini-batch)
Munch({'update_per_batch': True, 'mode': 'multi_step', 'milestones': [20, 40, 60], 'gamma': 0.1})
multi_step
INFO - Validation [   20/  391]   Loss 10.426115   Top1 0.000000   Top5 0.000000   BatchTime 0.662939
INFO - Validation [   40/  391]   Loss 9.851932   Top1 0.058594   Top5 0.390625   BatchTime 0.371889
INFO - Validation [   60/  391]   Loss 10.078457   Top1 0.039062   Top5 0.299479   BatchTime 0.277290
INFO - Validation [   80/  391]   Loss 9.897195   Top1 0.029297   Top5 0.234375   BatchTime 0.229647
INFO - Validation [  100/  391]   Loss 9.835794   Top1 0.023438   Top5 0.234375   BatchTime 0.200318
INFO - Validation [  120/  391]   Loss 9.867029   Top1 0.019531   Top5 0.208333   BatchTime 0.181561
INFO - Validation [  140/  391]   Loss 9.910343   Top1 0.033482   Top5 0.251116   BatchTime 0.167738
INFO - Validation [  160/  391]   Loss 9.933602   Top1 0.219727   Top5 0.517578   BatchTime 0.157179
INFO - Validation [  180/  391]   Loss 9.767149   Top1 0.199653   Top5 0.594618   BatchTime 0.148872
INFO - Validation [  200/  391]   Loss 9.629207   Top1 0.261719   Top5 0.882812   BatchTime 0.142723
INFO - Validation [  220/  391]   Loss 9.559465   Top1 0.255682   Top5 0.948153   BatchTime 0.137504
INFO - Validation [  240/  391]   Loss 9.480269   Top1 0.341797   Top5 1.103516   BatchTime 0.133368
INFO - Validation [  260/  391]   Loss 9.385702   Top1 0.357572   Top5 1.259014   BatchTime 0.129372
INFO - Validation [  280/  391]   Loss 9.332992   Top1 0.357143   Top5 1.283482   BatchTime 0.126233
INFO - Validation [  300/  391]   Loss 9.263727   Top1 0.390625   Top5 1.419271   BatchTime 0.123295
INFO - Validation [  320/  391]   Loss 9.216885   Top1 0.427246   Top5 1.542969   BatchTime 0.120555
INFO - Validation [  340/  391]   Loss 9.160497   Top1 0.418199   Top5 1.592371   BatchTime 0.117996
INFO - Validation [  360/  391]   Loss 9.129761   Top1 0.444878   Top5 1.740451   BatchTime 0.114791
INFO - Validation [  380/  391]   Loss 9.136724   Top1 0.425576   Top5 1.679688   BatchTime 0.111900
INFO - ==> Top1: 0.414    Top5: 1.678    Loss: 9.133
INFO - Scoreboard best 1 ==> Epoch [-1][Top1: 0.414   Top5: 1.678] Sparsity : 0.058
INFO - >>>>>>>> Epoch   0
INFO - Training: 1281167 samples (128 per mini-batch)
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
LsqQuan()
Parameter containing:
tensor(0.2599, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0666, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0226, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0788, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0176, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0898, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0175, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0898, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0318, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0088, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0324, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0140, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0324, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0221, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0058, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0218, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0053, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0218, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0125, device='cuda:0', requires_grad=True)
Parameter containing:
tensor(0.0057, device='cuda:0', requires_grad=True)
INFO - Training [0][   20/10010]   Loss 1.632292   Top1 62.539062   Top5 83.437500   BatchTime 0.447349   LR 0.010000
INFO - Training [0][   40/10010]   Loss 1.771442   Top1 59.902344   Top5 81.191406   BatchTime 0.311200   LR 0.010000
INFO - Training [0][   60/10010]   Loss 1.859123   Top1 57.955729   Top5 80.078125   BatchTime 0.265915   LR 0.010000
INFO - Training [0][   80/10010]   Loss 1.904540   Top1 56.767578   Top5 79.394531   BatchTime 0.243053   LR 0.010000
INFO - Training [0][  100/10010]   Loss 1.938317   Top1 56.000000   Top5 78.875000   BatchTime 0.229735   LR 0.010000
INFO - Training [0][  120/10010]   Loss 1.957465   Top1 55.475260   Top5 78.496094   BatchTime 0.220858   LR 0.010000
INFO - Training [0][  140/10010]   Loss 1.972206   Top1 55.066964   Top5 78.292411   BatchTime 0.214019   LR 0.010000
INFO - Training [0][  160/10010]   Loss 1.994113   Top1 54.829102   Top5 77.968750   BatchTime 0.208694   LR 0.010000
INFO - Training [0][  180/10010]   Loss 2.018512   Top1 54.383681   Top5 77.677951   BatchTime 0.204902   LR 0.010000
INFO - Training [0][  200/10010]   Loss 2.042055   Top1 53.996094   Top5 77.343750   BatchTime 0.201737   LR 0.010000
INFO - Training [0][  220/10010]   Loss 2.071565   Top1 53.373580   Top5 76.995739   BatchTime 0.199237   LR 0.010000
INFO - Training [0][  240/10010]   Loss 2.092395   Top1 52.939453   Top5 76.741536   BatchTime 0.197308   LR 0.010000
INFO - Training [0][  260/10010]   Loss 2.112844   Top1 52.566106   Top5 76.406250   BatchTime 0.195369   LR 0.010000
INFO - Training [0][  280/10010]   Loss 2.153775   Top1 51.816406   Top5 75.820312   BatchTime 0.194034   LR 0.010000
INFO - Training [0][  300/10010]   Loss 2.209781   Top1 50.835938   Top5 74.940104   BatchTime 0.192826   LR 0.010000
INFO - Training [0][  320/10010]   Loss 2.278693   Top1 49.685059   Top5 73.779297   BatchTime 0.191549   LR 0.010000
INFO - Training [0][  340/10010]   Loss 2.358386   Top1 48.295037   Top5 72.428768   BatchTime 0.190511   LR 0.010000
INFO - Training [0][  360/10010]   Loss 2.437011   Top1 46.927083   Top5 71.059028   BatchTime 0.189656   LR 0.010000
INFO - Training [0][  380/10010]   Loss 2.511652   Top1 45.670230   Top5 69.800576   BatchTime 0.188960   LR 0.010000
INFO - Training [0][  400/10010]   Loss 2.579904   Top1 44.507812   Top5 68.583984   BatchTime 0.188160   LR 0.010000
INFO - Training [0][  420/10010]   Loss 2.642311   Top1 43.497024   Top5 67.531622   BatchTime 0.187433   LR 0.010000
INFO - Training [0][  440/10010]   Loss 2.704725   Top1 42.485795   Top5 66.439986   BatchTime 0.186736   LR 0.010000
INFO - Training [0][  460/10010]   Loss 2.760224   Top1 41.603261   Top5 65.444973   BatchTime 0.186085   LR 0.010000
INFO - Training [0][  480/10010]   Loss 2.810210   Top1 40.773112   Top5 64.549154   BatchTime 0.185699   LR 0.010000
INFO - Training [0][  500/10010]   Loss 2.852961   Top1 40.043750   Top5 63.814062   BatchTime 0.185335   LR 0.010000
INFO - Training [0][  520/10010]   Loss 2.895581   Top1 39.299880   Top5 63.051382   BatchTime 0.184837   LR 0.010000
INFO - Training [0][  540/10010]   Loss 2.936811   Top1 38.593750   Top5 62.294560   BatchTime 0.184512   LR 0.010000
INFO - Training [0][  560/10010]   Loss 2.971971   Top1 37.998047   Top5 61.674107   BatchTime 0.184133   LR 0.010000
INFO - Training [0][  580/10010]   Loss 3.004406   Top1 37.464978   Top5 61.107220   BatchTime 0.183751   LR 0.010000
INFO - Training [0][  600/10010]   Loss 3.031320   Top1 36.977865   Top5 60.626302   BatchTime 0.183480   LR 0.010000
INFO - Training [0][  620/10010]   Loss 3.055969   Top1 36.543599   Top5 60.229335   BatchTime 0.183100   LR 0.010000
INFO - Training [0][  640/10010]   Loss 3.079154   Top1 36.127930   Top5 59.812012   BatchTime 0.182861   LR 0.010000
INFO - Training [0][  660/10010]   Loss 3.098836   Top1 35.782434   Top5 59.438920   BatchTime 0.182650   LR 0.010000
INFO - Training [0][  680/10010]   Loss 3.114554   Top1 35.483686   Top5 59.198070   BatchTime 0.182570   LR 0.010000
INFO - Training [0][  700/10010]   Loss 3.131845   Top1 35.179688   Top5 58.900670   BatchTime 0.183138   LR 0.010000
INFO - Training [0][  720/10010]   Loss 3.145761   Top1 34.913194   Top5 58.689236   BatchTime 0.183832   LR 0.010000
INFO - Training [0][  740/10010]   Loss 3.157162   Top1 34.675887   Top5 58.476562   BatchTime 0.183627   LR 0.010000
INFO - Training [0][  760/10010]   Loss 3.171322   Top1 34.452097   Top5 58.252467   BatchTime 0.183407   LR 0.010000
INFO - Training [0][  780/10010]   Loss 3.182932   Top1 34.270833   Top5 58.044872   BatchTime 0.183518   LR 0.010000
INFO - Training [0][  800/10010]   Loss 3.195133   Top1 34.065430   Top5 57.836914   BatchTime 0.183761   LR 0.010000
INFO - Training [0][  820/10010]   Loss 3.204707   Top1 33.881479   Top5 57.668636   BatchTime 0.183951   LR 0.010000
INFO - Training [0][  840/10010]   Loss 3.214062   Top1 33.690476   Top5 57.495350   BatchTime 0.184015   LR 0.010000
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff91ed82ca0>
Traceback (most recent call last):
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Traceback (most recent call last):
  File "main.py", line 175, in <module>
    main()
  File "main.py", line 123, in main
    t_top1, t_top5, t_loss, masking_loss = process.train(train_loader, model, criterion, optimizer,
  File "/home/ilena7440/LSQ/process.py", line 57, in train
    outputs = model(inputs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/model/resnet.py", line 217, in forward
    return self._forward_impl(x)
  File "/home/ilena7440/LSQ/model/resnet.py", line 207, in _forward_impl
    x = self.layer3(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/model/resnet.py", line 65, in forward
    identity = self.downsample(x)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/func.py", line 22, in forward
    quantized_weight = self.quan_w_fn(self.weight)
  File "/home/ilena7440/qilbertenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ilena7440/LSQ/quan/quantizer/lsq.py", line 118, in forward
    self.p.data.clamp_(0.,self.c.data)
KeyboardInterrupt