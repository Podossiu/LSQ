2022-11-02 03:33:18,654 - INFO  - Log file for this run: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_5_20221102-033318/MobileNetv2_imagenet_a8w8_5_20221102-033318.log
2022-11-02 03:33:18,660 - INFO  - TensorBoard data directory: /home/ilena7440/LSQ/out/MobileNetv2_imagenet_a8w8_5_20221102-033318/tb_runs
2022-11-02 03:33:20,693 - INFO  - Dataset `cifar10` size:
          Training Set = 50000 (391)
        Validation Set = 10000 (79)
              Test Set = 10000 (79)
2022-11-02 03:33:22,505 - INFO  - Created `MobileNetv2` model for `cifar10` dataset
          Use pre-trained model = True
2022-11-02 03:33:25,554 - INFO  - Inserted quantizers into the original model
2022-11-02 03:33:25,837 - INFO  - Optimizer: SGD (
           Parameter Group 0
               dampening: 0
               foreach: None
               lr: 0.05
               maximize: False
               momentum: 0.9
               nesterov: False
               weight_decay: 4e-05
           )
2022-11-02 03:33:25,837 - INFO  - LR scheduler: `MultiStepLr`
    Update per batch: True
             Group 0: 0.05

2022-11-02 03:33:25,837 - INFO  - >>>>>>>> Epoch -1 (pre-trained model evaluation)
2022-11-02 03:33:25,837 - INFO  - Validation: 10000 samples (128 per mini-batch)
2022-11-02 03:33:29,905 - INFO  - Validation [   20/   79]   Loss 18.551335   Top1 0.000000   Top5 0.000000   BatchTime 0.203335   
2022-11-02 03:33:30,494 - INFO  - Validation [   40/   79]   Loss 18.568195   Top1 0.000000   Top5 0.000000   BatchTime 0.116383   
2022-11-02 03:33:30,973 - INFO  - Validation [   60/   79]   Loss 18.637665   Top1 0.000000   Top5 0.000000   BatchTime 0.085586   
2022-11-02 03:33:31,579 - INFO  - ==> Top1: 0.000    Top5: 0.000    Loss: 18.632

2022-11-02 03:33:31,638 - INFO  - Scoreboard best 1 ==> Epoch [-1][Top1: 0.000   Top5: 0.000] Sparsity : 0.060
2022-11-02 03:33:31,638 - INFO  - >>>>>>>> Epoch   0
2022-11-02 03:33:31,639 - INFO  - Training: 50000 samples (128 per mini-batch)
2022-11-02 03:33:35,962 - INFO  - Training [0][   20/  391]   Loss 5.626907   Top1 7.578125   Top5 40.312500   BatchTime 0.216121   LR 0.050000   
2022-11-02 03:33:38,086 - INFO  - Training [0][   40/  391]   Loss 5.089859   Top1 8.417969   Top5 45.605469   BatchTime 0.161159   LR 0.050000   
2022-11-02 03:33:40,241 - INFO  - Training [0][   60/  391]   Loss nan   Top1 8.789062   Top5 47.005208   BatchTime 0.143339   LR 0.050000   
2022-11-02 03:33:42,349 - INFO  - Training [0][   80/  391]   Loss nan   Top1 8.974609   Top5 47.607422   BatchTime 0.133862   LR 0.050000   
